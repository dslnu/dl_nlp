<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Intro to PyTorch</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
  <script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Intro to PyTorch</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="imports" class="slide level2">
<h2>Imports</h2>
<div id="c6b9770b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a></a><span class="im">import</span> os</span>
<span id="cb1-3"><a></a><span class="im">import</span> math</span>
<span id="cb1-4"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a></a><span class="im">import</span> time</span>
<span id="cb1-6"><a></a></span>
<span id="cb1-7"><a></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-8"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-10"><a></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-11"><a></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-12"><a></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> to_rgba</span>
<span id="cb1-13"><a></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-14"><a></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-15"><a></a></span>
<span id="cb1-16"><a></a><span class="co">## Progress bar</span></span>
<span id="cb1-17"><a></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="basic-pytorch-import" class="slide level2">
<h2>Basic PyTorch import</h2>
<div id="6a1191c6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a></a><span class="bu">print</span>(<span class="st">"Using torch"</span>, torch.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using torch 2.6.0</code></pre>
</div>
</div>
</section>
<section id="set-the-seed" class="slide level2">
<h2>Set the seed</h2>
<div id="8c0c5d79" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>torch.manual_seed(<span class="dv">42</span>) <span class="co"># Setting the seed</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>&lt;torch._C.Generator at 0x135d22d10&gt;</code></pre>
</div>
</div>
</section>
<section id="tensors" class="slide level2">
<h2>Tensors</h2>
<p>Tensors</p>
<p>Tensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration (more on that later). The name “tensor” is a generalization of concepts you already know. For instance, a vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we will use tensors of various shapes and number of dimensions.</p>
<p>Most common functions you know from numpy can be used on tensors as well. Actually, since numpy arrays are so similar to tensors, we can convert most tensors to numpy arrays (and back) but we don’t need it too often.</p>
</section>
<section id="tensors-1" class="slide level2">
<h2>Tensors</h2>
<div id="0f206aa0" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>x <span class="op">=</span> torch.Tensor(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb6-2"><a></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])</code></pre>
</div>
</div>
</section>
<section id="tensors-2" class="slide level2">
<h2>Tensors</h2>
<p>The function <code>torch.Tensor</code> allocates memory for the desired tensor, but reuses any values that have already been in the memory. To directly assign values to the tensor during initialization, there are many alternatives including:</p>
<ul>
<li><code>torch.zeros</code>: Creates a tensor filled with zeros</li>
<li><code>torch.ones</code>: Creates a tensor filled with ones</li>
<li><code>torch.rand</code>: Creates a tensor with random values uniformly sampled between 0 and 1</li>
<li><code>torch.randn</code>: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1</li>
<li><code>torch.arange</code>: Creates a tensor containing the values <span class="math inline">\(N,N+1,N+2,...,M\)</span></li>
<li><code>torch.Tensor</code> (input list): Creates a tensor from the list elements you provide</li>
</ul>
</section>
<section id="tensors-3" class="slide level2">
<h2>Tensors</h2>
<div id="2c3f35d4" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="co"># Create a tensor from a (nested) list</span></span>
<span id="cb8-2"><a></a>x <span class="op">=</span> torch.Tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb8-3"><a></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1., 2.],
        [3., 4.]])</code></pre>
</div>
</div>
</section>
<section id="tensors-4" class="slide level2">
<h2>Tensors</h2>
<div id="42108d6d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="co"># Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]</span></span>
<span id="cb10-2"><a></a>x <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb10-3"><a></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[0.8823, 0.9150, 0.3829, 0.9593],
         [0.3904, 0.6009, 0.2566, 0.7936],
         [0.9408, 0.1332, 0.9346, 0.5936]],

        [[0.8694, 0.5677, 0.7411, 0.4294],
         [0.8854, 0.5739, 0.2666, 0.6274],
         [0.2696, 0.4414, 0.2969, 0.8317]]])</code></pre>
</div>
</div>
</section>
<section id="tensors-5" class="slide level2">
<h2>Tensors</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensor shape</strong></p>
</div>
<div class="callout-content">
<div id="5c2a98db" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>shape <span class="op">=</span> x.shape</span>
<span id="cb12-2"><a></a><span class="bu">print</span>(<span class="st">"Shape:"</span>, x.shape)</span>
<span id="cb12-3"><a></a></span>
<span id="cb12-4"><a></a>size <span class="op">=</span> x.size()</span>
<span id="cb12-5"><a></a><span class="bu">print</span>(<span class="st">"Size:"</span>, size)</span>
<span id="cb12-6"><a></a></span>
<span id="cb12-7"><a></a>dim1, dim2, dim3 <span class="op">=</span> x.size()</span>
<span id="cb12-8"><a></a><span class="bu">print</span>(<span class="st">"Size:"</span>, dim1, dim2, dim3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape: torch.Size([2, 3, 4])
Size: torch.Size([2, 3, 4])
Size: 2 3 4</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="tensors-6" class="slide level2">
<h2>Tensors</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>To Numpy and back again</strong></p>
</div>
<div class="callout-content">
<div id="b72545ff" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>np_arr <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb14-2"><a></a>tensor <span class="op">=</span> torch.from_numpy(np_arr)</span>
<span id="cb14-3"><a></a></span>
<span id="cb14-4"><a></a><span class="bu">print</span>(<span class="st">"Numpy array:"</span>, np_arr)</span>
<span id="cb14-5"><a></a><span class="bu">print</span>(<span class="st">"PyTorch tensor:"</span>, tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numpy array: [[1 2]
 [3 4]]
PyTorch tensor: tensor([[1, 2],
        [3, 4]])</code></pre>
</div>
</div>
<p>To transform a PyTorch tensor back to a numpy array, we can use the function .numpy() on tensors:</p>
<div id="2ee92f29" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a>tensor <span class="op">=</span> torch.arange(<span class="dv">4</span>)</span>
<span id="cb16-2"><a></a>np_arr <span class="op">=</span> tensor.numpy()</span>
<span id="cb16-3"><a></a></span>
<span id="cb16-4"><a></a><span class="bu">print</span>(<span class="st">"PyTorch tensor:"</span>, tensor)</span>
<span id="cb16-5"><a></a><span class="bu">print</span>(<span class="st">"Numpy array:"</span>, np_arr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch tensor: tensor([0, 1, 2, 3])
Numpy array: [0 1 2 3]</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU (more on GPU support in a later section). In case you have a tensor on GPU, you need to call <code>.cpu()</code> on the tensor beforehand. Hence, you get a line like <code>np_arr = tensor.cpu().numpy()</code>.</p>
</div>
</div>
</div>
</section>
<section id="operations" class="slide level2">
<h2>Operations</h2>
<p>Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the <a href="https://pytorch.org/docs/stable/tensors.html#">PyTorch documentation</a>, but we will review the most important ones here.</p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Addition</strong></p>
</div>
<div class="callout-content">
<div id="de8bcc32" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a>x1 <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb18-2"><a></a>x2 <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb18-3"><a></a>y <span class="op">=</span> x1 <span class="op">+</span> x2</span>
<span id="cb18-4"><a></a></span>
<span id="cb18-5"><a></a><span class="bu">print</span>(<span class="st">"X1"</span>, x1)</span>
<span id="cb18-6"><a></a><span class="bu">print</span>(<span class="st">"X2"</span>, x2)</span>
<span id="cb18-7"><a></a><span class="bu">print</span>(<span class="st">"Y"</span>, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X1 tensor([[0.1053, 0.2695, 0.3588],
        [0.1994, 0.5472, 0.0062]])
X2 tensor([[0.9516, 0.0753, 0.8860],
        [0.5832, 0.3376, 0.8090]])
Y tensor([[1.0569, 0.3448, 1.2448],
        [0.7826, 0.8848, 0.8151]])</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Calling x1 + x2 creates a <strong>new</strong> tensor containing the sum of the two inputs.</p>
</div>
</div>
</div>
</section>
<section id="operations-1" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>In-place</strong></p>
</div>
<div class="callout-content">
<p>We can also use in-place operations that are applied directly on the memory of a tensor. We therefore change the values of x2 without the chance to re-accessing the values of x2 before the operation.</p>
<div id="6863cd11" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a>x1 <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb20-2"><a></a>x2 <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb20-3"><a></a><span class="bu">print</span>(<span class="st">"X1 (before)"</span>, x1)</span>
<span id="cb20-4"><a></a><span class="bu">print</span>(<span class="st">"X2 (before)"</span>, x2)</span>
<span id="cb20-5"><a></a></span>
<span id="cb20-6"><a></a>x2.add_(x1)</span>
<span id="cb20-7"><a></a><span class="bu">print</span>(<span class="st">"X1 (after)"</span>, x1)</span>
<span id="cb20-8"><a></a><span class="bu">print</span>(<span class="st">"X2 (after)"</span>, x2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X1 (before) tensor([[0.5779, 0.9040, 0.5547],
        [0.3423, 0.6343, 0.3644]])
X2 (before) tensor([[0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895]])
X1 (after) tensor([[0.5779, 0.9040, 0.5547],
        [0.3423, 0.6343, 0.3644]])
X2 (after) tensor([[1.2884, 1.8504, 1.3437],
        [0.6237, 1.4230, 0.9539]])</code></pre>
</div>
</div>
</div>
</div>
</div>

<aside><div>
<p>In-place operations are usually marked with a underscore postfix (e.g.&nbsp;“add_” instead of “add”).</p>
</div></aside></section>
<section id="operations-2" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Shape change</strong></p>
</div>
<div class="callout-content">
<p>Another common operation aims at changing the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g.&nbsp;a tensor of size (6), or (3,2), …). In PyTorch, this operation is called <code>view</code>:</p>
<div id="cf97fcb5" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>)</span>
<span id="cb22-2"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([0, 1, 2, 3, 4, 5])</code></pre>
</div>
</div>
<div id="c0df3a43" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>x <span class="op">=</span> x.view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb24-2"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([[0, 1, 2],
        [3, 4, 5]])</code></pre>
</div>
</div>
<p>You can also swap dimensions:</p>
<div id="a9327cfc" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>x <span class="op">=</span> x.permute(<span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># Swapping dimension 0 and 1</span></span>
<span id="cb26-2"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([[0, 3],
        [1, 4],
        [2, 5]])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="operations-3" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multiplication</strong></p>
</div>
<div class="callout-content">
<p>Quite often, we have an input vector <span class="math inline">\(\mathbf{x}\)</span>, which is transformed using a learned weight matrix <span class="math inline">\(\mathbf{W}\)</span>. There are multiple ways and functions to perform matrix multiplication:</p>
<ul>
<li><code>torch.matmul</code>: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul">documentation</a>). Can also be written as <code>a @ b</code>, similar to numpy.</li>
<li><code>torch.mm</code>: Performs the matrix product over two matrices, but doesn’t support broadcasting (see <a href="https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm">documentation</a>)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="operations-4" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multiplication</strong></p>
</div>
<div class="callout-content">
<ul>
<li><code>torch.bmm</code>: Performs the matrix product with a support batch dimension. If the first tensor <span class="math inline">\(T\)</span> is of shape (<span class="math inline">\(b\times n\times m\)</span>), and the second tensor <span class="math inline">\(R\)</span> (<span class="math inline">\(b\times m\times p\)</span>), the output <span class="math inline">\(O\)</span> is of shape (<span class="math inline">\(b\times n\times p\)</span>), and has been calculated by performing <span class="math inline">\(b\)</span> matrix multiplications of the submatrices of <span class="math inline">\(T\)</span> and <span class="math inline">\(R\)</span>: <span class="math inline">\(O_i = T_i @ R_i\)</span></li>
<li><code>torch.einsum</code>: Performs matrix multiplications and more (i.e.&nbsp;sums of products) using the Einstein summation convention. Explanation of the Einstein sum can be found in assignment 1.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Usually, we use <code>torch.matmul</code> or <code>torch.bmm</code>.</p>
</div>
</div>
</div>
</section>
<section id="operations-5" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multiplication</strong></p>
</div>
<div class="callout-content">
<p>We can try a matrix multiplication with <code>torch.matmul</code>:</p>
<div id="26125235" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>)</span>
<span id="cb28-2"><a></a>x <span class="op">=</span> x.view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb28-3"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([[0, 1, 2],
        [3, 4, 5]])</code></pre>
</div>
</div>
<div id="8b2528c0" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a>W <span class="op">=</span> torch.arange(<span class="dv">9</span>).view(<span class="dv">3</span>, <span class="dv">3</span>) <span class="co"># We can also stack multiple operations in a single line</span></span>
<span id="cb30-2"><a></a><span class="bu">print</span>(<span class="st">"W"</span>, W)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>W tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])</code></pre>
</div>
</div>
<div id="fb1b9f95" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>h <span class="op">=</span> torch.matmul(x, W) <span class="co"># Verify the result by calculating it by hand too!</span></span>
<span id="cb32-2"><a></a><span class="bu">print</span>(<span class="st">"h"</span>, h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>h tensor([[15, 18, 21],
        [42, 54, 66]])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="operations-6" class="slide level2">
<h2>Operations</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Indexing</strong></p>
</div>
<div class="callout-content">
<p>We often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let’s try it:</p>
<div id="230070f3" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>x <span class="op">=</span> torch.arange(<span class="dv">12</span>).view(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb34-2"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])</code></pre>
</div>
</div>
<div id="add4a4c6" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a><span class="bu">print</span>(x[:, <span class="dv">1</span>])   <span class="co"># Second column</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1, 5, 9])</code></pre>
</div>
</div>
<div id="a87e4739" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="bu">print</span>(x[<span class="dv">0</span>])      <span class="co"># First row</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0, 1, 2, 3])</code></pre>
</div>
</div>
<div id="86afe0b9" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a><span class="bu">print</span>(x[:<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]) <span class="co"># First two rows, last column</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([3, 7])</code></pre>
</div>
</div>
<div id="b5fc54eb" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a><span class="bu">print</span>(x[<span class="dv">1</span>:<span class="dv">3</span>, :]) <span class="co"># Middle two rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computation" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dynamic computation graph: recap</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>PyTorch automatically gives us <strong>gradients/derivatives</strong> of functions that we define.</p></li>
<li><p>if our neural network would output a single scalar value, we would talk about taking the <strong>derivative</strong>, but you will see that quite often we will have <strong>multiple</strong> output variables (“values”); in that case we talk about <strong>gradients</strong>.</p></li>
<li><p>given an input <span class="math inline">\(\mathbf{x}\)</span>, we define our function by <strong>manipulating</strong> that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a <strong>computational graph</strong>. This graph shows how to arrive at our output from our input.</p></li>
<li><p>PyTorch is a <strong>define-by-run</strong> framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us. Thus, we create a dynamic computation graph along the way.</p></li>
<li><p>the only thing we have to do is to compute the <strong>output</strong>, and then we can ask PyTorch to automatically get the <strong>gradients</strong>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="computation-1" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gradients</strong></p>
</div>
<div class="callout-content">
<p>The first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients.</p>
<div id="6f2812f8" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a></a>x <span class="op">=</span> torch.ones((<span class="dv">3</span>,))</span>
<span id="cb44-2"><a></a><span class="bu">print</span>(x.requires_grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>False</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gradients: enabling</strong></p>
</div>
<div class="callout-content">
<p>We can change this for an existing tensor using the function <code>requires_grad_()</code> (underscore indicating that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument requires_grad=True to most initializers we have seen above.</p>
<div id="336538b4" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a></a>x.requires_grad_(<span class="va">True</span>)</span>
<span id="cb46-2"><a></a><span class="bu">print</span>(x.requires_grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computation-2" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation graph: example</strong></p>
</div>
<div class="callout-content">
<p>In order to get familiar with the concept of a computation graph, we will create one for the following function:</p>
<p><span class="math display">\[y = \frac{1}{\ell(x)}\sum_i \left[(x_i + 2)^2 + 3\right],\]</span></p>
<p>where we use <span class="math inline">\(\ell(x)\)</span> to denote the number of elements in <span class="math inline">\(x\)</span>. In other words, we are taking a mean here over the operation within the sum. You could imagine that <span class="math inline">\(x\)</span> are our parameters, and we want to optimize (either maximize or minimize) the output <span class="math inline">\(y\)</span>. For this, we want to obtain the gradients <span class="math inline">\(\partial y / \partial \mathbf{x}\)</span>. For our example, we’ll use <span class="math inline">\(\mathbf{x}=[0,1,2]\)</span> as our input.</p>
<div id="d87608ca" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a></a>x <span class="op">=</span> torch.arange(<span class="dv">3</span>, dtype<span class="op">=</span>torch.float32, requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># Only float tensors can have gradients</span></span>
<span id="cb48-2"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([0., 1., 2.], requires_grad=True)</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computation-3" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation graph: example</strong></p>
</div>
<div class="callout-content">
<p>Now let’s build the computation graph step by step.</p>
<div id="41ff7b28" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a></a>a <span class="op">=</span> x <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb50-2"><a></a>b <span class="op">=</span> a <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb50-3"><a></a>c <span class="op">=</span> b <span class="op">+</span> <span class="dv">3</span></span>
<span id="cb50-4"><a></a>y <span class="op">=</span> c.mean()</span>
<span id="cb50-5"><a></a><span class="bu">print</span>(<span class="st">"Y"</span>, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Y tensor(12.6667, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
</div>
</div>
</div>

<img data-src="img/pytorch_computation_graph.svg" class="r-stretch"></section>
<section id="computation-4" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation graph: example</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, <code>grad_fn</code>.</li>
<li>This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="computation-5" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation graph: example</strong></p>
</div>
<div class="callout-content">
<p>We can perform backpropagation on the computation graph by calling the function <code>backward()</code> on the last output, which effectively calculates the gradients for each tensor that has the property <code>requires_grad=True</code>:</p>
<div id="48a1dd4a" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a></a>y.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>x.grad</code> will now contain the gradient <span class="math inline">\(\partial y/ \partial \mathcal{x}\)</span>, and this gradient indicates how a change in <span class="math inline">\(\mathbf{x}\)</span> will affect output <span class="math inline">\(y\)</span> given the current input <span class="math inline">\(\mathbf{x}=[0,1,2]\)</span>:</p>
<div id="bb5a1b7c" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a><span class="bu">print</span>(x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.3333, 2.0000, 2.6667])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computation-6" class="slide level2">
<h2>Computation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation graph: example</strong></p>
</div>
<div class="callout-content">
<p>We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:</p>
<p><span class="math display">\[\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial c_i}\frac{\partial c_i}{\partial b_i}\frac{\partial b_i}{\partial a_i}\frac{\partial a_i}{\partial x_i}\]</span></p>
<p>Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:</p>
<p><span class="math display">\[
\frac{\partial a_i}{\partial x_i} = 1,\hspace{1cm}
\frac{\partial b_i}{\partial a_i} = 2\cdot a_i\hspace{1cm}
\frac{\partial c_i}{\partial b_i} = 1\hspace{1cm}
\frac{\partial y}{\partial c_i} = \frac{1}{3}
\]</span></p>
<p>Hence, with the input being <span class="math inline">\(\mathbf{x}=[0,1,2]\)</span>, our gradients are <span class="math inline">\(\partial y/\partial \mathbf{x}=[4/3,2,8/3]\)</span>. The previous code cell should have printed the same result.</p>
</div>
</div>
</div>
</section>
<section id="gpu-support" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>GPU overview</strong></p>
</div>
<div class="callout-content">
<p>A crucial feature of PyTorch is the support of GPUs, short for <strong>Graphics Processing Unit</strong>.</p>
<ul>
<li>A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks.</li>
<li>GPUs can accelerate the training of your network up to a factor of <span class="math inline">\(100\)</span> which is essential for large neural networks.</li>
<li>PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries <a href="https://developer.nvidia.com/cuda-zone">CUDA</a> and <a href="https://developer.nvidia.com/cudnn">cuDNN</a>).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="gpu-support-1" class="slide level2">
<h2>GPU support</h2>

<img data-src="img/comparison_CPU_GPU.png" class="r-stretch"></section>
<section id="gpu-support-2" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>GPU</strong></p>
</div>
<div class="callout-content">
<p>First, let’s check whether you have a GPU available:</p>
<div id="c0523dd6" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a></a>gpu_avail <span class="op">=</span> torch.cuda.is_available()</span>
<span id="cb55-2"><a></a><span class="bu">print</span>(<span class="ss">f"Is the Nvidia GPU available? </span><span class="sc">{</span>gpu_avail<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb55-3"><a></a></span>
<span id="cb55-4"><a></a>gpu_avail <span class="op">=</span> torch.mps.is_available()</span>
<span id="cb55-5"><a></a><span class="bu">print</span>(<span class="ss">f"Is the Apple GPU available? </span><span class="sc">{</span>gpu_avail<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Is the Nvidia GPU available? False
Is the Apple GPU available? True</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gpu-support-3" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pushing to GPU</strong></p>
</div>
<div class="callout-content">
<p>By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function <code>.to(...)</code>, or <code>.cuda()</code>.</p>
</div>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>It is often a good practice to define a <code>device</code> object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU.</p>
<p>We can specify the device as follows:</p>
<div id="47e35166" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a></a>device <span class="op">=</span> torch.device(<span class="st">"mps"</span>) <span class="cf">if</span> torch.mps.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb57-2"><a></a><span class="bu">print</span>(<span class="st">"Device"</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Device mps</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gpu-support-4" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pushing to GPU</strong></p>
</div>
<div class="callout-content">
<p>Let’s create a tensor and push it now:</p>
<div id="339daecd" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a></a>x <span class="op">=</span> torch.zeros(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb59-2"><a></a>x <span class="op">=</span> x.to(device)</span>
<span id="cb59-3"><a></a><span class="bu">print</span>(<span class="st">"X"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X tensor([[0., 0., 0.],
        [0., 0., 0.]], device='mps:0')</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The zero next to <code>mps</code> indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train.</p>
</div>
</div>
</div>
</section>
<section id="gpu-support-5" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>GPU-to-CPU comparison</strong></p>
</div>
<div class="callout-content">
<p>We can compare the runtime of a large matrix multiplication on the CPU with a operation on the GPU:</p>
<div id="6140e716" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">1000</span>)</span>
<span id="cb61-2"><a></a></span>
<span id="cb61-3"><a></a><span class="co">## CPU version</span></span>
<span id="cb61-4"><a></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb61-5"><a></a>_ <span class="op">=</span> torch.matmul(x, x)</span>
<span id="cb61-6"><a></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb61-7"><a></a><span class="bu">print</span>(<span class="ss">f"CPU time: </span><span class="sc">{</span>(end_time <span class="op">-</span> start_time)<span class="sc">:6.5f}</span><span class="ss">s"</span>)</span>
<span id="cb61-8"><a></a></span>
<span id="cb61-9"><a></a><span class="co">## GPU version</span></span>
<span id="cb61-10"><a></a>x <span class="op">=</span> x.to(device)</span>
<span id="cb61-11"><a></a>_ <span class="op">=</span> torch.matmul(x, x)  <span class="co"># First operation to 'burn in' GPU</span></span>
<span id="cb61-12"><a></a><span class="co"># CUDA is asynchronous, so we need to use different timing functions</span></span>
<span id="cb61-13"><a></a>start <span class="op">=</span> torch.mps.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-14"><a></a>end <span class="op">=</span> torch.mps.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-15"><a></a>start.record()</span>
<span id="cb61-16"><a></a>_ <span class="op">=</span> torch.matmul(x, x)</span>
<span id="cb61-17"><a></a>end.record()</span>
<span id="cb61-18"><a></a>torch.mps.synchronize()  <span class="co"># Waits for everything to finish running on the GPU</span></span>
<span id="cb61-19"><a></a><span class="bu">print</span>(<span class="ss">f"GPU time: </span><span class="sc">{</span><span class="fl">0.001</span> <span class="op">*</span> start<span class="sc">.</span>elapsed_time(end)<span class="sc">:6.5f}</span><span class="ss">s"</span>)  <span class="co"># Milliseconds to seconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU time: 0.00155s
GPU time: 0.00197s</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gpu-support-6" class="slide level2">
<h2>GPU support</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>GPU: seed sync</strong></p>
</div>
<div class="callout-content">
<p>When generating random numbers, the seed between CPU and GPU is <strong>not synchronized</strong>.</p>
<p>Hence, we need to set the seed on the GPU separately to ensure a <strong>reproducible</strong> code. Hence, we also set the seed on the GPU:</p>
<div id="fd7ee6d5" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a></a><span class="co"># GPU operations have a separate seed we also want to set</span></span>
<span id="cb63-2"><a></a><span class="cf">if</span> torch.mps.is_available():</span>
<span id="cb63-3"><a></a>    torch.mps.manual_seed(<span class="dv">42</span>)</span>
<span id="cb63-4"><a></a>    <span class="co">#torch.mps.manual_seed_all(42)</span></span>
<span id="cb63-5"><a></a></span>
<span id="cb63-6"><a></a><span class="co"># Additionally, some operations on a GPU are implemented stochastic for efficiency</span></span>
<span id="cb63-7"><a></a><span class="co"># We want to ensure that all operations are deterministic on GPU (if used) for reproducibility</span></span>
<span id="cb63-8"><a></a>torch.backends.mps.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb63-9"><a></a>torch.backends.mps.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Manual way</strong></p>
</div>
<div class="callout-content">
<p>If we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using <code>Tensors</code> (with <code>requires_grad=True</code>), ask PyTorch to calculate the gradients and then adjust the parameters.</p>
<p>But things can quickly get <strong>cumbersome</strong> if we have a lot of parameters.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Using torch.nn</strong></p>
</div>
<div class="callout-content">
<p>In PyTorch, there is a package called <code>torch.nn</code> that makes building neural networks more convenient.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-1" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example description</strong></p>
</div>
<div class="callout-content">
<p>Given two binary inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the label to predict is <span class="math inline">\(1\)</span> if either <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> is <span class="math inline">\(1\)</span> while the other is <span class="math inline">\(0\)</span>, or the label is <span class="math inline">\(0\)</span> in all other cases.</p>
<p>The example became famous by the fact that a <strong>single neuron</strong>, i.e.&nbsp;a linear classifier, <strong>cannot learn</strong> this simple function.</p>
<!-- To make it a little bit more interesting, we move the XOR into continuous space and introduce some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows: -->
</div>
</div>
</div>

<img data-src="img/continuous_xor.svg" class="r-stretch"></section>
<section id="continuous-xor-example-2" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Model</strong></p>
</div>
<div class="callout-content">
<p>The package <code>torch.nn</code> defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found <a href="https://pytorch.org/docs/stable/nn.html">here</a>. In case you need a certain network layer, check the documentation of the package first before writing the layer yourself as the package likely contains the code for it already. We import it below:</p>
<div id="d73c56f9" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Functional</strong></p>
</div>
<div class="callout-content">
<p>Additionally to <code>torch.nn</code>, there is also <code>torch.nn.functional</code>. It contains functions that are used in network layers. This is in contrast to <code>torch.nn</code> which defines them as <code>nn.Modules</code>, and <code>torch.nn</code> actually uses a lot of functionalities from <code>torch.nn.functional</code>.</p>
<div id="fbb25016" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-3" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>nn.Module</strong></p>
</div>
<div class="callout-content">
<p>In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:</p>
<div id="d413f6ce" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a></a><span class="kw">class</span> MyModule(nn.Module):</span>
<span id="cb66-2"><a></a></span>
<span id="cb66-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb66-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb66-5"><a></a>        <span class="co"># Some init for my module</span></span>
<span id="cb66-6"><a></a></span>
<span id="cb66-7"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb66-8"><a></a>        <span class="co"># Function for performing the calculation of the module.</span></span>
<span id="cb66-9"><a></a>        <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The forward function is where the computation of the module is taken place, and is executed when you call the module (<code>nn = MyModule(); nn(x)</code>). In the init function, we usually create the parameters of the module, using <code>nn.Parameter</code>, or defining other modules that are used in the forward function. The backward calculation is done automatically, but could be overwritten as well if wanted.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-4" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier</strong></p>
</div>
<div class="callout-content">
<p>We can now make use of the pre-defined modules in the <code>torch.nn</code> package, and define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer.</p>
</div>
</div>
</div>

<img data-src="img/small_neural_network.svg" class="r-stretch"></section>
<section id="continuous-xor-example-5" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier: Module def</strong></p>
</div>
<div class="callout-content">
<div id="6568546f" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a></a><span class="kw">class</span> SimpleClassifier(nn.Module):</span>
<span id="cb67-2"><a></a></span>
<span id="cb67-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, num_hidden, num_outputs):</span>
<span id="cb67-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb67-5"><a></a>        <span class="co"># Initialize the modules we need to build the network</span></span>
<span id="cb67-6"><a></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(num_inputs, num_hidden)</span>
<span id="cb67-7"><a></a>        <span class="va">self</span>.act_fn <span class="op">=</span> nn.Tanh()</span>
<span id="cb67-8"><a></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(num_hidden, num_outputs)</span>
<span id="cb67-9"><a></a></span>
<span id="cb67-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb67-11"><a></a>        <span class="co"># Perform the calculation of the model to determine the prediction</span></span>
<span id="cb67-12"><a></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb67-13"><a></a>        x <span class="op">=</span> <span class="va">self</span>.act_fn(x)</span>
<span id="cb67-14"><a></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb67-15"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Note that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-6" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier: Module creation</strong></p>
</div>
<div class="callout-content">
<div id="996f07fc" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a></a>model <span class="op">=</span> SimpleClassifier(num_inputs<span class="op">=</span><span class="dv">2</span>, num_hidden<span class="op">=</span><span class="dv">4</span>, num_outputs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb68-2"><a></a><span class="co"># Printing a module shows all its submodules</span></span>
<span id="cb68-3"><a></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SimpleClassifier(
  (linear1): Linear(in_features=2, out_features=4, bias=True)
  (act_fn): Tanh()
  (linear2): Linear(in_features=4, out_features=1, bias=True)
)</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-7" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier: Module parameters</strong></p>
</div>
<div class="callout-content">
<p>The parameters of a module can be obtained by using its <code>parameters()</code> functions, or <code>named_parameters()</code> to get a name to each parameter object. For our small neural network, we have the following parameters:</p>
<div id="a5549e23" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb70-2"><a></a>    <span class="bu">print</span>(<span class="ss">f"Parameter </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">, shape </span><span class="sc">{</span>param<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter linear1.weight, shape torch.Size([4, 2])
Parameter linear1.bias, shape torch.Size([4])
Parameter linear2.weight, shape torch.Size([1, 4])
Parameter linear2.bias, shape torch.Size([1])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-8" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier: Module parameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Each linear layer has a weight matrix of the shape <code>[output, input]</code>, and a bias of the shape <code>[output]</code>.</li>
<li>The tanh activation function does not have any parameters.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Parameters are only registered for <code>nn.Module</code> objects that are direct object attributes, i.e.&nbsp;<code>self.a = ...</code>. There are alternatives, like <code>nn.ModuleList</code>, <code>nn.ModuleDict</code> and <code>nn.Sequential</code>, that allow you to have different data structures of modules.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-9" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Simple classifier: data</strong></p>
</div>
<div class="callout-content">
<p>PyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package <code>torch.utils.data</code>.</p>
<div id="cd934fc0" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The data package defines two classes which are the standard interface for handling data in PyTorch:</p>
<ul>
<li><code>data.Dataset</code>: provides an uniform interface to access the training/test data</li>
<li><code>data.DataLoader</code>: makes sure to efficiently load and stack the data points from the dataset into batches during training.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-10" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset class</strong></p>
</div>
<div class="callout-content">
<p>To define a dataset in PyTorch, we simply specify two functions:</p>
<ul>
<li><code>__getitem__</code>: has to return the <span class="math inline">\(i\)</span>-th data point in the dataset</li>
<li><code>__len__</code>: returns the size of the dataset.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-11" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset class</strong></p>
</div>
<div class="callout-content">
<div id="7407dbab" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a></a><span class="kw">class</span> XORDataset(data.Dataset):</span>
<span id="cb73-2"><a></a></span>
<span id="cb73-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, std<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb73-4"><a></a>        <span class="co">"""</span></span>
<span id="cb73-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb73-6"><a></a><span class="co">            size - Number of data points we want to generate</span></span>
<span id="cb73-7"><a></a><span class="co">            std - Standard deviation of the noise (see generate_continuous_xor function)</span></span>
<span id="cb73-8"><a></a><span class="co">        """</span></span>
<span id="cb73-9"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-10"><a></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb73-11"><a></a>        <span class="va">self</span>.std <span class="op">=</span> std</span>
<span id="cb73-12"><a></a>        <span class="va">self</span>.generate_continuous_xor()</span>
<span id="cb73-13"><a></a></span>
<span id="cb73-14"><a></a>    <span class="kw">def</span> generate_continuous_xor(<span class="va">self</span>):</span>
<span id="cb73-15"><a></a>        <span class="co"># Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1</span></span>
<span id="cb73-16"><a></a>        <span class="co"># The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.</span></span>
<span id="cb73-17"><a></a>        <span class="co"># If x=y, the label is 0.</span></span>
<span id="cb73-18"><a></a>        data <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">2</span>, size<span class="op">=</span>(<span class="va">self</span>.size, <span class="dv">2</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb73-19"><a></a>        label <span class="op">=</span> (data.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">==</span> <span class="dv">1</span>).to(torch.<span class="bu">long</span>)</span>
<span id="cb73-20"><a></a>        <span class="co"># To make it slightly more challenging, we add a bit of gaussian noise to the data points.</span></span>
<span id="cb73-21"><a></a>        data <span class="op">+=</span> <span class="va">self</span>.std <span class="op">*</span> torch.randn(data.shape)</span>
<span id="cb73-22"><a></a></span>
<span id="cb73-23"><a></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb73-24"><a></a>        <span class="va">self</span>.label <span class="op">=</span> label</span>
<span id="cb73-25"><a></a></span>
<span id="cb73-26"><a></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb73-27"><a></a>        <span class="co"># Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]</span></span>
<span id="cb73-28"><a></a>        <span class="cf">return</span> <span class="va">self</span>.size</span>
<span id="cb73-29"><a></a></span>
<span id="cb73-30"><a></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb73-31"><a></a>        <span class="co"># Return the idx-th data point of the dataset</span></span>
<span id="cb73-32"><a></a>        <span class="co"># If we have multiple things to return (data point and label), we can return them as tuple</span></span>
<span id="cb73-33"><a></a>        data_point <span class="op">=</span> <span class="va">self</span>.data[idx]</span>
<span id="cb73-34"><a></a>        data_label <span class="op">=</span> <span class="va">self</span>.label[idx]</span>
<span id="cb73-35"><a></a>        <span class="cf">return</span> data_point, data_label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-12" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset class</strong></p>
</div>
<div class="callout-content">
<p>Let’s create and inspect:</p>
<div id="c61679cc" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a></a>dataset <span class="op">=</span> XORDataset(size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb74-2"><a></a><span class="bu">print</span>(<span class="st">"Size of dataset:"</span>, <span class="bu">len</span>(dataset))</span>
<span id="cb74-3"><a></a><span class="bu">print</span>(<span class="st">"Data point 0:"</span>, dataset[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Size of dataset: 200
Data point 0: (tensor([-0.1295,  0.8758]), tensor(1))</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-13" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset visualization</strong></p>
</div>
<div class="callout-content">
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Func definition</a></li><li><a href="#tabset-1-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div id="1aa40e0d" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a></a><span class="kw">def</span> visualize_samples(data, label):</span>
<span id="cb76-2"><a></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(data, torch.Tensor):</span>
<span id="cb76-3"><a></a>        data <span class="op">=</span> data.cpu().numpy()</span>
<span id="cb76-4"><a></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(label, torch.Tensor):</span>
<span id="cb76-5"><a></a>        label <span class="op">=</span> label.cpu().numpy()</span>
<span id="cb76-6"><a></a>    data_0 <span class="op">=</span> data[label <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb76-7"><a></a>    data_1 <span class="op">=</span> data[label <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb76-8"><a></a></span>
<span id="cb76-9"><a></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb76-10"><a></a>    plt.scatter(data_0[:,<span class="dv">0</span>], data_0[:,<span class="dv">1</span>], edgecolor<span class="op">=</span><span class="st">"#333"</span>, label<span class="op">=</span><span class="st">"Class 0"</span>)</span>
<span id="cb76-11"><a></a>    plt.scatter(data_1[:,<span class="dv">0</span>], data_1[:,<span class="dv">1</span>], edgecolor<span class="op">=</span><span class="st">"#333"</span>, label<span class="op">=</span><span class="st">"Class 1"</span>)</span>
<span id="cb76-12"><a></a>    plt.title(<span class="st">"Dataset samples"</span>)</span>
<span id="cb76-13"><a></a>    plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb76-14"><a></a>    plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb76-15"><a></a>    plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2">
<div id="0e03ec03" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a></a>visualize_samples(dataset.data, dataset.label)</span>
<span id="cb77-2"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec5_files/figure-revealjs/cell-45-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-14" class="slide level2 scrollable">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The data loader class</strong></p>
</div>
<div class="callout-content">
<p>The class <code>torch.utils.data.DataLoader</code> represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function <code>__getitem__</code>, and stacks its outputs as tensors over the first dimension to form a batch. We can configure our data loader with the following input arguments:</p>
<ul>
<li><code>batch_size</code>: Number of samples to stack per batch</li>
<li><code>shuffle</code>: If True, the data is returned in a random order. This is important during training for introducing stochasticity.</li>
<li><code>num_workers</code>: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g.&nbsp;large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.</li>
<li><code>pin_memory</code>: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.</li>
<li><code>drop_last</code>: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-15" class="slide level2 scrollable">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The data loader class</strong></p>
</div>
<div class="callout-content">
<p>Create:</p>
<div id="6424ecd0" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a></a>data_loader <span class="op">=</span> data.DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">8</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Fetch some data:</p>
<div id="70215f39" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a></a><span class="co"># next(iter(...)) catches the first batch of the data loader</span></span>
<span id="cb79-2"><a></a><span class="co"># If shuffle is True, this will return a different batch every time we run this cell</span></span>
<span id="cb79-3"><a></a><span class="co"># For iterating over the whole dataset, we can simple use "for batch in data_loader: ..."</span></span>
<span id="cb79-4"><a></a>data_inputs, data_labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_loader))</span>
<span id="cb79-5"><a></a></span>
<span id="cb79-6"><a></a><span class="co"># The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the</span></span>
<span id="cb79-7"><a></a><span class="co"># dimensions of the data point returned from the dataset class</span></span>
<span id="cb79-8"><a></a><span class="bu">print</span>(<span class="st">"Data inputs"</span>, data_inputs.shape, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, data_inputs)</span>
<span id="cb79-9"><a></a><span class="bu">print</span>(<span class="st">"Data labels"</span>, data_labels.shape, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, data_labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data inputs torch.Size([8, 2]) 
 tensor([[ 1.0122,  1.0697],
        [-0.0038,  1.0506],
        [ 0.9650,  0.9374],
        [ 0.9716,  0.7959],
        [ 0.0564,  1.0297],
        [-0.0850,  1.1061],
        [ 0.0873, -0.0852],
        [ 1.1155,  0.8814]])
Data labels torch.Size([8]) 
 tensor([0, 1, 0, 0, 1, 1, 0, 0])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-16" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Optimization</strong></p>
</div>
<div class="callout-content">
<p>After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:</p>
<ol type="1">
<li>Get a batch from the data loader</li>
<li>Obtain the predictions from the model for the batch</li>
<li>Calculate the loss based on the difference between predictions and labels</li>
<li>Backpropagation: calculate the gradients for every parameter with respect to the loss</li>
<li>Update the parameters of the model in the direction of the gradients</li>
</ol>
<p>We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-17" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Loss</strong></p>
</div>
<div class="callout-content">
<p>We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:</p>
<p><span class="math display">\[\mathcal{L}_{BCE} = -\sum_i \left[ y_i \log x_i + (1 - y_i) \log (1 - x_i) \right]\]</span></p>
<p>where <span class="math inline">\(y\)</span> are our labels, and <span class="math inline">\(x\)</span> our predictions, both in the range of <span class="math inline">\([0,1]\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-18" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Predefined funcs</strong></p>
</div>
<div class="callout-content">
<p>PyTorch already provides a list of predefined loss functions which we can use (see <a href="https://pytorch.org/docs/stable/nn.html#loss-functions">here</a> for a full list).</p>
<p>For BCE, PyTorch has two modules:</p>
<ul>
<li><code>nn.BCELoss()</code>: expects the inputs <span class="math inline">\(x\)</span> to be in the range <span class="math inline">\([0,1]\)</span>, i.e.&nbsp;the output of a sigmoid,</li>
<li><code>nn.BCEWithLogitsLoss()</code> combines a sigmoid layer and the BCE loss in a single class. This version is numerically more stable than using a plain Sigmoid followed by a BCE loss because of the logarithms applied in the loss function.</li>
</ul>
</div>
</div>
</div>

<aside><div>
<p>It is adviced to use loss functions applied on “logits” where possible (remember to not apply a sigmoid on the output of the model in this case!).</p>
</div></aside></section>
<section id="continuous-xor-example-19" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Loss</strong></p>
</div>
<div class="callout-content">
<p>For our model defined above, we therefore use the module <code>nn.BCEWithLogitsLoss</code>.</p>
<div id="71f48402" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a></a>loss_module <span class="op">=</span> nn.BCEWithLogitsLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-20" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stochastic Gradient Descent</strong></p>
</div>
<div class="callout-content">
<p>For updating the parameters, PyTorch provides the package <code>torch.optim</code> that has most popular optimizers implemented.</p>
<p><code>torch.optim.SGD</code> (<strong>Stochastic Gradient Descent</strong>). Updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss).</p>
<p>A good default value of the learning rate for a small network as ours is 0.1.</p>
<div id="d12c7e74" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a></a><span class="co"># Input to the optimizer are the parameters of the model: model.parameters()</span></span>
<span id="cb82-2"><a></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-21" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Optimizer</strong></p>
</div>
<div class="callout-content">
<p>The optimizer provides two useful functions:</p>
<ul>
<li><code>optimizer.step()</code>: updates the parameters based on the gradients as explained above.</li>
<li><code>optimizer.zero_grad()</code>: sets the gradients of all parameters to zero.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><code>zero_grad()</code> is a crucial pre-step before performing backpropagation. If we call the <code>backward</code> function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-22" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<p>Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size.</p>
<div id="3f9bf26d" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a></a>train_dataset <span class="op">=</span> XORDataset(size<span class="op">=</span><span class="dv">2500</span>)</span>
<span id="cb83-2"><a></a>train_data_loader <span class="op">=</span> data.DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, push to GPU:</p>
<div id="715e6eb3" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a></a><span class="co"># Push model to device. Has to be only done once</span></span>
<span id="cb84-2"><a></a>model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>SimpleClassifier(
  (linear1): Linear(in_features=2, out_features=4, bias=True)
  (act_fn): Tanh()
  (linear2): Linear(in_features=4, out_features=1, bias=True)
)</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-23" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<p>Set model to training mode via <code>model.train()</code> (there is also <code>model.eval()</code>):</p>
<div id="4b32d1f6" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a></a><span class="kw">def</span> train_model(model, optimizer, data_loader, loss_module, num_epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb86-2"><a></a>    <span class="co"># Set model to train mode</span></span>
<span id="cb86-3"><a></a>    model.train()</span>
<span id="cb86-4"><a></a></span>
<span id="cb86-5"><a></a>    <span class="co"># Training loop</span></span>
<span id="cb86-6"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_epochs)):</span>
<span id="cb86-7"><a></a>        <span class="cf">for</span> data_inputs, data_labels <span class="kw">in</span> data_loader:</span>
<span id="cb86-8"><a></a></span>
<span id="cb86-9"><a></a>            <span class="co">## Step 1: Move input data to device (only strictly necessary if we use GPU)</span></span>
<span id="cb86-10"><a></a>            data_inputs <span class="op">=</span> data_inputs.to(device)</span>
<span id="cb86-11"><a></a>            data_labels <span class="op">=</span> data_labels.to(device)</span>
<span id="cb86-12"><a></a></span>
<span id="cb86-13"><a></a>            <span class="co">## Step 2: Run the model on the input data</span></span>
<span id="cb86-14"><a></a>            preds <span class="op">=</span> model(data_inputs)</span>
<span id="cb86-15"><a></a>            preds <span class="op">=</span> preds.squeeze(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Output is [Batch size, 1], but we want [Batch size]</span></span>
<span id="cb86-16"><a></a></span>
<span id="cb86-17"><a></a>            <span class="co">## Step 3: Calculate the loss</span></span>
<span id="cb86-18"><a></a>            loss <span class="op">=</span> loss_module(preds, data_labels.<span class="bu">float</span>())</span>
<span id="cb86-19"><a></a></span>
<span id="cb86-20"><a></a>            <span class="co">## Step 4: Perform backpropagation</span></span>
<span id="cb86-21"><a></a>            <span class="co"># Before calculating the gradients, we need to ensure that they are all zero.</span></span>
<span id="cb86-22"><a></a>            <span class="co"># The gradients would not be overwritten, but actually added to the existing ones.</span></span>
<span id="cb86-23"><a></a>            optimizer.zero_grad()</span>
<span id="cb86-24"><a></a>            <span class="co"># Perform backpropagation</span></span>
<span id="cb86-25"><a></a>            loss.backward()</span>
<span id="cb86-26"><a></a></span>
<span id="cb86-27"><a></a>            <span class="co">## Step 5: Update the parameters</span></span>
<span id="cb86-28"><a></a>            optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-24" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<p>Train:</p>
<div id="ba65fc5a" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a></a>train_model(model, optimizer, train_data_loader, loss_module)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"468daba550fc4e2fb07fcc2aca4b9ea2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-25" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training: save a model</strong></p>
</div>
<div class="callout-content">
<p>After finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called <code>state_dict</code> from the model which contains all learnable parameters.</p>
<div id="ea046870" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a></a>state_dict <span class="op">=</span> model.state_dict()</span>
<span id="cb88-2"><a></a><span class="bu">print</span>(state_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OrderedDict({'linear1.weight': tensor([[-0.1582, -0.2596],
        [-3.3204,  2.7321],
        [-1.8136, -1.6776],
        [ 2.4255, -3.0768]], device='mps:0'), 'linear1.bias': tensor([ 0.8179, -1.2332,  0.1443, -1.0332], device='mps:0'), 'linear2.weight': tensor([[ 0.7300,  4.3084, -2.4952,  4.2797]], device='mps:0'), 'linear2.bias': tensor([1.6509], device='mps:0')})</code></pre>
</div>
</div>
<p>To save the state dictionary, we can use <code>torch.save</code>:</p>
<div id="0f9d0095" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a></a><span class="co"># torch.save(object, filename). For the filename, any extension can be used</span></span>
<span id="cb90-2"><a></a>torch.save(state_dict, <span class="st">"our_model.tar"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-26" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training: load a model</strong></p>
</div>
<div class="callout-content">
<p>To load a model from a state dict, we use the function torch.load to load the state dict from the disk, and the module function load_state_dict to overwrite our parameters with the new values:</p>
<div id="b5594720" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a></a><span class="co"># Load state dict from the disk (make sure it is the same name as above)</span></span>
<span id="cb91-2"><a></a>state_dict <span class="op">=</span> torch.load(<span class="st">"our_model.tar"</span>)</span>
<span id="cb91-3"><a></a></span>
<span id="cb91-4"><a></a><span class="co"># Create a new model and load the state</span></span>
<span id="cb91-5"><a></a>new_model <span class="op">=</span> SimpleClassifier(num_inputs<span class="op">=</span><span class="dv">2</span>, num_hidden<span class="op">=</span><span class="dv">4</span>, num_outputs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb91-6"><a></a>new_model.load_state_dict(state_dict)</span>
<span id="cb91-7"><a></a></span>
<span id="cb91-8"><a></a><span class="co"># Verify that the parameters are the same</span></span>
<span id="cb91-9"><a></a><span class="bu">print</span>(<span class="st">"Original model</span><span class="ch">\n</span><span class="st">"</span>, model.state_dict())</span>
<span id="cb91-10"><a></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Loaded model</span><span class="ch">\n</span><span class="st">"</span>, new_model.state_dict())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original model
 OrderedDict({'linear1.weight': tensor([[-0.1582, -0.2596],
        [-3.3204,  2.7321],
        [-1.8136, -1.6776],
        [ 2.4255, -3.0768]], device='mps:0'), 'linear1.bias': tensor([ 0.8179, -1.2332,  0.1443, -1.0332], device='mps:0'), 'linear2.weight': tensor([[ 0.7300,  4.3084, -2.4952,  4.2797]], device='mps:0'), 'linear2.bias': tensor([1.6509], device='mps:0')})

Loaded model
 OrderedDict({'linear1.weight': tensor([[-0.1582, -0.2596],
        [-3.3204,  2.7321],
        [-1.8136, -1.6776],
        [ 2.4255, -3.0768]]), 'linear1.bias': tensor([ 0.8179, -1.2332,  0.1443, -1.0332]), 'linear2.weight': tensor([[ 0.7300,  4.3084, -2.4952,  4.2797]]), 'linear2.bias': tensor([1.6509])})</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-27" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Evaluation</strong></p>
</div>
<div class="callout-content">
<p>Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader.</p>
<div id="bc71888d" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a></a>test_dataset <span class="op">=</span> XORDataset(size<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb93-2"><a></a><span class="co"># drop_last -&gt; Don't drop the last batch although it is smaller than 128</span></span>
<span id="cb93-3"><a></a>test_data_loader <span class="op">=</span> data.DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As metric, we will use accuracy which is calculated as follows:</p>
<p><span class="math display">\[acc = \frac{\#\text{correct predictions}}{\#\text{all predictions}} = \frac{TP+TN}{TP+TN+FP+FN}\]</span></p>
<p>where TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives.</p>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-28" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Evaluation</strong></p>
</div>
<div class="callout-content">
<ul>
<li>we don’t need to keep track of the computation graph as we don’t intend to calculate the gradients. In PyTorch this is done via <code>with torch.no_grad(): ...</code>.</li>
<li>remember to additionally set the model to eval mode.</li>
</ul>
<div id="9bddfbc8" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a></a><span class="kw">def</span> eval_model(model, data_loader):</span>
<span id="cb94-2"><a></a>    model.<span class="bu">eval</span>() <span class="co"># Set model to eval mode</span></span>
<span id="cb94-3"><a></a>    true_preds, num_preds <span class="op">=</span> <span class="fl">0.</span>, <span class="fl">0.</span></span>
<span id="cb94-4"><a></a></span>
<span id="cb94-5"><a></a>    <span class="cf">with</span> torch.no_grad(): <span class="co"># Deactivate gradients for the following code</span></span>
<span id="cb94-6"><a></a>        <span class="cf">for</span> data_inputs, data_labels <span class="kw">in</span> data_loader:</span>
<span id="cb94-7"><a></a></span>
<span id="cb94-8"><a></a>            <span class="co"># Determine prediction of model on dev set</span></span>
<span id="cb94-9"><a></a>            data_inputs, data_labels <span class="op">=</span> data_inputs.to(device), data_labels.to(device)</span>
<span id="cb94-10"><a></a>            preds <span class="op">=</span> model(data_inputs)</span>
<span id="cb94-11"><a></a>            preds <span class="op">=</span> preds.squeeze(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb94-12"><a></a>            preds <span class="op">=</span> torch.sigmoid(preds) <span class="co"># Sigmoid to map predictions between 0 and 1</span></span>
<span id="cb94-13"><a></a>            pred_labels <span class="op">=</span> (preds <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">long</span>() <span class="co"># Binarize predictions to 0 and 1</span></span>
<span id="cb94-14"><a></a></span>
<span id="cb94-15"><a></a>            <span class="co"># Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)</span></span>
<span id="cb94-16"><a></a>            true_preds <span class="op">+=</span> (pred_labels <span class="op">==</span> data_labels).<span class="bu">sum</span>()</span>
<span id="cb94-17"><a></a>            num_preds <span class="op">+=</span> data_labels.shape[<span class="dv">0</span>]</span>
<span id="cb94-18"><a></a></span>
<span id="cb94-19"><a></a>    acc <span class="op">=</span> true_preds <span class="op">/</span> num_preds</span>
<span id="cb94-20"><a></a>    <span class="bu">print</span>(<span class="ss">f"Accuracy of the model: </span><span class="sc">{</span><span class="fl">100.0</span><span class="op">*</span>acc<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-xor-example-29" class="slide level2">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Evaluation</strong></p>
</div>
<div class="callout-content">
<div id="9fe8f602" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a></a>eval_model(model, test_data_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy of the model: 100.00%</code></pre>
</div>
</div>
</div>
</div>
</div>

<img data-src="img/whoa.png" class="r-stretch"></section>
<section id="continuous-xor-example-30" class="slide level2 scrollable">
<h2>Continuous XOR example</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualizing classification boundaries</strong></p>
</div>
<div class="callout-content">
<p>To visualize what our model has learned, we can perform a prediction for every data point in a range of <span class="math inline">\([-0.5, 1.5]\)</span>, and visualize the predicted class (class 0 is blue and class 1 is orange).</p>
<div id="9cf9b92d" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a></a><span class="at">@torch.no_grad</span>() <span class="co"># Decorator, same effect as "with torch.no_grad(): ..." over the whole function.</span></span>
<span id="cb97-2"><a></a><span class="kw">def</span> visualize_classification(model, data, label):</span>
<span id="cb97-3"><a></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(data, torch.Tensor):</span>
<span id="cb97-4"><a></a>        data <span class="op">=</span> data.cpu().numpy()</span>
<span id="cb97-5"><a></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(label, torch.Tensor):</span>
<span id="cb97-6"><a></a>        label <span class="op">=</span> label.cpu().numpy()</span>
<span id="cb97-7"><a></a>    data_0 <span class="op">=</span> data[label <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb97-8"><a></a>    data_1 <span class="op">=</span> data[label <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb97-9"><a></a></span>
<span id="cb97-10"><a></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>), dpi<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb97-11"><a></a>    plt.scatter(data_0[:,<span class="dv">0</span>], data_0[:,<span class="dv">1</span>], edgecolor<span class="op">=</span><span class="st">"#333"</span>, label<span class="op">=</span><span class="st">"Class 0"</span>)</span>
<span id="cb97-12"><a></a>    plt.scatter(data_1[:,<span class="dv">0</span>], data_1[:,<span class="dv">1</span>], edgecolor<span class="op">=</span><span class="st">"#333"</span>, label<span class="op">=</span><span class="st">"Class 1"</span>)</span>
<span id="cb97-13"><a></a>    plt.title(<span class="st">"Dataset samples"</span>)</span>
<span id="cb97-14"><a></a>    plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb97-15"><a></a>    plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb97-16"><a></a>    plt.legend()</span>
<span id="cb97-17"><a></a></span>
<span id="cb97-18"><a></a>    <span class="co"># Let's make use of a lot of operations we have learned above</span></span>
<span id="cb97-19"><a></a>    model.to(device)</span>
<span id="cb97-20"><a></a>    c0 <span class="op">=</span> torch.Tensor(to_rgba(<span class="st">"C0"</span>)).to(device)</span>
<span id="cb97-21"><a></a>    c1 <span class="op">=</span> torch.Tensor(to_rgba(<span class="st">"C1"</span>)).to(device)</span>
<span id="cb97-22"><a></a>    x1 <span class="op">=</span> torch.arange(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>, step<span class="op">=</span><span class="fl">0.01</span>, device<span class="op">=</span>device)</span>
<span id="cb97-23"><a></a>    x2 <span class="op">=</span> torch.arange(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>, step<span class="op">=</span><span class="fl">0.01</span>, device<span class="op">=</span>device)</span>
<span id="cb97-24"><a></a>    xx1, xx2 <span class="op">=</span> torch.meshgrid(x1, x2, indexing<span class="op">=</span><span class="st">'ij'</span>)  <span class="co"># Meshgrid function as in numpy</span></span>
<span id="cb97-25"><a></a>    model_inputs <span class="op">=</span> torch.stack([xx1, xx2], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb97-26"><a></a>    preds <span class="op">=</span> model(model_inputs)</span>
<span id="cb97-27"><a></a>    preds <span class="op">=</span> torch.sigmoid(preds)</span>
<span id="cb97-28"><a></a>    output_image <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> preds) <span class="op">*</span> c0[<span class="va">None</span>,<span class="va">None</span>] <span class="op">+</span> preds <span class="op">*</span> c1[<span class="va">None</span>,<span class="va">None</span>]  <span class="co"># Specifying "None" in a dimension creates a new one</span></span>
<span id="cb97-29"><a></a>    output_image <span class="op">=</span> output_image.cpu().numpy()  <span class="co"># Convert to numpy array. This only works for tensors on CPU, hence first push to CPU</span></span>
<span id="cb97-30"><a></a>    plt.imshow(output_image, origin<span class="op">=</span><span class="st">'lower'</span>, extent<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>))</span>
<span id="cb97-31"><a></a>    plt.grid(<span class="va">False</span>)</span>
<span id="cb97-32"><a></a>    <span class="cf">return</span> fig</span>
<span id="cb97-33"><a></a></span>
<span id="cb97-34"><a></a>_ <span class="op">=</span> visualize_classification(model, dataset.data, dataset.label)</span>
<span id="cb97-35"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec5_files/figure-revealjs/cell-60-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="additional-features" class="slide level2">
<h2>Additional features</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensorboard</strong></p>
</div>
<div class="callout-content">
<p>TensorBoard is a logging and visualization tool that is a popular choice for training deep learning models.</p>
<div id="cd372649" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a></a><span class="co"># Import tensorboard logger from PyTorch</span></span>
<span id="cb98-2"><a></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb98-3"><a></a></span>
<span id="cb98-4"><a></a><span class="co"># Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook</span></span>
<span id="cb98-5"><a></a><span class="op">%</span>load_ext tensorboard</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="additional-features-1" class="slide level2">
<h2>Additional features</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensorboard API</strong></p>
</div>
<div class="callout-content">
<ul>
<li>We start the logging process by creating a new object, <code>writer = SummaryWriter(...)</code>, where we specify the directory in which the logging file should be saved</li>
<li>With this object, we can log different aspects of our model by calling functions of the style <code>writer.add_...</code>.</li>
<li>For example, we can visualize the computation graph with the function <code>writer.add_graph</code>, or add a scalar value like the loss with <code>writer.add_scalar</code>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="additional-features-2" class="slide level2">
<h2>Additional features</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensorboard</strong></p>
</div>
<div class="callout-content">
<div id="5d4196b4" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a></a><span class="kw">def</span> train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs<span class="op">=</span><span class="dv">100</span>, logging_dir<span class="op">=</span><span class="st">'runs/our_experiment'</span>):</span>
<span id="cb99-2"><a></a>    <span class="co"># Create TensorBoard logger</span></span>
<span id="cb99-3"><a></a>    writer <span class="op">=</span> SummaryWriter(logging_dir)</span>
<span id="cb99-4"><a></a>    model_plotted <span class="op">=</span> <span class="va">False</span></span>
<span id="cb99-5"><a></a></span>
<span id="cb99-6"><a></a>    <span class="co"># Set model to train mode</span></span>
<span id="cb99-7"><a></a>    model.train()</span>
<span id="cb99-8"><a></a></span>
<span id="cb99-9"><a></a>    <span class="co"># Training loop</span></span>
<span id="cb99-10"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_epochs)):</span>
<span id="cb99-11"><a></a>        epoch_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb99-12"><a></a>        <span class="cf">for</span> data_inputs, data_labels <span class="kw">in</span> data_loader:</span>
<span id="cb99-13"><a></a></span>
<span id="cb99-14"><a></a>            <span class="co">## Step 1: Move input data to device (only strictly necessary if we use GPU)</span></span>
<span id="cb99-15"><a></a>            data_inputs <span class="op">=</span> data_inputs.to(device)</span>
<span id="cb99-16"><a></a>            data_labels <span class="op">=</span> data_labels.to(device)</span>
<span id="cb99-17"><a></a></span>
<span id="cb99-18"><a></a>            <span class="co"># For the very first batch, we visualize the computation graph in TensorBoard</span></span>
<span id="cb99-19"><a></a>            <span class="cf">if</span> <span class="kw">not</span> model_plotted:</span>
<span id="cb99-20"><a></a>                writer.add_graph(model, data_inputs)</span>
<span id="cb99-21"><a></a>                model_plotted <span class="op">=</span> <span class="va">True</span></span>
<span id="cb99-22"><a></a></span>
<span id="cb99-23"><a></a>            <span class="co">## Step 2: Run the model on the input data</span></span>
<span id="cb99-24"><a></a>            preds <span class="op">=</span> model(data_inputs)</span>
<span id="cb99-25"><a></a>            preds <span class="op">=</span> preds.squeeze(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Output is [Batch size, 1], but we want [Batch size]</span></span>
<span id="cb99-26"><a></a></span>
<span id="cb99-27"><a></a>            <span class="co">## Step 3: Calculate the loss</span></span>
<span id="cb99-28"><a></a>            loss <span class="op">=</span> loss_module(preds, data_labels.<span class="bu">float</span>())</span>
<span id="cb99-29"><a></a></span>
<span id="cb99-30"><a></a>            <span class="co">## Step 4: Perform backpropagation</span></span>
<span id="cb99-31"><a></a>            <span class="co"># Before calculating the gradients, we need to ensure that they are all zero.</span></span>
<span id="cb99-32"><a></a>            <span class="co"># The gradients would not be overwritten, but actually added to the existing ones.</span></span>
<span id="cb99-33"><a></a>            optimizer.zero_grad()</span>
<span id="cb99-34"><a></a>            <span class="co"># Perform backpropagation</span></span>
<span id="cb99-35"><a></a>            loss.backward()</span>
<span id="cb99-36"><a></a></span>
<span id="cb99-37"><a></a>            <span class="co">## Step 5: Update the parameters</span></span>
<span id="cb99-38"><a></a>            optimizer.step()</span>
<span id="cb99-39"><a></a></span>
<span id="cb99-40"><a></a>            <span class="co">## Step 6: Take the running average of the loss</span></span>
<span id="cb99-41"><a></a>            epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb99-42"><a></a></span>
<span id="cb99-43"><a></a>        <span class="co"># Add average loss to TensorBoard</span></span>
<span id="cb99-44"><a></a>        epoch_loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb99-45"><a></a>        writer.add_scalar(<span class="st">'training_loss'</span>,</span>
<span id="cb99-46"><a></a>                          epoch_loss,</span>
<span id="cb99-47"><a></a>                          global_step <span class="op">=</span> epoch <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb99-48"><a></a></span>
<span id="cb99-49"><a></a>        <span class="co"># Visualize prediction and add figure to TensorBoard</span></span>
<span id="cb99-50"><a></a>        <span class="co"># Since matplotlib figures can be slow in rendering, we only do it every 10th epoch</span></span>
<span id="cb99-51"><a></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb99-52"><a></a>            fig <span class="op">=</span> visualize_classification(model, val_dataset.data, val_dataset.label)</span>
<span id="cb99-53"><a></a>            writer.add_figure(<span class="st">'predictions'</span>,</span>
<span id="cb99-54"><a></a>                              fig,</span>
<span id="cb99-55"><a></a>                              global_step <span class="op">=</span> epoch <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb99-56"><a></a></span>
<span id="cb99-57"><a></a>    writer.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="additional-features-3" class="slide level2">
<h2>Additional features</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensorboard</strong></p>
</div>
<div class="callout-content">
<p>Train:</p>
<div id="85e5476c" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a></a>model <span class="op">=</span> SimpleClassifier(num_inputs<span class="op">=</span><span class="dv">2</span>, num_hidden<span class="op">=</span><span class="dv">4</span>, num_outputs<span class="op">=</span><span class="dv">1</span>).to(device)</span>
<span id="cb100-2"><a></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb100-3"><a></a>train_model_with_logger(model, optimizer, train_data_loader, loss_module, val_dataset<span class="op">=</span>dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e701db9ab21646ec9805e5755d02af45","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="additional-features-4" class="slide level2">
<h2>Additional features</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensorboard</strong></p>
</div>
<div class="callout-content">
<p>The TensorBoard file in the folder <code>runs/our_experiment</code> now contains a loss curve, the computation graph of our network, and a visualization of the learned predictions over number of epochs.</p>
<div id="7294f0b5" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a></a><span class="op">%</span>tensorboard <span class="op">--</span>logdir runs<span class="op">/</span>our_experiment</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>



<img data-src="img/tensorboard_screenshot.png" class="r-stretch"></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"dfeb146abfa7566d06e3260dc51741a028c92925d9a164cf153da95cae43b4f3"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="application/vnd.jupyter.widget-state+json">
    {"state":{"15ac3429983e4c279e55d6fc6e1a5340":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42216de74462420187feef551cea5081":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468daba550fc4e2fb07fcc2aca4b9ea2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59e8728945744bee822fee2d7e4179ef","IPY_MODEL_8a468d54edeb4fd29ebb4dd63242f4df","IPY_MODEL_e86e67f125934707b0314062c6a2c6ed"],"layout":"IPY_MODEL_42216de74462420187feef551cea5081","tabbable":null,"tooltip":null}},"4bc54e74904443d8ba1f592a7f9babb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"59e8728945744bee822fee2d7e4179ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fe1ce72b813e4ecd8de58f4f0d637d06","placeholder":"​","style":"IPY_MODEL_4bc54e74904443d8ba1f592a7f9babb7","tabbable":null,"tooltip":null,"value":"100%"}},"5dff2ab174a148269737f601117db576":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69e2a10ca5de47fe98d9c89fada29bef":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75512a0ddc8a453299ca914d31017019":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_15ac3429983e4c279e55d6fc6e1a5340","placeholder":"​","style":"IPY_MODEL_d7a1c79e5eb444feb50ca9ec6716872f","tabbable":null,"tooltip":null,"value":" 100/100 [00:04&lt;00:00, 23.38it/s]"}},"8a468d54edeb4fd29ebb4dd63242f4df":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5dff2ab174a148269737f601117db576","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da4e98f0657746a0a4a39c8be573ab8d","tabbable":null,"tooltip":null,"value":100}},"984c1bb9555a4e388a8143c58831492c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a2f1c5ece40b48fa9243086c846d3de0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a60b86eea7d04261a02fc5bed48dc4bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"aece0cc1c56d4adc9ac1a5f3a8aeb5dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_bbfcf467f88f48b997cffd71ce0a9629","placeholder":"​","style":"IPY_MODEL_a60b86eea7d04261a02fc5bed48dc4bf","tabbable":null,"tooltip":null,"value":"100%"}},"bbfcf467f88f48b997cffd71ce0a9629":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccbe5b8f41aa4840b0e28cda15af0c59":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7a1c79e5eb444feb50ca9ec6716872f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"da4e98f0657746a0a4a39c8be573ab8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da9ad1ce75084ec4ab6203abe237a6fa":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e701db9ab21646ec9805e5755d02af45":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aece0cc1c56d4adc9ac1a5f3a8aeb5dc","IPY_MODEL_eb7ff95279554c669efe0971ac352de4","IPY_MODEL_75512a0ddc8a453299ca914d31017019"],"layout":"IPY_MODEL_69e2a10ca5de47fe98d9c89fada29bef","tabbable":null,"tooltip":null}},"e86e67f125934707b0314062c6a2c6ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a2f1c5ece40b48fa9243086c846d3de0","placeholder":"​","style":"IPY_MODEL_984c1bb9555a4e388a8143c58831492c","tabbable":null,"tooltip":null,"value":" 100/100 [00:02&lt;00:00, 47.88it/s]"}},"eb7ff95279554c669efe0971ac352de4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_da9ad1ce75084ec4ab6203abe237a6fa","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ccbe5b8f41aa4840b0e28cda15af0c59","tabbable":null,"tooltip":null,"value":100}},"fe1ce72b813e4ecd8de58f4f0d637d06":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>