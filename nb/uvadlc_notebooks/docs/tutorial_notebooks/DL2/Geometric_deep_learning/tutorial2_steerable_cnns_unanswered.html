<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>GDL - Steerable CNNs – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prerequisite-knowledge" id="toc-prerequisite-knowledge" class="nav-link active" data-scroll-target="#prerequisite-knowledge">Prerequisite Knowledge</a></li>
  <li><a href="#representation-theory-and-harmonic-analysis-of-compact-groups" id="toc-representation-theory-and-harmonic-analysis-of-compact-groups" class="nav-link" data-scroll-target="#representation-theory-and-harmonic-analysis-of-compact-groups">1. Representation Theory and Harmonic Analysis of Compact Groups</a>
  <ul class="collapse">
  <li><a href="#group-representation" id="toc-group-representation" class="nav-link" data-scroll-target="#group-representation">1.1 Group Representation</a></li>
  <li><a href="#fourier-transform" id="toc-fourier-transform" class="nav-link" data-scroll-target="#fourier-transform">1.2 Fourier Transform</a></li>
  </ul></li>
  <li><a href="#from-group-cnns-to-steerable-cnns" id="toc-from-group-cnns-to-steerable-cnns" class="nav-link" data-scroll-target="#from-group-cnns-to-steerable-cnns">2. From Group CNNs to Steerable CNNs</a>
  <ul class="collapse">
  <li><a href="#feature-fields" id="toc-feature-fields" class="nav-link" data-scroll-target="#feature-fields">2.1 Feature Fields</a></li>
  <li><a href="#general-steerable-cnns" id="toc-general-steerable-cnns" class="nav-link" data-scroll-target="#general-steerable-cnns">General Steerable CNNs</a></li>
  <li><a href="#defining-a-steerable-cnn" id="toc-defining-a-steerable-cnn" class="nav-link" data-scroll-target="#defining-a-steerable-cnn">2.2 Defining a Steerable CNN</a></li>
  <li><a href="#steerable-cnn-with-infinite-group-g" id="toc-steerable-cnn-with-infinite-group-g" class="nav-link" data-scroll-target="#steerable-cnn-with-infinite-group-g">2.3 Steerable CNN with infinite group <span class="math inline">\(G\)</span></a></li>
  </ul></li>
  <li><a href="#build-and-train-steerable-cnns" id="toc-build-and-train-steerable-cnns" class="nav-link" data-scroll-target="#build-and-train-steerable-cnns">3. Build and Train Steerable CNNs</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#so2-equivariant-architecture" id="toc-so2-equivariant-architecture" class="nav-link" data-scroll-target="#so2-equivariant-architecture"><span class="math inline">\(SO(2)\)</span> equivariant architecture</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">GDL - Steerable CNNs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab"></a><br>
<strong>Empty notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns_unanswered.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github Unanswered"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns_unanswered.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab Unanswered"></a><br>
<strong>Authors:</strong> Gabriele Cesa</p>
<p>During the lectures, you have learnt that the symmetries of a machine learning task can be modelled with <strong>groups</strong>. In the previous tutorial, you have also studied the framework of <em>Group-Convolutional Neural Networks</em> (<strong>GCNNs</strong>), which describes a neural architecture design equivariant to general groups.</p>
<p>The feature maps of a GCNN are functions over the elements of the group. A naive implementation of group-convolution requires computing and storing a response for each group element. For this reason, the GCNN framework is not particularly convenient to implement networks equivariant to groups with infinite elements.</p>
<p>Steerable CNNs are a more general framework which solves this issue. The key idea is that, instead of storing the value of a feature map on each group element, the model stores the <em>Fourier transform</em> of this feature map, up to a finite number of frequencies.</p>
<p>In this tutorial, we will first introduce some Representation theory and Fourier theory (<em>non-commutative harmonic analysis</em>) and, then, we will explore how this idea is used in practice to implement Steerable CNNs.</p>
<section id="prerequisite-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="prerequisite-knowledge">Prerequisite Knowledge</h2>
<p>Throughout this tutorial, we will assume you are already familiar with some concepts of <strong>group theory</strong>, such as <em>groups</em>, <em>group actions</em> (in particular <em>on functions</em>), <em>semi-direct product</em> and <em>order of a group</em>, as well as basic <strong>linear algebra</strong>.</p>
<p>We start by importing the necessary packages. You can run the following command to install all the requirements:</p>
<p><code>&gt; pip install torch torchvision numpy matplotlib git+https://github.com/AMLab-Amsterdam/lie_learn escnn scipy</code></p>
<div id="cell-5" class="cell" data-outputid="e7f72cca-bb3b-4226-b681-f30b1db7d9e9" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(precision<span class="op">=</span><span class="dv">3</span>, suppress<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">10000</span>, threshold<span class="op">=</span><span class="dv">100000</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># If the fonts in the plots are incorrectly rendered, comment out the next two lines</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.error <span class="im">import</span> HTTPError</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../../saved_models/DL2/GDL"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/n2/p4m4lkjn60j1ky5sffq81s_m0000gn/T/ipykernel_1797/1932627903.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`
  set_matplotlib_formats('svg', 'pdf') # For export</code></pre>
</div>
</div>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create checkpoint path if it doesn't exist yet</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>os.makedirs(CHECKPOINT_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Files to download</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>pretrained_files <span class="op">=</span> [</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steerable_c4-pretrained.ckpt"</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steerable_so2-pretrained.ckpt"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steerable_c4-accuracies.npy"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steerable_so2-accuracies.npy"</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Github URL where saved models are stored for this tutorial</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>base_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/phlippe/saved_models/main/DL2/GDL/"</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># For each file, check whether it already exists. If not, try downloading it.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> file_name <span class="kw">in</span> pretrained_files:</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    file_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, file_name)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(file_path):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        file_url <span class="op">=</span> base_url <span class="op">+</span> file_name</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>file_url<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            urllib.request.urlretrieve(file_url, file_path)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> HTTPError <span class="im">as</span> e:</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Something went wrong. Please contact the author with the full output including the following error:</span><span class="ch">\n</span><span class="st">"</span>, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="representation-theory-and-harmonic-analysis-of-compact-groups" class="level2">
<h2 class="anchored" data-anchor-id="representation-theory-and-harmonic-analysis-of-compact-groups">1. Representation Theory and Harmonic Analysis of Compact Groups</h2>
<p>We will make use of the <code>escnn</code> <a href="https://github.com/QUVA-Lab/escnn">library</a> throughout this tutorial. You can also find its documentation <a href="https://quva-lab.github.io/escnn/">here</a>.</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> escnn.group <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Google Colab does not have escnn installed by default. Hence, we do it here if necessary</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>AMLab<span class="op">-</span>Amsterdam<span class="op">/</span>lie_learn escnn</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> escnn.group <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First, let’s create a group. We will use the <em>Cyclic Group</em> <span class="math inline">\(G=C_8\)</span> as an example. This group contains the <span class="math inline">\(8\)</span> planar rotations by multiples of <span class="math inline">\(\frac{2\pi}{8}\)</span>. In <code>escnn</code>, a groups are instances of the abstract class <code>escnn.group.Group</code>, which provides some useful functionalities. We instantiate groups via a <em>factory method</em>. To build the cyclic group of order <span class="math inline">\(8\)</span>, we use this factory method:</p>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> cyclic_group(N<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We can verify that the order of this group is 8:</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>G.order()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A group is a collection of group elements together with a binary operation to combine them. This is implemented in the class <code>escnn.group.GroupElement</code>. We can access the <em>identity</em> element <span class="math inline">\(e \in G\)</span> as</p>
<div id="cell-13" class="cell" data-outputid="2e72acb6-75a6-4b80-c14c-d611862c2780">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>G.identity</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>or sample a random element as</p>
<div id="cell-15" class="cell" data-outputid="7015660e-6c61-4ba9-9dc7-8023424371bf">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>G.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Group elements can be combined via the binary operator <code>@</code>; we can also take the inverse of an element using <code>~</code>:</p>
<div id="cell-17" class="cell" data-outputid="7031e4cc-07ea-4cc7-e4f1-78dd3d7a274d">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> G.sample()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> G.sample()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a <span class="op">@</span> b)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="op">~</span>a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Representation theory is a fundamental element in Steerable CNNs and to construct a Fourier theory over groups. In this first section, we will introduce the essential concepts.</p>
<section id="group-representation" class="level3">
<h3 class="anchored" data-anchor-id="group-representation">1.1 Group Representation</h3>
<p>A <strong>linear group representation</strong> <span class="math inline">\(\rho\)</span> of a compact group <span class="math inline">\(G\)</span> on a vector space (called <em>representation space</em>) <span class="math inline">\(\mathbb{R}^d\)</span> is a <em>group homomorphism</em> from <span class="math inline">\(G\)</span> to the general linear group <span class="math inline">\(GL(\mathbb{R}^d)\)</span>, i.e.&nbsp;it is a map <span class="math inline">\(\rho : G \to \mathbb{R}^{d \times d}\)</span> such that: <span class="math display">\[\rho(g_1 g_2) = \rho(g1) \rho(g2) \quad \forall g_1,g_2 \in G \ .\]</span></p>
<p>In other words, <span class="math inline">\(\rho(g)\)</span> is a <span class="math inline">\(d \times d\)</span> <em>invertible</em> matrix. We refer to <span class="math inline">\(d\)</span> as the <em>size</em> of the representation.</p>
<section id="example-the-trivial-representation" class="level4">
<h4 class="anchored" data-anchor-id="example-the-trivial-representation">Example: the Trivial Representation</h4>
<p>The simplest example of <em>group representation</em> is the <strong>trivial representation</strong> which maps every element to <span class="math inline">\(1 \in \mathbb{R}\)</span>, i.e.&nbsp;<span class="math inline">\(\rho: g \mapsto 1\)</span>. One can verify that it satisfies the condition above. We can construct this representation as follows:</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.trivial_representation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>rho</code> is an instance of <code>escnn.group.Representation</code>. This class provides some functionalities to work with group representations. We can also use it as a callable function to compute the representation of a group element; this will return a squared matrix as a <code>numpy.array</code>. Let verify that the trivial representation does indeed verify the condition above:</p>
<div id="cell-23" class="cell" data-outputid="2b66b62b-ad2e-4ee2-c41e-f2b41dc1cd0f">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> G.sample()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> G.sample()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g1) <span class="op">@</span> rho(g2))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g1 <span class="op">@</span> g2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the trivial representation has size <span class="math inline">\(1\)</span>:</p>
<div id="cell-25" class="cell" data-outputid="771d8636-b40a-4cf6-ecbf-f63e1e3d99ff">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>rho.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="example-rotations" class="level4">
<h4 class="anchored" data-anchor-id="example-rotations">Example: rotations</h4>
<p>Another common example of group representations is given by 2D rotations. Let <span class="math inline">\(SO(2)\)</span> be the group of all planar rotations; note that we can identify each rotation by an angle <span class="math inline">\(\theta \in [0, 2\pi)\)</span>. Then, the standard representation of planar rotations as <span class="math inline">\(2\times 2\)</span> rotation matrices is a representation of <span class="math inline">\(SO(2)\)</span>:</p>
<p><span class="math display">\[
    \rho: r_{\theta} \mapsto \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) \end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(r_\theta \in SO(2)\)</span> is a counter-clockwise rotation by <span class="math inline">\(\theta\)</span>. Let’s try to build this group and, then, verify that this is a representation:</p>
<div id="cell-27" class="cell" data-outputid="3f46f3ab-7a96-4fb9-8056-780328015d64">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> so2_group()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.standard_representation()</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> G.sample()</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> G.sample()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'g1=</span><span class="sc">{</span>g1<span class="sc">}</span><span class="ss">, g2=</span><span class="sc">{</span>g2<span class="sc">}</span><span class="ss">, g1 * g2 = </span><span class="sc">{</span>g1 <span class="op">@</span> g2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'rho(g1) @ rho(g2)'</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g1) <span class="op">@</span> rho(g2))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'rho(g1 * g2)'</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g1 <span class="op">@</span> g2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="question-1" class="level4">
<h4 class="anchored" data-anchor-id="question-1">QUESTION 1</h4>
<p>Show that any representation <span class="math inline">\(\rho: G \to \mathbb{R}^{d \times d}\)</span> also satisfies the following two properties:</p>
<ul>
<li><p>let <span class="math inline">\(e \in G\)</span> be the identity element. Then, <span class="math inline">\(\rho(e)\)</span> is the identity matrix of size <span class="math inline">\(d\)</span>.</p></li>
<li><p>let <span class="math inline">\(g \in G\)</span> and <span class="math inline">\(g^{-1}\)</span> be its inverse (i.e.&nbsp;<span class="math inline">\(g \cdot g^{-1} = e\)</span>). Then, <span class="math inline">\(\rho(g^{-1}) = \rho(g)^{-1}\)</span>.</p></li>
</ul>
</section>
<section id="answer-1" class="level4">
<h4 class="anchored" data-anchor-id="answer-1">ANSWER 1</h4>
<hr>
</section>
<section id="direct-sum" class="level4">
<h4 class="anchored" data-anchor-id="direct-sum">Direct Sum</h4>
<p>We can combine representations to build a larger representation via the <strong>direct sum</strong>.</p>
<p>Given representations <span class="math inline">\(\rho_1 : G \to \mathbb{R}^{d_1 \times d_1}\)</span> and <span class="math inline">\(\rho_2 : G \to \mathbb{R}^{d_2 \times d_2}\)</span>, their <em>direct sum</em> <span class="math inline">\(\rho_1 \oplus \rho_2: G \to \mathbb{R}^{(d_1 + d_2) \times (d_1 + d_2)}\)</span> is defined as</p>
<p><span class="math display">\[
    (\rho_1 \oplus \rho_2)(g) = \begin{bmatrix}\rho_1(g) &amp; 0 \\ 0 &amp; \rho_2(g) \end{bmatrix}
\]</span></p>
<p>Its action is therefore given by the independent actions of <span class="math inline">\(\rho_1\)</span> and <span class="math inline">\(\rho_2\)</span> on the orthogonal subspaces <span class="math inline">\(\mathbb{R}^{d_1}\)</span> and <span class="math inline">\(\mathbb{R}^{d_2}\)</span> of <span class="math inline">\(\mathbb{R}^{d_1 + d_2}\)</span>.</p>
<p>Let’s see an example:</p>
<div id="cell-30" class="cell" data-outputid="8865e8f6-853f-477b-c066-36257934a22e">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>rho_sum <span class="op">=</span> rho <span class="op">+</span> rho</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> G.sample()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho_sum(g))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the direct sum of two representations has size equal to the sum of their sizes:</p>
<div id="cell-32" class="cell" data-outputid="5029e93a-64fd-4bb9-a90a-f66aae40d698">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>rho.size, rho_sum.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can combine arbitrary many representations in this way, e.g.&nbsp;<span class="math inline">\(\rho \oplus \rho \oplus \rho \oplus \rho\)</span>:</p>
<div id="cell-34" class="cell" data-outputid="c380a8ae-e561-4ae0-e3cc-02ac197d8d47">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>rho_sum <span class="op">=</span> rho <span class="op">+</span> rho <span class="op">+</span> rho <span class="op">+</span> rho</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or, more simply:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>rho_sum <span class="op">=</span> directsum([rho, rho, rho, rho])</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>rho_sum.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-regular-representation" class="level4">
<h4 class="anchored" data-anchor-id="the-regular-representation">The Regular Representation</h4>
<p>Another important representation is the <strong>regular representation</strong>. The regular representation describes the action of a group <span class="math inline">\(G\)</span> on the vector space of functions over the group <span class="math inline">\(G\)</span>. Assume for the moment that the group <span class="math inline">\(G\)</span> is <em>finite</em>, i.e.&nbsp;<span class="math inline">\(|G| &lt; \infty\)</span>.</p>
<p>The set of functions over <span class="math inline">\(G\)</span> is equivalent to the vector space <span class="math inline">\(\mathbb{R}^{|G|}\)</span>. We can indeed interpret a vector <span class="math inline">\(\mathbf{f} \in \mathbb{R}^{|G|}\)</span> as a function over <span class="math inline">\(G\)</span>, where the <span class="math inline">\(i\)</span>-th entry of <span class="math inline">\(\mathbf{f}\)</span> is interpreted as the value of the function on the <span class="math inline">\(i\)</span>-th element <span class="math inline">\(g_i \in G\)</span>.</p>
<p>The <strong>regular representation</strong> of <span class="math inline">\(G\)</span> is a <span class="math inline">\(|G|\)</span> dimensional representation. Recall the left action of <span class="math inline">\(G\)</span> on a function <span class="math inline">\(f: G \to \mathbb{R}\)</span>:</p>
<p><span class="math display">\[
[g.f](h) := f(g^{-1} h)
\]</span></p>
<p>The new function <span class="math inline">\(g.f\)</span> is still a function over <span class="math inline">\(G\)</span> and belongs to the same vector space. If we represent the function <span class="math inline">\(f\)</span> as a vector <span class="math inline">\(\mathbf{f}\)</span>, the vector representing the function <span class="math inline">\(g.f\)</span> will have permuted entries with respect to <span class="math inline">\(\mathbf{f}\)</span>. This permutation is the regular representation of <span class="math inline">\(g \in G\)</span>.</p>
<hr>
</section>
<section id="question-2" class="level4">
<h4 class="anchored" data-anchor-id="question-2">QUESTION 2</h4>
<p>Show that the space of functions over <span class="math inline">\(G\)</span> is a vector space. To do so, show that functions satisfy the properties of a vector space; see <a href="https://en.wikipedia.org/wiki/Vector_space#Notation_and_definition">here</a>.</p>
</section>
<section id="answer-2" class="level4">
<h4 class="anchored" data-anchor-id="answer-2">ANSWER 2</h4>
<hr>
<p>For finite groups, we can generate this representation. We assume that the <span class="math inline">\(i\)</span>-th entry is associated with the element of <span class="math inline">\(G=C_8\)</span> corresponing to a rotation by <span class="math inline">\(i \frac{2\pi}{8}\)</span>.</p>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> cyclic_group(<span class="dv">8</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.regular_representation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-39" class="cell" data-outputid="1523946e-b39b-4b76-87ad-9d4200f02172">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that the size of the representation is equal to the group's order |G|</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>rho.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>the identity element maps a function to itself, so the entries are not permuted</p>
<div id="cell-41" class="cell" data-outputid="f4ac2e16-e2fe-406e-e4b1-2d075e95820d">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>rho(G.identity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The regular representation of the rotation by <span class="math inline">\(1\frac{2\pi}{8}\)</span> just cyclically shifts each entry to the next position since <span class="math inline">\(r_{1\frac{2\pi}{8}}^{-1} r_{i\frac{2\pi}{8}} = r_{(i-1)\frac{2\pi}{8}}\)</span>:</p>
<div id="cell-43" class="cell" data-outputid="ff6bda6b-688d-4c26-f7a8-57b2873c867d">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>rho(G.element(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see an example of the action on a function. We consider a function which is zero on all group elements apart from the identity (<span class="math inline">\(i=0\)</span>).</p>
<div id="cell-45" class="cell" data-outputid="18b9c5a7-e0e4-4648-aa8c-531cc3ef6043">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.zeros(<span class="dv">8</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>f[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Observe that <span class="math inline">\(\rho(e) \mathbf{f} = \mathbf{f}\)</span>, where <span class="math inline">\(e = 0\frac{2\pi}{8}\)</span> is the identity element.</p>
<div id="cell-47" class="cell" data-outputid="82d4ac93-f061-4459-a6f1-2b14b71f82ff">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>rho(G.identity) <span class="op">@</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math inline">\(\mathbf{f}\)</span> is non-zero only on the element <span class="math inline">\(e\)</span>. If an element <span class="math inline">\(g\)</span> acts on this function, it moves the non-zero value to the entry associated with <span class="math inline">\(g\)</span>:</p>
<div id="cell-49" class="cell" data-outputid="697efea0-0164-4f99-99cf-98bf5eb914a5">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>rho(G.element(<span class="dv">1</span>)) <span class="op">@</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-50" class="cell" data-outputid="0405ee43-8115-4ca7-d762-e76b6c2ee84e">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>rho(G.element(<span class="dv">6</span>)) <span class="op">@</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="question-3" class="level4">
<h4 class="anchored" data-anchor-id="question-3">QUESTION 3</h4>
<p>Prove the result above.</p>
</section>
<section id="answer-3" class="level4">
<h4 class="anchored" data-anchor-id="answer-3">ANSWER 3</h4>
<hr>
</section>
<section id="equivalent-representations" class="level4">
<h4 class="anchored" data-anchor-id="equivalent-representations">Equivalent Representations</h4>
<p>Two representations <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\rho'\)</span> of a group <span class="math inline">\(G\)</span> on the same vector space <span class="math inline">\(\mathbb{R}^d\)</span> are called <em>equivalent</em> (or <strong>isomorphic</strong>) if and only if they are related by a change of basis <span class="math inline">\(Q \in \mathbb{R}^{d \times d}\)</span>, i.e.&nbsp; <span class="math display">\[ \forall g \in G \quad \rho(g) = Q \rho'(g) Q^{-1} \ . \]</span></p>
<p>Equivalent representations behave similarly since their composition is <em>basis-independent</em> as seen by <span class="math display">\[ \rho'(g_1) \rho'(g_2) = Q \rho(g_1)Q^{−1}Q \rho(g_2)Q^{−1} = Q \rho(g_1)\rho(g_2)Q^{−1} \ .\]</span></p>
<p><em>Direct sum</em> and <em>change of basis matrices</em> provide a way to combine representations to construct larger and more complex representations. In the next example, we concatenate two trivial representations and two regular representations and apply a random change of basis <span class="math inline">\(Q\)</span>. The final representation is formally defined as: <span class="math display">\[
\rho(g) = Q
\left(
\rho_\text{trivial} \oplus
\rho_\text{regular} \oplus
\rho_\text{regular} \oplus
\rho_\text{trivial}
\right)
Q^{-1}
\]</span></p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> G.trivial_representation.size <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> G.regular_representation.size <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.random.randn(d, d)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> directsum(</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    [G.trivial_representation, G.regular_representation, G.regular_representation, G.trivial_representation],</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    change_of_basis<span class="op">=</span>Q</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-54" class="cell" data-outputid="37863ec9-218a-4efc-b4c5-35056ca43a23">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>rho.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="irreducible-representations-or-irreps" class="level4">
<h4 class="anchored" data-anchor-id="irreducible-representations-or-irreps">Irreducible Representations (or <em>Irreps</em>)</h4>
<p>Under minor conditions, any representation can be decomposed in this way, that is, any representation <span class="math inline">\(\rho\)</span> of a compact group <span class="math inline">\(G\)</span> can be written as a <em>direct sum</em> of a number of smaller representations, up to a <em>change of basis</em>. These “smaller representations” can not be decomposed further and play a very important role in the theory of group representations and steerable CNNs and are called <strong>irreducible representations</strong>, or simply <strong>irreps</strong>.</p>
<p>The set of <em>irreducible representations</em> of a group <span class="math inline">\(G\)</span> is generally denoted as <span class="math inline">\(\hat{G}\)</span>. We will often use the notation <span class="math inline">\(\hat{G} = \{\rho_j\}_j\)</span> to index this set.</p>
<p>We can access the irreps of a group via the <code>irrep()</code> method. The <em>trivial representation</em> is <em>always</em> an irreducible representation. For <span class="math inline">\(G=C_8\)</span>, we access it with the index <span class="math inline">\(j=0\)</span>:</p>
<div id="cell-57" class="cell" data-outputid="25e5efe8-7d3f-4d7c-edeb-f370cb185813">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rho_0 <span class="op">=</span> G.irrep(<span class="dv">0</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho_0 <span class="op">==</span> G.trivial_representation)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>rho_0(G.sample())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The next irrep <span class="math inline">\(j=1\)</span> gives the representation of <span class="math inline">\(i\frac{2\pi}{8}\)</span> as the <span class="math inline">\(2 \times 2\)</span> rotation matrix by <span class="math inline">\(\theta = i\frac{2\pi}{8}\)</span>:</p>
<div id="cell-59" class="cell" data-outputid="7182b6b5-5bb6-44fc-90b6-28b3ed16e8ec">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.irrep(<span class="dv">1</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> G.sample()</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(g)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Irreducible representations provide the building blocks to construct any representation <span class="math inline">\(\rho\)</span> via direct sums and change of basis, i.e: <span class="math display">\[ \rho = Q \left( \bigoplus_{j \in \mathcal{I}} \rho_j \right) Q^{-1} \]</span></p>
<p>where <span class="math inline">\(\mathcal{I}\)</span> is an index set (possibly with repetitions) over <span class="math inline">\(\hat{G}\)</span>.</p>
<p>Internally, any <code>escnn.group.Representation</code> is indeed implemented as a list of irreps (representing the index set <span class="math inline">\(\mathcal{I}\)</span>) and a change of basis <span class="math inline">\(Q\)</span>. An irrep is identified by a <em>tuple</em> <code>id</code>.</p>
<p>Let’s see an example. Let’s take the regular representaiton of <span class="math inline">\(C_8\)</span> and check its decomposition into irreps:</p>
<div id="cell-62" class="cell" data-outputid="2bc03f54-a9df-4991-e38d-c658208f44d8">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.regular_representation</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>rho.irreps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell" data-outputid="a00ad28b-9508-4d86-85cd-a0151f998abe">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>rho.change_of_basis</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-outputid="71731f2a-c4a0-4280-a7ee-f9fc5bc6cf41">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's access second irrep</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>rho_id <span class="op">=</span> rho.irreps[<span class="dv">1</span>]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>rho_1 <span class="op">=</span> G.irrep(<span class="op">*</span>rho_id)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we verify it is the irrep j=1 we described before</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>rho_1(g)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, let’s verify that this direct sum and this change of basis indeed yield the regular representation</p>
<div id="cell-66" class="cell" data-outputid="0fd4dc9f-c5bc-41dc-a4c2-57d1cf4bd5f7">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate all the irreps in rho.irreps:</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>irreps <span class="op">=</span> [</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>          G.irrep(<span class="op">*</span>irrep)(g) <span class="cf">for</span> irrep <span class="kw">in</span> rho.irreps</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># build the direct sum</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>direct_sum <span class="op">=</span> np.asarray(scipy.sparse.block_diag(irreps, <span class="bu">format</span><span class="op">=</span><span class="st">'csc'</span>).todense())</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Regular representation of'</span>, g)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho(g))</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Direct sum of the irreps:'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(direct_sum)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Apply the change of basis on the direct sum of the irreps:'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho.change_of_basis <span class="op">@</span> direct_sum <span class="op">@</span> rho.change_of_basis_inv)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Are the two representations equal?'</span>, np.allclose(rho(g), rho.change_of_basis <span class="op">@</span> direct_sum <span class="op">@</span> rho.change_of_basis_inv))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="fourier-transform" class="level3">
<h3 class="anchored" data-anchor-id="fourier-transform">1.2 Fourier Transform</h3>
<p>We can finally approach the harmonic analysis of functions over a group <span class="math inline">\(G\)</span>.</p>
<p>Note that a representation <span class="math inline">\(\rho: G \to \mathbb{R}^{d \times d}\)</span> can be interpreted as a collection of <span class="math inline">\(d^2\)</span> functions over <span class="math inline">\(G\)</span>, one for each matrix entry of <span class="math inline">\(\rho\)</span>. The <strong>Peter-Weyl theorem</strong> states that the collection of functions in the matrix entries of all irreps <span class="math inline">\(\hat{G}\)</span> of a group <span class="math inline">\(G\)</span> spans the space of all (square-integrable) functions over <span class="math inline">\(G\)</span>.</p>
<p>This result gives us a way to parameterize functions over the group. This is the focus of this section. In particular, this is useful to parameterize functions over groups with infinite elements.</p>
<p>In this section, we will first consider the <em>dihedral group</em> <span class="math inline">\(D_8\)</span> as example. This is the group containing the <span class="math inline">\(8\)</span> planar rotations by angles multiple of <span class="math inline">\(\frac{2\pi}{8}\)</span> and <em>reflection</em> along the <span class="math inline">\(X\)</span> axis. The group contains in total <span class="math inline">\(16\)</span> elements (<span class="math inline">\(8\)</span> normal rotations and <span class="math inline">\(8\)</span> rotations preceeded by the reflection).</p>
<div id="cell-69" class="cell" data-outputid="6e0f22e0-4f75-4853-93a1-b331db57621a">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> dihedral_group(<span class="dv">8</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>G.order()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-70" class="cell" data-outputid="7cfd707d-b4cf-4e60-a599-107f32cb9303">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># element representing the reflection (-) and no rotations</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>G.reflection</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-71" class="cell" data-outputid="bc5425a9-e75c-45be-8916-76826fa9412d">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># element representing a rotation by pi/2 (i.e. 2 * 2pi/8) and no reflections (+)</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>G.element((<span class="dv">0</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-72" class="cell" data-outputid="b6e709d1-d63a-44c2-afae-2634a0e8f314">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reflection followed by a rotation by pi/2</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(G.element((<span class="dv">0</span>, <span class="dv">2</span>)) <span class="op">@</span> G.reflection)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we can also directly generate this element as</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(G.element((<span class="dv">1</span>, <span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-73" class="cell" data-outputid="06b0b4bb-2228-4042-e5f2-f049a46c7ac9">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a rotation by pi/2 followed by a reflection is equivalent to a reclection followed by a rotation by 6*2pi/8</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>G.reflection <span class="op">@</span> G.element((<span class="dv">0</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The list of all elements in the group is obtaied as:</p>
<div id="cell-75" class="cell" data-outputid="3fff3785-148c-4a68-958e-dcf18c9a4d02">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>G.elements</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="fourier-and-inverse-fourier-transform" class="level4">
<h4 class="anchored" data-anchor-id="fourier-and-inverse-fourier-transform">Fourier and Inverse Fourier Transform</h4>
<p>For most groups, the entries of the irreps don’t only span the space of functions but form also a basis (i.e.&nbsp;these functions are mutually orthogonal to each other). Therefore, we can write a function <span class="math inline">\(f: G \to \mathbb{R}\)</span> as <span class="math display">\[ f(g) = \sum_{\rho_j \in \hat{G}} \sum_{m,n &lt; d_j} w_{j,m,n} \cdot \sqrt{d_j} [\rho_j(g)]_{mn}\]</span></p>
<p>where <span class="math inline">\(d_j\)</span> is the dimension of the irrep <span class="math inline">\(\rho_j\)</span>, while <span class="math inline">\(m, n\)</span> index the <span class="math inline">\(d_j^2\)</span> entries of <span class="math inline">\(\rho_j\)</span>. The coefficients <span class="math inline">\(\{ w_{j, m, n} \in \mathbb{R} \}_{j, m, n}\)</span> parameterize the function <span class="math inline">\(f\)</span> on this basis. The <span class="math inline">\(\sqrt{d_j}\)</span> is a scalar factor to ensure the basis is normalized.</p>
<p>We rewrite this expression in a cleaner form by using the following fact. If <span class="math inline">\(A, B \in \mathbb{R}^{d \times d}\)</span>, then <span class="math display">\[\text{Tr}(A^T B) = \sum_{m, n &lt; d} A_{mn} B_{mn} \in \mathbb{R} \ .\]</span></p>
<p>By definining <span class="math inline">\(\hat{f}(\rho_j) \in \mathbb{R}^{d_j \times d_j}\)</span> as the matrix containing the <span class="math inline">\(d_j^2\)</span> coefficients <span class="math inline">\(\{ w_{j, m, n} \in \mathbb{R} \}_{m, n &lt; d_j}\)</span>, we can express the <strong>Inverse Fourier Transform</strong> as: <span class="math display">\[ f(g) = \sum_{\rho_j \in \hat{G}} \sqrt{d_j} \text{Tr}\left(\rho_j(g)^T \hat{f}(\rho_j)\right) \]</span></p>
<p>Similarly, we can project a general function <span class="math inline">\(f: G \to \mathbb{R}\)</span> on an element <span class="math inline">\(\rho_{j,m,n}: G \to \mathbb{R}\)</span> of the basis via: <span class="math display">\[ w_{j,m,n} = \frac{1}{|G|} \sum_{g \in G} f(g) \sqrt{d_j} [\rho_j(g)]_{m, n} \ . \]</span></p>
<p>The projection over all entries of <span class="math inline">\(\rho_j\)</span> can be more cleanly written as follows: <span class="math display">\[ \hat{f}(\rho_j) = \frac{1}{|G|} \sum_{g \in G} f(g) \sqrt{d_j} \rho_j(g) \ . \]</span></p>
<p>which we refer to as <strong>Fourier Transform</strong>.</p>
<p>If the group <span class="math inline">\(G\)</span> is <em>infinite</em>, we replace the average over the group elements with an <em>integral</em> over them: <span class="math display">\[ \hat{f}(\rho_j) = \int_G f(g) \sqrt{d_j} \rho_j(g) dg \ , \]</span></p>
<p>For a finite group <span class="math inline">\(G\)</span>, we can access all its irreps by using the <code>Group.irreps()</code> method. Let’s see an example:</p>
<div id="cell-78" class="cell" data-outputid="47077591-ba51-4af3-f9a7-22537f13c730">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>irreps <span class="op">=</span> G.irreps()</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The dihedral group D8 has </span><span class="sc">{</span><span class="bu">len</span>(irreps)<span class="sc">}</span><span class="ss"> irreps'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-79" class="cell" data-outputid="c2fe9573-2a4b-47c2-8f69-627d24f163f1">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the first one, is the 1-dimensional trivial representation</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(irreps[<span class="dv">0</span>] <span class="op">==</span> G.trivial_representation <span class="op">==</span> G.irrep(<span class="dv">0</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="question-4" class="level4">
<h4 class="anchored" data-anchor-id="question-4">QUESTION 4</h4>
<p>We can now implement the Fourier Transform and the Inverse Fourier Transform for the Dihedral Group <span class="math inline">\(D_8\)</span>. Using the equations above, implement the following methods:</p>
<hr>
<div id="cell-81" class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fourier_transform_D8(f: np.array):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the method gets in input a function on the elements of D_8</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and should return a dictionary mapping each irrep's `id` to the corresponding Fourier Transform</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The i-th element of `f` stores the value of the function on the group element `G.elements[i]`</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> dihedral_group(<span class="dv">8</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> f.shape <span class="op">==</span> (<span class="dv">16</span>,), f.shape</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    ft <span class="op">=</span> {}</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># INSERT YOUR CODE HERE:</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ft</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-82" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inverse_fourier_transform_D8(ft: <span class="bu">dict</span>):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the method gets in input a dictionary mapping each irrep's `id` to the corresponding Fourier Transform</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and should return the function `f` on the elements of D_8</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The i-th element of `f` stores the value of the function on the group element `G.elements[i]`</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> dihedral_group(<span class="dv">8</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.zeros(<span class="dv">16</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># INSERT YOUR CODE HERE:</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now want to verify that the <strong>Fourier Transform</strong> and the <strong>Inverse Fourier Transform</strong> are inverse of each other:</p>
<div id="cell-84" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.random.randn(<span class="dv">16</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>ft <span class="op">=</span> fourier_transform_D8(f)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>new_f <span class="op">=</span> inverse_fourier_transform_D8(ft)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(f, new_f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="parameterizing-functions-over-infinite-groups" class="level4">
<h4 class="anchored" data-anchor-id="parameterizing-functions-over-infinite-groups">Parameterizing functions over infinite groups</h4>
<p>This allows us to also parameterize functions over infinite groups, such as <span class="math inline">\(O(2)\)</span>, i.e.&nbsp;the group of all planar rotations and reflections.</p>
<div id="cell-86" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> o2_group()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-87" class="cell" data-outputid="bcb72d1e-b721-4b3a-c840-2f208ed50a54">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the group has infinite many elements, so the `order` method just returns -1</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>G.order()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The equations remain the same, but this group has an <em>infinite</em> number of <em>irreps</em>. We can, however, parameterize a function over the group by only considering a finite number of irreps in the sum inside the definition of <em>Inverse Fourier Transform</em>. Let <span class="math inline">\(\tilde{G} \subset \hat{G}\)</span> be a finite subset of the irreps of <span class="math inline">\(G\)</span>. We can then write the following transforms within the subspace of functions spanned only by the entries of the irreps in <span class="math inline">\(\tilde{G}\)</span>.</p>
<p><strong>Inverse Fourier Transform</strong>: <span class="math display">\[ f(g) = \sum_{\rho_j \in \tilde{G}} \sqrt{d_j} \text{Tr}\left(\rho_j(g)^T \hat{f}(\rho_j)\right) \]</span></p>
<p>and <strong>Fourier Transform</strong>: <span class="math display">\[ \hat{f}(\rho_j) = \int_G f(g) \sqrt{d_j} \rho_j(g) dg \ , \]</span></p>
<hr>
</section>
<section id="question-5" class="level4">
<h4 class="anchored" data-anchor-id="question-5">QUESTION 5</h4>
<p>We can now implement the Inverse Fourier Transform for the Orthogonal Group <span class="math inline">\(O(2)\)</span>. Since the group has infinite many elements, we can not store the values the function take on each element. Instead, we just sample the function on a particular element of the group:</p>
<hr>
<div id="cell-90" class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inverse_fourier_transform_O2(g: GroupElement, ft: <span class="bu">dict</span>):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the method gets in input a dictionary mapping each irrep's `id` to the corresponding Fourier Transform</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and a group element `g` </span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The method should return the value of the function evaluated on `g`.</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> o2_group()</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># INSERT YOUR CODE HERE:</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s plot a function. First we generate a random function by using a few irreps.</p>
<div id="cell-92" class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>irreps <span class="op">=</span> [G.irrep(<span class="dv">0</span>, <span class="dv">0</span>)] <span class="op">+</span> [G.irrep(<span class="dv">1</span>, j) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)]</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>ft <span class="op">=</span> {</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    rho.<span class="bu">id</span>: np.random.randn(rho.size, rho.size)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rho <span class="kw">in</span> irreps</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, we generate a grid on the group where to evaluate the function, i.e.&nbsp;we choose a finite set of element of <span class="math inline">\(G\)</span>. Like the Dihedral group, <span class="math inline">\(O(2)\)</span> contains rotations (parameterized by an angle <span class="math inline">\(\theta \in [0, 2\pi)\)</span>) and a reflection followed by any rotation. For example:</p>
<div id="cell-94" class="cell" data-outputid="8e23da8f-5f22-479c-d213-28dc6729db9e">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>G.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To build our grid, we sample <span class="math inline">\(100\)</span> rotations and <span class="math inline">\(100\)</span> rotations preceeded by a reflection:</p>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> [i<span class="op">*</span><span class="dv">2</span><span class="op">*</span>np.pi<span class="op">/</span>N <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>grid_rot <span class="op">=</span> [G.element((<span class="dv">0</span>, theta)) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>grid_refl <span class="op">=</span> [G.element((<span class="dv">1</span>, theta)) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now evaluate the function over all these elements and, finally, plot it:</p>
<div id="cell-98" class="cell" data-outputid="df6451eb-5bf3-4cd7-821c-5870da350b96">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>f_rot <span class="op">=</span> [</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_rot</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>f_refl <span class="op">=</span> [</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_refl</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_rot, label<span class="op">=</span><span class="st">'rotations'</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_refl, label<span class="op">=</span><span class="st">'reflection + rotations'</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'theta [0, 2pi)'</span>)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(g)'</span>)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Observe that using more irreps allows one to parameterize more flexible functions. Let’s try to add some more:</p>
<div id="cell-100" class="cell" data-outputid="bfb65984-5582-41d1-a392-0283fb620aab">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>irreps <span class="op">=</span> [G.irrep(<span class="dv">0</span>, <span class="dv">0</span>)] <span class="op">+</span> [G.irrep(<span class="dv">1</span>, j) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>)]</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>ft <span class="op">=</span> {</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    rho.<span class="bu">id</span>: np.random.randn(rho.size, rho.size)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rho <span class="kw">in</span> irreps</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>f_rot <span class="op">=</span> [</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_rot</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>f_refl <span class="op">=</span> [</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_refl</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_rot, label<span class="op">=</span><span class="st">'rotations'</span>)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_refl, label<span class="op">=</span><span class="st">'reflection + rotations'</span>)</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'theta [0, 2pi)'</span>)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(g)'</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="fourier-transform-of-shifted-functions" class="level4">
<h4 class="anchored" data-anchor-id="fourier-transform-of-shifted-functions">Fourier Transform of shifted functions</h4>
<p>Recall that a group element <span class="math inline">\(g \in G\)</span> can act on a function <span class="math inline">\(f: G \to \mathbb{R}\)</span> as: <span class="math display">\[ [g.f](h) = f(g^{-1}h) \ .\]</span></p>
<p>The Fourier transform defined before has the convenient property that the Fourier transform of <span class="math inline">\(f\)</span> and of <span class="math inline">\([g.f]\)</span> are related as follows: <span class="math display">\[\widehat{g.f}(\rho_j) = \rho_j(g) \widehat{f} \]</span></p>
<p>for any irrep <span class="math inline">\(\rho_j\)</span>.</p>
<hr>
</section>
<section id="question-6" class="level4">
<h4 class="anchored" data-anchor-id="question-6">QUESTION 6</h4>
<p>Prove the property above.</p>
</section>
<section id="answer-6" class="level4">
<h4 class="anchored" data-anchor-id="answer-6">ANSWER 6</h4>
<hr>
<p>We can verify this property visually:</p>
<div id="cell-104" class="cell" data-outputid="f3fb9db8-c3ae-4c1d-c500-91e6c1c2608d">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>irreps <span class="op">=</span> [G.irrep(<span class="dv">0</span>, <span class="dv">0</span>)] <span class="op">+</span> [G.irrep(<span class="dv">1</span>, j) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>)]</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># first, we generate a random function, as earlier</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>ft <span class="op">=</span> {</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    rho.<span class="bu">id</span>: np.random.randn(rho.size, rho.size)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rho <span class="kw">in</span> irreps</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="co"># second, we sample a random group element `g`</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> G.sample()</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Transforming the function with g=</span><span class="sc">{</span>g<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co"># finally, we transform the Fourier coefficients as in the equations above:</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>gft <span class="op">=</span> {</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    rho.<span class="bu">id</span>: rho(g) <span class="op">@</span> ft[rho.<span class="bu">id</span>]</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rho <span class="kw">in</span> irreps</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's now visualize the two functions:</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>f_rot <span class="op">=</span> [</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_rot</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>f_refl <span class="op">=</span> [</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, ft) <span class="cf">for</span> g <span class="kw">in</span> grid_refl</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>gf_rot <span class="op">=</span> [</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, gft) <span class="cf">for</span> g <span class="kw">in</span> grid_rot</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>gf_refl <span class="op">=</span> [</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    inverse_fourier_transform_O2(g, gft) <span class="cf">for</span> g <span class="kw">in</span> grid_refl</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_rot, label<span class="op">=</span><span class="st">'rotations'</span>)</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, f_refl, label<span class="op">=</span><span class="st">'reflection + rotations'</span>)</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'theta [0, 2pi)'</span>)</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(g)'</span>)</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'f'</span>)</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, gf_rot, label<span class="op">=</span><span class="st">'rotations'</span>)</span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, gf_refl, label<span class="op">=</span><span class="st">'reflection + rotations'</span>)</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'theta [0, 2pi)'</span>)</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(g)'</span>)</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'g.f'</span>)</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="from-the-fourier-transform-to-the-regular-representation" class="level4">
<h4 class="anchored" data-anchor-id="from-the-fourier-transform-to-the-regular-representation">From the Fourier Transform to the Regular Representation</h4>
<p>For simplicity, we can stack all the Fourier coefficients (the output of the Fourier transform, that is, the input of the inverse Fourier transform) into a unique vector. We define the vector <span class="math inline">\(\mathbf{f}\)</span> as the stack of the columns of each Fourier coefficients matrix <span class="math inline">\(f(\rho_j)\)</span>.</p>
<p>Let’s first introduce some notation. We denote the stack of two vectors <span class="math inline">\(\mathbf{v_1}, \mathbf{v_2}\)</span> as <span class="math inline">\(\mathbf{v_1} \oplus \mathbf{v_2}\)</span>. The use of <span class="math inline">\(\oplus\)</span> is not random: if <span class="math inline">\(\rho_1\)</span> is a representation acting on <span class="math inline">\(\mathbf{v_1}\)</span> and <span class="math inline">\(\rho_2\)</span> is a representation acting on <span class="math inline">\(\mathbf{v_2}\)</span>, then the <em>direct sum</em> <span class="math inline">\(\rho_1 \oplus \rho_2\)</span> acts on the concatenated vector <span class="math inline">\(\mathbf{v_1} \oplus \mathbf{v_2}\)</span>.</p>
<p>Second, we denote by <span class="math inline">\(\text{vec}(A)\)</span> the vector which is the stack of the columns of a matrix <span class="math inline">\(A\)</span>. In <code>numpy</code>, this is written as <code>A.T.reshape(-1)</code>, where the transpose is necessary since <code>numpy</code> stacks rows by default.</p>
<p>Then, we write: <span class="math display">\[ \mathbf{f} = \bigoplus_{\rho_j} \text{vec}(\hat{f}(\rho_j)) \ .\]</span></p>
<p>Moreover, by using <span class="math inline">\(\widehat{g.f}(\rho_j) = \rho_j(g) \hat{f}(\rho_j)\)</span>, we see that the vector containing the coefficients of the function <span class="math inline">\([g.f]\)</span> will be: <span class="math display">\[
\bigoplus_{\rho_j} \text{vec}(\rho_j(g) \hat{f}(\rho_j)) =
\bigoplus_{\rho_j} \left(\bigoplus^{d_j} \rho_j(g)\right) \text{vec}(\hat{f}(\rho_j))
\]</span></p>
<p>In other words, the group <span class="math inline">\(G\)</span> is acting on the vector <span class="math inline">\(\mathbf{f}\)</span> with the following representation: <span class="math display">\[
\rho(g) = \bigoplus_{\rho_j} \bigoplus^{d_j} \rho_j(g)
\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(\rho(g) \mathbf{f}\)</span> is the vector containing the Fourier coefficients of the function <span class="math inline">\([g.f]\)</span>.</p>
<p>Note that, essentially, the representation <span class="math inline">\(\rho\)</span> acts on a vector space containing functions over <span class="math inline">\(G\)</span>. This should remind you of the <strong>regular representation</strong> we defined for <em>finite groups</em> earlier. Indeed, it turns out that, if <span class="math inline">\(G\)</span> is finite, the representation <span class="math inline">\(\rho\)</span> we have just constructed is <strong>isomorphic</strong> (<em>equivalent</em>) to the <em>regular representation</em> defined earlier. The change of basis <span class="math inline">\(Q\)</span> is a matrix which performs the Fourier transform, while <span class="math inline">\(Q^{-1}\)</span> performs the inverse Fourier transform. More formally: <span class="math display">\[ \rho_\text{reg}(g) = Q^{-1} \left(\bigoplus_{\rho_j} \bigoplus^{d_j} \rho_j(g) \right) Q \]</span></p>
<p>where each irrep <span class="math inline">\(\rho_j\)</span> is repeated <span class="math inline">\(d_j\)</span> times, i.e.&nbsp;a number of times equal to its size.</p>
<p><strong>Intuition</strong>: recall that a function <span class="math inline">\(f : G \to \mathbb{R}\)</span> is just a vector living in a vector space. Such vector can be expressed with respect to any basis for this vector space. The first time we introduced the <em>regular representation</em> for finite groups, we chose a basis where each axis is associated with a group element; the action of <span class="math inline">\(G\)</span> is realized in this basis by a permutation of all the axes. Here, instead, we defined a basis for the same vector space where <span class="math inline">\(G\)</span> acts indipendently on different subsets of the axes, i.e.&nbsp;the action of <span class="math inline">\(G\)</span> is a block-diagonal matrix (the direct sum of irreps). This is often a more convenient choice of basis as we will see later.</p>
<p>Let verify this equivalence for the Dihdral group <span class="math inline">\(D_8\)</span>:</p>
<div id="cell-109" class="cell" data-outputid="6f4e2ccf-5b2e-4bbc-ef69-d315c516e45e">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> dihedral_group(<span class="dv">8</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>rho_irreps <span class="op">=</span> []</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rho_j <span class="kw">in</span> G.irreps():</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    d_j <span class="op">=</span> rho_j.size</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># repeat each irrep a number of times equal to its size</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    rho_irreps <span class="op">+=</span> [rho_j]<span class="op">*</span>d_j</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> directsum(rho_irreps)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The representations have the same size:'</span>)</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho.size, G.regular_representation.size)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'And contain the same irreps:'</span>)</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rho.irreps)</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(G.regular_representation.irreps)</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Fourier transform matrix:</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> G.regular_representation.change_of_basis</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a><span class="co"># inverse Fourier transform matrix:</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>Qinv <span class="op">=</span> G.regular_representation.change_of_basis_inv</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a><span class="co"># let's check that the two representations are indeed equivalent</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> G.sample()</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>rho_g <span class="op">=</span> rho(g)</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>reg_g <span class="op">=</span> G.regular_representation(g)</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Are the two representations equivalent?'</span>, np.allclose(Q <span class="op">@</span> rho_g <span class="op">@</span> Qinv, reg_g))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When <span class="math inline">\(G\)</span> is not finite, we can not explicitly store the regular representation <span class="math inline">\(\rho_\text{reg}\)</span> or the Fourier transform matrix <span class="math inline">\(Q\)</span>, since they are infinite dimensional. Nevertheless, as we have done earlier, we can just consider a subset of all functions, spanned only by a finite number of irreps. We can sample the function on any group element via the Inverse Fourier Transform when needed, without the need to compute the full Inverse Fourier Transform <span class="math inline">\(Q^{-1}\)</span> to store all values.</p>
<p>This is the underlying idea we will exploit later to build GCNNs equivariant to infinite groups.</p>
<p>We can easily generate this representation as (<code>bl_regular_representation</code> stands for “band-limited”, since only a limited subset of irreps, i.e.&nbsp;frequencies, is used):</p>
<div id="cell-112" class="cell" data-outputid="097d2d0d-09aa-431d-c292-e42a51837d10">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> o2_group()</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.bl_regular_representation(<span class="dv">7</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>rho.irreps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="irreps-with-redundant-entries-the-case-of-so2" class="level4">
<h4 class="anchored" data-anchor-id="irreps-with-redundant-entries-the-case-of-so2">Irreps with redundant entries: the case of <span class="math inline">\(SO(2)\)</span></h4>
<p>We need to conclude with a final note about the Fourier transform. When we introduced it earlier, we said that the entries of the irreps form a <strong>basis</strong> for the functions over <em>most</em> groups. Indeed, there exists some groups where the entries of the irreps are partially redundant and, therefore, form an <em>overcomplete</em> basis. This is the case, for example, of the group of planar rotations <span class="math inline">\(SO(2)\)</span> (or the group of <span class="math inline">\(N\)</span> discrete rotations <span class="math inline">\(C_N\)</span>). Indeed, an irrep of <span class="math inline">\(SO(2)\)</span> has form:</p>
<p><span class="math display">\[
\rho_j(r_\theta) = \begin{bmatrix}
    \cos(j \cdot \theta) &amp; -\sin(j \cdot \theta) \\
    \sin(j \cdot \theta) &amp; \cos(j \cdot \theta) \\
\end{bmatrix}
\]</span></p>
<p>for <span class="math inline">\(\theta \in [0, 2\pi)\)</span>, where the integer <span class="math inline">\(j \in \mathbb{N}\)</span> is interpreted as the rotational <em>frequency</em>.</p>
<p>You can observe that the two columns of <span class="math inline">\(\rho_j(r_\theta)\)</span> contain redundant elements and span the same <span class="math inline">\(2\)</span> dimensional space of functions. It is indeed sufficient to consider only one of the two columns to parameterize functions over <span class="math inline">\(SO(2)\)</span>. This also means that the irrep <span class="math inline">\(\rho_j\)</span> appears only once (instead of <span class="math inline">\(d_j=2\)</span> times) in the regular representation.</p>
<p>We don’t generally need to worry much about this, since we can generate the representation as earlier:</p>
<div id="cell-114" class="cell" data-outputid="3b491658-28f6-498c-8f03-b98cd8d48630">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> so2_group()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> G.bl_regular_representation(<span class="dv">7</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># observe that each irrep is now repeated only once, even if some are 2-dimensional</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>rho.irreps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="from-group-cnns-to-steerable-cnns" class="level2">
<h2 class="anchored" data-anchor-id="from-group-cnns-to-steerable-cnns">2. From Group CNNs to Steerable CNNs</h2>
<p>We consider a GCNN equivariant to a <em>semi-direct</em> product group <span class="math inline">\(\mathbb{R}^n \rtimes G\)</span>, with compact group <span class="math inline">\(G \leq O(n)\)</span>. This setting covers equivariance to <strong>isometries</strong> (distance preserving transformations) of the Euclidean space <span class="math inline">\(\mathbb{R}^n\)</span>; in particular, it includes equivariance to <em>translations</em> in <span class="math inline">\(\mathbb{R}^n\)</span> and to a origin-preserving symmetry <span class="math inline">\(G\)</span> (e.g.&nbsp;rotations or reflections in <span class="math inline">\(n\)</span>-dimensions). We call <span class="math inline">\(G\)</span> a <strong>point group</strong>.</p>
<p>If <span class="math inline">\(G=O(n)\)</span>, the group of all rotations and reflections in <span class="math inline">\(\mathbb{R}^n\)</span>, then <span class="math inline">\(E(n) = \mathbb{R}^n \rtimes O(n)\)</span> is called the <strong>Euclidean group</strong>, and includes all isometries of <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<section id="feature-fields" class="level3">
<h3 class="anchored" data-anchor-id="feature-fields">2.1 Feature Fields</h3>
<p>In a GCNN, a feature map is a signal <span class="math inline">\(f: \mathbb{R}^n \times G \to \mathbb{R}\)</span>. The action of an element <span class="math inline">\((x, g) \in \mathbb{R}^n \rtimes G\)</span> is:</p>
<p><span class="math display">\[ [(x, g).f](y,h):= f(g^{-1}(y-x), g^{-1}h) \]</span></p>
<p>where <span class="math inline">\(x, y \in \mathbb{R}^n\)</span> and <span class="math inline">\(g, h \in G\)</span>.</p>
<hr>
<section id="question-7" class="level4">
<h4 class="anchored" data-anchor-id="question-7">QUESTION 7</h4>
<p>Prove the action has indeed this form.</p>
</section>
<section id="answer-7" class="level4">
<h4 class="anchored" data-anchor-id="answer-7">ANSWER 7</h4>
<hr>
<p>In a GCNN, a feature map <span class="math inline">\(f\)</span> is stored as a multi-dimensional array with an axis for each of the <span class="math inline">\(n\)</span> spatial dimensions and one for the group <span class="math inline">\(G\)</span>.</p>
<p>In a steerable CNN, we replace the <span class="math inline">\(G\)</span> axis with a “Fourier” axis, which contains <span class="math inline">\(c\)</span> Fourier coefficients used to parameterize a function over <span class="math inline">\(G\)</span>, as described in the previous section. Again, let’s call <span class="math inline">\(\rho: G \to \mathbb{R}^{c \times c}\)</span> the representation of <span class="math inline">\(G\)</span> acting on these <span class="math inline">\(c\)</span> coefficients. The result is equivalent to a standard GCNN if <span class="math inline">\(G\)</span> is finite (and we have <span class="math inline">\(c = |G|\)</span>), but we can now also use infinite <span class="math inline">\(G\)</span>, such as <span class="math inline">\(SO(2)\)</span>.</p>
<p>A feature map <span class="math inline">\(f\)</span> can now be interpreted as a vector field on the space <span class="math inline">\(\mathbb{R}^n\)</span>, i.e.: <span class="math display">\[ f: \mathbb{R}^n \to \mathbb{R}^c \]</span></p>
<p>which assigns a <span class="math inline">\(c\)</span>-dimensional feature vector <span class="math inline">\(f(x)\in\mathbb{R}^c\)</span> to each spatial position <span class="math inline">\(x\in\mathbb{R}^n\)</span>. We call such vector field a <strong>feature vector field</strong>.</p>
<p>The action of <span class="math inline">\(\mathbb{R}^n \rtimes G\)</span> on one such feature vector field is defined as:</p>
<p><span class="math display">\[ [(x, g).f](y):= \rho(g) f(g^{-1}(y-x)) \]</span></p>
<p>where <span class="math inline">\(x, y \in \mathbb{R}^n\)</span> and <span class="math inline">\(g \in G\)</span>.</p>
<hr>
</section>
<section id="question-8" class="level4">
<h4 class="anchored" data-anchor-id="question-8">QUESTION 8</h4>
<p>Prove that this is indeed the right action of <span class="math inline">\(\mathbb{R}^n \rtimes G\)</span> on the feature vector field <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^c\)</span>. Recall the action of this group over the functions of the form <span class="math inline">\(\underline{f}: \mathbb{R}^n \rtimes G \to \mathbb{R}\)</span> that we described earlier. Moreover, note that the vector <span class="math inline">\(f(x) \in \mathbb{R}^c\)</span> contains the <span class="math inline">\(c\)</span> Fourier coefficients of the function <span class="math inline">\(\underline{f}(x, \cdot) : G \to \mathbb{R}\)</span> along its <span class="math inline">\(G\)</span> axis, i.e.: <span class="math display">\[
    f(x) = \bigoplus_{\rho_j} \text{vec}\left(\widehat{\underline{f}(x, \cdot)}(\rho_j)\right)
\]</span></p>
</section>
<section id="answer-8" class="level4">
<h4 class="anchored" data-anchor-id="answer-8">ANSWER 8:</h4>
<hr>
</section>
</section>
<section id="general-steerable-cnns" class="level3">
<h3 class="anchored" data-anchor-id="general-steerable-cnns">General Steerable CNNs</h3>
<p>The framework of Steerable CNNs is actually more general and allows for any representation <span class="math inline">\(\rho\)</span> of <span class="math inline">\(G\)</span>. A different choice of <span class="math inline">\(\rho\)</span> generally require some structural change in the architecture, e.g.&nbsp;by adapting the non-linearity used to ensure equivariance. Anyways, for simplicity, we will stick with the Fourier example in this tutorial.</p>
<p>Throughout the rest of this tutorial, we will assume <span class="math inline">\(n=2\)</span> for simplicity. That means we will be working for example with planar images and with the isometries of the plane (2D rotations or mirroring). The actions of <span class="math inline">\(g \in G=SO(2)\)</span> on two examples of feature vector fields over <span class="math inline">\(\mathbb{R}^2\)</span> are shown next. On the left, <span class="math inline">\(\rho\)</span> is the trivial representation of <span class="math inline">\(SO(2)\)</span> while, on the right, <span class="math inline">\(\rho\)</span> is the representation of <span class="math inline">\(SO(2)\)</span> as <span class="math inline">\(2\times 2\)</span> rotation matrices.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/QUVA-Lab/e2cnn/raw/master/visualizations/feature_fields.png" class="img-fluid figure-img"></p>
<figcaption>feature field examples</figcaption>
</figure>
</div>
</section>
<section id="defining-a-steerable-cnn" class="level3">
<h3 class="anchored" data-anchor-id="defining-a-steerable-cnn">2.2 Defining a Steerable CNN</h3>
<p>We can now proceed with building a Steerable CNN. First we import some other useful packages.</p>
<div id="cell-126" class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> escnn <span class="im">import</span> group</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> escnn <span class="im">import</span> gspaces</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> escnn <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First, we need to choose the group <span class="math inline">\(G\)</span> of point symmetries (reflections and rotations) which are being considered. All of these choices are subgroups <span class="math inline">\(G\leq O(2)\)</span> of the orthogonal group.</p>
<p>For simplicity, we first consider the <em>finite</em> group <span class="math inline">\(G=C_4\)</span>, which models the <span class="math inline">\(4\)</span> <em>rotations</em> by angle <span class="math inline">\(\theta \in \big\{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\big\}\)</span>. Because these are perfect symmetries of the grid, transforming an image with this group does not require any interpolation. We will later extend our examples to an infinite group such as <span class="math inline">\(SO(2)\)</span> or <span class="math inline">\(O(2)\)</span>.</p>
<p>Recall that a semi-direct product <span class="math inline">\(\mathbb{R}^2 \rtimes G\)</span> is defined by <span class="math inline">\(G\)</span> but also by the action of <span class="math inline">\(G\)</span> on <span class="math inline">\(\mathbb{R}^2\)</span>. We determine both the <strong>point group</strong> <span class="math inline">\(G\)</span> and its <strong>action on the space</strong> <span class="math inline">\(\mathbb{R}^2\)</span> by instantiating a subclass of <code>gspace.GSpace</code>. For the rotational action of <span class="math inline">\(G=C_4\)</span> on <span class="math inline">\(\mathbb{R}^2\)</span> this is done by:</p>
<div id="cell-128" class="cell" data-outputid="63cb72f2-e088-4b42-ce08-d2c26db47d19">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>r2_act <span class="op">=</span> gspaces.rot2dOnR2(N<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>r2_act</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-129" class="cell" data-outputid="53885ae7-8d46-4b58-92be-1b4a76cd4856">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can access the group G as</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> r2_act.fibergroup</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having specified the symmetry transformation on the <em>base space</em> <span class="math inline">\(\mathbb{R}^2\)</span>, we next need to define the representation <span class="math inline">\(\rho: G \to \mathbb{R}^{c \times c}\)</span> which describes how a <strong>feature vector field</strong> <span class="math inline">\(f : \mathbb{R}^2 \to \mathbb{R}^c\)</span> transforms under the action of <span class="math inline">\(G\)</span>. This transformation law of feature fields is implemented by <code>nn.FieldType</code>.</p>
<p>We instantiate the <code>nn.FieldType</code> modeling a GCNN feature by passing it the <code>gspaces.GSpace</code> instance and the <em>regular representation</em> of <span class="math inline">\(G=C_4\)</span>. We call a feature field associated with the regular representation <span class="math inline">\(\rho_\text{reg}\)</span> a <strong>regular feature field</strong>.</p>
<div id="cell-131" class="cell" data-outputid="b758e594-2a4f-48df-ab7b-1211009a02f4">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>feat_type <span class="op">=</span> nn.FieldType(r2_act, [G.regular_representation])</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>feat_type</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Recall that the regular representation of a finite group <span class="math inline">\(G\)</span> built by <code>G.regular_representation</code> is a permutation matrix of shape <span class="math inline">\(|G| \times |G|\)</span>:</p>
<div id="cell-133" class="cell" data-outputid="6c3af2a4-5e21-479f-c362-0f4b36af49a7">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>G.regular_representation(G.sample())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="deep-feature-spaces" class="level4">
<h4 class="anchored" data-anchor-id="deep-feature-spaces">Deep Feature spaces</h4>
<p>The deep feature spaces of a GCNN typically comprise multiple channels. Similarly, the feature spaces of a steerable CNN can include multiple independent feature fields. This is achieved via <strong>direc sum</strong>, but stacking multiple copies of <span class="math inline">\(\rho\)</span>.</p>
<p>For example, we can use <span class="math inline">\(3\)</span> copies of the regular representation <span class="math inline">\(\rho_\text{reg}: G \to \mathbb{R}^{|G|}\)</span>. The full feature space is in this case modeled as a <em>stacked</em> field <span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}^{3|G|}\)</span> which transforms according to the <strong>direct sum</strong> of three regular representations:</p>
<p><span class="math display">\[
\rho(r_\theta)
    \ =\ \rho_\text{reg}(r_\theta) \oplus \rho_\text{reg}(r_\theta) \oplus \rho_\text{reg}(r_\theta)
    \ =\ \begin{bmatrix}
            \rho_\text{reg}(\theta) &amp; 0 &amp; 0 \\
            0 &amp; \rho_\text{reg}(\theta) &amp; 0 \\
            0 &amp; 0 &amp; \rho_\text{reg}(\theta) \\
          \end{bmatrix}
          \quad\in\ \mathbb{R}^{3|G| \times 3|G|}
\]</span></p>
<p>We instantiate a <code>nn.FieldType</code> composed of <span class="math inline">\(3\)</span> regular representations by passing the full field representation as a list of three regular representations:</p>
<div id="cell-135" class="cell" data-outputid="f109a7d2-c4c3-47ec-c298-c5a8d8e118bf">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Technically, one can also construct the direct-sum representation G.regular_representation + G.regular_representation + G.regular_representation as done </span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># before. Passing a list containing 3 copies of G.regular_representation allows for more efficient implementation of certain operations internally.</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>feat_type <span class="op">=</span> nn.FieldType(r2_act, [G.regular_representation]<span class="op">*</span><span class="dv">3</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>feat_type</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="input-features" class="level4">
<h4 class="anchored" data-anchor-id="input-features">Input Features</h4>
<p>Each hidden layer of a steerable CNN has its own transformation law which the user needs to specify (equivalent to the choice of number of channels in each layer of a conventional CNN). The <em>input</em> and <em>output</em> of a steerable CNN are also feature fields and their type (i.e.&nbsp;transformation law) is typically determined by the inference task.</p>
<p>The most common example is that of gray-scale input images. A rotation of a gray-scale image is performed by moving each pixel to a new position without changing their intensity values. The invariance of the scalar pixel values under rotations is modeled by the <strong>trivial representation</strong> <span class="math inline">\(\rho_0: G\to\mathbb{R},\ g\mapsto 1\)</span> of <span class="math inline">\(G\)</span> and identifies them as <strong>scalar fields</strong>. Formally, a scalar field is a function <span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}\)</span> mapping to a feature vector with <span class="math inline">\(c=1\)</span> channels. A rotation <span class="math inline">\(r_\theta \in C_4\)</span> transforms this scalar field as</p>
<p><span class="math display">\[ \big[r_{\theta}\,. f\big](x)
   \ :=\ \rho_0(r_\theta)\,f\big(r_\theta^{-1}x\big)
   \ =\ 1\cdot f\big(r_\theta^{-1}x\big)
   \ =\ f\big(r_\theta^{-1}x\big) \ .
\]</span></p>
<p>We instantiate the <code>nn.FieldType</code> modeling a gray-scale image by passing it the trivial representation of <span class="math inline">\(G\)</span>:</p>
<div id="cell-137" class="cell" data-outputid="79acd2c7-599d-4b54-ca45-146b25f5c25d">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>feat_type_in <span class="op">=</span> nn.FieldType(r2_act, [G.trivial_representation])</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>feat_type_in</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="equivariant-layers" class="level4">
<h4 class="anchored" data-anchor-id="equivariant-layers">Equivariant Layers</h4>
<p>When we build a model <strong>equivariant</strong> to a group <span class="math inline">\(G\)</span>, we require that the output produced by the model transforms consistently when the input transforms under the action of an element <span class="math inline">\(g \in G\)</span>. For a function <span class="math inline">\(F\)</span> (e.g.&nbsp;a neural network), the <strong>equivariance constraint</strong> requires:</p>
<p><span class="math display">\[ \mathcal{T}^\text{out}_g \big[F(x)\big]\ =\ F\big(\mathcal{T}^\text{in}_g[x]\big) \quad \forall g\in G\]</span></p>
<p>where <span class="math inline">\(\mathcal{T}^\text{in}_g\)</span> is the transformation of the input by the group element <span class="math inline">\(g\)</span> while <span class="math inline">\(\mathcal{T}^\text{out}_g\)</span> is the transformation of the output by the same element. The <em>field type</em> <code>feat_type_in</code> we have just defined above precisely describes <span class="math inline">\(\mathcal{T}^\text{in}\)</span>. The transformation law <span class="math inline">\(\mathcal{T}^\text{out}\)</span> of the output of the first layer is similarly chosen by defining an instance <code>feat_type_out</code> of <code>nn.FieldType</code>.</p>
<p>For example, let’s use <span class="math inline">\(3\)</span> <em>regular feature fields</em> in output:</p>
<div id="cell-139" class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>feat_type_out <span class="op">=</span> nn.FieldType(r2_act, [G.regular_representation]<span class="op">*</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a shortcut, we can also use:</p>
<div id="cell-141" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>feat_type_in <span class="op">=</span> nn.FieldType(r2_act, [r2_act.trivial_repr])</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>feat_type_out <span class="op">=</span> nn.FieldType(r2_act, [r2_act.regular_repr]<span class="op">*</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once having defined how the input and output feature spaces should transform, we can build neural network functions as <strong>equivariant modules</strong>. These are implemented as subclasses of an abstract base class <code>nn.EquivariantModule</code> which itself inherits from <code>torch.nn.Module</code>.</p>
<p><strong>Equivariant Convolution Layer</strong>: We start by instantiating a convolutional layer that maps between fields of types <code>feat_type_in</code> and <code>feat_type_out</code>.</p>
<p>Let <span class="math inline">\(\rho_\text{in}: G \to \mathbb{R}^{c_\text{in} \times c_\text{in}}\)</span> and <span class="math inline">\(\rho_\text{out}: G \to \mathbb{R}^{c_\text{out} \times c_\text{out}}\)</span> be respectively the representations of <span class="math inline">\(G\)</span> associated with <code>feat_type_in</code> and <code>feat_type_out</code>. Then, an equivariant convolution layer is a standard convolution layer with a filter <span class="math inline">\(k: \mathbb{R}^2 \to \mathbb{R}^{c_\text{out} \times c_\text{in}}\)</span> (note the number of input and output channels) which satisfies a particular <strong>steerability constraint</strong>: <span class="math display">\[
\forall g \in G, x \in \mathbb{R}^2 \quad k(g.x) = \rho_\text{out}(g) k(x) \rho_\text{in}(g)^{-1}
\]</span></p>
<p>In particular, the use of convolution guarantees the translation equivariance, while the fact the filters satisfy this steerability constraint guarantees the <span class="math inline">\(G\)</span>-equivairance.</p>
<hr>
</section>
<section id="question-9" class="level4">
<h4 class="anchored" data-anchor-id="question-9">QUESTION 9</h4>
<p>Show that if a filter <span class="math inline">\(k: \mathbb{R}^2 \to \mathbb{R}^{c_\text{out} \times c_\text{in}}\)</span> satisfies the constraint above, the convolution with it is equivariant to <span class="math inline">\(G\)</span>, i.e.&nbsp;show that <span class="math display">\[
  f_\text{out} = k \star f_\text{in} \implies [g.f_\text{out}] = k \star [g.f_\text{in}]
\]</span></p>
<p>for all <span class="math inline">\(g \in G\)</span>.</p>
<p>The action on the features <span class="math inline">\(f_\text{in}\)</span> and <span class="math inline">\(f_\text{out}\)</span> is the one previously defined, i.e: <span class="math display">\[
  [g.f_\text{in}](x) = \rho_\text{in}(g) f(g^{-1}x)
\]</span></p>
<p>and <span class="math display">\[
  [g.f_\text{out}](x) = \rho_\text{out}(g) f(g^{-1}x)
\]</span></p>
<p>while the convolution is defined as <span class="math display">\[
  f_\text{out}(y) = [k \star f_\text{in}](y) = \int_{\mathbb{R}^2} k(x-y) f_\text{in}(x) dx
\]</span></p>
</section>
<section id="answer-9" class="level4">
<h4 class="anchored" data-anchor-id="answer-9">ANSWER 9</h4>
<hr>
<p>The steerability constraint restricts the space of possible learnable filters to a smaller space of equivariant filters. Solving this constraint goes beyond the scope of this tutorial; fortunately, the <code>nn.R2Conv</code> module takes care of properly parameterizing the filter <span class="math inline">\(k\)</span> such that it satisfies the constraint.</p>
<div id="cell-145" class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.R2Conv(feat_type_in, feat_type_out, kernel_size<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each equivariant module has an input and output type. As a function (<code>.forward()</code>), it <em>requires</em> its inputs to transform according to its input type and is guaranteed to return feature fields associated with its output type. To prevent the user from accidentally feeding an incorrectly transforming input field into an equivariant module, we perform a dynamic type checking. In order to do so, we define <strong>geometric tensors</strong> as data containers. They are wrapping a <em>PyTorch</em> <code>torch.Tensor</code> to augment them with an instance of <code>FieldType</code>.</p>
<p>Let’s build a few random 32x32 gray-scale images and wrap them into an <code>nn.GeometricTensor</code>:</p>
<div id="cell-147" class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">32</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="co"># FieldType is a callable object; its call method can be used to wrap PyTorch tensors into GeometricTensors</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> feat_type_in(x)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">isinstance</span>(x.tensor, torch.Tensor)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">isinstance</span>(x, nn.GeometricTensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As usually done in <em>PyTorch</em>, an image or feature map is stored in a 4-dimensional array of shape BxCxHxW, where B is the batch-size, C is the number of channels and W and H are the spatial dimensions.</p>
<p>We can feed a geometric tensor to an equivariant module as we feed normal tensors in <em>PyTorch</em>’s modules:</p>
<div id="cell-150" class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> conv(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can verify that the output is indeed associated with the output type of the convolutional layer:</p>
<div id="cell-152" class="cell" data-scrolled="true">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> y.<span class="bu">type</span> <span class="op">==</span> feat_type_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets check whether the output transforms as described by the output type when the input transforms according to the input type. The <span class="math inline">\(G\)</span>-transformation of a geometric tensor is hereby conveniently done by calling <code>nn.GeometricTensor.transform()</code>.</p>
<div id="cell-154" class="cell" data-scrolled="false">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for each group element</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># transform the input with the current group element according to the input type</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    x_transformed <span class="op">=</span> x.transform(g)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># feed the transformed input in the convolutional layer</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed <span class="op">=</span> conv(x_transformed)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the result should be equivalent to rotating the output produced in the </span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># previous block according to the output type</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x <span class="op">=</span> y.transform(g)</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(y_from_x_transformed.tensor, y_transformed_from_x.tensor, atol<span class="op">=</span><span class="fl">1e-5</span>), g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Any network operation is required to be equivariant. <code>escnn.nn</code> provides a wide range of equivariant network modules which guarantee this behavior.</p>
<p><strong>Non-Linearities</strong>: As an example, we will next apply an <em>equivariant nonlinearity</em> to the output feature field of the convolution. Since the regular representations of a finite group <span class="math inline">\(G\)</span> consists of permutation matrices, any pointwise nonlinearity like <em>ReLUs</em> is equivariant. Note that this is <em>not</em> the case for many other choices of representations / field types!</p>
<p>We instantiate a <code>escnn.nn.ReLU</code>, which, as an <code>nn.EquivariantModule</code>, requires to be informed about its input type to be able to perform the type checking. Here we are passing <code>feat_type_out</code>, the output of the equivariant convolution layer, as input type. It is not necessary to pass an output type to the nonlinearity since this is here determined by its input type.</p>
<div id="cell-156" class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>relu <span class="op">=</span> nn.ReLU(feat_type_out)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> relu(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can verify the equivariance again:</p>
<div id="cell-158" class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for each group element</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    y_transformed <span class="op">=</span> y.transform(g)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    z_from_y_transformed <span class="op">=</span> relu(y_transformed)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    z_transformed_from_y <span class="op">=</span> z.transform(g)</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(z_from_y_transformed.tensor, z_transformed_from_y.tensor, atol<span class="op">=</span><span class="fl">1e-5</span>), g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Deeper Models</strong>: In <em>deep learning</em> we usually want to stack multiple layers to build a deep model. As long as each layer is equivariant and consecutive layers are compatible, the equivariance property is preserved by induction.</p>
<p>The compatibility of two consecutive layers requires the output type of the first layer to be equal to the input type of the second layer.</p>
<p>In case we feed an input with the wrong type to a module, an error is raised:</p>
<div id="cell-160" class="cell" data-outputid="ec815185-1d47-4893-9980-f1cde085e68b">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>layer1 <span class="op">=</span> nn.R2Conv(feat_type_in, feat_type_out, kernel_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>layer2 <span class="op">=</span> nn.ReLU(feat_type_in) <span class="co"># the input type of the ReLU should be the output type of the convolution</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> feat_type_in(torch.randn(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> layer2(layer1(x))</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">AssertionError</span> <span class="im">as</span> e:</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Simple deeper architectures can be built using a <strong>SequentialModule</strong>:</p>
<div id="cell-162" class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>feat_type_in <span class="op">=</span> nn.FieldType(r2_act, [r2_act.trivial_repr])</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>feat_type_hid <span class="op">=</span> nn.FieldType(r2_act, <span class="dv">8</span><span class="op">*</span>[r2_act.regular_repr])</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>feat_type_out <span class="op">=</span> nn.FieldType(r2_act, <span class="dv">2</span><span class="op">*</span>[r2_act.regular_repr])</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_in, feat_type_hid, kernel_size<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    nn.InnerBatchNorm(feat_type_hid),</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(feat_type_hid, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_hid, feat_type_hid, kernel_size<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    nn.InnerBatchNorm(feat_type_hid),</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(feat_type_hid, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_hid, feat_type_out, kernel_size<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>).<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As every layer is equivariant and consecutive layers are compatible, the whole model is equivariant.</p>
<div id="cell-164" class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">17</span>, <span class="dv">17</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> feat_type_in(x)</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> model(x)</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="co"># for each group element</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    x_transformed <span class="op">=</span> x.transform(g)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed <span class="op">=</span> model(x_transformed)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x <span class="op">=</span> y.transform(g)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(y_from_x_transformed.tensor, y_transformed_from_x.tensor, atol<span class="op">=</span><span class="fl">1e-5</span>), g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Invariant Pooling Layer</strong>: Usually, at the end of the model we want to produce a single feature vector to use for classification. To do so, it is common to pool over the spatial dimensions, e.g.&nbsp;via average pooling.</p>
<p>This produces (approximatively) translation-invariant feature vectors.</p>
<div id="cell-166" class="cell" data-outputid="854beb2d-8989-46a6-ad37-9d720366fda3">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># average pooling with window size 11</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>avgpool <span class="op">=</span> nn.PointwiseAvgPool(feat_type_out, <span class="dv">11</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> avgpool(model(x))</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In our case, the feature vectors <span class="math inline">\(f(x)\in\mathbb{R}^c\)</span> associated to each point <span class="math inline">\(x\in\mathbb{R}^2\)</span> have a well defined transformation law. The output of the model now transforms according to <code>feat_type_out</code> (here two <span class="math inline">\(C_4\)</span> regular fields, i.e.&nbsp;8 channels). For our choice of regular representations (which are permutation representations) the channels in the feature vectors associated to each point permute when the input is rotated.</p>
<div id="cell-168" class="cell" data-outputid="93f07723-a8b2-4840-e368-f241adf42e41">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'rotation by </span><span class="sc">{</span>g<span class="sc">}</span><span class="ss">:'</span>, y.transform(g).tensor[<span class="dv">0</span>, ...].detach().numpy().squeeze())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Many learning tasks require to build models which are <strong>invariant</strong> under rotations. We can compute invariant features from the output of the model using an <strong>invariant map</strong>. For instance, we can take the maximum value within each regular field. We do so using <code>nn.GroupPooling</code>:</p>
<div id="cell-170" class="cell" data-outputid="cae3c6ff-9883-42ae-a9ab-6b7b0e73cd50">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>invariant_map <span class="op">=</span> nn.GroupPooling(feat_type_out)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> invariant_map(avgpool(model(x)))</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'rotation by </span><span class="sc">{</span>g<span class="sc">}</span><span class="ss">:'</span>, y.transform(g).tensor[<span class="dv">0</span>, ...].detach().numpy().squeeze())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-171" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for each group element</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> G.elements:</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rotated the input image</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    x_transformed <span class="op">=</span> x.transform(g)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed <span class="op">=</span> invariant_map(avgpool(model(x_transformed)))</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x <span class="op">=</span> y <span class="co"># no .transform(g) needed since y should be invariant!</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check that the output did not change</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that here we are not rotating the original output y as before</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> torch.allclose(y_from_x_transformed.tensor, y_transformed_from_x.tensor, atol<span class="op">=</span><span class="fl">1e-6</span>), g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="steerable-cnn-with-infinite-group-g" class="level3">
<h3 class="anchored" data-anchor-id="steerable-cnn-with-infinite-group-g">2.3 Steerable CNN with infinite group <span class="math inline">\(G\)</span></h3>
<p>We can now repeat the same constructions with <span class="math inline">\(G\)</span> being an infinite group, e.g.&nbsp;the group of all planar rotations <span class="math inline">\(G=SO(2)\)</span>.</p>
<div id="cell-173" class="cell" data-outputid="1afbd84c-9635-47c6-bb07-5cdc32e91c1b">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use N=-1 to indicate all rotations</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>r2_act <span class="op">=</span> gspaces.rot2dOnR2(N<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>r2_act</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-174" class="cell" data-outputid="da0b5e6e-a6be-43e5-9a2f-e1ac7d099e9d">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> r2_act.fibergroup</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-175" class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For simplicity we take a single-channel gray-scale image in input and we output a single-channel gray-scale image, i.e. we use scalar fields in input and output</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>feat_type_in <span class="op">=</span> nn.FieldType(r2_act, [G.trivial_representation])</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>feat_type_out <span class="op">=</span> nn.FieldType(r2_act, [G.trivial_representation])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As intermidiate feature types, we want to use again the <em>regular representation</em>. Because <span class="math inline">\(G\)</span> has an infinite number of elements, we use use the Fourier transform idea described earlier. For example, we will use the first three irreps of <span class="math inline">\(G=SO(2)\)</span>, which contains cosines and sines of frequency <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. Earlier, we built this representation as</p>
<p><code>rho = G.bl_regular_representation(2)</code></p>
<p>To apply a non-linearity, e.g.&nbsp;ELU, we can use the <em>Inverse Fourier Transform</em> to sample the function, apply the non-linearity and, finally, compute the <em>Fourier Transform</em> to recover the coeffients. Because <span class="math inline">\(G\)</span> has infinite elements, the Fourier Transform requires an integral over <span class="math inline">\(G\)</span>; this can be <strong>approximated</strong> by a sum over a finite number of samples. The more samples one take, the better the approximation will be, although this also increase the computational cost.</p>
<p>Fortunately, the class <code>nn.FourierELU</code> takes care of most of these details. We can just specify which <code>irreps</code> to consider (<code>G.bl_irreps(2)</code> returns the list of irreps up to frequency <code>2</code>), the number of <code>channels</code> (i.e.&nbsp;copies of the regular representation) and the number <code>N</code> of elements of <span class="math inline">\(G\)</span> where to sample the function:</p>
<div id="cell-177" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>nonlinearity <span class="op">=</span> nn.FourierELU(r2_act, <span class="dv">16</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">2</span>), N<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we do not need to pre-define the feature type: FourierELU will create it internally and we can just access it as</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>feat_type_hid <span class="op">=</span> nonlinearity.in_type</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># note also the its input and output types are the same</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> nonlinearity.in_type <span class="op">==</span> nonlinearity.out_type</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s build a simple <span class="math inline">\(G=SO(2)\)</span> equivariant model:</p>
<div id="cell-179" class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>equivariant_so2_model <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_in, feat_type_hid, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>    nn.IIDBatchNorm2d(feat_type_hid),</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    nonlinearity,</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_hid, feat_type_hid, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    nn.IIDBatchNorm2d(feat_type_hid),</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    nonlinearity,</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    nn.R2Conv(feat_type_hid, feat_type_out, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>).<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and check its equivariance to a few elements of <span class="math inline">\(SO(2)\)</span>:</p>
<div id="cell-181" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">23</span>, <span class="dv">23</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> feat_type_in(x)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> equivariant_so2_model(x)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># check equivariance to N=16 rotations</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> G.element(i<span class="op">*</span><span class="dv">2</span><span class="op">*</span>np.pi<span class="op">/</span>N)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>        x_transformed <span class="op">=</span> x.transform(g)</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        y_from_x_transformed <span class="op">=</span> equivariant_so2_model(x_transformed)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>        y_transformed_from_x <span class="op">=</span> y.transform(g)</span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> torch.allclose(y_from_x_transformed.tensor, y_transformed_from_x.tensor, atol<span class="op">=</span><span class="fl">1e-3</span>), g</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error! The model is not equivariant!'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<section id="question-10" class="level4">
<h4 class="anchored" data-anchor-id="question-10">QUESTION 10</h4>
<p>The model is not perfectly equivariant to <span class="math inline">\(G=SO(2)\)</span> ! Why is this an expected behaviour?</p>
</section>
<section id="answer-10" class="level4">
<h4 class="anchored" data-anchor-id="answer-10">ANSWER 10</h4>
<hr>
<p>While the model can not be perfectly equivariant, we can compare it with a <em>conventional CNN</em> baseline. Let’s build a CNN similar to our equivariant model but which is not constrained to be equivariant:</p>
<div id="cell-184" class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>conventional_model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>    torch.nn.Conv2d(feat_type_in.size, feat_type_hid.size, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    torch.nn.BatchNorm2d(feat_type_hid.size),</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    torch.nn.ELU(),</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    torch.nn.Conv2d(feat_type_hid.size, feat_type_hid.size, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    torch.nn.BatchNorm2d(feat_type_hid.size),</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    torch.nn.ELU(),</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    torch.nn.Conv2d(feat_type_hid.size, feat_type_out.size, kernel_size<span class="op">=</span><span class="dv">7</span>),</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>).<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To compare the two models, we compute their <em>equivariance error</em> for a few elements of <span class="math inline">\(G\)</span>. We define the equivariance error of a model <span class="math inline">\(F\)</span> with respect to a group element <span class="math inline">\(g \in G\)</span> and an input <span class="math inline">\(x\)</span> as: <span class="math display">\[
  \epsilon_g(F) = \frac{||F(g.X) - g.F(X)||_2}{||F(x)||_2}
\]</span></p>
<p>Note that this is a form of <em>relative</em> error. Let’s now compute the equivariance error of the two models:</p>
<div id="cell-186" class="cell" data-outputid="571eb7cb-aadc-4da9-821d-8ded785eebae">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's generate a random image of shape W x W</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> <span class="dv">37</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, W, W)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Because a rotation by an angle smaller than 90 degrees moves pixels outsize the image, we mask out all pixels outside the central disk</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to do this both for the input and the output</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_mask(W):</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>    center_mask <span class="op">=</span> np.zeros((<span class="dv">2</span>, W, W))</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    center_mask[<span class="dv">1</span>, :, :] <span class="op">=</span> np.arange(<span class="dv">0</span>, W) <span class="op">-</span> W <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    center_mask[<span class="dv">0</span>, :, :] <span class="op">=</span> np.arange(<span class="dv">0</span>, W) <span class="op">-</span> W <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>    center_mask[<span class="dv">0</span>, :, :] <span class="op">=</span> center_mask[<span class="dv">0</span>, :, :].T</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>    center_mask <span class="op">=</span> center_mask[<span class="dv">0</span>, :, :] <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> center_mask[<span class="dv">1</span>, :, :] <span class="op">**</span> <span class="dv">2</span> <span class="op">&lt;</span> <span class="fl">.9</span><span class="op">*</span>(W <span class="op">//</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>    center_mask <span class="op">=</span> torch.tensor(center_mask.reshape(<span class="dv">1</span>, <span class="dv">1</span>, W, W), dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> center_mask</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a><span class="co"># create the mask for the input</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>input_center_mask <span class="op">=</span> build_mask(W)</span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a><span class="co"># mask the input image</span></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x <span class="op">*</span> input_center_mask</span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> feat_type_in(x)</span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the output of both models</span></span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>y_equivariant <span class="op">=</span> equivariant_so2_model(x)</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>y_conventional <span class="op">=</span> feat_type_out(conventional_model(x.tensor))</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a><span class="co"># create the mask for the output images</span></span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>output_center_mask <span class="op">=</span> build_mask(y_equivariant.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a><span class="co"># We evaluate the equivariance error on N=100 rotations</span></span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a>error_equivariant <span class="op">=</span> []</span>
<span id="cb85-36"><a href="#cb85-36" aria-hidden="true" tabindex="-1"></a>error_conventional <span class="op">=</span> []</span>
<span id="cb85-37"><a href="#cb85-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-38"><a href="#cb85-38" aria-hidden="true" tabindex="-1"></a><span class="co"># for each of the N rotations</span></span>
<span id="cb85-39"><a href="#cb85-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb85-40"><a href="#cb85-40" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> G.element(i <span class="op">/</span> N <span class="op">*</span> <span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb85-41"><a href="#cb85-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-42"><a href="#cb85-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rotate the input</span></span>
<span id="cb85-43"><a href="#cb85-43" aria-hidden="true" tabindex="-1"></a>    x_transformed <span class="op">=</span> x.transform(g)</span>
<span id="cb85-44"><a href="#cb85-44" aria-hidden="true" tabindex="-1"></a>    x_transformed.tensor <span class="op">*=</span> input_center_mask</span>
<span id="cb85-45"><a href="#cb85-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-46"><a href="#cb85-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># F(g.X)  feed the transformed images in both models</span></span>
<span id="cb85-47"><a href="#cb85-47" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed_equivariant <span class="op">=</span> equivariant_so2_model(x_transformed).tensor</span>
<span id="cb85-48"><a href="#cb85-48" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed_conventional <span class="op">=</span> conventional_model(x_transformed.tensor)</span>
<span id="cb85-49"><a href="#cb85-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-50"><a href="#cb85-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># g.F(x)  transform the output of both models</span></span>
<span id="cb85-51"><a href="#cb85-51" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x_equivariant <span class="op">=</span> y_equivariant.transform(g)</span>
<span id="cb85-52"><a href="#cb85-52" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x_conventional <span class="op">=</span> y_conventional.transform(g)</span>
<span id="cb85-53"><a href="#cb85-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-54"><a href="#cb85-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mask all the outputs</span></span>
<span id="cb85-55"><a href="#cb85-55" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed_equivariant <span class="op">=</span> y_from_x_transformed_equivariant <span class="op">*</span> output_center_mask</span>
<span id="cb85-56"><a href="#cb85-56" aria-hidden="true" tabindex="-1"></a>    y_from_x_transformed_conventional <span class="op">=</span> y_from_x_transformed_conventional <span class="op">*</span> output_center_mask</span>
<span id="cb85-57"><a href="#cb85-57" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x_equivariant <span class="op">=</span> y_transformed_from_x_equivariant.tensor <span class="op">*</span> output_center_mask</span>
<span id="cb85-58"><a href="#cb85-58" aria-hidden="true" tabindex="-1"></a>    y_transformed_from_x_conventional <span class="op">=</span> y_transformed_from_x_conventional.tensor <span class="op">*</span> output_center_mask</span>
<span id="cb85-59"><a href="#cb85-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-60"><a href="#cb85-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the relative error of both models</span></span>
<span id="cb85-61"><a href="#cb85-61" aria-hidden="true" tabindex="-1"></a>    rel_error_equivariant <span class="op">=</span> torch.norm(y_from_x_transformed_equivariant <span class="op">-</span> y_transformed_from_x_equivariant).item() <span class="op">/</span> torch.norm(y_equivariant.tensor).item()</span>
<span id="cb85-62"><a href="#cb85-62" aria-hidden="true" tabindex="-1"></a>    rel_error_conventional <span class="op">=</span> torch.norm(y_from_x_transformed_conventional <span class="op">-</span> y_transformed_from_x_conventional).item() <span class="op">/</span> torch.norm(y_conventional.tensor).item()</span>
<span id="cb85-63"><a href="#cb85-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-64"><a href="#cb85-64" aria-hidden="true" tabindex="-1"></a>    error_equivariant.append(rel_error_equivariant)</span>
<span id="cb85-65"><a href="#cb85-65" aria-hidden="true" tabindex="-1"></a>    error_conventional.append(rel_error_conventional)</span>
<span id="cb85-66"><a href="#cb85-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-67"><a href="#cb85-67" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the error of both models as a function of the rotation angle theta</span></span>
<span id="cb85-68"><a href="#cb85-68" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb85-69"><a href="#cb85-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-70"><a href="#cb85-70" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> [i<span class="op">*</span><span class="dv">2</span><span class="op">*</span>np.pi <span class="op">/</span> N <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb85-71"><a href="#cb85-71" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, error_equivariant, label<span class="op">=</span><span class="st">'SO(2)-Steerable CNN'</span>)</span>
<span id="cb85-72"><a href="#cb85-72" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, error_conventional, label<span class="op">=</span><span class="st">'Conventional CNN'</span>)</span>
<span id="cb85-73"><a href="#cb85-73" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Equivariant vs Conventional CNNs'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb85-74"><a href="#cb85-74" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$g = r_\theta$'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb85-75"><a href="#cb85-75" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Equivariance Error'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb85-76"><a href="#cb85-76" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb85-77"><a href="#cb85-77" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb85-78"><a href="#cb85-78" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="build-and-train-steerable-cnns" class="level2">
<h2 class="anchored" data-anchor-id="build-and-train-steerable-cnns">3. Build and Train Steerable CNNs</h2>
<p>Finally, we will proceed with implementing a <strong>Steerable CNN</strong> and train it on rotated MNIST.</p>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<p>We will evaluate the model on the <em>rotated</em> MNIST dataset. First, we download the (non-rotated) MNIST 12k data:</p>
<div id="cell-190" class="cell" data-outputid="83ca8de9-b943-4bde-8831-bb745f83650e">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download the dataset</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget <span class="op">-</span>nc http:<span class="op">//</span>www.iro.umontreal.ca<span class="op">/~</span>lisa<span class="op">/</span>icml2007data<span class="op">/</span>mnist.<span class="bu">zip</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="co"># uncompress the zip file</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip <span class="op">-</span>n mnist.<span class="bu">zip</span> <span class="op">-</span>d mnist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, we build the dataset and some utility functions:</p>
<div id="cell-192" class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> RandomRotation</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> Pad</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> Resize</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> Compose</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-193" class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MnistDataset(Dataset):</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mode, rotated: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> mode <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'test'</span>]</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mode <span class="op">==</span> <span class="st">"train"</span>:</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span> <span class="op">=</span> <span class="st">"mnist/mnist_train.amat"</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span> <span class="op">=</span> <span class="st">"mnist/mnist_test.amat"</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.loadtxt(<span class="bu">file</span>)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> data[:, :<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).astype(np.float32)</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>               </span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># images are padded to have shape 29x29.</span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this allows to use odd-size filters with stride 2 when downsampling a feature map in the model</span></span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>        pad <span class="op">=</span> Pad((<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>), fill<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to reduce interpolation artifacts (e.g. when testing the model on rotated images),</span></span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we upsample an image by a factor of 3, rotate it and finally downsample it again</span></span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>        resize1 <span class="op">=</span> Resize(<span class="dv">87</span>) <span class="co"># to upsample</span></span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>        resize2 <span class="op">=</span> Resize(<span class="dv">29</span>) <span class="co"># to downsample</span></span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a>        totensor <span class="op">=</span> ToTensor()</span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rotated:</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.images <span class="op">=</span> torch.empty((images.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">29</span>, <span class="dv">29</span>))        </span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(images.shape[<span class="dv">0</span>]), leave<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> images[i]</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> Image.fromarray(img, mode<span class="op">=</span><span class="st">'F'</span>)</span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a>                r <span class="op">=</span> (np.random.rand() <span class="op">*</span> <span class="fl">360.</span>)</span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.images[i] <span class="op">=</span> totensor(resize2(resize1(pad(img)).rotate(r, Image.BILINEAR))).reshape(<span class="dv">1</span>, <span class="dv">29</span>, <span class="dv">29</span>)</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.images <span class="op">=</span> torch.zeros((images.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">29</span>, <span class="dv">29</span>))        </span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.images[:, :, :<span class="dv">28</span>, :<span class="dv">28</span>] <span class="op">=</span> torch.tensor(images).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> data[:, <span class="op">-</span><span class="dv">1</span>].astype(np.int64)</span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_samples <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a>        image, label <span class="op">=</span> <span class="va">self</span>.images[index], <span class="va">self</span>.labels[index]</span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-194" class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the random seed for reproducibility</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="co"># build the rotated training and test datasets</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>mnist_train <span class="op">=</span> MnistDataset(mode<span class="op">=</span><span class="st">'train'</span>, rotated<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(mnist_train, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>mnist_test <span class="op">=</span> MnistDataset(mode<span class="op">=</span><span class="st">'test'</span>, rotated<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(mnist_test, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for testing purpose, we also build a version of the test set with *non*-rotated digits</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>raw_mnist_test <span class="op">=</span> MnistDataset(mode<span class="op">=</span><span class="st">'test'</span>, rotated<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="so2-equivariant-architecture" class="level3">
<h3 class="anchored" data-anchor-id="so2-equivariant-architecture"><span class="math inline">\(SO(2)\)</span> equivariant architecture</h3>
<p>We now build an <span class="math inline">\(SO(2)\)</span> equivariant CNN.</p>
<p>Because the inputs are still gray-scale images, the input type of the model is again a <em>scalar field</em>. In the intermidiate layers, we will use <em>regular fields</em>, such that the models are equivalent to <em>group-equivariant convolutional neural networks</em> (GCNNs).</p>
<p>The final classification is performed by a fully connected layer.</p>
<div id="cell-197" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SO2SteerableCNN(torch.nn.Module):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SO2SteerableCNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the model is equivariant under all planar rotations</span></span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.r2_act <span class="op">=</span> gspaces.rot2dOnR2(N<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the group SO(2)</span></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.G: SO2 <span class="op">=</span> <span class="va">self</span>.r2_act.fibergroup</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the input image is a scalar field, corresponding to the trivial representation</span></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> nn.FieldType(<span class="va">self</span>.r2_act, [<span class="va">self</span>.r2_act.trivial_repr])</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we store the input type for wrapping the images into a geometric tensor during the forward pass</span></span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_type <span class="op">=</span> in_type</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We need to mask the input image since the corners are moved outside the grid under rotations</span></span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mask <span class="op">=</span> nn.MaskModule(in_type, <span class="dv">29</span>, margin<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 1</span></span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first we build the non-linear layer, which also constructs the right feature type</span></span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we choose 8 feature fields, each transforming under the regular representation of SO(2) up to frequency 3</span></span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># When taking the ELU non-linearity, we sample the feature fields on N=16 points</span></span>
<span id="cb90-26"><a href="#cb90-26" aria-hidden="true" tabindex="-1"></a>        activation1 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">8</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-27"><a href="#cb90-27" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation1.in_type</span>
<span id="cb90-28"><a href="#cb90-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-29"><a href="#cb90-29" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">7</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-30"><a href="#cb90-30" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-31"><a href="#cb90-31" aria-hidden="true" tabindex="-1"></a>            activation1,</span>
<span id="cb90-32"><a href="#cb90-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-33"><a href="#cb90-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-34"><a href="#cb90-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 2</span></span>
<span id="cb90-35"><a href="#cb90-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb90-36"><a href="#cb90-36" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block1.out_type</span>
<span id="cb90-37"><a href="#cb90-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the second convolution layer are 16 regular feature fields</span></span>
<span id="cb90-38"><a href="#cb90-38" aria-hidden="true" tabindex="-1"></a>        activation2 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">16</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-39"><a href="#cb90-39" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation2.in_type</span>
<span id="cb90-40"><a href="#cb90-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-41"><a href="#cb90-41" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-42"><a href="#cb90-42" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-43"><a href="#cb90-43" aria-hidden="true" tabindex="-1"></a>            activation2</span>
<span id="cb90-44"><a href="#cb90-44" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-45"><a href="#cb90-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to reduce the downsampling artifacts, we use a Gaussian smoothing filter</span></span>
<span id="cb90-46"><a href="#cb90-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool1 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-47"><a href="#cb90-47" aria-hidden="true" tabindex="-1"></a>            nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb90-48"><a href="#cb90-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-49"><a href="#cb90-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-50"><a href="#cb90-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 3</span></span>
<span id="cb90-51"><a href="#cb90-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb90-52"><a href="#cb90-52" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block2.out_type</span>
<span id="cb90-53"><a href="#cb90-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the third convolution layer are 32 regular feature fields</span></span>
<span id="cb90-54"><a href="#cb90-54" aria-hidden="true" tabindex="-1"></a>        activation3 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">32</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-55"><a href="#cb90-55" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation3.in_type        </span>
<span id="cb90-56"><a href="#cb90-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-57"><a href="#cb90-57" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-58"><a href="#cb90-58" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-59"><a href="#cb90-59" aria-hidden="true" tabindex="-1"></a>            activation3</span>
<span id="cb90-60"><a href="#cb90-60" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-61"><a href="#cb90-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-62"><a href="#cb90-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 4</span></span>
<span id="cb90-63"><a href="#cb90-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb90-64"><a href="#cb90-64" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block3.out_type</span>
<span id="cb90-65"><a href="#cb90-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the fourth convolution layer are 64 regular feature fields</span></span>
<span id="cb90-66"><a href="#cb90-66" aria-hidden="true" tabindex="-1"></a>        activation4 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">32</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-67"><a href="#cb90-67" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation4.in_type        </span>
<span id="cb90-68"><a href="#cb90-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block4 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-69"><a href="#cb90-69" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-70"><a href="#cb90-70" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-71"><a href="#cb90-71" aria-hidden="true" tabindex="-1"></a>            activation4</span>
<span id="cb90-72"><a href="#cb90-72" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-73"><a href="#cb90-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool2 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-74"><a href="#cb90-74" aria-hidden="true" tabindex="-1"></a>            nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb90-75"><a href="#cb90-75" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-76"><a href="#cb90-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-77"><a href="#cb90-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 5</span></span>
<span id="cb90-78"><a href="#cb90-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb90-79"><a href="#cb90-79" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block4.out_type</span>
<span id="cb90-80"><a href="#cb90-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the fifth convolution layer are 96 regular feature fields</span></span>
<span id="cb90-81"><a href="#cb90-81" aria-hidden="true" tabindex="-1"></a>        activation5 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">64</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-82"><a href="#cb90-82" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation5.in_type      </span>
<span id="cb90-83"><a href="#cb90-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block5 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-84"><a href="#cb90-84" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-85"><a href="#cb90-85" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-86"><a href="#cb90-86" aria-hidden="true" tabindex="-1"></a>            activation5</span>
<span id="cb90-87"><a href="#cb90-87" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-88"><a href="#cb90-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-89"><a href="#cb90-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 6</span></span>
<span id="cb90-90"><a href="#cb90-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb90-91"><a href="#cb90-91" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block5.out_type</span>
<span id="cb90-92"><a href="#cb90-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the sixth convolution layer are 64 regular feature fields</span></span>
<span id="cb90-93"><a href="#cb90-93" aria-hidden="true" tabindex="-1"></a>        activation6 <span class="op">=</span> nn.FourierELU(<span class="va">self</span>.r2_act, <span class="dv">64</span>, irreps<span class="op">=</span>G.bl_irreps(<span class="dv">3</span>), N<span class="op">=</span><span class="dv">16</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-94"><a href="#cb90-94" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation6.in_type  </span>
<span id="cb90-95"><a href="#cb90-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block6 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb90-96"><a href="#cb90-96" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb90-97"><a href="#cb90-97" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb90-98"><a href="#cb90-98" aria-hidden="true" tabindex="-1"></a>            activation6</span>
<span id="cb90-99"><a href="#cb90-99" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-100"><a href="#cb90-100" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool3 <span class="op">=</span> nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb90-101"><a href="#cb90-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-102"><a href="#cb90-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of output invariant channels</span></span>
<span id="cb90-103"><a href="#cb90-103" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb90-104"><a href="#cb90-104" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-105"><a href="#cb90-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># last 1x1 convolution layer, which maps the regular fields to c=64 invariant scalar fields</span></span>
<span id="cb90-106"><a href="#cb90-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is essential to provide *invariant* features in the final classification layer</span></span>
<span id="cb90-107"><a href="#cb90-107" aria-hidden="true" tabindex="-1"></a>        output_invariant_type <span class="op">=</span> nn.FieldType(<span class="va">self</span>.r2_act, c<span class="op">*</span>[<span class="va">self</span>.r2_act.trivial_repr])</span>
<span id="cb90-108"><a href="#cb90-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.invariant_map <span class="op">=</span> nn.R2Conv(out_type, output_invariant_type, kernel_size<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb90-109"><a href="#cb90-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-110"><a href="#cb90-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully Connected classifier</span></span>
<span id="cb90-111"><a href="#cb90-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fully_net <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb90-112"><a href="#cb90-112" aria-hidden="true" tabindex="-1"></a>            torch.nn.BatchNorm1d(c),</span>
<span id="cb90-113"><a href="#cb90-113" aria-hidden="true" tabindex="-1"></a>            torch.nn.ELU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb90-114"><a href="#cb90-114" aria-hidden="true" tabindex="-1"></a>            torch.nn.Linear(c, n_classes),</span>
<span id="cb90-115"><a href="#cb90-115" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-116"><a href="#cb90-116" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb90-117"><a href="#cb90-117" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>: torch.Tensor):</span>
<span id="cb90-118"><a href="#cb90-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># wrap the input tensor in a GeometricTensor</span></span>
<span id="cb90-119"><a href="#cb90-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (associate it with the input type)</span></span>
<span id="cb90-120"><a href="#cb90-120" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.input_type(<span class="bu">input</span>)</span>
<span id="cb90-121"><a href="#cb90-121" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-122"><a href="#cb90-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mask out the corners of the input image</span></span>
<span id="cb90-123"><a href="#cb90-123" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mask(x)</span>
<span id="cb90-124"><a href="#cb90-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-125"><a href="#cb90-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply each equivariant block</span></span>
<span id="cb90-126"><a href="#cb90-126" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-127"><a href="#cb90-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each layer has an input and an output type</span></span>
<span id="cb90-128"><a href="#cb90-128" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A layer takes a GeometricTensor in input.</span></span>
<span id="cb90-129"><a href="#cb90-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This tensor needs to be associated with the same representation of the layer's input type</span></span>
<span id="cb90-130"><a href="#cb90-130" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb90-131"><a href="#cb90-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each layer outputs a new GeometricTensor, associated with the layer's output type.</span></span>
<span id="cb90-132"><a href="#cb90-132" aria-hidden="true" tabindex="-1"></a>        <span class="co"># As a result, consecutive layers need to have matching input/output types</span></span>
<span id="cb90-133"><a href="#cb90-133" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block1(x)</span>
<span id="cb90-134"><a href="#cb90-134" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block2(x)</span>
<span id="cb90-135"><a href="#cb90-135" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool1(x)</span>
<span id="cb90-136"><a href="#cb90-136" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-137"><a href="#cb90-137" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block3(x)</span>
<span id="cb90-138"><a href="#cb90-138" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block4(x)</span>
<span id="cb90-139"><a href="#cb90-139" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool2(x)</span>
<span id="cb90-140"><a href="#cb90-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-141"><a href="#cb90-141" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block5(x)</span>
<span id="cb90-142"><a href="#cb90-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block6(x)</span>
<span id="cb90-143"><a href="#cb90-143" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-144"><a href="#cb90-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pool over the spatial dimensions</span></span>
<span id="cb90-145"><a href="#cb90-145" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool3(x)</span>
<span id="cb90-146"><a href="#cb90-146" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-147"><a href="#cb90-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># extract invariant features</span></span>
<span id="cb90-148"><a href="#cb90-148" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.invariant_map(x)</span>
<span id="cb90-149"><a href="#cb90-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-150"><a href="#cb90-150" aria-hidden="true" tabindex="-1"></a>        <span class="co"># unwrap the output GeometricTensor</span></span>
<span id="cb90-151"><a href="#cb90-151" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (take the Pytorch tensor and discard the associated representation)</span></span>
<span id="cb90-152"><a href="#cb90-152" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.tensor</span>
<span id="cb90-153"><a href="#cb90-153" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-154"><a href="#cb90-154" aria-hidden="true" tabindex="-1"></a>        <span class="co"># classify with the final fully connected layer</span></span>
<span id="cb90-155"><a href="#cb90-155" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fully_net(x.reshape(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb90-156"><a href="#cb90-156" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-157"><a href="#cb90-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="equivariance-test-before-training" class="level4">
<h4 class="anchored" data-anchor-id="equivariance-test-before-training">Equivariance Test before training</h4>
<p>Let’s instantiate the model:</p>
<div id="cell-200" class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SO2SteerableCNN().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The model is now randomly initialized. Therefore, we do not expect it to produce the right class probabilities.</p>
<p>However, the model should still produce the same output for rotated versions of the same image. This is true for rotations by multiples of <span class="math inline">\(\frac{\pi}{2}\)</span>, but is only approximate for other rotations.</p>
<p>Let’s test it on a random test image: we feed <span class="math inline">\(N=20\)</span> rotated versions of the first image in the test set and print the output logits of the model for each of them.</p>
<div id="cell-202" class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_model_single_image(model: torch.nn.Module, x: torch.Tensor, N: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>):</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    np.set_printoptions(linewidth<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Image.fromarray(x.cpu().numpy()[<span class="dv">0</span>], mode<span class="op">=</span><span class="st">'F'</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to reduce interpolation artifacts (e.g. when testing the model on rotated images),</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we upsample an image by a factor of 3, rotate it and finally downsample it again</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    resize1 <span class="op">=</span> Resize(<span class="dv">87</span>) <span class="co"># to upsample</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    resize2 <span class="op">=</span> Resize(<span class="dv">29</span>) <span class="co"># to downsample</span></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    totensor <span class="op">=</span> ToTensor()</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> resize1(x)</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the `model` on N rotated versions of the input image `x`</span></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'##########################################################################################'</span>)</span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>    header <span class="op">=</span> <span class="st">'angle  |  '</span> <span class="op">+</span> <span class="st">'  '</span>.join([<span class="st">"</span><span class="sc">{:5d}</span><span class="st">"</span>.<span class="bu">format</span>(d) <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)])</span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(header)</span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a>            x_transformed <span class="op">=</span> totensor(resize2(x.rotate(r<span class="op">*</span><span class="fl">360.</span><span class="op">/</span>N, Image.BILINEAR))).reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">29</span>, <span class="dv">29</span>)</span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a>            x_transformed <span class="op">=</span> x_transformed.to(device)</span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> model(x_transformed)</span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(<span class="st">'cpu'</span>).numpy().squeeze()</span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a>            angle <span class="op">=</span> r <span class="op">*</span> <span class="fl">360.</span> <span class="op">/</span> N</span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"</span><span class="sc">{:6.1f}</span><span class="st"> : </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(angle, y))</span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'##########################################################################################'</span>)</span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-203" class="cell" data-outputid="df35553d-1a1c-4133-d851-6fbbfd1a7652">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># retrieve the first image from the test set</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(raw_mnist_test))</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the model</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>test_model_single_image(model, x, N<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output of the model is already almost invariant but we observe small fluctuations in the outputs. This is the effect of the discretization artifacts (e.g.&nbsp;the pixel grid can not be perfectly rotated by any angle without interpolation) and can not be completely removed.</p>
</section>
<section id="training-the-model" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model">Training the model</h4>
<p>Let’s train the model now. The procedure is the same used to train a normal <em>PyTorch</em> architecture:</p>
<div id="cell-206" class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build the training and test function</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(model: torch.nn.Module):</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># test over the full rotated test set</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x, t) <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader):</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(device)</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>            t <span class="op">=</span> t.to(device)</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> model(x)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>            _, prediction <span class="op">=</span> torch.<span class="bu">max</span>(y.data, <span class="dv">1</span>)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> t.shape[<span class="dv">0</span>]</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (prediction <span class="op">==</span> t).<span class="bu">sum</span>().item()</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct<span class="op">/</span>total<span class="op">*</span><span class="fl">100.</span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model: torch.nn.Module, lr<span class="op">=</span><span class="fl">1e-4</span>, wd<span class="op">=</span><span class="fl">1e-4</span>, checkpoint_path: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>        checkpoint_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, checkpoint_path)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> os.path.isfile(checkpoint_path):</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>        model.load_state_dict(torch.load(checkpoint_path))</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>    loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr, weight_decay<span class="op">=</span>wd)</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">21</span>)):</span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x, t) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(device)</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>            t <span class="op">=</span> t.to(device)</span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> model(x)</span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_function(y, t)</span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> x, y, t, loss</span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> test(model)    </span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a>        torch.save(model.state_dict(), checkpoint_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, train the <span class="math inline">\(SO(2)\)</span> equivariant model:</p>
<div id="cell-208" class="cell" data-outputid="24e813d8-a1f2-44f8-c7d8-28186434e302">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set the seed manually for reproducibility</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SO2SteerableCNN().to(device)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>train(model, checkpoint_path<span class="op">=</span><span class="st">"steerable_so2-pretrained.ckpt"</span>)</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test(model)    </span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-209" class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_model_rotations(model: torch.nn.Module, N: <span class="bu">int</span> <span class="op">=</span> <span class="dv">24</span>, M: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2000</span>, checkpoint_path: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the `model` on N rotated versions of the first M images in the test set</span></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>        checkpoint_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, checkpoint_path)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> os.path.isfile(checkpoint_path):</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>        accuracies <span class="op">=</span> np.load(checkpoint_path)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> accuracies.tolist()</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to reduce interpolation artifacts (e.g. when testing the model on rotated images),</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we upsample an image by a factor of 3, rotate it and finally downsample it again</span></span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>    resize1 <span class="op">=</span> Resize(<span class="dv">87</span>) <span class="co"># to upsample</span></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>    resize2 <span class="op">=</span> Resize(<span class="dv">29</span>) <span class="co"># to downsample</span></span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>    totensor <span class="op">=</span> ToTensor()</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>    accuracies <span class="op">=</span> []</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> tqdm(<span class="bu">range</span>(N)):</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>            total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>                x, t <span class="op">=</span> raw_mnist_test[i]</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> Image.fromarray(x.numpy()[<span class="dv">0</span>], mode<span class="op">=</span><span class="st">'F'</span>)</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> totensor(resize2(resize1(x).rotate(r<span class="op">*</span><span class="fl">360.</span><span class="op">/</span>N, Image.BILINEAR))).reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">29</span>, <span class="dv">29</span>).to(device)</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> x.to(device)</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> model(x)</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>                _, prediction <span class="op">=</span> torch.<span class="bu">max</span>(y.data, <span class="dv">1</span>)</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a>                total <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>                correct <span class="op">+=</span> (prediction <span class="op">==</span> t).<span class="bu">sum</span>().item()</span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a>            accuracies.append(correct<span class="op">/</span>total<span class="op">*</span><span class="fl">100.</span>)</span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-45"><a href="#cb96-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> checkpoint_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb96-46"><a href="#cb96-46" aria-hidden="true" tabindex="-1"></a>        np.save(checkpoint_path, np.array(accuracies))</span>
<span id="cb96-47"><a href="#cb96-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-48"><a href="#cb96-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracies</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-210" class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>accs_so2 <span class="op">=</span> test_model_rotations(model, <span class="dv">16</span>, <span class="dv">10000</span>, checkpoint_path<span class="op">=</span><span class="st">"steerable_so2-accuracies.npy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-211" class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the accuracy of as a function of the rotation angle theta applied to the test set</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> [i<span class="op">*</span><span class="dv">2</span><span class="op">*</span>np.pi <span class="op">/</span> N <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, accs_so2 <span class="op">+</span> [accs_so2[<span class="dv">0</span>]])</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'SO(2)-Steerable CNN'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'Test rotation $\theta \in [0, 2\pi)$'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Even after training, the model is not perfectly <span class="math inline">\(SO(2)\)</span> equivariant, but we observe the accuracy is rather stable to rotations.</p>
</section>
<section id="c_4-equivariant-architecture" class="level4">
<h4 class="anchored" data-anchor-id="c_4-equivariant-architecture"><span class="math inline">\(C_4\)</span> equivariant architecture</h4>
<p>For comparison, let’s build a similar architecture equivariant only to <span class="math inline">\(N=4\)</span> rotations.</p>
<div id="cell-214" class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNSteerableCNN(torch.nn.Module):</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNSteerableCNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the model is equivariant to rotations by multiples of 2pi/N</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.r2_act <span class="op">=</span> gspaces.rot2dOnR2(N<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the input image is a scalar field, corresponding to the trivial representation</span></span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> nn.FieldType(<span class="va">self</span>.r2_act, [<span class="va">self</span>.r2_act.trivial_repr])</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we store the input type for wrapping the images into a geometric tensor during the forward pass</span></span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_type <span class="op">=</span> in_type</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We need to mask the input image since the corners are moved outside the grid under rotations</span></span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mask <span class="op">=</span> nn.MaskModule(in_type, <span class="dv">29</span>, margin<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 1</span></span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first we build the non-linear layer, which also constructs the right feature type</span></span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we choose 8 feature fields, each transforming under the regular representation of C_4</span></span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>        activation1 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">8</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation1.in_type</span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">7</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-27"><a href="#cb99-27" aria-hidden="true" tabindex="-1"></a>            activation1,</span>
<span id="cb99-28"><a href="#cb99-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-29"><a href="#cb99-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-30"><a href="#cb99-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 2</span></span>
<span id="cb99-31"><a href="#cb99-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb99-32"><a href="#cb99-32" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block1.out_type</span>
<span id="cb99-33"><a href="#cb99-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the second convolution layer are 16 regular feature fields</span></span>
<span id="cb99-34"><a href="#cb99-34" aria-hidden="true" tabindex="-1"></a>        activation2 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">16</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-35"><a href="#cb99-35" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation2.in_type</span>
<span id="cb99-36"><a href="#cb99-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-37"><a href="#cb99-37" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-38"><a href="#cb99-38" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-39"><a href="#cb99-39" aria-hidden="true" tabindex="-1"></a>            activation2</span>
<span id="cb99-40"><a href="#cb99-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-41"><a href="#cb99-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool1 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-42"><a href="#cb99-42" aria-hidden="true" tabindex="-1"></a>            nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb99-43"><a href="#cb99-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-44"><a href="#cb99-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-45"><a href="#cb99-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 3</span></span>
<span id="cb99-46"><a href="#cb99-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb99-47"><a href="#cb99-47" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block2.out_type</span>
<span id="cb99-48"><a href="#cb99-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the third convolution layer are 32 regular feature fields</span></span>
<span id="cb99-49"><a href="#cb99-49" aria-hidden="true" tabindex="-1"></a>        activation3 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">32</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-50"><a href="#cb99-50" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation3.in_type        </span>
<span id="cb99-51"><a href="#cb99-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-52"><a href="#cb99-52" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-53"><a href="#cb99-53" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-54"><a href="#cb99-54" aria-hidden="true" tabindex="-1"></a>            activation3</span>
<span id="cb99-55"><a href="#cb99-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-56"><a href="#cb99-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-57"><a href="#cb99-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 4</span></span>
<span id="cb99-58"><a href="#cb99-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb99-59"><a href="#cb99-59" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block3.out_type</span>
<span id="cb99-60"><a href="#cb99-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the fourth convolution layer are 32 regular feature fields</span></span>
<span id="cb99-61"><a href="#cb99-61" aria-hidden="true" tabindex="-1"></a>        activation4 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">32</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-62"><a href="#cb99-62" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation4.in_type        </span>
<span id="cb99-63"><a href="#cb99-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block4 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-64"><a href="#cb99-64" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-65"><a href="#cb99-65" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-66"><a href="#cb99-66" aria-hidden="true" tabindex="-1"></a>            activation4</span>
<span id="cb99-67"><a href="#cb99-67" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-68"><a href="#cb99-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool2 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-69"><a href="#cb99-69" aria-hidden="true" tabindex="-1"></a>            nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb99-70"><a href="#cb99-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-71"><a href="#cb99-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-72"><a href="#cb99-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 5</span></span>
<span id="cb99-73"><a href="#cb99-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb99-74"><a href="#cb99-74" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block4.out_type</span>
<span id="cb99-75"><a href="#cb99-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the fifth convolution layer are 64 regular feature fields</span></span>
<span id="cb99-76"><a href="#cb99-76" aria-hidden="true" tabindex="-1"></a>        activation5 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">64</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-77"><a href="#cb99-77" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation5.in_type      </span>
<span id="cb99-78"><a href="#cb99-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block5 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-79"><a href="#cb99-79" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-80"><a href="#cb99-80" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-81"><a href="#cb99-81" aria-hidden="true" tabindex="-1"></a>            activation5</span>
<span id="cb99-82"><a href="#cb99-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-83"><a href="#cb99-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-84"><a href="#cb99-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convolution 6</span></span>
<span id="cb99-85"><a href="#cb99-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the old output type is the input type to the next layer</span></span>
<span id="cb99-86"><a href="#cb99-86" aria-hidden="true" tabindex="-1"></a>        in_type <span class="op">=</span> <span class="va">self</span>.block5.out_type</span>
<span id="cb99-87"><a href="#cb99-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the output type of the sixth convolution layer are 64 regular feature fields</span></span>
<span id="cb99-88"><a href="#cb99-88" aria-hidden="true" tabindex="-1"></a>        activation6 <span class="op">=</span> nn.ELU(nn.FieldType(<span class="va">self</span>.r2_act, <span class="dv">64</span><span class="op">*</span>[<span class="va">self</span>.r2_act.regular_repr]), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb99-89"><a href="#cb99-89" aria-hidden="true" tabindex="-1"></a>        out_type <span class="op">=</span> activation6.in_type  </span>
<span id="cb99-90"><a href="#cb99-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block6 <span class="op">=</span> nn.SequentialModule(</span>
<span id="cb99-91"><a href="#cb99-91" aria-hidden="true" tabindex="-1"></a>            nn.R2Conv(in_type, out_type, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb99-92"><a href="#cb99-92" aria-hidden="true" tabindex="-1"></a>            nn.IIDBatchNorm2d(out_type),</span>
<span id="cb99-93"><a href="#cb99-93" aria-hidden="true" tabindex="-1"></a>            activation6</span>
<span id="cb99-94"><a href="#cb99-94" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-95"><a href="#cb99-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool3 <span class="op">=</span> nn.PointwiseAvgPoolAntialiased(out_type, sigma<span class="op">=</span><span class="fl">0.66</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb99-96"><a href="#cb99-96" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-97"><a href="#cb99-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of output invariant channels</span></span>
<span id="cb99-98"><a href="#cb99-98" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb99-99"><a href="#cb99-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-100"><a href="#cb99-100" aria-hidden="true" tabindex="-1"></a>        output_invariant_type <span class="op">=</span> nn.FieldType(<span class="va">self</span>.r2_act, c<span class="op">*</span>[<span class="va">self</span>.r2_act.trivial_repr])</span>
<span id="cb99-101"><a href="#cb99-101" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.invariant_map <span class="op">=</span> nn.R2Conv(out_type, output_invariant_type, kernel_size<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb99-102"><a href="#cb99-102" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-103"><a href="#cb99-103" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-104"><a href="#cb99-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fully Connected classifier</span></span>
<span id="cb99-105"><a href="#cb99-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fully_net <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb99-106"><a href="#cb99-106" aria-hidden="true" tabindex="-1"></a>            torch.nn.BatchNorm1d(c),</span>
<span id="cb99-107"><a href="#cb99-107" aria-hidden="true" tabindex="-1"></a>            torch.nn.ELU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb99-108"><a href="#cb99-108" aria-hidden="true" tabindex="-1"></a>            torch.nn.Linear(c, n_classes),</span>
<span id="cb99-109"><a href="#cb99-109" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-110"><a href="#cb99-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb99-111"><a href="#cb99-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>: torch.Tensor):</span>
<span id="cb99-112"><a href="#cb99-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># wrap the input tensor in a GeometricTensor</span></span>
<span id="cb99-113"><a href="#cb99-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (associate it with the input type)</span></span>
<span id="cb99-114"><a href="#cb99-114" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.input_type(<span class="bu">input</span>)</span>
<span id="cb99-115"><a href="#cb99-115" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-116"><a href="#cb99-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mask out the corners of the input image</span></span>
<span id="cb99-117"><a href="#cb99-117" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mask(x)</span>
<span id="cb99-118"><a href="#cb99-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-119"><a href="#cb99-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply each equivariant block</span></span>
<span id="cb99-120"><a href="#cb99-120" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-121"><a href="#cb99-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each layer has an input and an output type</span></span>
<span id="cb99-122"><a href="#cb99-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A layer takes a GeometricTensor in input.</span></span>
<span id="cb99-123"><a href="#cb99-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This tensor needs to be associated with the same representation of the layer's input type</span></span>
<span id="cb99-124"><a href="#cb99-124" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb99-125"><a href="#cb99-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each layer outputs a new GeometricTensor, associated with the layer's output type.</span></span>
<span id="cb99-126"><a href="#cb99-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># As a result, consecutive layers need to have matching input/output types</span></span>
<span id="cb99-127"><a href="#cb99-127" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block1(x)</span>
<span id="cb99-128"><a href="#cb99-128" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block2(x)</span>
<span id="cb99-129"><a href="#cb99-129" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool1(x)</span>
<span id="cb99-130"><a href="#cb99-130" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-131"><a href="#cb99-131" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block3(x)</span>
<span id="cb99-132"><a href="#cb99-132" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block4(x)</span>
<span id="cb99-133"><a href="#cb99-133" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool2(x)</span>
<span id="cb99-134"><a href="#cb99-134" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-135"><a href="#cb99-135" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block5(x)</span>
<span id="cb99-136"><a href="#cb99-136" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block6(x)</span>
<span id="cb99-137"><a href="#cb99-137" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-138"><a href="#cb99-138" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pool over the spatial dimensions</span></span>
<span id="cb99-139"><a href="#cb99-139" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool3(x)</span>
<span id="cb99-140"><a href="#cb99-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-141"><a href="#cb99-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># extract invariant features</span></span>
<span id="cb99-142"><a href="#cb99-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.invariant_map(x)</span>
<span id="cb99-143"><a href="#cb99-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-144"><a href="#cb99-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># unwrap the output GeometricTensor</span></span>
<span id="cb99-145"><a href="#cb99-145" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (take the Pytorch tensor and discard the associated representation)</span></span>
<span id="cb99-146"><a href="#cb99-146" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.tensor</span>
<span id="cb99-147"><a href="#cb99-147" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-148"><a href="#cb99-148" aria-hidden="true" tabindex="-1"></a>        <span class="co"># classify with the final fully connected layer</span></span>
<span id="cb99-149"><a href="#cb99-149" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fully_net(x.reshape(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb99-150"><a href="#cb99-150" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-151"><a href="#cb99-151" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instantiate and train the <span class="math inline">\(C_4\)</span> equivariant model:</p>
<div id="cell-216" class="cell" data-scrolled="true">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>model_c4 <span class="op">=</span> CNSteerableCNN().to(device)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>train(model_c4, checkpoint_path<span class="op">=</span><span class="st">"steerable_c4-pretrained.ckpt"</span>)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test(model_c4)    </span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>accs_c4 <span class="op">=</span> test_model_rotations(model_c4, <span class="dv">16</span>, <span class="dv">10000</span>, checkpoint_path<span class="op">=</span><span class="st">"steerable_c4-accuracies.npy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, let’s compare the performance of both models on the rotated test sets:</p>
<div id="cell-218" class="cell">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the accuracy of as a function of the rotation angle theta applied to the test set</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">16</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> [i<span class="op">*</span><span class="dv">2</span><span class="op">*</span>np.pi <span class="op">/</span> N <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, accs_so2 <span class="op">+</span> [accs_so2[<span class="dv">0</span>]], label<span class="op">=</span><span class="vs">r'$SO(2)$-Steerable CNN'</span>)</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, accs_c4 <span class="op">+</span> [accs_c4[<span class="dv">0</span>]], label<span class="op">=</span><span class="vs">r'$C_4$-Steerable CNN'</span>)</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r'$C_4$ vs $SO(2)$ Steerable CNNs'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'Test rotation ($\theta \in [0, 2\pi)$)'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>While perfect equivariance to <span class="math inline">\(SO(2)\)</span> is not achievable due to the discretizations, the <span class="math inline">\(SO(2)\)</span> equivariant architecture is more stable over the rotations of the test set than the <span class="math inline">\(C_4\)</span> model. Moreover, since <span class="math inline">\(C_4\)</span> is the only perfect symmetry of the pixel grid and since <span class="math inline">\(C_4 &lt; SO(2)\)</span>, the <span class="math inline">\(SO(2)\)</span> equivariant architecture is also perfectly equivariant to rotations by multiples of <span class="math inline">\(\pi/2\)</span>.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this tutorial, you first leart about <em>group representation theory</em> and the <em>Fourier Transform</em> over compact groups. These are the mathematical tools used to formalize Steerable CNNs.</p>
<p>In the second part of this tutorial, you learnt about <em>steerable feature fields</em> and <em>steerable CNNs</em>. In particular, the previously defined Fourier transform allowed us to build a steerable CNN which is equivalent to a Group-Convolutional Neural Network (GCNN) equivariant to translations and the continuous group <span class="math inline">\(G=SO(2)\)</span> of rotations.</p>
<p>In our steerable CNNs, we mostly leveraged the <em>regular representation</em> of the group <span class="math inline">\(G\)</span>, but the framework of steerable CNNs allows for a variety of representations. If you are interested in knowing more about steerable CNNs, this is a (non-exhaustive) list of relevant works you can check out:</p>
<ul>
<li><a href="https://arxiv.org/abs/1612.08498">Steerable CNNs</a></li>
<li><a href="https://arxiv.org/abs/1612.04642">Harmonic Networks: Deep Translation and Rotation Equivariance</a></li>
<li><a href="https://arxiv.org/abs/1807.02547">3D Steerable CNNs</a></li>
<li><a href="https://arxiv.org/abs/1802.08219">Tensor Field Networks</a></li>
<li><a href="https://arxiv.org/abs/1811.02017">A General Theory of Equivariant CNNs on Homogeneous Spaces</a></li>
<li><a href="https://arxiv.org/abs/1906.04015">Cormorant: Covariant Molecular Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1911.08251">General E(2)-Equivariant Steerable CNNs</a></li>
<li><a href="https://openreview.net/forum?id=WE4qe9xlnQw">A Program to Build E(N)-Equivariant Steerable CNNs</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>