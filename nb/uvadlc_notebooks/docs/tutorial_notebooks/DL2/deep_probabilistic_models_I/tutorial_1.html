<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DPM1 - Deep Probabilistic Models I – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#general-notes" id="toc-general-notes" class="nav-link active" data-scroll-target="#general-notes">0. General notes</a>
  <ul class="collapse">
  <li><a href="#ilos" id="toc-ilos" class="nav-link" data-scroll-target="#ilos">0.1 ILOs</a></li>
  <li><a href="#setting-up" id="toc-setting-up" class="nav-link" data-scroll-target="#setting-up">0.2 Setting up</a></li>
  </ul></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">1. Data</a>
  <ul class="collapse">
  <li><a href="#sentiment" id="toc-sentiment" class="nav-link" data-scroll-target="#sentiment">1.1 Sentiment</a></li>
  <li><a href="#age" id="toc-age" class="nav-link" data-scroll-target="#age">1.2 Age</a></li>
  <li><a href="#syntactic-categories" id="toc-syntactic-categories" class="nav-link" data-scroll-target="#syntactic-categories">1.3 Syntactic categories</a></li>
  <li><a href="#vocabulary" id="toc-vocabulary" class="nav-link" data-scroll-target="#vocabulary">1.4 Vocabulary</a></li>
  <li><a href="#corpus-and-data-loader" id="toc-corpus-and-data-loader" class="nav-link" data-scroll-target="#corpus-and-data-loader">1.5 Corpus and Data Loader</a></li>
  </ul></li>
  <li><a href="#text-encoders" id="toc-text-encoders" class="nav-link" data-scroll-target="#text-encoders">2. Text encoders</a></li>
  <li><a href="#probabilistic-model" id="toc-probabilistic-model" class="nav-link" data-scroll-target="#probabilistic-model">3. Probabilistic model</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation">4. Parameter estimation</a></li>
  <li><a href="#decision-rules" id="toc-decision-rules" class="nav-link" data-scroll-target="#decision-rules">5. Decision rules</a></li>
  <li><a href="#training-procedure" id="toc-training-procedure" class="nav-link" data-scroll-target="#training-procedure">6. Training procedure</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">7. Examples</a>
  <ul class="collapse">
  <li><a href="#categorical" id="toc-categorical" class="nav-link" data-scroll-target="#categorical">7.1 Categorical</a></li>
  <li><a href="#poisson" id="toc-poisson" class="nav-link" data-scroll-target="#poisson">7.2 Poisson</a></li>
  <li><a href="#sequence-labelling" id="toc-sequence-labelling" class="nav-link" data-scroll-target="#sequence-labelling">7.3 Sequence labelling</a></li>
  </ul></li>
  <li><a href="#exercise-design-your-own-deep-probabilistic-model-optional-non-graded" id="toc-exercise-design-your-own-deep-probabilistic-model-optional-non-graded" class="nav-link" data-scroll-target="#exercise-design-your-own-deep-probabilistic-model-optional-non-graded">8. Exercise: design your own deep probabilistic model (optional, non-graded)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DPM1 - Deep Probabilistic Models I</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab"></a><br>
<strong>Recordings:</strong> <a href="https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/57053ad80d2847888ef9aefcba2574a61d">Lecture 1.1</a> &amp; <a href="https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/a30fffc800524283809bd69b27a11a351d">Lecture 1.2</a><br>
<strong>Authors</strong>: Wilker Aziz &amp; Bryan Eikema</p>
<section id="general-notes" class="level2">
<h2 class="anchored" data-anchor-id="general-notes">0. General notes</h2>
<p>This notebook illustrates the concepts discussed in the module <strong>Deep probabilistic models I</strong> offered within DL2.</p>
<p>The examples in the notebook are based on NLP datasets, but the concepts are general enough that you can apply them in any domain. At the end of the notebook, we invite you to try your favourite dataset (as long as you can design an encoder for the data type you are interested in, transferring ideas from this notebook should be fairly simple).</p>
<p>The notebook starts with modelling univariate response variables conditionally, given a high-dimensional and structured input. We look into a nominal response variable and then into a numerical one. Next, we look into a structured response variable.</p>
<section id="ilos" class="level3">
<h3 class="anchored" data-anchor-id="ilos">0.1 ILOs</h3>
<ul>
<li>Prescribe joint distributions using PyTorch</li>
<li>Estimate the parameters of the model via maximum likelihood estimation</li>
<li>Implement a decision rule</li>
</ul>
</section>
<section id="setting-up" class="level3">
<h3 class="anchored" data-anchor-id="setting-up">0.2 Setting up</h3>
<div id="cell-6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> nltk</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> sklearn</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Install nltk and sklearn if necessary</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet nltk, sklearn</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> nltk</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seed_all(seed<span class="op">=</span><span class="dv">42</span>):    </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>seed_all()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">1. Data</h2>
<p>In this tutorial we will design conditional models involving structured data. We use text as an example and design</p>
<ol type="1">
<li>a classifier</li>
<li>an ordinal regressor</li>
<li>a sequence labeller</li>
</ol>
<p>At the end of the tutorial you are encouraged to design similar models for a different type of data (for example, consider a different modality of data).</p>
<div id="cell-8" class="cell" data-outputid="57d80aa0-36a1-4b4e-8e8c-1c521446e4c8" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'treebank'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'brown'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'universal_tagset'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package treebank to /home/phillip/nltk_data...
[nltk_data]   Package treebank is already up-to-date!
[nltk_data] Downloading package brown to /home/phillip/nltk_data...
[nltk_data]   Unzipping corpora/brown.zip.
[nltk_data] Downloading package punkt to /home/phillip/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package universal_tagset to
[nltk_data]     /home/phillip/nltk_data...
[nltk_data]   Package universal_tagset is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>True</code></pre>
</div>
</div>
<section id="sentiment" class="level3">
<h3 class="anchored" data-anchor-id="sentiment">1.1 Sentiment</h3>
<div id="cell-10" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> treebank, brown</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>brown</code> corpus contains texts organized into 15 genres.</p>
<div id="cell-12" class="cell" data-outputid="ef805f58-4d6b-4bdb-f489-8a83c1f0acfc" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>brown.categories()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>['adventure',
 'belles_lettres',
 'editorial',
 'fiction',
 'government',
 'hobbies',
 'humor',
 'learned',
 'lore',
 'mystery',
 'news',
 'religion',
 'reviews',
 'romance',
 'science_fiction']</code></pre>
</div>
</div>
<p>The dataset is already preprocessed for us:</p>
<div id="cell-14" class="cell" data-outputid="6b8eda51-83e5-4531-ae33-d4e637e00116" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'fiction' examples"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">3</span>), brown.sents(categories<span class="op">=</span>[<span class="st">'fiction'</span>])):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i, x)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'religion' examples"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">3</span>), brown.sents(categories<span class="op">=</span>[<span class="st">'religion'</span>])):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i, x)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'learned' examples"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">3</span>), brown.sents(categories<span class="op">=</span>[<span class="st">'learned'</span>])):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>'fiction' examples
0 ['Thirty-three']
1 ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.']
2 ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']

'religion' examples
0 ['As', 'a', 'result', ',', 'although', 'we', 'still', 'make', 'use', 'of', 'this', 'distinction', ',', 'there', 'is', 'much', 'confusion', 'as', 'to', 'the', 'meaning', 'of', 'the', 'basic', 'terms', 'employed', '.']
1 ['Just', 'what', 'is', 'meant', 'by', '``', 'spirit', "''", 'and', 'by', '``', 'matter', "''", '?', '?']
2 ['The', 'terms', 'are', 'generally', 'taken', 'for', 'granted', 'as', 'though', 'they', 'referred', 'to', 'direct', 'and', 'axiomatic', 'elements', 'in', 'the', 'common', 'experience', 'of', 'all', '.']

'learned' examples
0 ['1', '.']
1 ['Introduction']
2 ['It', 'has', 'recently', 'become', 'practical', 'to', 'use', 'the', 'radio', 'emission', 'of', 'the', 'moon', 'and', 'planets', 'as', 'a', 'new', 'source', 'of', 'information', 'about', 'these', 'bodies', 'and', 'their', 'atmospheres', '.']</code></pre>
</div>
</div>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_nltk_categorised_corpus(corpus, categories, max_length<span class="op">=</span><span class="dv">30</span>, num_heldout<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shuffle and split a corpus.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    corpus: a corpus of tagged sequences, each sequence is a pair, each pair is a token and a tag.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    max_length: discard sentences longer than this</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Return: </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">        (training word sequences, training tag sequences), </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">        (dev word sequences, dev tag sequences), </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">        (test word sequences, test tag sequences),         </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> []</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, c <span class="kw">in</span> <span class="bu">enumerate</span>(categories):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        seqs <span class="op">=</span> corpus.sents(categories<span class="op">=</span>[c])</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        sentences.extend(seqs)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        labels.extend(<span class="bu">len</span>(seqs) <span class="op">*</span> [k])    </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># do not change the seed in here    </span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> np.random.RandomState(<span class="dv">42</span>).permutation(np.arange(<span class="bu">len</span>(sentences)))   </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    shuffled_sentences <span class="op">=</span> [[w <span class="cf">for</span> w <span class="kw">in</span> sentences[i]] <span class="cf">for</span> i <span class="kw">in</span> order <span class="cf">if</span> <span class="bu">len</span>(sentences[i]) <span class="op">&lt;=</span> max_length]    </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    shuffled_labels <span class="op">=</span> [labels[i] <span class="cf">for</span> i <span class="kw">in</span> order <span class="cf">if</span> <span class="bu">len</span>(sentences[i]) <span class="op">&lt;=</span> max_length] </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (shuffled_sentences[<span class="dv">2</span><span class="op">*</span>num_heldout:], shuffled_labels[<span class="dv">2</span><span class="op">*</span>num_heldout:]), (shuffled_sentences[num_heldout:<span class="dv">2</span><span class="op">*</span>num_heldout], shuffled_labels[num_heldout:<span class="dv">2</span><span class="op">*</span>num_heldout]), (shuffled_sentences[:num_heldout], shuffled_labels[:num_heldout])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-outputid="efd43d5c-2663-4256-c1fe-8cf35c029c99" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>(cat_training_x, cat_training_y), (cat_dev_x, cat_dev_y), (cat_test_x, cat_test_y) <span class="op">=</span> split_nltk_categorised_corpus(brown, brown.categories(), num_heldout<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of sentences: training=</span><span class="sc">{</span><span class="bu">len</span>(cat_training_x)<span class="sc">}</span><span class="ss"> dev=</span><span class="sc">{</span><span class="bu">len</span>(cat_dev_x)<span class="sc">}</span><span class="ss"> test=</span><span class="sc">{</span><span class="bu">len</span>(cat_test_x)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of sentences: training=44685 dev=1000 test=1000
CPU times: user 3.07 s, sys: 59.2 ms, total: 3.13 s
Wall time: 3.13 s</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-outputid="b919ca11-3030-424b-c3e6-2481fe66bda5" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(cat_training_x), <span class="bu">len</span>(cat_dev_x), <span class="bu">len</span>(cat_test_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(44685, 1000, 1000)</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-outputid="8ad96d01-4afa-43f2-ac0e-6b3bda3b7ea7" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>cat_training_y[<span class="dv">0</span>], cat_training_x[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(1,
 ['That',
  'is',
  'why',
  'the',
  'members',
  'of',
  'the',
  'beat',
  'generation',
  'proudly',
  'assume',
  'the',
  'title',
  'of',
  'the',
  'holy',
  'barbarians',
  ';',
  ';'])</code></pre>
</div>
</div>
</section>
<section id="age" class="level3">
<h3 class="anchored" data-anchor-id="age">1.2 Age</h3>
<p><a href="http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf">Schler et al (2006)</a> collected a dataset of blog posts annotated for the age of the author, we will use a subset of that dataset. The complete dataset can be found <a href="https://www.kaggle.com/rtatman/blog-authorship-corpus">on Kaggle</a>.</p>
<p>Warning: we will use this dataset to illustrate Poisson regression, but note that in general age may be a protected attribute in certain applications, and one should carefully consider the implications of designing and deploying an age detection system.</p>
<div id="cell-20" class="cell" data-outputid="666c6c5f-9d51-4ba0-f9df-94addf5e0356" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>surfdrive.surf.nl<span class="op">/</span>files<span class="op">/</span>index.php<span class="op">/</span>s<span class="op">/</span><span class="dv">2</span><span class="er">xWdFxnewjN9gsq</span><span class="op">/</span>download <span class="op">-</span>O blog<span class="op">-</span>authorship.json.gz</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>gzip <span class="op">-</span>d blog<span class="op">-</span>authorship.json.gz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2022-04-05 17:42:50--  https://surfdrive.surf.nl/files/index.php/s/2xWdFxnewjN9gsq/download
Resolving surfdrive.surf.nl (surfdrive.surf.nl)... 145.100.27.67, 2001:610:108:203b:0:a11:da7a:5afe
Connecting to surfdrive.surf.nl (surfdrive.surf.nl)|145.100.27.67|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 25699440 (25M) [application/gzip]
Saving to: ‘blog-authorship.json.gz’

blog-authorship.jso 100%[===================&gt;]  24.51M  16.1MB/s    in 1.5s    

2022-04-05 17:42:52 (16.1 MB/s) - ‘blog-authorship.json.gz’ saved [25699440/25699440]
</code></pre>
</div>
</div>
<div id="cell-21" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"blog-authorship.json"</span>) <span class="im">as</span> f:</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    blog_data <span class="op">=</span> json.load(f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-22" class="cell" data-outputid="118aca69-7279-4047-d883-b511a2a550e9" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>blog_data.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>dict_keys(['source', 'ack', 'subset', 'format', 'training', 'dev', 'test'])</code></pre>
</div>
</div>
<div id="cell-23" class="cell" data-outputid="3a37bd30-fd61-47ce-936d-528d46bcd44a" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>blog_data[<span class="st">'source'</span>], blog_data[<span class="st">'ack'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>('[Blog authorship corpus](https://www.kaggle.com/rtatman/blog-authorship-corpus)',
 '[Schler et al (2006)](http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf)')</code></pre>
</div>
</div>
<p>We already have a training/dev/test split, each data point is a pair and the text is not preprocessed for us:</p>
<div id="cell-25" class="cell" data-outputid="5910fea0-93b3-482f-9fc8-e43e8d8b3169" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(blog_data[<span class="st">'training'</span>]), <span class="bu">len</span>(blog_data[<span class="st">'dev'</span>]), <span class="bu">len</span>(blog_data[<span class="st">'test'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(200000, 40000, 49174)</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-outputid="f448c512-e315-4f33-a044-cf97166796f1" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>blog_data[<span class="st">'training'</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>['I hate exams considering I just failed one. Yippee', 16]</code></pre>
</div>
</div>
<p>We need to tokenize the data for use in deep learning. We will use the NLTK word tokenizer for this. We also lowercase the inputs and shuffle the training data. This will take some 3 minutes.</p>
<div id="cell-28" class="cell" data-outputid="43d13643-6830-461b-d33f-f265febacbed" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize and shuffle the data.</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> np.random.RandomState(<span class="dv">42</span>).permutation(np.arange(<span class="bu">len</span>(blog_data[<span class="st">"training"</span>])))   </span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>blog_training_x <span class="op">=</span> [word_tokenize(blog_data[<span class="st">"training"</span>][i][<span class="dv">0</span>].lower()) <span class="cf">for</span> i <span class="kw">in</span> order]</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>blog_training_y <span class="op">=</span> [[blog_data[<span class="st">"training"</span>][i][<span class="dv">1</span>]] <span class="cf">for</span> i <span class="kw">in</span> order]</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>blog_dev_x <span class="op">=</span> [word_tokenize(x[<span class="dv">0</span>].lower()) <span class="cf">for</span> x <span class="kw">in</span> blog_data[<span class="st">"dev"</span>]]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>blog_dev_y <span class="op">=</span> [[x[<span class="dv">1</span>]] <span class="cf">for</span> x <span class="kw">in</span> blog_data[<span class="st">"dev"</span>]]</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>blog_test_x <span class="op">=</span> [word_tokenize(x[<span class="dv">0</span>].lower()) <span class="cf">for</span> x <span class="kw">in</span> blog_data[<span class="st">"test"</span>]]</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>blog_test_y <span class="op">=</span> [[x[<span class="dv">1</span>]] <span class="cf">for</span> x <span class="kw">in</span> blog_data[<span class="st">"test"</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1min 10s, sys: 306 ms, total: 1min 11s
Wall time: 1min 11s</code></pre>
</div>
</div>
<div id="cell-29" class="cell" data-outputid="00609d42-6017-43b6-f8b9-bc73eb5df13a" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example pre-processed data point</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"input: </span><span class="sc">{</span>blog_training_x[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"age of author: </span><span class="sc">{</span>blog_training_y[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>input: ['urllink', 'this', 'is', 'my', 'friend', 'sam', '.', 'he', "'s", 'a', 'little', 'weird', '.', 'he', "'s", 'nice', 'though', '.', 'but', 'weird', '.']
age of author: [17]</code></pre>
</div>
</div>
</section>
<section id="syntactic-categories" class="level3">
<h3 class="anchored" data-anchor-id="syntactic-categories">1.3 Syntactic categories</h3>
<p>From NLTK, we will also take the <code>treebank</code> corpus where words are annotated with their syntactic categories (parts of speech, or POS tags).</p>
<p>The method <code>tagged_sents</code> will give us a view of tokenized sentences with their token lag tag annotation:</p>
<div id="cell-33" class="cell" data-outputid="b769ecc8-8add-4658-e671-971aaa846e7a" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>example <span class="op">=</span> treebank.tagged_sents(tagset<span class="op">=</span><span class="st">'universal'</span>)[<span class="dv">0</span>]  <span class="co"># 'universal' here refers to the style of tags</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>example</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>[('Pierre', 'NOUN'),
 ('Vinken', 'NOUN'),
 (',', '.'),
 ('61', 'NUM'),
 ('years', 'NOUN'),
 ('old', 'ADJ'),
 (',', '.'),
 ('will', 'VERB'),
 ('join', 'VERB'),
 ('the', 'DET'),
 ('board', 'NOUN'),
 ('as', 'ADP'),
 ('a', 'DET'),
 ('nonexecutive', 'ADJ'),
 ('director', 'NOUN'),
 ('Nov.', 'NOUN'),
 ('29', 'NUM'),
 ('.', '.')]</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_nltk_tagged_corpus(corpus, max_length<span class="op">=</span><span class="dv">30</span>, num_heldout<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shuffle and split a corpus.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">    corpus: a corpus of tagged sequences, each sequence is a pair, each pair is a token and a tag.</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">    max_length: discard sentences longer than this</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Return: </span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">        (training word sequences, training tag sequences), </span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        (dev word sequences, dev tag sequences), </span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">        (test word sequences, test tag sequences),         </span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    tagged_sentences <span class="op">=</span> corpus.tagged_sents(tagset<span class="op">=</span><span class="st">'universal'</span>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># do not change the seed in here    </span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> np.random.RandomState(<span class="dv">42</span>).permutation(np.arange(<span class="bu">len</span>(tagged_sentences)))    </span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    word_sequences <span class="op">=</span> [[w.lower() <span class="cf">for</span> w, t <span class="kw">in</span> tagged_sentences[i]] <span class="cf">for</span> i <span class="kw">in</span> order <span class="cf">if</span> <span class="bu">len</span>(tagged_sentences[i]) <span class="op">&lt;=</span> max_length]    </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    tag_sequences <span class="op">=</span> [[t <span class="cf">for</span> w, t <span class="kw">in</span> tagged_sentences[i]] <span class="cf">for</span> i <span class="kw">in</span> order <span class="cf">if</span> <span class="bu">len</span>(tagged_sentences[i]) <span class="op">&lt;=</span> max_length]    </span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (word_sequences[<span class="dv">2</span><span class="op">*</span>num_heldout:], tag_sequences[<span class="dv">2</span><span class="op">*</span>num_heldout:]), (word_sequences[num_heldout:<span class="dv">2</span><span class="op">*</span>num_heldout], tag_sequences[num_heldout:<span class="dv">2</span><span class="op">*</span>num_heldout]), (word_sequences[:num_heldout], tag_sequences[:num_heldout])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For <code>treebank</code> this will take about 10 seconds.</p>
<div id="cell-36" class="cell" data-outputid="040a3f0a-fdac-46d7-cdba-929983380ac1" data-execution_count="20">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>(tagger_training_x, tagger_training_y), (tagger_dev_x, tagger_dev_y), (tagger_test_x, tagger_test_y) <span class="op">=</span> split_nltk_tagged_corpus(treebank, num_heldout<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of sentences: training=</span><span class="sc">{</span><span class="bu">len</span>(tagger_training_x)<span class="sc">}</span><span class="ss"> dev=</span><span class="sc">{</span><span class="bu">len</span>(tagger_dev_x)<span class="sc">}</span><span class="ss"> test=</span><span class="sc">{</span><span class="bu">len</span>(tagger_test_x)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of sentences: training=2486 dev=100 test=100
CPU times: user 2.38 s, sys: 116 ms, total: 2.5 s
Wall time: 2.52 s</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-outputid="36947f85-8f6e-4987-db1e-2c1027d322ef" data-execution_count="21">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"# A few training sentences</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):    </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x_</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>tagger_training_x[n]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"y_</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>tagger_training_y[n]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A few training sentences

x_0 = ['they', 'know', '0', 'he', 'is', 'generally', 'opposed', 'to', 'cop-killer', 'bullets', ',', 'but', 'that', 'he', 'had', 'some', 'reservations', 'about', 'the', 'language', 'in', 'the', 'legislation', '.', "''"]
y_0 = ['PRON', 'VERB', 'X', 'PRON', 'VERB', 'ADV', 'VERB', 'PRT', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', '.', '.']

x_1 = ['california', "'s", 'education', 'department', 'suspects', 'adult', 'responsibility', 'for', 'erasures', 'at', '40', 'schools', 'that', '*t*-85', 'changed', 'wrong', 'answers', 'to', 'right', 'ones', 'on', 'a', 'statewide', 'test', '.']
y_1 = ['NOUN', 'PRT', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NUM', 'NOUN', 'DET', 'X', 'VERB', 'ADJ', 'NOUN', 'PRT', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.']

x_2 = ['the', 'loan', 'may', 'be', 'extended', '*-1', 'by', 'the', 'mcalpine', 'group', 'for', 'an', 'additional', 'year', 'with', 'an', 'increase', 'in', 'the', 'conversion', 'price', 'to', '$', '2.50', '*u*', 'a', 'share', '.']
y_2 = ['DET', 'NOUN', 'VERB', 'VERB', 'VERB', 'X', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'PRT', '.', 'NUM', 'X', 'DET', 'NOUN', '.']
</code></pre>
</div>
</div>
</section>
<section id="vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="vocabulary">1.4 Vocabulary</h3>
<p>As always when dealing with NLP models, we need an object to maintain our vocabulary of known tokens. We will rely on word-tokenization.</p>
<p>Our vocabulary class will maintain the set of known tokens, and a dictionary to convert tokens to codes and codes back to tokens. The class will also take care of some special symbols (e.g., BOS, EOS, UNK, PAD).</p>
<p>Finally, if later on you test your model on sentences that are not word tokenized, you can use <code>nlt.tokenize.word_tokenize</code> or any other tokenizer you like (as long as the level of tokenization is similar to the one you used for training your model.</p>
<p>This class will be used for maintaining both the vocabulary of known tokens and the set of known tags for the tagger data.</p>
<div id="cell-40" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter, OrderedDict</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Vocab:</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, corpus: <span class="bu">list</span>, min_freq<span class="op">=</span><span class="dv">1</span>):        </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus: list of documents, each document is a list of tokens, each token is a string</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">        min_freq: words that occur less than this value are discarded</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count word occurrences</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        counter <span class="op">=</span> Counter(chain(<span class="op">*</span>corpus))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sort them by frequency</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        sorted_by_freq_tuples <span class="op">=</span> <span class="bu">sorted</span>(counter.items(), key<span class="op">=</span><span class="kw">lambda</span> pair: pair[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Special tokens</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pad_token <span class="op">=</span> <span class="st">"-PAD-"</span> <span class="co"># used to fill sequences in a batch to maximum sequence length</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bos_token <span class="op">=</span> <span class="st">"-BOS-"</span> <span class="co"># begin of sequence</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eos_token <span class="op">=</span> <span class="st">"-EOS-"</span> <span class="co"># end of sequence</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unk_token <span class="op">=</span> <span class="st">"-UNK-"</span> <span class="co"># unknown symbol</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pad_id <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bos_id <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eos_id <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unk_id <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.known_symbols <span class="op">=</span> [<span class="va">self</span>.pad_token, <span class="va">self</span>.bos_token, <span class="va">self</span>.eos_token, <span class="va">self</span>.unk_token]</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Vocabulary</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2id <span class="op">=</span> OrderedDict()                </span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2id[<span class="va">self</span>.pad_token] <span class="op">=</span> <span class="va">self</span>.pad_id        </span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2id[<span class="va">self</span>.bos_token] <span class="op">=</span> <span class="va">self</span>.bos_id</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2id[<span class="va">self</span>.eos_token] <span class="op">=</span> <span class="va">self</span>.eos_id</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2id[<span class="va">self</span>.unk_token] <span class="op">=</span> <span class="va">self</span>.unk_id</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_freq <span class="op">=</span> min_freq</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> w, n <span class="kw">in</span> sorted_by_freq_tuples: </span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> n <span class="op">&gt;=</span> min_freq: <span class="co"># discard infrequent words</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.word2id[w] <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.known_symbols)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.known_symbols.append(w)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.counts.append(n)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the counts as well</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts <span class="op">=</span> np.array(<span class="va">self</span>.counts)</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.known_symbols)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, word: <span class="bu">str</span>): </span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the id (int) of a word (str)</span></span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.word2id.get(word, <span class="va">self</span>.unk_id)</span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, doc: <span class="bu">list</span>, add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">False</span>, pad_right<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Transform a document into a numpy array of integer token identifiers.</span></span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a><span class="co">        doc: list of tokens, each token is a string</span></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="co">        add_bos: whether to add the BOS token</span></span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a><span class="co">        add_eos: whether to add the EOS token</span></span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a><span class="co">        pad_right: number of suffix padding tokens </span></span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: a list of codes (possibly with BOS and EOS added as well as padding)</span></span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.word2id.get(w, <span class="va">self</span>.unk_id) <span class="cf">for</span> w <span class="kw">in</span> chain([<span class="va">self</span>.bos_token] <span class="op">*</span> <span class="bu">int</span>(add_bos), doc, [<span class="va">self</span>.eos_token] <span class="op">*</span> <span class="bu">int</span>(add_eos), [<span class="va">self</span>.pad_token] <span class="op">*</span> pad_right)]</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> batch_encode(<span class="va">self</span>, docs: <span class="bu">list</span>, add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a><span class="co">        Transform a batch of documents into a numpy array of integer token identifiers.</span></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a><span class="co">        This will pad the shorter documents to the length of the longest document.</span></span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a><span class="co">        docs: a list of documents</span></span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a><span class="co">        add_bos: whether to add the BOS token</span></span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a><span class="co">        add_eos: whether to add the EOS token</span></span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a><span class="co">        pad_right: number of suffix padding tokens</span></span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: numpy array with shape [len(docs), longest_doc + add_bos + add_eos]</span></span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>        max_len <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(doc) <span class="cf">for</span> doc <span class="kw">in</span> docs)</span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array([<span class="va">self</span>.encode(doc, add_bos<span class="op">=</span>add_bos, add_eos<span class="op">=</span>add_eos, pad_right<span class="op">=</span>max_len<span class="op">-</span><span class="bu">len</span>(doc)) <span class="cf">for</span> doc <span class="kw">in</span> docs])</span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids, strip_pad<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Transform a np.array document into a list of tokens.</span></span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a><span class="co">        ids: np.array with shape [num_tokens] </span></span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a><span class="co">        strip_pad: whether PAD tokens should be deleted from the output</span></span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: list of strings with size [num_tokens - num_padding]</span></span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> strip_pad:</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> [<span class="va">self</span>.known_symbols[<span class="bu">id</span>] <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> ids <span class="cf">if</span> <span class="bu">id</span> <span class="op">!=</span> <span class="va">self</span>.pad_id]</span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb39-94"><a href="#cb39-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> [<span class="va">self</span>.known_symbols[<span class="bu">id</span>] <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> ids]</span>
<span id="cb39-95"><a href="#cb39-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-96"><a href="#cb39-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> batch_decode(<span class="va">self</span>, docs, strip_pad<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-97"><a href="#cb39-97" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-98"><a href="#cb39-98" aria-hidden="true" tabindex="-1"></a><span class="co">        Transform a np.array collection of documents into a collection of lists of tokens.</span></span>
<span id="cb39-99"><a href="#cb39-99" aria-hidden="true" tabindex="-1"></a><span class="co">        ids: np.array with shape [num_docs, max_length] </span></span>
<span id="cb39-100"><a href="#cb39-100" aria-hidden="true" tabindex="-1"></a><span class="co">        strip_pad: whether PAD tokens should be deleted from the output</span></span>
<span id="cb39-101"><a href="#cb39-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-102"><a href="#cb39-102" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: list of documents, each a list of tokens, each token a string</span></span>
<span id="cb39-103"><a href="#cb39-103" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-104"><a href="#cb39-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.decode(doc, strip_pad<span class="op">=</span>strip_pad) <span class="cf">for</span> doc <span class="kw">in</span> docs]    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see how this works on the tagger corpus:</p>
<div id="cell-42" class="cell" data-outputid="b85f1343-991d-4c75-b5a8-e03add25d791" data-execution_count="23">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We get a vocabulary for words</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>word_vocab <span class="op">=</span> Vocab(tagger_training_x, min_freq<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and a vocabulary for tags</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>tag_vocab <span class="op">=</span> Vocab(tagger_training_y, min_freq<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># You can see their sizes V and C:</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(word_vocab), <span class="bu">len</span>(tag_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>(3358, 16)</code></pre>
</div>
</div>
<p>The <code>encode</code> method turns a sequence of (str) symbols into a sequence of (int) codes:</p>
<p>We can also have <code>encode</code> add some special symbols for us (but remember to be consistent, you should always have token sequences and tag sequences that match in length):</p>
<p>We can also encode and decode entire batches of sequences. This will use pad symbols/codes to make the sequences in the same batch have the same length:</p>
<div id="cell-46" class="cell" data-outputid="81f3973a-024a-4d78-a16f-a4adfb0a2333" data-execution_count="24">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>word_vocab.batch_encode(tagger_training_x[:<span class="dv">3</span>], add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>array([[  45,  907,   13,   36,   18,  600, 1078,    8, 1651, 1652,    6,
          41,   19,   36,   66,   71, 2194,   55,    5, 2195,   10,    5,
         487,    4,   21,    2,    0,    0,    0],
       [ 488,   14, 1309,  156, 2196,    3,    3,   16,    3,   31,  449,
         908,   19,    3,  601,  909,  772,    8,  343,  910,   26,    9,
        2197,  344,    4,    2,    0,    0,    0],
       [   5,  542,  129,   40, 2198,   12,   34,    5, 1310,  264,   16,
          44,  378,   54,   37,   44,  324,   10,    5, 2199,  186,    8,
          25, 1311,   23,    9,  111,    4,    2]])</code></pre>
</div>
</div>
<div id="cell-47" class="cell" data-outputid="b69d2f4a-a522-4754-beb6-1c7bf6a89654" data-execution_count="25">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>word_vocab.batch_decode(word_vocab.batch_encode(tagger_training_x[:<span class="dv">3</span>], add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">True</span>), strip_pad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>[['they',
  'know',
  '0',
  'he',
  'is',
  'generally',
  'opposed',
  'to',
  'cop-killer',
  'bullets',
  ',',
  'but',
  'that',
  'he',
  'had',
  'some',
  'reservations',
  'about',
  'the',
  'language',
  'in',
  'the',
  'legislation',
  '.',
  "''",
  '-EOS-'],
 ['california',
  "'s",
  'education',
  'department',
  'suspects',
  '-UNK-',
  '-UNK-',
  'for',
  '-UNK-',
  'at',
  '40',
  'schools',
  'that',
  '-UNK-',
  'changed',
  'wrong',
  'answers',
  'to',
  'right',
  'ones',
  'on',
  'a',
  'statewide',
  'test',
  '.',
  '-EOS-'],
 ['the',
  'loan',
  'may',
  'be',
  'extended',
  '*-1',
  'by',
  'the',
  'mcalpine',
  'group',
  'for',
  'an',
  'additional',
  'year',
  'with',
  'an',
  'increase',
  'in',
  'the',
  'conversion',
  'price',
  'to',
  '$',
  '2.50',
  '*u*',
  'a',
  'share',
  '.',
  '-EOS-']]</code></pre>
</div>
</div>
</section>
<section id="corpus-and-data-loader" class="level3">
<h3 class="anchored" data-anchor-id="corpus-and-data-loader">1.5 Corpus and Data Loader</h3>
<p>We will be developing our models in torch, thus we need to wrap our corpus into a <code>Dataset</code> and a <code>DataLoader</code>:</p>
<div id="cell-49" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextRegressionCorpus(Dataset):</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Use this to give torch access to a corpus of documents annotated with a simple response variable (e.g., category or real number)</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co">    This class will also know the vocab objects for tokens</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co">    and it will take care of coding strings into integers consistently.</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, corpus_x, corpus_y, vocab_x: Vocab):</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">        In PyTorch we better always manipulate numerical codes, rather than text.</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">        So, our Corpus object will contain a vocab that converts words to codes.</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_x: token sequences</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_y: response values</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_x: vocabulary of input symbols</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.corpus_x <span class="op">=</span> <span class="bu">list</span>(corpus_x)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.corpus_y <span class="op">=</span> <span class="bu">list</span>(corpus_y)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_x) <span class="op">!=</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_y):</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"I need pairs"</span>)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_x <span class="op">=</span> vocab_x</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Size of the corpus in number of sequence pairs"""</span></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_x)</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return corpus_x[idx] and corpus_y[idx] converted to codes and with the EOS code in the end"""</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.vocab_x.encode(<span class="va">self</span>.corpus_x[idx], add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.corpus_y[idx]</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, y</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pad_to_longest(<span class="va">self</span>, pairs, pad_id<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Take a list of coded sequences and returns a torch tensor where </span></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a><span class="co">        every sentence has the same length (by means of using PAD tokens)</span></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>        longest <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x, y <span class="kw">in</span> pairs)</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>        batch_x <span class="op">=</span> torch.tensor([x <span class="op">+</span> [<span class="va">self</span>.vocab_x.pad_id] <span class="op">*</span> (longest <span class="op">-</span> <span class="bu">len</span>(x)) <span class="cf">for</span> x, y <span class="kw">in</span> pairs])         </span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>        batch_y <span class="op">=</span> torch.tensor([y <span class="cf">for</span> x, y <span class="kw">in</span> pairs]) </span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> batch_x, batch_y</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ParallelCorpus(Dataset):</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="co">    Use this to give torch access to a corpus of sequence pairs.</span></span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a><span class="co">    This class will also know the vocab objects for the two streams. </span></span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a><span class="co">    and it will take care of coding strings into integers consistently.</span></span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, corpus_x, corpus_y, vocab_x: Vocab, vocab_y: Vocab):</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a><span class="co">        In PyTorch we better always manipulate numerical codes, rather than text.</span></span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a><span class="co">        So, our Corpus object will contain a vocab that converts words to codes.</span></span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_x: token sequences</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_y: tag sequences</span></span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_x: vocabulary for token sequences</span></span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_y: vocabulary for tag sequences</span></span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.corpus_x <span class="op">=</span> <span class="bu">list</span>(corpus_x)</span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.corpus_y <span class="op">=</span> <span class="bu">list</span>(corpus_y)</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_x) <span class="op">==</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_y), <span class="st">"I need sequence pairs"</span></span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_x <span class="op">=</span> vocab_x</span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_y <span class="op">=</span> vocab_y</span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Size of the corpus in number of sequence pairs"""</span></span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.corpus_x)</span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a><span class="co">        Return corpus_x[idx] and corpus_y[idx] converted to codes </span></span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a><span class="co">        the latter has the EOS code in the end            </span></span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.vocab_x.encode(<span class="va">self</span>.corpus_x[idx], add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.vocab_y.encode(<span class="va">self</span>.corpus_y[idx], add_bos<span class="op">=</span><span class="va">False</span>, add_eos<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, y</span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pad_to_longest(<span class="va">self</span>, pairs, pad_id<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Take a list of coded sequences and returns a torch tensor where </span></span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a><span class="co">        every sentence has the same length (by means of using PAD tokens)</span></span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-88"><a href="#cb46-88" aria-hidden="true" tabindex="-1"></a>        longest_x <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x, y <span class="kw">in</span> pairs)</span>
<span id="cb46-89"><a href="#cb46-89" aria-hidden="true" tabindex="-1"></a>        longest_y <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(y) <span class="cf">for</span> x, y <span class="kw">in</span> pairs)</span>
<span id="cb46-90"><a href="#cb46-90" aria-hidden="true" tabindex="-1"></a>        batch_x <span class="op">=</span> torch.tensor([x <span class="op">+</span> [<span class="va">self</span>.vocab_x.pad_id] <span class="op">*</span> (longest_x <span class="op">-</span> <span class="bu">len</span>(x)) <span class="cf">for</span> x, y <span class="kw">in</span> pairs]) </span>
<span id="cb46-91"><a href="#cb46-91" aria-hidden="true" tabindex="-1"></a>        batch_y <span class="op">=</span> torch.tensor([y <span class="op">+</span> [<span class="va">self</span>.vocab_y.pad_id] <span class="op">*</span> (longest_y <span class="op">-</span> <span class="bu">len</span>(y)) <span class="cf">for</span> x, y <span class="kw">in</span> pairs]) </span>
<span id="cb46-92"><a href="#cb46-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> batch_x, batch_y    </span>
<span id="cb46-93"><a href="#cb46-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-94"><a href="#cb46-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-95"><a href="#cb46-95" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TaggedCorpus(ParallelCorpus):</span>
<span id="cb46-96"><a href="#cb46-96" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-97"><a href="#cb46-97" aria-hidden="true" tabindex="-1"></a><span class="co">    Use this to give torch access to a corpus of tagged sequences.</span></span>
<span id="cb46-98"><a href="#cb46-98" aria-hidden="true" tabindex="-1"></a><span class="co">    This class will also know the vocab objects for tokens and tags, </span></span>
<span id="cb46-99"><a href="#cb46-99" aria-hidden="true" tabindex="-1"></a><span class="co">    and it will take care of coding strings into integers consistently.</span></span>
<span id="cb46-100"><a href="#cb46-100" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-101"><a href="#cb46-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-102"><a href="#cb46-102" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, corpus_x, corpus_y, vocab_x: Vocab, vocab_y: Vocab):</span>
<span id="cb46-103"><a href="#cb46-103" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb46-104"><a href="#cb46-104" aria-hidden="true" tabindex="-1"></a><span class="co">        In PyTorch we better always manipulate numerical codes, rather than text.</span></span>
<span id="cb46-105"><a href="#cb46-105" aria-hidden="true" tabindex="-1"></a><span class="co">        So, our Corpus object will contain a vocab that converts words to codes.</span></span>
<span id="cb46-106"><a href="#cb46-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-107"><a href="#cb46-107" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_x: token sequences</span></span>
<span id="cb46-108"><a href="#cb46-108" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus_y: tag sequences</span></span>
<span id="cb46-109"><a href="#cb46-109" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_x: vocabulary for token sequences</span></span>
<span id="cb46-110"><a href="#cb46-110" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_y: vocabulary for tag sequences</span></span>
<span id="cb46-111"><a href="#cb46-111" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb46-112"><a href="#cb46-112" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(corpus_x, corpus_y, vocab_x, vocab_y)</span>
<span id="cb46-113"><a href="#cb46-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">all</span>(<span class="bu">len</span>(x) <span class="op">==</span> <span class="bu">len</span>(y) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(corpus_x, corpus_y)), <span class="st">"A sequence pair should match in number of steps"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We join our input and output data into torch <code>Dataset</code> objects for training, development and testing. Note that training, development and testing data share the same vocabularies, which were constructed using the training set alone. We can wrap the tagger data around a <code>Dataset</code> object as:</p>
<div id="cell-51" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>tagger_training <span class="op">=</span> TaggedCorpus(tagger_training_x, tagger_training_y, word_vocab, tag_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s an example of how we get a <code>DataLoader</code> for a corpus, we simply choose the <code>Dataset</code> object we want (training/dev/test), the batch size we want, whether we need shuffling (e.g., for training batches in SGD), and how we “glue” data points of different length together (i.e., a function such as <code>pad_to_longest</code> which <code>TextRegressionCorpus</code> and <code>TaggedCorpus</code> provide for us).</p>
<div id="cell-53" class="cell" data-outputid="8e55f218-4ee9-4d7d-b446-ad808075152f" data-execution_count="28">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>batcher <span class="op">=</span> DataLoader(tagger_training, batch_size<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>tagger_training.pad_to_longest)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"# This is how the tagged sequences in a batch come out of the data loader</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(batch_x, batch_y):</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(x)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(y)        </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"# And we can always decode them for inspection</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># stripping padding makes it easier to read the examples</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(word_vocab.batch_decode(batch_x, strip_pad<span class="op">=</span><span class="va">True</span>), tag_vocab.batch_decode(batch_y, strip_pad<span class="op">=</span><span class="va">True</span>)):</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(x)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(y)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># This is how the tagged sequences in a batch come out of the data loader

tensor([  77, 2535,    5,  318,    6,   43,  776,    4,    2,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])
tensor([14,  5,  8,  4,  6, 14,  4,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0])

tensor([   3,    3,   11,    3,   98,   13,   45,   29,    3,    7,  101,  918,
         175,   34, 1491,   15,    8,    3,   49,    3,  748,  213,    4,    2])
tensor([ 4,  4, 15,  4,  5, 10, 14,  5,  9,  7,  8,  4, 10,  7,  4, 10, 13,  5,
        15,  5,  4,  4,  6,  2])

tensor([ 46, 882,  69,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0])
tensor([4, 4, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

# And we can always decode them for inspection

['i', 'loved', 'the', 'school', ',', 'its', 'history', '.', '-EOS-']
['PRON', 'VERB', 'DET', 'NOUN', '.', 'PRON', 'NOUN', '.', '-EOS-']

['-UNK-', '-UNK-', 'and', '-UNK-', 'say', '0', 'they', 'are', '-UNK-', 'of', 'any', 'efforts', '*ich*-1', 'by', 'mcgraw-hill', '*', 'to', '-UNK-', 'or', '-UNK-', 'scoring', 'high', '.', '-EOS-']
['NOUN', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'X', 'PRON', 'VERB', 'ADJ', 'ADP', 'DET', 'NOUN', 'X', 'ADP', 'NOUN', 'X', 'PRT', 'VERB', 'CONJ', 'VERB', 'NOUN', 'NOUN', '.', '-EOS-']

['new', 'account', ':', '-EOS-']
['NOUN', 'NOUN', '.', '-EOS-']
</code></pre>
</div>
</div>
</section>
</section>
<section id="text-encoders" class="level2">
<h2 class="anchored" data-anchor-id="text-encoders">2. Text encoders</h2>
<p>In NLP applications, we often have to <em>encode</em> a piece of text. For example, that is the case in text classification and regression, as well as in sequence labelling.</p>
<p>Assume for example, a text classifier that takes a document <span class="math inline">\(x_{1:l} = \langle x_1, \ldots, x_l \rangle\)</span>, where each token <span class="math inline">\(x_i \in \mathcal W\)</span> is from a finite vocabulary of <span class="math inline">\(V\)</span> tokens and predicts a distribution over <span class="math inline">\(C\)</span> classes from a set <span class="math inline">\(\mathcal T = \{1, \ldots, C\}\)</span>. An encoding function can map <span class="math inline">\(x_{1:l}\)</span> to a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(\mathbf u\)</span>, which we can then transform to a <span class="math inline">\(C\)</span>-dimensional vector of scores using an affine transformation, which in turn we can constrain to the probability simplex via softmax:</p>
<p><span class="math display">\[\begin{align}
Y|X_{1:l}=x_{1:l} &amp;\sim \mathrm{Categorical}(\mathbf g(x_{1:l}; \theta)) \\
\mathbf u &amp;= \mathrm{encode}_D(x_{1:l}; \theta_{\text{enc}}) \\
\mathbf s &amp;= \mathrm{affine}_C(\mathbf u; \theta_{\text{out}}) \\
\mathbf g(x_{1:l}; \theta) &amp;= \mathrm{softmax}(\mathbf s)
\end{align}\]</span></p>
<p>Here I use a subscript to indicate the dimensionality of the output of the function, the named parameters after <code>;</code> are the trainable parameters of the function.</p>
<p>Generally text encoders may return a single output vector per document, or one vector per word in the document.</p>
<div id="cell-55" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, output_dim):</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._output_dim <span class="op">=</span> output_dim</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_dim(<span class="va">self</span>):</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._output_dim</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Return a tensor of shape [batch_size, max_length, output_dim] or</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="co">            [batch_size, output_dim]</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implement me!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We develop a very basic text encoder based on a bidirectional LSTM. In principle, any powerful encoder (including pretrained encoders) can be used, we go ahead with the BiLSTM to keep the tutorial lightweight.</p>
<div id="cell-57" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextEncoder(Encoder):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Given a predictor x this NN parameterises the pdf of the random variable Y|X=x.</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co">    In other words, it predicts the conditional distribution P(Y|X=x).</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co">    When reduce_mean is True the forward will return a single output vector per document.</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">    When reduce_mean is False one vector per word in the document is returned instead.</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, word_embed_dim: <span class="bu">int</span>, hidden_size: <span class="bu">int</span>, reduce_mean<span class="op">=</span><span class="va">False</span>, pad_id<span class="op">=</span><span class="dv">0</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="dv">2</span> <span class="op">*</span> hidden_size)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pad_id <span class="op">=</span> pad_id </span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word_embed_dim <span class="op">=</span> word_embed_dim</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size  </span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word_embed <span class="op">=</span> nn.Embedding(<span class="va">self</span>.vocab_size, embedding_dim<span class="op">=</span>word_embed_dim)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.LSTM(</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>            input_size<span class="op">=</span>word_embed_dim,</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span>hidden_size,</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reduce_mean <span class="op">=</span> reduce_mean</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We begin by embedding the tokens        </span></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, embed_dim]</span></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.word_embed(x)</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, 2*hidden_size*num_layers]</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>        h, _ <span class="op">=</span> <span class="va">self</span>.encoder(h)</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.reduce_mean: <span class="co"># average pooling</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> torch.where((x <span class="op">==</span> <span class="va">self</span>.pad_id).unsqueeze(<span class="op">-</span><span class="dv">1</span>), torch.zeros_like(h), h)            </span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> torch.<span class="bu">sum</span>(h, axis<span class="op">=-</span><span class="dv">2</span>) <span class="op">/</span> torch.<span class="bu">sum</span>((x <span class="op">!=</span> <span class="va">self</span>.pad_id).<span class="bu">float</span>(), <span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="probabilistic-model" class="level2">
<h2 class="anchored" data-anchor-id="probabilistic-model">3. Probabilistic model</h2>
<p>A probabilistic model prescribes the probability measure of a random experiment, in this tutorial we will design models by explicitly parameterising a probability density function (pdf using NNs. When our random variables (rvs) are multivariate or structured we will pick a factorisation of the joint pdf, decide on the statistical family of each factor, and parameterise the factors using NNs.</p>
<p>Generically, we will be modelling the distribution of some random variable <span class="math inline">\(Y\)</span> (univariate, multivariate, structured) conditioned on an assignment of some random variable <span class="math inline">\(X=x\)</span> (typically structured).</p>
<p>For example, <span class="math inline">\(x\)</span> may be a piece of text or an image. The response variable may be a category, a numerical measurement, a vector of attributes/measurements, a data structure (e.g., sequence, tree, graph).</p>
<p>For us, a probabilistic model can at the very least * assign probability density to an assignment <span class="math inline">\(Y=y\)</span> given <span class="math inline">\(X=x\)</span> * sample an assignment of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> and we will use the forward method of a PyTorch Module to parameterise the relevant conditional probability distributions.</p>
<p>Occasionally, we will be able to support other operations such as finding the median, mean, mode, etc.</p>
<div id="cell-59" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A probabilistic model predicts the probability measure of a random experiment. </span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co">    We will predict the distribution of the random variable Y conditioned on </span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co">    an assignment to the random variable X=x. </span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co">    As a modelling mechanism we will use pmfs and pdfs, hence predicting </span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co">    the distribution P(Y|X=x) requires mapping from x to the parameters of a </span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="co">    pmf/pdf for Y, or, in case of multivariate/structured data, a joint pdf </span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co">    whose factorisation we decide upon. </span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="co">    For us, a model needs to satisfy the following desiderata:</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="co">    * parameterising the pdf is tractable</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="co">    * assessing the pdf for a given outcome Y=y|X=x is tractable</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="co">    * drawing samples from P(Y|X=x) is tractable</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="co">    * the pdf p(y|x) is differentiable with respect to the parameters of the </span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="co">      NN that predicts its parameter(s) </span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a><span class="co">      </span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="co">      </span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a><span class="co">    The only aspects of the Model's API that is fixed are:</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a><span class="co">    * log_prob(x, y) -&gt; tensor of floats, one value per instance in the batch</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a><span class="co">    * sample(x, sample_size) -&gt; batch of samples</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a><span class="co">    * forward(...) -&gt; a torch.distribution object</span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a><span class="co">        the signature of the forward method may vary in subclasses</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, event_shape<span class="op">=</span><span class="bu">tuple</span>()):</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a><span class="co">        The event_shape is the shape of the outcome of the rv Y.        </span></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._event_shape <span class="op">=</span> event_shape</span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> event_shape(<span class="va">self</span>):</span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._event_shape</span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_parameters(<span class="va">self</span>):</span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(np.prod(theta.shape) <span class="cf">for</span> theta <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, ...]</span></span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Each type of model will have a different implementation here"</span>)</span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="bu">tuple</span>()):</span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, ...]</span></span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Return a batch of samples from Y|X=x</span></span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Each type of model will have a different implementation here"</span>)</span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, x, y):</span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the log pdf for Y=y|X=x for each pair in the batch.</span></span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a><span class="co">        x: batch_shape + event_shape_x</span></span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a><span class="co">        y: batch_shape + event_shape</span></span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-63"><a href="#cb52-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict the conditional probability distribution using the forward function.</span></span>
<span id="cb52-64"><a href="#cb52-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will return one such probability distribution per batch element.</span></span>
<span id="cb52-65"><a href="#cb52-65" aria-hidden="true" tabindex="-1"></a>        cpds <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x)</span>
<span id="cb52-66"><a href="#cb52-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-67"><a href="#cb52-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Computer the log probability of each element in the batch.</span></span>
<span id="cb52-68"><a href="#cb52-68" aria-hidden="true" tabindex="-1"></a>        logp <span class="op">=</span> cpds.log_prob(y) <span class="co"># [batch_size]        </span></span>
<span id="cb52-69"><a href="#cb52-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-70"><a href="#cb52-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logp  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="parameter-estimation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimation">4. Parameter estimation</h2>
<p>We will estimate parameters using maximum likelihood estimation (MLE), via gradient-based search. This means we need to assess the model’s likelihood given a dataset (or batch) of observations and the likelihood function must be tractable and differentiable with respect to the NN parameters.</p>
<div id="cell-61" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(<span class="va">self</span>, x, y):   </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""    </span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co">    No matter the probabilistic model, the loss is the negative log likelihood </span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co">    of the parameters estimated on a single batch:</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co">        - 1/batch_size * \sum_{s} log P(y[s]|x[s], theta)</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co">    x: batch_shape + event_shape_x</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y: batch_shape + event_shape</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="va">self</span>.log_prob(x<span class="op">=</span>x, y<span class="op">=</span>y).mean(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-62" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distortion(model, dl, device):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrapper for estimating distortion using all data points in a data loader.</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    total_log_prob <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    data_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> dl:</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>            total_log_prob <span class="op">=</span> total_log_prob <span class="op">+</span> model.log_prob(batch_x.to(device), batch_y.to(device)).<span class="bu">sum</span>()</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>            data_size <span class="op">+=</span> batch_x.shape[<span class="dv">0</span>]</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> total_log_prob <span class="op">/</span> data_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="decision-rules" class="level2">
<h2 class="anchored" data-anchor-id="decision-rules">5. Decision rules</h2>
<p>A rational decision maker chooses her actions to maximise expected utility.</p>
<p>Let <span class="math inline">\(u(y, c)\)</span> quantify the benefit in choosing <span class="math inline">\(c \in \mathcal Y\)</span> when <span class="math inline">\(y\)</span> is the truth. When deciding under uncertainty, we solve</p>
<p><span class="math display">\[\begin{align}
y^\star &amp;= \arg\max_{c \in \mathcal Y}~\mathbb E[u(Y, c)|X=x]
\end{align}\]</span></p>
<p>where the expectation is with respect to the pdf <span class="math inline">\(p_{Y|X=x}\)</span>.</p>
<p>For certain utility functions and pdf combinations, this can be solved in closed form. In many cases there is not tractable algorithm for this decision problem, then approximations are needed.</p>
<p>When the utility function is <span class="math inline">\(u(y, c) = [y = c]\)</span> (which evaluates to 1 if <span class="math inline">\(y\)</span> and <span class="math inline">\(c\)</span> are the same, and 0 otherwise), <span class="math inline">\(y^\star\)</span> corresponds to the mode of <span class="math inline">\(p_{Y|X=x}\)</span>.</p>
<p>In structure prediction, utility functions may reward partial/structural similarity between <span class="math inline">\(y\)</span> and <span class="math inline">\(c\)</span>. For example, a utility function for strings may be based on Levenshtein distance/similarity.</p>
<div id="cell-64" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecisionRule:</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, model, x):</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co">        This function should map from a model and input, and the resulting predicted probability distribution,</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co">        to a single outcome.</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>    </span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implement me!"</span>)      </span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExactMode(DecisionRule):</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co">    This decision rule returns the most probable outcome under the predicted probability distribution, assuming</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="co">    a unimodal or discrete probability distribution. </span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, model, x):</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model.mode(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have some helper code for predictions using the batches in a data loader:</p>
<div id="cell-66" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(model, rule, dl, device, return_targets<span class="op">=</span><span class="va">False</span>, strip_pad<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrapper for predictions using a decision rule.</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co">    model: one of our taggers</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co">    dl: a data loader for the heldout data</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="co">    device: the PyTorch device where the model is stored</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">    return_targets: also return the targets from the data loader</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co">        you can use this when the actual targets are in the dataloader (e.g., for dev set)</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Return </span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co">        * a list of predictions, each a sequence of tags (already decoded)</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co">        * if return_targets=True, additionally return a list of targets, each a sequence of tags (already decoded)</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    all_preds <span class="op">=</span> []</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    all_targets <span class="op">=</span> []</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> dl:</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> rule(model, batch_x.to(device))</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>            all_preds.extend(preds.cpu().numpy())</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>            all_targets.extend(batch_y.cpu().numpy())</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> return_targets:</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> all_preds, all_targets</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> all_preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-procedure" class="level2">
<h2 class="anchored" data-anchor-id="training-procedure">6. Training procedure</h2>
<p>The training procedure is always the exact same, no matter the model, so we abstract it for you.</p>
<div id="cell-69" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributions <span class="im">as</span> td</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> opt</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-70" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, mean_squared_error, mean_absolute_error, median_absolute_error</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flatten(seq):</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""flattens a python list"""</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(chain.from_iterable(seq))</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report_regression(y_true, y_pred):</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"MSE"</span>: mean_squared_error(y_true, y_pred), <span class="st">"MAE"</span>: mean_absolute_error(y_true, y_pred), <span class="st">"MdAE"</span>: median_absolute_error(y_true, y_pred)}</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report_classification(y_true, y_pred):</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classification_report(y_true, y_pred, output_dict<span class="op">=</span><span class="va">True</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report_tagging(y_true, y_pred, score_pad<span class="op">=</span><span class="va">False</span>, pad_id<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> score_pad:</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>        pairs <span class="op">=</span> [(t, p) <span class="cf">for</span> t, p <span class="kw">in</span> <span class="bu">zip</span>(flatten(y_true), flatten(y_pred)) <span class="cf">if</span> t <span class="op">!=</span> pad_id]</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>        y_true <span class="op">=</span> [t <span class="cf">for</span> t, p <span class="kw">in</span> pairs]</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> [p <span class="cf">for</span> t, p <span class="kw">in</span> pairs]</span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classification_report(y_true, y_pred, output_dict<span class="op">=</span><span class="va">True</span>, zero_division<span class="op">=</span><span class="dv">0</span>)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-71" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_neural_model(model: Model, optimiser, decision_rule: DecisionRule, </span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    training_data, dev_data, </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">200</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    report_fn<span class="op">=</span><span class="va">None</span>, report_metrics<span class="op">=</span>[], </span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co">    model: pytorch model</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co">    optimiser: pytorch optimiser</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co">    training_corpus: a TaggedCorpus for trianing</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="co">    dev_corpus: a TaggedCorpus for dev</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co">    batch_size: use more if you have more memory</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co">    num_epochs: use more for improved convergence</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a><span class="co">    check_every: use less to check performance on dev set more often</span></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="co">    device: where we run the experiment</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a log of quantities computed during training (for plotting)</span></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We use the training data in random order for parameter estimation</span></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>    batcher <span class="op">=</span> DataLoader(training_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>training_data.pad_to_longest)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We use the dev data for evaluation during training (no need for randomisation here)</span></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>    dev_batcher <span class="op">=</span> DataLoader(dev_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>dev_data.pad_to_longest)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>    total_steps <span class="op">=</span> num_epochs <span class="op">*</span> <span class="bu">len</span>(batcher)</span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>    log[<span class="st">'D'</span>].append(distortion(model, dev_batcher, device<span class="op">=</span>device).item())</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> report_fn:</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>        preds, targets <span class="op">=</span> predict(</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a>            model,       </span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>            decision_rule,  </span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>            dev_batcher, </span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>            device<span class="op">=</span>device, </span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>            return_targets<span class="op">=</span><span class="va">True</span></span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a>        )    </span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>        report <span class="op">=</span> report_fn(targets, preds)</span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> metric <span class="kw">in</span> report_metrics:            </span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a>            log[metric].append(report[metric])</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb59-43"><a href="#cb59-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tqdm(<span class="bu">range</span>(total_steps)) <span class="im">as</span> bar:</span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a>                model.train()</span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a>                optimiser.zero_grad()</span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a>                L <span class="op">=</span> loss(model, batch_x.to(device), batch_y.to(device))</span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a>                                        </span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a>                L.backward()</span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a>                optimiser.step()</span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a>                bar_dict <span class="op">=</span> OrderedDict()</span>
<span id="cb59-56"><a href="#cb59-56" aria-hidden="true" tabindex="-1"></a>                bar_dict[<span class="st">'loss'</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>L<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb59-57"><a href="#cb59-57" aria-hidden="true" tabindex="-1"></a>                bar_dict[<span class="st">'D'</span>] <span class="op">=</span>  <span class="ss">f"</span><span class="sc">{</span>log[<span class="st">'D'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb59-58"><a href="#cb59-58" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric <span class="kw">in</span> report_metrics:</span>
<span id="cb59-59"><a href="#cb59-59" aria-hidden="true" tabindex="-1"></a>                    bar_dict[metric] <span class="op">=</span>  <span class="st">"</span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(log[metric][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb59-60"><a href="#cb59-60" aria-hidden="true" tabindex="-1"></a>                bar.set_postfix(bar_dict)</span>
<span id="cb59-61"><a href="#cb59-61" aria-hidden="true" tabindex="-1"></a>                bar.update()  </span>
<span id="cb59-62"><a href="#cb59-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-63"><a href="#cb59-63" aria-hidden="true" tabindex="-1"></a>                log[<span class="st">'loss'</span>].append(L.item())</span>
<span id="cb59-64"><a href="#cb59-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-65"><a href="#cb59-65" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> step <span class="op">%</span> check_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb59-66"><a href="#cb59-66" aria-hidden="true" tabindex="-1"></a>                    model.<span class="bu">eval</span>()</span>
<span id="cb59-67"><a href="#cb59-67" aria-hidden="true" tabindex="-1"></a>                    log[<span class="st">'D'</span>].append(distortion(model, dev_batcher, device<span class="op">=</span>device).item())</span>
<span id="cb59-68"><a href="#cb59-68" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb59-69"><a href="#cb59-69" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> report_fn:</span>
<span id="cb59-70"><a href="#cb59-70" aria-hidden="true" tabindex="-1"></a>                        preds, targets <span class="op">=</span> predict(</span>
<span id="cb59-71"><a href="#cb59-71" aria-hidden="true" tabindex="-1"></a>                            model,   </span>
<span id="cb59-72"><a href="#cb59-72" aria-hidden="true" tabindex="-1"></a>                            decision_rule,      </span>
<span id="cb59-73"><a href="#cb59-73" aria-hidden="true" tabindex="-1"></a>                            dev_batcher, </span>
<span id="cb59-74"><a href="#cb59-74" aria-hidden="true" tabindex="-1"></a>                            device<span class="op">=</span>device, </span>
<span id="cb59-75"><a href="#cb59-75" aria-hidden="true" tabindex="-1"></a>                            return_targets<span class="op">=</span><span class="va">True</span></span>
<span id="cb59-76"><a href="#cb59-76" aria-hidden="true" tabindex="-1"></a>                        )    </span>
<span id="cb59-77"><a href="#cb59-77" aria-hidden="true" tabindex="-1"></a>                        report <span class="op">=</span> report_fn(targets, preds)</span>
<span id="cb59-78"><a href="#cb59-78" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> metric <span class="kw">in</span> report_metrics:            </span>
<span id="cb59-79"><a href="#cb59-79" aria-hidden="true" tabindex="-1"></a>                            log[metric].append(report[metric])</span>
<span id="cb59-80"><a href="#cb59-80" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb59-81"><a href="#cb59-81" aria-hidden="true" tabindex="-1"></a>                step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb59-82"><a href="#cb59-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-83"><a href="#cb59-83" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb59-84"><a href="#cb59-84" aria-hidden="true" tabindex="-1"></a>    log[<span class="st">'D'</span>].append(distortion(model, dev_batcher, device<span class="op">=</span>device).item())</span>
<span id="cb59-85"><a href="#cb59-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-86"><a href="#cb59-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> report_fn:</span>
<span id="cb59-87"><a href="#cb59-87" aria-hidden="true" tabindex="-1"></a>        preds, targets <span class="op">=</span> predict(</span>
<span id="cb59-88"><a href="#cb59-88" aria-hidden="true" tabindex="-1"></a>            model, </span>
<span id="cb59-89"><a href="#cb59-89" aria-hidden="true" tabindex="-1"></a>            decision_rule,        </span>
<span id="cb59-90"><a href="#cb59-90" aria-hidden="true" tabindex="-1"></a>            dev_batcher, </span>
<span id="cb59-91"><a href="#cb59-91" aria-hidden="true" tabindex="-1"></a>            device<span class="op">=</span>device, </span>
<span id="cb59-92"><a href="#cb59-92" aria-hidden="true" tabindex="-1"></a>            return_targets<span class="op">=</span><span class="va">True</span></span>
<span id="cb59-93"><a href="#cb59-93" aria-hidden="true" tabindex="-1"></a>        )    </span>
<span id="cb59-94"><a href="#cb59-94" aria-hidden="true" tabindex="-1"></a>        report <span class="op">=</span> report_fn(targets, preds)</span>
<span id="cb59-95"><a href="#cb59-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> metric <span class="kw">in</span> report_metrics:            </span>
<span id="cb59-96"><a href="#cb59-96" aria-hidden="true" tabindex="-1"></a>            log[metric].append(report[metric])</span>
<span id="cb59-97"><a href="#cb59-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-98"><a href="#cb59-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log            </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="examples" class="level2">
<h2 class="anchored" data-anchor-id="examples">7. Examples</h2>
<section id="categorical" class="level3">
<h3 class="anchored" data-anchor-id="categorical">7.1 Categorical</h3>
<p>Here we design a probabilistic model for a Categorical response variable <span class="math inline">\(Y\)</span> given a document <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{align}
Y|X=x &amp;\sim \mathrm{Categorical}(\mathbf g(x; \theta)) \\
\mathbf u &amp;= \mathrm{encode}_D(x; \theta_{\text{enc}}) \\
\mathbf s &amp;= \mathrm{affine}_C(\mathbf u; \theta_{\text{out}}) \\
\mathbf g(x; \theta) &amp;= \mathrm{softmax}(\mathbf s)
\end{align}\]</span></p>
<div id="cell-74" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CategoricalModel(Model):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Given a predictor x this NN parameterises the pdf of the random variable Y|X=x,</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co">    where Y is Categorically distributed.</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">    In other words, it predicts the conditional distribution P(Y|X=x).</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, support_size, hidden_size: <span class="bu">int</span>, encoder: Encoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="bu">tuple</span>())</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.support_size <span class="op">=</span> support_size</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We have a simple neural network that maps from the text encodings to</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># logits for the Categorical distribution.</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits_predictor <span class="op">=</span> nn.Sequential(</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(encoder.output_dim, hidden_size),</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, support_size)</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We begin by encoding the tokens</span></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x) <span class="co"># batch_shape, (enc_dim,)</span></span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We use the logit predictor network to transform those into logits.</span></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.logits_predictor(h) <span class="co"># batch_shape, (support_size,)</span></span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We return a Categorical distribution with the predicted logits as its parameters.</span></span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Categorical(logits<span class="op">=</span>logits) </span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="bu">tuple</span>()):</span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>            cpd <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x)        </span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> cpd.sample(sample_size)</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mode(<span class="va">self</span>, x):</span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Predict the Categorical distribution of P(Y|X=x)</span></span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>            cpd <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x)</span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We can easily obtain the exact mode by taking the argmax over logits.</span></span>
<span id="cb60-46"><a href="#cb60-46" aria-hidden="true" tabindex="-1"></a>            mode <span class="op">=</span> torch.argmax(cpd.logits, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb60-47"><a href="#cb60-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb60-48"><a href="#cb60-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> mode</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s sanity check our implementation by doing a forwards pass through a randomly initialized text encoder and Categorical model:</p>
<div id="cell-76" class="cell" data-outputid="e01473ea-bf3f-4f71-82dd-cb02bbdc5e8f" data-execution_count="40">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> TextEncoder(<span class="bu">len</span>(word_vocab), <span class="dv">7</span>, <span class="dv">5</span>, <span class="va">True</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>cat_model <span class="op">=</span> CategoricalModel(<span class="dv">3</span>, <span class="dv">12</span>, encoder)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(batch_x.shape)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cat_model(batch_x).logits)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cat_model.mode(batch_x))</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cat_model.sample(batch_x))</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([3, 28])
tensor([[-1.2122, -1.1352, -0.9647],
        [-1.1917, -1.1365, -0.9799],
        [-1.2199, -1.1338, -0.9599]], grad_fn=&lt;SubBackward0&gt;)
tensor([2, 2, 2])
tensor([1, 2, 1])</code></pre>
</div>
</div>
<div id="cell-77" class="cell" data-outputid="570337de-048b-459a-df36-5c3b63f6bc8d" data-execution_count="41">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We already set up the data at the start of this notebook. We are only left to construct the word vocabulary.</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>cat_vocab <span class="op">=</span> Vocab(cat_training_x, min_freq<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(cat_vocab))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>22734</code></pre>
</div>
</div>
<div id="cell-78" class="cell" data-outputid="d8902cd5-58a8-41f7-de91-87a638781543" data-execution_count="42">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let's train a model on this data.</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="co"># First, reset random number generators</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>seed_all()</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU acceleration</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CategoricalModel(</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>    support_size<span class="op">=</span><span class="bu">len</span>(brown.categories()),</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    encoder<span class="op">=</span>TextEncoder(</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>        vocab_size<span class="op">=</span><span class="bu">len</span>(cat_vocab),</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        word_embed_dim<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        reduce_mean<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        pad_id<span class="op">=</span>cat_vocab.pad_id</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct an Adam optimiser</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> opt.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">5e-3</span>)</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model"</span>)</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Report number of parameters</span></span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>model<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_neural_model(</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>    model, optimiser, </span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    decision_rule<span class="op">=</span>ExactMode(),</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>TextRegressionCorpus(cat_training_x, cat_training_y, cat_vocab), </span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>TextRegressionCorpus(cat_dev_x, cat_dev_y, cat_vocab),</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>    report_fn<span class="op">=</span>report_classification,</span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>    report_metrics<span class="op">=</span>[<span class="st">'accuracy'</span>],</span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">500</span>, num_epochs<span class="op">=</span><span class="dv">20</span>, check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss and validation checks</span></span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'loss'</span>])), log[<span class="st">'loss'</span>])</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">'steps'</span>)</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">'training loss'</span>)</span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'D'</span>])), log[<span class="st">'D'</span>])</span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">'steps (in 100s)'</span>)</span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">'D given dev'</span>)</span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'accuracy'</span>])), log[<span class="st">'accuracy'</span>])</span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">'steps (in 10s)'</span>)</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">'dev acc'</span>)</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model
CategoricalModel(
  (encoder): TextEncoder(
    (word_embed): Embedding(22734, 100)
    (encoder): LSTM(100, 64, batch_first=True, bidirectional=True)
  )
  (logits_predictor): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=32, out_features=15, bias=True)
  )
)
Model size: 2,363,015 parameters</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"48a5c58a04ce48b99d2e434497ecdeed","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-43-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s use the model for some predictions.</p>
<div id="cell-80" class="cell" data-outputid="72202cd8-a538-48b7-bb2f-044ea16a0e55" data-execution_count="43">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the test dataloader</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>cat_test_corpus <span class="op">=</span> TextRegressionCorpus(cat_test_x, cat_test_y, cat_vocab)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>cat_test_dl <span class="op">=</span> DataLoader(cat_test_corpus, batch_size<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>cat_test_corpus.pad_to_longest)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>batch_x, batch_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(cat_test_dl))</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the mode to form predictions.</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>decision_rule <span class="op">=</span> ExactMode()</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> decision_rule(model, batch_x.to(my_device))</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, pred, truth <span class="kw">in</span> <span class="bu">zip</span>(batch_x, batch_y, predictions):</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"input: </span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(cat_vocab.decode(x, strip_pad<span class="op">=</span><span class="va">True</span>))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"prediction: </span><span class="sc">{</span>brown<span class="sc">.</span>categories()[pred]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"truth: </span><span class="sc">{</span>brown<span class="sc">.</span>categories()[truth]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>input: Have you examined this problem of increasing consumer sophistication from the standpoint of your own company ? ? -EOS-
prediction: hobbies
truth: lore

input: Some have plenty of money -- some have very little money . -EOS-
prediction: lore
truth: mystery

input: Thus far the advances made have been almost entirely along functional lines . -EOS-
prediction: belles_lettres
truth: learned
</code></pre>
</div>
</div>
<div id="cell-81" class="cell" data-outputid="3293b7f4-6903-4387-a069-bc36186780e0" data-execution_count="44">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can also visualize the predicted Categoricals and use them for sampling or input</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># to any arbitrary decision rule.</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    batch_x, batch_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(cat_test_dl))</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    cpds <span class="op">=</span> model(batch_x.to(my_device))</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, x, y, predicted_probs <span class="kw">in</span> <span class="bu">zip</span>(axes, batch_x, batch_y, cpds.probs):</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    sns.barplot(data<span class="op">=</span>pd.DataFrame({<span class="st">"category"</span>: brown.categories(), <span class="st">"prob"</span>: predicted_probs.cpu()}),</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">"category"</span>, y<span class="op">=</span><span class="st">"prob"</span>, ax<span class="op">=</span>ax)</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"</span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(cat_vocab.decode(x, strip_pad<span class="op">=</span><span class="va">True</span>))<span class="sc">}</span><span class="ss">, truth = </span><span class="sc">{</span>brown<span class="sc">.</span>categories()[y]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-45-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="poisson" class="level3">
<h3 class="anchored" data-anchor-id="poisson">7.2 Poisson</h3>
<p>Here we design a probabilistic model for an ordinal response variable <span class="math inline">\(Y\)</span> given a document <span class="math inline">\(x\)</span>, for which we choose to model with a Poisson pmf:</p>
<p><span class="math display">\[\begin{align}
Y|X=x &amp;\sim \mathrm{Poisson}(g(x; \theta)) \\
\mathbf u &amp;= \mathrm{encode}_D(x; \theta_{\text{enc}}) \\
s &amp;= \mathrm{affine}_1(\mathbf u; \theta_{\text{out}}) \\
g(x; \theta) &amp;= \mathrm{softplus}(s)
\end{align}\]</span></p>
<p>The Poisson rate parameter must be strictly positive, thus we use a softplus output.</p>
<div id="cell-83" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PoissonModel(Model):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This model predicts a conditional Poisson distribution Y|X=x for ordinal data.</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size: <span class="bu">int</span>, encoder: Encoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="bu">tuple</span>())</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The poisson distribution has 1 parameter: its rate. This needs to be a positive real number.</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rate_predictor <span class="op">=</span> nn.Sequential(</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>            nn.Linear(encoder.output_dim, hidden_size),</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, <span class="dv">1</span>),</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>            nn.Softplus()</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):        </span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a>        rate <span class="op">=</span> <span class="va">self</span>.rate_predictor(h)</span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Poisson(rate<span class="op">=</span>rate) </span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="bu">tuple</span>()):</span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a>            cpd <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x)</span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> cpd.sample(sample_size)</span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mode(<span class="va">self</span>, x):</span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a>            cpd <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x)</span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a>            rate <span class="op">=</span> cpd.rate</span>
<span id="cb70-35"><a href="#cb70-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> torch.floor(rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-84" class="cell" data-outputid="090f9022-b4f2-4dae-b907-62d7b5fac658" data-execution_count="46">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's train on the age data using a conditional Poisson model.</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to construct the vocabulary for this data.</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>blog_vocab <span class="op">=</span> Vocab(blog_training_x, min_freq<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(cat_vocab)<span class="sc">:,}</span><span class="ss"> words in the vocabulary."</span>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the random seeds</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>seed_all()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU acceleration</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a similar model as before, but predicting a Poisson distribution instead.</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PoissonModel(</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    encoder<span class="op">=</span>TextEncoder(</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>        vocab_size<span class="op">=</span><span class="bu">len</span>(blog_vocab),</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>        word_embed_dim<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        reduce_mean<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        pad_id<span class="op">=</span>blog_vocab.pad_id</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an Adam optimiser</span></span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> opt.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">5e-3</span>)</span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model"</span>)</span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Report number of parameters</span></span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>model<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_neural_model(</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a>    model, optimiser, </span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a>    decision_rule<span class="op">=</span>ExactMode(),</span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>TextRegressionCorpus(blog_training_x, blog_training_y, blog_vocab), </span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>TextRegressionCorpus(blog_dev_x, blog_dev_y, blog_vocab),</span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a>    report_fn<span class="op">=</span>report_regression,</span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>    report_metrics<span class="op">=</span>[<span class="st">"MSE"</span>, <span class="st">"MAE"</span>, <span class="st">"MdAE"</span>],</span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">500</span>, num_epochs<span class="op">=</span><span class="dv">1</span>, check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb71-45"><a href="#cb71-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-46"><a href="#cb71-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-47"><a href="#cb71-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss and validation checks</span></span>
<span id="cb71-48"><a href="#cb71-48" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb71-49"><a href="#cb71-49" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'loss'</span>])), log[<span class="st">'loss'</span>])</span>
<span id="cb71-50"><a href="#cb71-50" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">'steps'</span>)</span>
<span id="cb71-51"><a href="#cb71-51" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">'training loss'</span>)</span>
<span id="cb71-52"><a href="#cb71-52" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'D'</span>])), log[<span class="st">'D'</span>])</span>
<span id="cb71-53"><a href="#cb71-53" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">'steps (in 100s)'</span>)</span>
<span id="cb71-54"><a href="#cb71-54" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">'D given dev'</span>)</span>
<span id="cb71-55"><a href="#cb71-55" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'MAE'</span>])), log[<span class="st">'MAE'</span>])</span>
<span id="cb71-56"><a href="#cb71-56" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">'steps (in 10s)'</span>)</span>
<span id="cb71-57"><a href="#cb71-57" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">'dev mean absolute error'</span>)</span>
<span id="cb71-58"><a href="#cb71-58" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'MdAE'</span>])), log[<span class="st">'MdAE'</span>])</span>
<span id="cb71-59"><a href="#cb71-59" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_xlabel(<span class="st">'steps (in 10s)'</span>)</span>
<span id="cb71-60"><a href="#cb71-60" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_ylabel(<span class="st">'dev median absolute error'</span>)</span>
<span id="cb71-61"><a href="#cb71-61" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb71-62"><a href="#cb71-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>22,734 words in the vocabulary.
Model
PoissonModel(
  (encoder): TextEncoder(
    (word_embed): Embedding(83532, 100)
    (encoder): LSTM(100, 64, batch_first=True, bidirectional=True)
  )
  (rate_predictor): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=32, out_features=1, bias=True)
    (5): Softplus(beta=1, threshold=20)
  )
)
Model size: 8,442,353 parameters</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5c1e9ca7bb764ad9b0502bf6e5dd2a02","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-47-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-85" class="cell" data-outputid="9bdcd44d-33ac-478c-c9c6-180b8c573dd9" data-scrolled="true" data-execution_count="47">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the test dataloader</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>blog_test_corpus <span class="op">=</span> TextRegressionCorpus(blog_test_x, blog_test_y, blog_vocab)</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>blog_test_dl <span class="op">=</span> DataLoader(blog_test_corpus, batch_size<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>blog_test_corpus.pad_to_longest)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>batch_x, batch_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(blog_test_dl))</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We can use the mode again to form predictions.</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>decision_rule <span class="op">=</span> ExactMode()</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> decision_rule(model, batch_x.to(my_device))</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, pred, truth <span class="kw">in</span> <span class="bu">zip</span>(batch_x, batch_y, predictions):</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"input: </span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(blog_vocab.decode(x, strip_pad<span class="op">=</span><span class="va">True</span>))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"prediction: </span><span class="sc">{</span>pred<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"truth: </span><span class="sc">{</span>truth<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>input: every day deserves a good urllink insult . but , no one insults like shakespeare ... : drunkenness is his best virtue , for he will be swine drunk , and in his sleep he does little harm , save to his -UNK- about him . -EOS-
prediction: 26
truth: 24.0

input: here 's something i never thought i 'd never see : an epa ad mocking the idea of good fuel -UNK- in cars . urllink http : -UNK- maybe the appliance department should talk to the transportation department and get everyone on the same page . -EOS-
prediction: 17
truth: 24.0

input: i ca n't tolerate bitches ... im just allergic to them . they have no quality in starting a conversation and worse of all , a disgrace to women around the world . its in me ... i ca n't tolerate them . heard of a saying , respect yourself ... before others respect you .. very the true ... -EOS-
prediction: 24
truth: 24.0
</code></pre>
</div>
</div>
<div id="cell-86" class="cell" data-outputid="007344e2-57a1-496f-b5c9-aa61dfacafe7" data-execution_count="48">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can also visualize the pmf of the predicted conditional Poisson distributions.</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    batch_x, batch_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(blog_test_dl))</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    cpds <span class="op">=</span> model(batch_x.to(my_device))</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    mode <span class="op">=</span> model.mode(batch_x.to(my_device))</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">0</span>, <span class="dv">50</span>).repeat(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> cpds.log_prob(x.to(my_device)).exp()</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>    axes[i].plot(x[i].cpu(), pmf[i].cpu().numpy(), ls<span class="op">=</span><span class="st">'-.'</span>, color<span class="op">=</span><span class="st">'b'</span>, label<span class="op">=</span><span class="st">'pmf'</span>)</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>    axes[i].axvline(mode[i].cpu().numpy(), ls<span class="op">=</span><span class="st">"--"</span>, color<span class="op">=</span><span class="st">'b'</span>, label<span class="op">=</span><span class="st">'mode'</span>)</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    axes[i].axvline(batch_y[i].numpy(), color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'truth'</span>)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xlabel(<span class="st">"age"</span>)</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    axes[i].set_ylabel(<span class="st">"pmf"</span>)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f"</span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(blog_vocab.decode(batch_x[i], strip_pad<span class="op">=</span><span class="va">True</span>))[:<span class="dv">40</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    axes[i].legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-49-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sequence-labelling" class="level3">
<h3 class="anchored" data-anchor-id="sequence-labelling">7.3 Sequence labelling</h3>
<p>In sequence labelling we have two sequences of equal length: a word sequence <span class="math inline">\(x_{1:l}\)</span> and a tag sequence <span class="math inline">\(y_{1:l}\)</span>.</p>
<p>The word sequence <span class="math inline">\(x_{1:l} = \langle x_1, \ldots, x_l \rangle\)</span>, where <span class="math inline">\(l\)</span> is the sequence length, is such that token <span class="math inline">\(x_i\)</span> belongs to a vocabulary <span class="math inline">\(\mathcal W\)</span> of <span class="math inline">\(V\)</span> known words.</p>
<p>The tag sequence <span class="math inline">\(y_{1:l} = \langle y_1, \ldots, y_l \rangle\)</span>, where <span class="math inline">\(l\)</span> is the same length as the document <span class="math inline">\(x_{1:l}\)</span>, is such that each tag <span class="math inline">\(y_i\)</span> belongs to a vocabulary <span class="math inline">\(\mathcal T\)</span> of <span class="math inline">\(C\)</span> known tags.</p>
<p>As always, our models are probability distributions, but this time we need a distribution over a space of sequences. A sequence is a finite length object, but it cannot be seen as a fixed dimensional vector (a multivariate random variable), for its length is not at all fixed. A sequence is best seen as a data structure. To prescribe a distribution over a general data structure, we can decompose this structure into parts, prescribe distributions for the parts, and combine them into a single pdf.</p>
<p>Chain rule allows us to re-express a pdf over an arbitrary sample space (with multivariate or structured outcomes) as a product of pdfs, each over a simpler space space (e.g., with univariate outcomes):</p>
<p><span class="math display">\[\begin{align}
p(y_{1:l}|x_{1:l}, \theta) &amp;= \prod_{i=1}^l p(y_i|x_{1:l}, y_1^{i-1}, \theta) \\
&amp;=  \prod_{i=1}^l \mathrm{Categorical}(y_i|\mathbf g(x_{1:l}, y_1^{i-1}; \theta))
\end{align}\]</span></p>
<p>where, on the right-hand side of the first equation, we have pdfs (or pfms) for each step of the sequence. In the second row, we use a Categorical pmf since each step of the sequence is a simple categorical variable. Each pmf is parameterised in context using some trainable function <span class="math inline">\(\mathbf g\)</span> with parameters <span class="math inline">\(\theta\)</span>.</p>
<p>The above is a <em>factorisation</em> of the joint pdf. This is one valid factorisation, and this one does not make any conditional independence assumptions (i.e., it conditions on all parts of the structure already generated). We could simplify the factorisation by making such conditional independence (or Markov) assumptions. For example, we might use only <span class="math inline">\(x_{1:l}\)</span> and the previous step <span class="math inline">\(y_{i-1}\)</span> when parameterising the distribution of <span class="math inline">\(Y_i\)</span>. These assumptions are justified on a case by case basis depending on application domain.</p>
<div id="cell-88" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tagger(Model):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, tagset_size: <span class="bu">int</span>, pad_id<span class="op">=</span><span class="dv">0</span>, bos_id<span class="op">=</span><span class="dv">1</span>, eos_id<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._tagset_size <span class="op">=</span> tagset_size</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._pad <span class="op">=</span> pad_id</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._bos <span class="op">=</span> bos_id</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._eos <span class="op">=</span> eos_id</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocab_size(<span class="va">self</span>):</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._vocab_size</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tagset_size(<span class="va">self</span>):</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._tagset_size</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pad(<span class="va">self</span>):</span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._pad</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> bos(<span class="va">self</span>):</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._bos</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> eos(<span class="va">self</span>):</span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._eos</span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_parameters(<span class="va">self</span>):</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(np.prod(theta.shape) <span class="cf">for</span> theta <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, y_in):</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a><span class="co">        To predict the distribution of Y[i], an autoregressive model conditions</span></span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a><span class="co">         on both x and y[:i]. Normally, y_in is prepended with some BOS code so</span></span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a><span class="co">         that both sequences have the same length.</span></span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a><span class="co">        y_in: [batch_size, max_length]</span></span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Return a batch of cpds, one per step.</span></span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Each type of tagger will have a different implementation here"</span>)</span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, x, y):</span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the log conditional probability of each tag sequence in a batch.</span></span>
<span id="cb76-50"><a href="#cb76-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-51"><a href="#cb76-51" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb76-52"><a href="#cb76-52" aria-hidden="true" tabindex="-1"></a><span class="co">        y: [batch_size, max_length]</span></span>
<span id="cb76-53"><a href="#cb76-53" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb76-54"><a href="#cb76-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shift the output sequence and prepend the BOS code</span></span>
<span id="cb76-55"><a href="#cb76-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (after all, we do not want to condition on what we need to predict)</span></span>
<span id="cb76-56"><a href="#cb76-56" aria-hidden="true" tabindex="-1"></a>        batch_size, max_len <span class="op">=</span> y.shape</span>
<span id="cb76-57"><a href="#cb76-57" aria-hidden="true" tabindex="-1"></a>        bos <span class="op">=</span> torch.full((batch_size, <span class="dv">1</span>), <span class="va">self</span>.bos, device<span class="op">=</span>y.device)</span>
<span id="cb76-58"><a href="#cb76-58" aria-hidden="true" tabindex="-1"></a>        y_in <span class="op">=</span> torch.cat([bos, y], <span class="dv">1</span>)[:,:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb76-59"><a href="#cb76-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-60"><a href="#cb76-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># one C-dimensional Categorical cpd for each token in the batch</span></span>
<span id="cb76-61"><a href="#cb76-61" aria-hidden="true" tabindex="-1"></a>        cpds <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x, y_in<span class="op">=</span>y_in)</span>
<span id="cb76-62"><a href="#cb76-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length]        </span></span>
<span id="cb76-63"><a href="#cb76-63" aria-hidden="true" tabindex="-1"></a>        logp <span class="op">=</span> cpds.log_prob(y)</span>
<span id="cb76-64"><a href="#cb76-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size]</span></span>
<span id="cb76-65"><a href="#cb76-65" aria-hidden="true" tabindex="-1"></a>        logp <span class="op">=</span> torch.where(y <span class="op">!=</span> <span class="va">self</span>.pad, logp, torch.zeros_like(logp)).<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb76-66"><a href="#cb76-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logp  </span>
<span id="cb76-67"><a href="#cb76-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-68"><a href="#cb76-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> greedy(<span class="va">self</span>, x):</span>
<span id="cb76-69"><a href="#cb76-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb76-70"><a href="#cb76-70" aria-hidden="true" tabindex="-1"></a><span class="co">        For each cpd Y[i]|X=x, predicts the mode of the cpd.</span></span>
<span id="cb76-71"><a href="#cb76-71" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb76-72"><a href="#cb76-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-73"><a href="#cb76-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: tag sequences [batch_size, max_length]</span></span>
<span id="cb76-74"><a href="#cb76-74" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb76-75"><a href="#cb76-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Each type of tagger differs here"</span>)</span>
<span id="cb76-76"><a href="#cb76-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-77"><a href="#cb76-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="bu">tuple</span>()):</span>
<span id="cb76-78"><a href="#cb76-78" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb76-79"><a href="#cb76-79" aria-hidden="true" tabindex="-1"></a><span class="co">        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.</span></span>
<span id="cb76-80"><a href="#cb76-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-81"><a href="#cb76-81" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_len]</span></span>
<span id="cb76-82"><a href="#cb76-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-83"><a href="#cb76-83" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: tag sequences with shape [batch_size, max_len] if sample_size is None</span></span>
<span id="cb76-84"><a href="#cb76-84" aria-hidden="true" tabindex="-1"></a><span class="co">            else with shape [sample_size, batch_size, max_len]</span></span>
<span id="cb76-85"><a href="#cb76-85" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb76-86"><a href="#cb76-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Each type of tagger differs here"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-89" class="cell" data-outputid="ab37a60c-4d40-4cca-c1c8-c8bde0240223" data-execution_count="50">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we get a vocabulary for words</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>word_vocab <span class="op">=</span> Vocab(tagger_training_x, min_freq<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and a vocabulary for tags</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>tag_vocab <span class="op">=</span> Vocab(tagger_training_y, min_freq<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="co"># you can see their sizes V and C:</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(word_vocab), <span class="bu">len</span>(tag_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>(3358, 16)</code></pre>
</div>
</div>
<section id="independent-c-way-classification" class="level4">
<h4 class="anchored" data-anchor-id="independent-c-way-classification">7.3.1 Independent C-way classification</h4>
<p>Our first tagger is in fact just a <span class="math inline">\(C\)</span>-way classifier that we use to predict a distribution over <span class="math inline">\(C\)</span> tags for different positions of an input sequence conditioned on the entire input sequence.</p>
<p>Here is the model of the <span class="math inline">\(i\)</span>th tag given <span class="math inline">\(x_{1:l}\)</span>: <span class="math display">\[\begin{align}
Y_i | S=x_{1:l} &amp;\sim \mathrm{Categorical}(\mathbf g(i, x_{1:l}; \theta))
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf g\)</span> is a neural network. For example: <span class="math display">\[\begin{align}
\mathbf e_j &amp;= \mathrm{embed}_D(x_j; \theta_{\text{in}})  &amp; j \in \{1, \ldots, l\}\\
\mathbf u_{1:l} &amp;= \mathrm{birnn}_{2K}(\mathbf e_{1:l}; \theta_{\text{bienc}})\\
\mathbf s_i &amp;= \mathrm{affine}_C(\mathbf u_i; \theta_{\text{out}})\\
\mathbf g(i, x_{1:l}) &amp;= \mathrm{softmax}(\mathbf s_i)
\end{align}\]</span></p>
<p>The bidirection RNN layer concatenates the states of two independent RNN layers, one that processes the sequence from left-to-right, another that processes it from right-to-left.</p>
<p>Note how this model ignores every other tag in the sequence.</p>
<div id="cell-91" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicTagger(Tagger):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, tagset_size, word_embed_dim: <span class="bu">int</span>, hidden_size: <span class="bu">int</span>, p_drop<span class="op">=</span><span class="dv">0</span>, pad_id<span class="op">=</span><span class="dv">0</span>, bos_id<span class="op">=</span><span class="dv">1</span>, eos_id<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""        </span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_size: number of known words</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="co">        tagset_size: number of known tags</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a><span class="co">        word_embed_dim: dimensionality of word embeddings</span></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: dimensionality of hidden layers</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="co">        recurrent_encoder: enable recurrent encoder</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a><span class="co">        bidirectional_encoder: for a recurrent encoder, make it bidirectional</span></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(vocab_size<span class="op">=</span>vocab_size, tagset_size<span class="op">=</span>tagset_size, pad_id<span class="op">=</span>pad_id, bos_id<span class="op">=</span>bos_id, eos_id<span class="op">=</span>eos_id)</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word_embed_dim <span class="op">=</span> word_embed_dim</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size        </span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TextEncoder(</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>            vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>            word_embed_dim<span class="op">=</span>word_embed_dim,</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span>hidden_size,</span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>            reduce_mean<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>            pad_id<span class="op">=</span>pad_id,</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the bidirectional LSTM encoder produces outputs of size 2*hidden_size</span></span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># thus our linear layer must take 2*hidden_size inputs</span></span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits_predictor <span class="op">=</span> nn.Sequential(</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.encoder.output_dim, <span class="va">self</span>.tagset_size)</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, y_in<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameterise the conditional distributions over Y[i] given the entire word sequence x.        </span></span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a><span class="co">        y: not used by this class</span></span>
<span id="cb79-36"><a href="#cb79-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-37"><a href="#cb79-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: a batch of C-dimensional Categorical distributions, one per step of the sequence.</span></span>
<span id="cb79-38"><a href="#cb79-38" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb79-39"><a href="#cb79-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We begin by encoding the tokens </span></span>
<span id="cb79-40"><a href="#cb79-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, enc_dim]</span></span>
<span id="cb79-41"><a href="#cb79-41" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb79-42"><a href="#cb79-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># finally, per step of the sequence we predict logits for the possible tags</span></span>
<span id="cb79-43"><a href="#cb79-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, tagset_size]        </span></span>
<span id="cb79-44"><a href="#cb79-44" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.logits_predictor(h)</span>
<span id="cb79-45"><a href="#cb79-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and convert those logits to Categorical distributions</span></span>
<span id="cb79-46"><a href="#cb79-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Categorical(logits<span class="op">=</span>s)</span>
<span id="cb79-47"><a href="#cb79-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-48"><a href="#cb79-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> greedy(<span class="va">self</span>, x):</span>
<span id="cb79-49"><a href="#cb79-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb79-50"><a href="#cb79-50" aria-hidden="true" tabindex="-1"></a><span class="co">        For each cpd Y[i]|X=x, predicts the mode of the cpd.</span></span>
<span id="cb79-51"><a href="#cb79-51" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_length]</span></span>
<span id="cb79-52"><a href="#cb79-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-53"><a href="#cb79-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: tag sequences [batch_size, max_length]</span></span>
<span id="cb79-54"><a href="#cb79-54" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb79-55"><a href="#cb79-55" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb79-56"><a href="#cb79-56" aria-hidden="true" tabindex="-1"></a>        max_length <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb79-57"><a href="#cb79-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb79-58"><a href="#cb79-58" aria-hidden="true" tabindex="-1"></a>            cpds <span class="op">=</span> <span class="va">self</span>(x)</span>
<span id="cb79-59"><a href="#cb79-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># [batch_size, max_length]</span></span>
<span id="cb79-60"><a href="#cb79-60" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> torch.argmax(cpds.probs, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb79-61"><a href="#cb79-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if a position in x is padded, it should be padded in y</span></span>
<span id="cb79-62"><a href="#cb79-62" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> torch.where(x <span class="op">!=</span> <span class="va">self</span>.pad, y_pred, torch.zeros_like(y_pred) <span class="op">+</span> <span class="va">self</span>.pad)</span>
<span id="cb79-63"><a href="#cb79-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> y_pred</span>
<span id="cb79-64"><a href="#cb79-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-65"><a href="#cb79-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mode(<span class="va">self</span>, x):</span>
<span id="cb79-66"><a href="#cb79-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.greedy(x)</span>
<span id="cb79-67"><a href="#cb79-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-68"><a href="#cb79-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb79-69"><a href="#cb79-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb79-70"><a href="#cb79-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.</span></span>
<span id="cb79-71"><a href="#cb79-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-72"><a href="#cb79-72" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, max_len]</span></span>
<span id="cb79-73"><a href="#cb79-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-74"><a href="#cb79-74" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: tag sequences with shape [batch_size, max_len] if sample_size is None</span></span>
<span id="cb79-75"><a href="#cb79-75" aria-hidden="true" tabindex="-1"></a><span class="co">            else with shape [sample_size, batch_size, max_len]</span></span>
<span id="cb79-76"><a href="#cb79-76" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb79-77"><a href="#cb79-77" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb79-78"><a href="#cb79-78" aria-hidden="true" tabindex="-1"></a>        max_length <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb79-79"><a href="#cb79-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb79-80"><a href="#cb79-80" aria-hidden="true" tabindex="-1"></a>            cpds <span class="op">=</span> <span class="va">self</span>(x)</span>
<span id="cb79-81"><a href="#cb79-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> sample_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb79-82"><a href="#cb79-82" aria-hidden="true" tabindex="-1"></a>                shape <span class="op">=</span> (<span class="dv">1</span>,)</span>
<span id="cb79-83"><a href="#cb79-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb79-84"><a href="#cb79-84" aria-hidden="true" tabindex="-1"></a>                shape <span class="op">=</span> (sample_size,)</span>
<span id="cb79-85"><a href="#cb79-85" aria-hidden="true" tabindex="-1"></a>            <span class="co"># [sample_size, batch_size, max_length]</span></span>
<span id="cb79-86"><a href="#cb79-86" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> cpds.sample(shape)</span>
<span id="cb79-87"><a href="#cb79-87" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if a position in x is padding, it must be padded in y too</span></span>
<span id="cb79-88"><a href="#cb79-88" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> torch.where(x.unsqueeze(<span class="dv">0</span>) <span class="op">!=</span> <span class="va">self</span>.pad, y_pred, torch.zeros_like(y_pred) <span class="op">+</span> <span class="va">self</span>.pad)</span>
<span id="cb79-89"><a href="#cb79-89" aria-hidden="true" tabindex="-1"></a>            <span class="co"># takes care of output shape</span></span>
<span id="cb79-90"><a href="#cb79-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> sample_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb79-91"><a href="#cb79-91" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> y_pred.squeeze(<span class="dv">0</span>)</span>
<span id="cb79-92"><a href="#cb79-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb79-93"><a href="#cb79-93" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> y_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-92" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_basic_tagger(training_x, training_y, vocab_x, vocab_y):</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    seed_all()</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    toy_uni_tagger <span class="op">=</span> BasicTagger(</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>        vocab_size<span class="op">=</span><span class="bu">len</span>(vocab_x), </span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>        tagset_size<span class="op">=</span><span class="bu">len</span>(vocab_y), </span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        word_embed_dim<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    )    </span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">type</span>(toy_uni_tagger(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])))) <span class="kw">is</span> td.Categorical</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_uni_tagger.log_prob(</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), </span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_y.batch_encode(training_y[:<span class="dv">2</span>]))</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        ).shape <span class="op">==</span> (<span class="dv">2</span>,)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss(toy_uni_tagger,</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), </span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_y.batch_encode(training_y[:<span class="dv">2</span>]))</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        ).shape <span class="op">==</span> <span class="bu">tuple</span>()</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_uni_tagger.sample(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>]))).shape <span class="op">==</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_uni_tagger.sample(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), <span class="dv">3</span>).shape <span class="op">==</span> (<span class="dv">3</span>,) <span class="op">+</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_uni_tagger.greedy(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>]))).shape <span class="op">==</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a>test_basic_tagger(tagger_training_x, tagger_training_y, word_vocab, tag_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="autoregressive-tagger" class="level4">
<h4 class="anchored" data-anchor-id="autoregressive-tagger">7.3.2 Autoregressive tagger</h4>
<p>When predicting the distribution of <span class="math inline">\(Y_i\)</span>, an autoregressive tagger conditions on the tag sequence already generated thus far, hence it makes no Markov assumption. This is the model of the <span class="math inline">\(i\)</span>th tag:</p>
<p><span class="math display">\[\begin{align}
Y_i | S=x_{1:l}, H=y_1^{i-1} &amp;\sim \mathrm{Categorical}(\mathbf g(i, x_{1:l}, y_1^{i-1}; \theta))
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf g\)</span> is a neural network. For example:</p>
<p><span class="math display">\[\begin{align}
\mathbf e_j &amp;= \mathrm{embed}_{D_1}(x_j; \theta_{\text{words}}) &amp; j \in \{1, \ldots, l\}\\
\mathbf t_k &amp;= \mathrm{embed}_{D_2}(y_k; \theta_{\text{tags}}) &amp; k &lt; i\\
\mathbf u_{1:l} &amp;= \mathrm{birnn}_{2K}(\mathbf e_{1:l}; \theta_{\text{bienc}})\\
\mathbf v_i &amp;= \mathrm{rnnstep}_K(\mathbf v_{i-1},  \mathbf t_{i-1}; \theta_{\text{dec}})\\
\mathbf s_i &amp;= \mathrm{ffnn}_C(\mathrm{concat}(\mathbf u_i, \mathbf v_i); \theta_{\text{out}})\\
\mathbf g(i, x_{1:l}) &amp;= \mathrm{softmax}(\mathbf s_i)
\end{align}\]</span></p>
<p>Again, we have two different embedding layers, one for words and one for tags. Again, we use a bidirectional rnn to encode the whole document. Now, for the <span class="math inline">\(i\)</span>th position, we use an RNN generator/decoder cell to encode the complete history of previous tags. We then predict the logits by using an FFNN to combine the history encoding with the document encoding for position <span class="math inline">\(i\)</span>.</p>
<p>There’s yet another way to parameterise this model, in which we let the RNN decoder compose the features of the history with the features of the document:</p>
<p><span class="math display">\[\begin{align}
\mathbf e_j &amp;= \mathrm{embed}_{D_1}(x_j; \theta_{\text{words}}) &amp; j \in \{1, \ldots, l\}\\
\mathbf t_k &amp;= \mathrm{embed}_{D_2}(y_k; \theta_{\text{tags}}) &amp; k &lt; i\\
\mathbf u_{1:l} &amp;= \mathrm{birnn}_{2K}(\mathbf e_{1:l}; \theta_{\text{bienc}})\\
\mathbf v_i &amp;= \mathrm{rnnstep}_K(\mathbf v_{i-1}, \mathrm{concat}(\mathbf u_i, \mathbf t_{i-1}); \theta_{\text{dec}})\\
\mathbf s_i &amp;= \mathrm{affine}_C(\mathbf v_i; \theta_{\text{out}})\\
\mathbf g(i, x_{1:l}) &amp;= \mathrm{softmax}(\mathbf s_i)
\end{align}\]</span></p>
<p>Both are very good options.</p>
<div id="cell-94" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AutoregressiveTagger(Tagger):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, tagset_size: <span class="bu">int</span>, word_embed_dim: <span class="bu">int</span>, tag_embed_dim: <span class="bu">int</span>, hidden_size: <span class="bu">int</span>, p_drop<span class="op">=</span><span class="fl">0.</span>, pad_id<span class="op">=</span><span class="dv">0</span>, bos_id<span class="op">=</span><span class="dv">1</span>, eos_id<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co">        ngram_size: longest ngram (for tag sequence)</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_size: number of known words</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="co">        tagset_size: number of known tags</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="co">        word_embed_dim: dimensionality of word embeddings</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="co">        tag_embed_dim: dimensionality of tag embeddings (needed to encode the history of ngram_size-1 tags)</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: dimensionality of hidden layers</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(vocab_size<span class="op">=</span>vocab_size, tagset_size<span class="op">=</span>tagset_size, pad_id<span class="op">=</span>pad_id, bos_id<span class="op">=</span>bos_id, eos_id<span class="op">=</span>eos_id)        </span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word_embed_dim <span class="op">=</span> word_embed_dim</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tag_embed_dim <span class="op">=</span> tag_embed_dim</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TextEncoder(</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>            vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>            word_embed_dim<span class="op">=</span>word_embed_dim,</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span>hidden_size,</span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>            reduce_mean<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>            pad_id<span class="op">=</span>pad_id</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we need to embed tags in the history </span></span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tag_embed <span class="op">=</span> nn.Embedding(tagset_size, embedding_dim<span class="op">=</span>tag_embed_dim) </span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.LSTM(</span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a>            input_size<span class="op">=</span>tag_embed_dim,</span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span>hidden_size,</span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">False</span></span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for each position i, we need to combine the encoding of x[i] in context </span></span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># as well as the history of ngram_size-1 tags</span></span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we use a FFNN for that:</span></span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits_predictor <span class="op">=</span> nn.Sequential(</span>
<span id="cb81-38"><a href="#cb81-38" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb81-39"><a href="#cb81-39" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.encoder.output_dim <span class="op">+</span> hidden_size, hidden_size),</span>
<span id="cb81-40"><a href="#cb81-40" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb81-41"><a href="#cb81-41" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb81-42"><a href="#cb81-42" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, tagset_size),</span>
<span id="cb81-43"><a href="#cb81-43" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb81-44"><a href="#cb81-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-45"><a href="#cb81-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_state <span class="op">=</span> nn.Sequential(</span>
<span id="cb81-46"><a href="#cb81-46" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb81-47"><a href="#cb81-47" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.encoder.output_dim, hidden_size),</span>
<span id="cb81-48"><a href="#cb81-48" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()</span>
<span id="cb81-49"><a href="#cb81-49" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb81-50"><a href="#cb81-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_cell <span class="op">=</span> nn.Sequential(</span>
<span id="cb81-51"><a href="#cb81-51" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb81-52"><a href="#cb81-52" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.encoder.output_dim, hidden_size),</span>
<span id="cb81-53"><a href="#cb81-53" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()</span>
<span id="cb81-54"><a href="#cb81-54" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb81-55"><a href="#cb81-55" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb81-56"><a href="#cb81-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, y_in):</span>
<span id="cb81-57"><a href="#cb81-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb81-58"><a href="#cb81-58" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameterise the conditional distributions over Y[i] given history y[:i] and all of x.</span></span>
<span id="cb81-59"><a href="#cb81-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-60"><a href="#cb81-60" aria-hidden="true" tabindex="-1"></a><span class="co">        This procedure takes care that the ith output distribution conditions only on the n-1 observations before y[i].</span></span>
<span id="cb81-61"><a href="#cb81-61" aria-hidden="true" tabindex="-1"></a><span class="co">        It also takes care of padding to the left with BOS symbols.</span></span>
<span id="cb81-62"><a href="#cb81-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-63"><a href="#cb81-63" aria-hidden="true" tabindex="-1"></a><span class="co">        x: word sequences [batch_size, max_length]</span></span>
<span id="cb81-64"><a href="#cb81-64" aria-hidden="true" tabindex="-1"></a><span class="co">        y_in: history of tag sequences  [batch_size, max_length]</span></span>
<span id="cb81-65"><a href="#cb81-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-66"><a href="#cb81-66" aria-hidden="true" tabindex="-1"></a><span class="co">        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.</span></span>
<span id="cb81-67"><a href="#cb81-67" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb81-68"><a href="#cb81-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-69"><a href="#cb81-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Let's start by encoding the conditioning sequences        </span></span>
<span id="cb81-70"><a href="#cb81-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, enc_dim]</span></span>
<span id="cb81-71"><a href="#cb81-71" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> <span class="va">self</span>.encoder(x)        </span>
<span id="cb81-72"><a href="#cb81-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-73"><a href="#cb81-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># here we pad the tag sequence with BOS on the left</span></span>
<span id="cb81-74"><a href="#cb81-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_len, tag_emb_dim]</span></span>
<span id="cb81-75"><a href="#cb81-75" aria-hidden="true" tabindex="-1"></a>        t_in <span class="op">=</span> <span class="va">self</span>.tag_embed(y_in)</span>
<span id="cb81-76"><a href="#cb81-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-77"><a href="#cb81-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encode the histories</span></span>
<span id="cb81-78"><a href="#cb81-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_len, hidden_size]</span></span>
<span id="cb81-79"><a href="#cb81-79" aria-hidden="true" tabindex="-1"></a>        v, _ <span class="op">=</span> <span class="va">self</span>.decoder(t_in)</span>
<span id="cb81-80"><a href="#cb81-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-81"><a href="#cb81-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now we can combine the encodings of x and the encodings of histories, we do so via concatenation</span></span>
<span id="cb81-82"><a href="#cb81-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># since there's a fixed number of such encodings per step of the sequence</span></span>
<span id="cb81-83"><a href="#cb81-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, 3*hidden_size]</span></span>
<span id="cb81-84"><a href="#cb81-84" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> torch.cat([u, v], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb81-85"><a href="#cb81-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We are now ready to map the state of each step of the sequence to a C-dimensional vector of logits</span></span>
<span id="cb81-86"><a href="#cb81-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we do so using our FFNN</span></span>
<span id="cb81-87"><a href="#cb81-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch_size, max_length, tagset_size]</span></span>
<span id="cb81-88"><a href="#cb81-88" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.logits_predictor(u)</span>
<span id="cb81-89"><a href="#cb81-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-90"><a href="#cb81-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Categorical(logits<span class="op">=</span>s)</span>
<span id="cb81-91"><a href="#cb81-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-92"><a href="#cb81-92" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> greedy(<span class="va">self</span>,x):</span>
<span id="cb81-93"><a href="#cb81-93" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb81-94"><a href="#cb81-94" aria-hidden="true" tabindex="-1"></a><span class="co">        Draws a number of samples from the model, each sample is a complete sequence.</span></span>
<span id="cb81-95"><a href="#cb81-95" aria-hidden="true" tabindex="-1"></a><span class="co">        We impose a maximum number of steps, to avoid infinite loops.</span></span>
<span id="cb81-96"><a href="#cb81-96" aria-hidden="true" tabindex="-1"></a><span class="co">        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.</span></span>
<span id="cb81-97"><a href="#cb81-97" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb81-98"><a href="#cb81-98" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb81-99"><a href="#cb81-99" aria-hidden="true" tabindex="-1"></a>        max_length <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb81-100"><a href="#cb81-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-101"><a href="#cb81-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb81-102"><a href="#cb81-102" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the beginning we do not know the tag sequence</span></span>
<span id="cb81-103"><a href="#cb81-103" aria-hidden="true" tabindex="-1"></a>            <span class="co"># but NNs work with fixed dimensional tensors, </span></span>
<span id="cb81-104"><a href="#cb81-104" aria-hidden="true" tabindex="-1"></a>            <span class="co"># so we allocate a tensor full of BOS codes</span></span>
<span id="cb81-105"><a href="#cb81-105" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> torch.full((batch_size, max_length <span class="op">+</span> <span class="dv">1</span>), <span class="va">self</span>.bos, device<span class="op">=</span><span class="va">self</span>.tag_embed.weight.device) </span>
<span id="cb81-106"><a href="#cb81-106" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Per step</span></span>
<span id="cb81-107"><a href="#cb81-107" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb81-108"><a href="#cb81-108" aria-hidden="true" tabindex="-1"></a>                <span class="co"># we parameterise a cpd for Y[i]|X=x</span></span>
<span id="cb81-109"><a href="#cb81-109" aria-hidden="true" tabindex="-1"></a>                <span class="co"># note that the forward method takes care of not conditioning on y[i] itself</span></span>
<span id="cb81-110"><a href="#cb81-110" aria-hidden="true" tabindex="-1"></a>                <span class="co"># and only using the ngram_size-1 previous tags</span></span>
<span id="cb81-111"><a href="#cb81-111" aria-hidden="true" tabindex="-1"></a>                <span class="co"># at this point, the tag y[i] is a dummy code</span></span>
<span id="cb81-112"><a href="#cb81-112" aria-hidden="true" tabindex="-1"></a>                <span class="co"># the forward method recomputes all cds in the batch, this will include the cpd for Y[i]</span></span>
<span id="cb81-113"><a href="#cb81-113" aria-hidden="true" tabindex="-1"></a>                <span class="co"># [batch_size, max_len, C] </span></span>
<span id="cb81-114"><a href="#cb81-114" aria-hidden="true" tabindex="-1"></a>                cpds <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x, y_in<span class="op">=</span>y[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb81-115"><a href="#cb81-115" aria-hidden="true" tabindex="-1"></a>                <span class="co"># we get their modes via argmax</span></span>
<span id="cb81-116"><a href="#cb81-116" aria-hidden="true" tabindex="-1"></a>                <span class="co"># [batch_size, max_len]</span></span>
<span id="cb81-117"><a href="#cb81-117" aria-hidden="true" tabindex="-1"></a>                modes <span class="op">=</span> torch.argmax(cpds.probs, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb81-118"><a href="#cb81-118" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb81-119"><a href="#cb81-119" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Here we update the current token to the freshly obtained mode</span></span>
<span id="cb81-120"><a href="#cb81-120" aria-hidden="true" tabindex="-1"></a>                <span class="co">#  and also replace the token by 0 (pad) in case the sentence is already complete</span></span>
<span id="cb81-121"><a href="#cb81-121" aria-hidden="true" tabindex="-1"></a>                y[:, i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> modes[:, i]     </span>
<span id="cb81-122"><a href="#cb81-122" aria-hidden="true" tabindex="-1"></a>            <span class="co"># discard the BOS token</span></span>
<span id="cb81-123"><a href="#cb81-123" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y[:,<span class="dv">1</span>:]           </span>
<span id="cb81-124"><a href="#cb81-124" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where we had a PAD token in x, we change the y token to PAD too</span></span>
<span id="cb81-125"><a href="#cb81-125" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> torch.where(x <span class="op">!=</span> <span class="va">self</span>.pad, y, torch.zeros_like(y) <span class="op">+</span> <span class="va">self</span>.pad)</span>
<span id="cb81-126"><a href="#cb81-126" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb81-127"><a href="#cb81-127" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> y</span>
<span id="cb81-128"><a href="#cb81-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-129"><a href="#cb81-129" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mode(<span class="va">self</span>, x):</span>
<span id="cb81-130"><a href="#cb81-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"The search for the mode of the autoregressive tagger is intractable, consider using `greedy` or a sampling-based approximation. "</span>)</span>
<span id="cb81-131"><a href="#cb81-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-132"><a href="#cb81-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _sample(<span class="va">self</span>, x):</span>
<span id="cb81-133"><a href="#cb81-133" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb81-134"><a href="#cb81-134" aria-hidden="true" tabindex="-1"></a><span class="co">        Draws a number of samples from the model, each sample is a complete sequence.</span></span>
<span id="cb81-135"><a href="#cb81-135" aria-hidden="true" tabindex="-1"></a><span class="co">        We impose a maximum number of steps, to avoid infinite loops.</span></span>
<span id="cb81-136"><a href="#cb81-136" aria-hidden="true" tabindex="-1"></a><span class="co">        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.</span></span>
<span id="cb81-137"><a href="#cb81-137" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb81-138"><a href="#cb81-138" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb81-139"><a href="#cb81-139" aria-hidden="true" tabindex="-1"></a>        max_length <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb81-140"><a href="#cb81-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-141"><a href="#cb81-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb81-142"><a href="#cb81-142" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the beginning we do not know the tag sequence</span></span>
<span id="cb81-143"><a href="#cb81-143" aria-hidden="true" tabindex="-1"></a>            <span class="co"># but NNs work with fixed dimensional tensors, </span></span>
<span id="cb81-144"><a href="#cb81-144" aria-hidden="true" tabindex="-1"></a>            <span class="co"># so we allocate a tensor full of BOS codes</span></span>
<span id="cb81-145"><a href="#cb81-145" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> torch.full((batch_size, max_length <span class="op">+</span> <span class="dv">1</span>), <span class="va">self</span>.bos, device<span class="op">=</span><span class="va">self</span>.tag_embed.weight.device) </span>
<span id="cb81-146"><a href="#cb81-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-147"><a href="#cb81-147" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Per step</span></span>
<span id="cb81-148"><a href="#cb81-148" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb81-149"><a href="#cb81-149" aria-hidden="true" tabindex="-1"></a>                <span class="co"># we parameterise a cpd for Y[i]|X=x</span></span>
<span id="cb81-150"><a href="#cb81-150" aria-hidden="true" tabindex="-1"></a>                <span class="co"># note that the forward method takes care of not conditioning on y[i] itself</span></span>
<span id="cb81-151"><a href="#cb81-151" aria-hidden="true" tabindex="-1"></a>                <span class="co"># and only using the ngram_size-1 previous tags</span></span>
<span id="cb81-152"><a href="#cb81-152" aria-hidden="true" tabindex="-1"></a>                <span class="co"># at this point, the tag y[i] is a dummy code</span></span>
<span id="cb81-153"><a href="#cb81-153" aria-hidden="true" tabindex="-1"></a>                <span class="co"># the forward method recomputes all cds in the batch, this will include the cpd for Y[i]</span></span>
<span id="cb81-154"><a href="#cb81-154" aria-hidden="true" tabindex="-1"></a>                <span class="co"># we get their modes via argmax</span></span>
<span id="cb81-155"><a href="#cb81-155" aria-hidden="true" tabindex="-1"></a>                <span class="co"># [batch_size, max_len, C]</span></span>
<span id="cb81-156"><a href="#cb81-156" aria-hidden="true" tabindex="-1"></a>                cpds <span class="op">=</span> <span class="va">self</span>(x<span class="op">=</span>x, y_in<span class="op">=</span>y[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb81-157"><a href="#cb81-157" aria-hidden="true" tabindex="-1"></a>                <span class="co"># [batch_size, max_len]</span></span>
<span id="cb81-158"><a href="#cb81-158" aria-hidden="true" tabindex="-1"></a>                samples <span class="op">=</span> cpds.sample()</span>
<span id="cb81-159"><a href="#cb81-159" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb81-160"><a href="#cb81-160" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Here we update the current token to the freshly obtained mode</span></span>
<span id="cb81-161"><a href="#cb81-161" aria-hidden="true" tabindex="-1"></a>                <span class="co">#  and also replace the token by 0 (pad) in case the sentence is already complete</span></span>
<span id="cb81-162"><a href="#cb81-162" aria-hidden="true" tabindex="-1"></a>                y[:, i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> samples[:,i]</span>
<span id="cb81-163"><a href="#cb81-163" aria-hidden="true" tabindex="-1"></a>            <span class="co"># discard the BOS token</span></span>
<span id="cb81-164"><a href="#cb81-164" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y[:,<span class="dv">1</span>:]</span>
<span id="cb81-165"><a href="#cb81-165" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where we had a PAD token in x, we change the y token to PAD too</span></span>
<span id="cb81-166"><a href="#cb81-166" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> torch.where(x <span class="op">!=</span> <span class="va">self</span>.pad, y, torch.zeros_like(y) <span class="op">+</span> <span class="va">self</span>.pad)</span>
<span id="cb81-167"><a href="#cb81-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-168"><a href="#cb81-168" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> y</span>
<span id="cb81-169"><a href="#cb81-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-170"><a href="#cb81-170" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb81-171"><a href="#cb81-171" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb81-172"><a href="#cb81-172" aria-hidden="true" tabindex="-1"></a><span class="co">        Draws a number of samples from the model, each sample is a complete sequence.</span></span>
<span id="cb81-173"><a href="#cb81-173" aria-hidden="true" tabindex="-1"></a><span class="co">        We impose a maximum number of steps, to avoid infinite loops.</span></span>
<span id="cb81-174"><a href="#cb81-174" aria-hidden="true" tabindex="-1"></a><span class="co">        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.</span></span>
<span id="cb81-175"><a href="#cb81-175" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb81-176"><a href="#cb81-176" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb81-177"><a href="#cb81-177" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._sample(x)</span>
<span id="cb81-178"><a href="#cb81-178" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb81-179"><a href="#cb81-179" aria-hidden="true" tabindex="-1"></a>            samples <span class="op">=</span> [<span class="va">self</span>._sample(x) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(sample_size)]</span>
<span id="cb81-180"><a href="#cb81-180" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> torch.stack(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-95" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_autoreg_tagger(training_x, training_y, vocab_x, vocab_y):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    seed_all()</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    toy_ar_tagger <span class="op">=</span> AutoregressiveTagger(</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>        vocab_size<span class="op">=</span><span class="bu">len</span>(vocab_x), </span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>        tagset_size<span class="op">=</span><span class="bu">len</span>(vocab_y), </span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>        word_embed_dim<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        tag_embed_dim<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    )    </span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">type</span>(toy_ar_tagger(</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])),</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        torch.from_numpy(vocab_y.batch_encode(training_y[:<span class="dv">2</span>])))</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>    ) <span class="kw">is</span> td.Categorical</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_ar_tagger.log_prob(</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), </span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_y.batch_encode(training_y[:<span class="dv">2</span>]))</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>        ).shape <span class="op">==</span> (<span class="dv">2</span>,)</span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss(toy_ar_tagger,</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), </span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>            torch.from_numpy(vocab_y.batch_encode(training_y[:<span class="dv">2</span>]))</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        ).shape <span class="op">==</span> <span class="bu">tuple</span>()</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_ar_tagger.sample(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>]))).shape <span class="op">==</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_ar_tagger.sample(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>])), <span class="dv">3</span>).shape <span class="op">==</span> (<span class="dv">3</span>,) <span class="op">+</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> toy_ar_tagger.greedy(torch.from_numpy(vocab_x.batch_encode(training_x[:<span class="dv">2</span>]))).shape <span class="op">==</span> vocab_x.batch_encode(training_x[:<span class="dv">2</span>]).shape</span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>test_autoreg_tagger(tagger_training_x, tagger_training_y, word_vocab, tag_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Because labelling is a chain of classification decisions, we can also evaluate our tagger in terms of accuracy of its decisions. For that we need a decision rule. Normally, in NLP, we use the most probable tag sequence as a decision. That is, given a sentence <span class="math inline">\(x_{1:l}\)</span> we search in the space <span class="math inline">\(\{1, \ldots, C\}^l\)</span> of all tag sequences of length <span class="math inline">\(l\)</span>, for the one sequence that the model assigns highest probability to (i.e., the <em>mode</em> of the conditional distribution):</p>
<p><span class="math display">\[\begin{align}
y^\star &amp;= \arg\max_{c_{1:l} \in \{1, \ldots, C\}^l}~ \log P(G=c_{1:l}|S=x_{1:l})
\end{align}\]</span></p>
<p>This search is defined over an extremely large space and is generally not tractable. For some types of tagger, because of their conditional independence assumptions, this search may be doable in polynomial time (as a function of sequence length), for others this is not at all possible.</p>
<p>For the basic tagger, which treats the tags as independent given the sentence, this search can be done exactly, because greedily maximising each step independently is equivalently to maximising the joint assignment of the entire sequence for that model.</p>
<p><strong>Search for the unigram tagger</strong></p>
<p>We search for the best tag in each position, which takes time <span class="math inline">\(\mathcal O(C)\)</span> per position,</p>
<p><span class="math display">\[\begin{align}
y^\star_i &amp;= \arg\max_{c \in \{1, \ldots, C\}}~ \log P(Y_i=c|S=x_{1:l})
\end{align}\]</span></p>
<p>and put them together in a sequence. The total operation takes time <span class="math inline">\(\mathcal O(l \times C)\)</span>.</p>
<p><strong>Search for the autoregressive tagger</strong></p>
<p>The autoregressive tagger makes no conditional independence assumptions, and the search problem is genuinely intractable for this model. Being intractable means there is not efficient algorithm known to be able to handle it. In fact, the current hypothesis is that an efficient (by efficient we mean that it runs in polynomial time as a function of <span class="math inline">\(l\)</span>) is actually impossible in standard computer architectures. Problems of this kind are called NP-complete.</p>
<p>For this tutorial, we will again use the greedy approximation:</p>
<p><span class="math display">\[\begin{align}
\hat y_i &amp;= \arg\max_{c \in \{1, \ldots, C\}}~ \log P(Y_i=c|S=x_{1:l}, H=\hat y_1^{i-1})
\end{align}\]</span></p>
<p>where we solve the argmax locally per position in order from left-to-right. For each step <span class="math inline">\(Y_i\)</span> we condition on the already predicted argmax for all the preceding steps.</p>
<hr>
<p>Once we have a search algorithm in place to make predictions we can compute accuracy and/or other metrics common for classification.</p>
<div id="cell-97" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GreedyMode(DecisionRule):</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, model, x):</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model.greedy(x)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On GPU, this should take just about 2 minutes:</p>
<div id="cell-99" class="cell" data-outputid="3a549abe-87a9-46ca-ecd6-a356bca54f3a" data-execution_count="56">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>seed_all() <span class="co"># reset random number generators before creating your model and training it</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>tagger <span class="op">=</span> BasicTagger(</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(word_vocab), </span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    tagset_size<span class="op">=</span><span class="bu">len</span>(tag_vocab), </span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    word_embed_dim<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an Adam optimiser</span></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> opt.Adam(tagger.parameters(), lr<span class="op">=</span><span class="fl">5e-3</span>)</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model"</span>)</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tagger)</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a><span class="co"># report number of parameters</span></span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>tagger<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_neural_model(</span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>    tagger, optimiser, </span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a>    decision_rule<span class="op">=</span>ExactMode(), </span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>TaggedCorpus(tagger_training_x, tagger_training_y, word_vocab, tag_vocab), </span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>TaggedCorpus(tagger_dev_x, tagger_dev_y, word_vocab, tag_vocab), </span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a>    report_fn<span class="op">=</span>report_tagging, report_metrics<span class="op">=</span>[<span class="st">'accuracy'</span>],</span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">10</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss and validation checks</span></span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'loss'</span>])), log[<span class="st">'loss'</span>])</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">'steps'</span>)</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">'training loss'</span>)</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'D'</span>])), log[<span class="st">'D'</span>])</span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">'steps (in 100s)'</span>)</span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">'D given dev'</span>)</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'accuracy'</span>])), log[<span class="st">'accuracy'</span>])</span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">'steps (in 10s)'</span>)</span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">'dev acc'</span>)</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model
BasicTagger(
  (encoder): TextEncoder(
    (word_embed): Embedding(3358, 4)
    (encoder): LSTM(4, 8, batch_first=True, bidirectional=True)
  )
  (logits_predictor): Sequential(
    (0): Dropout(p=0, inplace=False)
    (1): Linear(in_features=16, out_features=16, bias=True)
  )
)
Model size: 14,600 parameters</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9bfdc58b44824f4e9347dd5ea052e737","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-57-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-100" class="cell" data-outputid="7d896d5d-333d-4c5a-e294-3c0691c103d9" data-execution_count="57">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>seed_all() <span class="co"># reset random number generators before creating your model and training it</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>ar_tagger <span class="op">=</span> AutoregressiveTagger(</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(word_vocab), </span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    tagset_size<span class="op">=</span><span class="bu">len</span>(tag_vocab), </span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>    word_embed_dim<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    tag_embed_dim<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an Adam optimiser</span></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> opt.Adam(ar_tagger.parameters(), lr<span class="op">=</span><span class="fl">5e-3</span>)</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model"</span>)</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ar_tagger)</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a><span class="co"># report number of parameters</span></span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>ar_tagger<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_neural_model(</span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a>    ar_tagger, optimiser, </span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a>    decision_rule<span class="op">=</span>GreedyMode(), </span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>TaggedCorpus(tagger_training_x, tagger_training_y, word_vocab, tag_vocab), </span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>TaggedCorpus(tagger_dev_x, tagger_dev_y, word_vocab, tag_vocab), </span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a>    report_fn<span class="op">=</span>report_tagging, report_metrics<span class="op">=</span>[<span class="st">'accuracy'</span>],</span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">10</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb86-33"><a href="#cb86-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-34"><a href="#cb86-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss and validation checks</span></span>
<span id="cb86-35"><a href="#cb86-35" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb86-36"><a href="#cb86-36" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'loss'</span>])), log[<span class="st">'loss'</span>])</span>
<span id="cb86-37"><a href="#cb86-37" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">'steps'</span>)</span>
<span id="cb86-38"><a href="#cb86-38" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">'training loss'</span>)</span>
<span id="cb86-39"><a href="#cb86-39" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'D'</span>])), log[<span class="st">'D'</span>])</span>
<span id="cb86-40"><a href="#cb86-40" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">'steps (in 100s)'</span>)</span>
<span id="cb86-41"><a href="#cb86-41" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">'D given dev'</span>)</span>
<span id="cb86-42"><a href="#cb86-42" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.arange(<span class="bu">len</span>(log[<span class="st">'accuracy'</span>])), log[<span class="st">'accuracy'</span>])</span>
<span id="cb86-43"><a href="#cb86-43" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">'steps (in 10s)'</span>)</span>
<span id="cb86-44"><a href="#cb86-44" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">'dev acc'</span>)</span>
<span id="cb86-45"><a href="#cb86-45" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb86-46"><a href="#cb86-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model
AutoregressiveTagger(
  (encoder): TextEncoder(
    (word_embed): Embedding(3358, 4)
    (encoder): LSTM(4, 8, batch_first=True, bidirectional=True)
  )
  (tag_embed): Embedding(16, 4)
  (decoder): LSTM(4, 8, batch_first=True)
  (logits_predictor): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=24, out_features=8, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=8, out_features=16, bias=True)
  )
  (init_state): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=16, out_features=8, bias=True)
    (2): Tanh()
  )
  (init_cell): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=16, out_features=8, bias=True)
    (2): Tanh()
  )
)
Model size: 15,456 parameters</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"92eaf5fb34d14e0d8634dba9c41c400e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_1_files/figure-html/cell-58-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-101" class="cell" data-outputid="db3a58c1-e717-4640-bafe-293580a136c6" data-execution_count="58">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>tagger_dev_data <span class="op">=</span> TaggedCorpus(tagger_dev_x, tagger_dev_y, word_vocab, tag_vocab)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>tagger_dev_dl <span class="op">=</span> DataLoader(tagger_dev_data, batch_size<span class="op">=</span><span class="dv">1</span>, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>tagger_dev_data.pad_to_longest)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> tagger_dev_dl:</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sample"</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    y_out <span class="op">=</span> ar_tagger.sample(batch_x.to(my_device))    </span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(word_vocab.batch_decode(batch_x, strip_pad<span class="op">=</span><span class="va">True</span>), tag_vocab.batch_decode(y_out, strip_pad<span class="op">=</span><span class="va">False</span>)):</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">" "</span>.join(<span class="ss">f"</span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> w, t <span class="kw">in</span> <span class="bu">zip</span>(x, y)))</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Greedy"</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    y_out <span class="op">=</span> ar_tagger.greedy(batch_x.to(my_device))    </span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(word_vocab.batch_decode(batch_x, strip_pad<span class="op">=</span><span class="va">True</span>), tag_vocab.batch_decode(y_out, strip_pad<span class="op">=</span><span class="va">False</span>)):</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">" "</span>.join(<span class="ss">f"</span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> w, t <span class="kw">in</span> <span class="bu">zip</span>(x, y)))</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample
-UNK-/NOUN ,/. a/DET -UNK-/NOUN -UNK-/VERB the/DET -UNK-/NOUN of/ADP -UNK-/ADJ -UNK-/NOUN ,/. is/VERB marketed/VERB *-1/X as/ADP a/DET $/. -UNK-/NUM *u*/X -UNK-/VERB for/ADP -UNK-/ADJ -UNK-/NOUN ./. -EOS-/-EOS-

Greedy
-UNK-/NOUN ,/. a/DET -UNK-/NOUN -UNK-/VERB the/DET -UNK-/NOUN of/ADP -UNK-/ADJ -UNK-/NOUN ,/. is/VERB marketed/VERB *-1/X as/ADP a/DET $/. -UNK-/NUM *u*/X -UNK-/VERB for/ADP -UNK-/NOUN -UNK-/NOUN ./. -EOS-/-EOS-</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="exercise-design-your-own-deep-probabilistic-model-optional-non-graded" class="level2">
<h2 class="anchored" data-anchor-id="exercise-design-your-own-deep-probabilistic-model-optional-non-graded">8. Exercise: design your own deep probabilistic model (optional, non-graded)</h2>
<p>Now it’s time to design your own probabilistic model. The objective is to find a dataset and design a probabilistic model for it, parameterised by a neural network. Be conscious about the distributions you choose to model your data with, make sure they are appropriate for the data! Use maximum likelihood estimation for estimating neural network parameters. Also, be conscious about the decision rule you use. Can you obtain the mode exactly and is it appropriate under your model? Is there perhaps another criterion more sensible? Can you think of a way to use maximisation of expected utility for your model? As we have already provided many helper classes for learning features from text data, we encourage you to look into textual datasets, but note the data you are modelling can still be of any domain: ordinal, real numbers, discrete, structured, etc. Some good places to look for such data are:</p>
<ul>
<li>NLTK: https://www.nltk.org/nltk_data/</li>
<li>HuggingFace: https://huggingface.co/datasets</li>
<li>Kaggle: https://www.kaggle.com/datasets</li>
</ul>
<p>Remarks on other types of data:</p>
<ul>
<li>some structured data are fixed-dimensional (e.g., an image of size HxWxC), some models are built upon a chain rule factorisation just like the sequence labeller above, but their parameterisation might exploit the fact that the dimensionality is fixed (a good example is a <a href="https://arxiv.org/abs/1502.03509">MADE</a>, you can ask Wilker more about those);</li>
<li>some structured data are made of continuous parts (e.g., a time series in the finance domain, or in medical applications), in those cases your conditional factors are from a numerical family (e.g., Poisson, Normal, Gamma, etc), sometimes you need a distribution more powerful than the typical unimodal exponential family, in those cases you can use a mixture Model (we discuss those in the second module in the series) or a <a href="https://arxiv.org/abs/1912.02762">normalising flow</a> (you can ask Wilker more about those, but note that Stratis will discuss NFs along with other advanced generative models)</li>
</ul>


</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"0409946d94e0482d8243d968e28dfe47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0459e8ee035c47a9ae4f30ed77b7eae4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1122936450c9426dad96d06758aa27d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eee0d5ef5482493fb94c4448e3766429","max":2490,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8c1f5b28f154c2493476a8acf13a883","value":2490}},"16979a9ae8fe4f5a8b72de005bd430a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84f2eb3c15d24a69971ee9e90228c38f","placeholder":"​","style":"IPY_MODEL_207e4eda7ade4f2db9df20a827b270ad","value":"100%"}},"207e4eda7ade4f2db9df20a827b270ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23568fdc5e234cd1b6ec7d7a9cc63f39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4851518491e34a45a1999f156a8319b5","placeholder":"​","style":"IPY_MODEL_5d6e6a2e29084d14ae1cb46b5ec52adc","value":"100%"}},"249eb32ff5fb4498ba83dbd697aba32d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32b7065e4c244c22b7cd5b69e2305777":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"427361bf79ac4ab1980ccd29d450e604":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4312d5ff4e2a49ebbb9e1d2fd86226ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47a9d27c793549e7a9f6cfe8315d8d21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4851518491e34a45a1999f156a8319b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49bccad39d1b4f1690181bcd2b1307f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50b9f55188de444f83ba5b8a03c0a6bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23568fdc5e234cd1b6ec7d7a9cc63f39","IPY_MODEL_ef0d354416714ad08e5ecb2a996b59fe","IPY_MODEL_cc4e5930621440459849e2ed1b116ca7"],"layout":"IPY_MODEL_8fe3ac02462b4221b8d658bc6ec6c839"}},"50ed4edb9e8e463ea0fbc5fc4838f8bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47a9d27c793549e7a9f6cfe8315d8d21","placeholder":"​","style":"IPY_MODEL_bf0d02ce7a5b43b8b0f45d8cff7ec981","value":"100%"}},"51bddb1821dc4f87bfb3c0ec8915e0b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3268e24e96143a8aaae1e5e5bd9c50a","placeholder":"​","style":"IPY_MODEL_86c0a8e93234405db7e39356cd653460","value":" 2490/2490 [01:17&lt;00:00, 56.50it/s, loss=2.02, D=4.69, accuracy=0.91]"}},"53845fb5bcc94fc39255f0b5161876c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58d460c295e9466ab846f5002507a6f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_427361bf79ac4ab1980ccd29d450e604","placeholder":"​","style":"IPY_MODEL_0459e8ee035c47a9ae4f30ed77b7eae4","value":" 2490/2490 [00:32&lt;00:00, 60.35it/s, loss=2.68, D=4.83, accuracy=0.93]"}},"5d6e6a2e29084d14ae1cb46b5ec52adc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66d4bf21b3d440a0a0e90a3a49da21fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_249eb32ff5fb4498ba83dbd697aba32d","placeholder":"​","style":"IPY_MODEL_bce4caf6bb52457b88327fc6b44c2365","value":"100%"}},"68072d9827564899ac9b09aed4590109":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_914e7b562c7141209f6ec82f362455e4","max":1800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49bccad39d1b4f1690181bcd2b1307f7","value":1800}},"77a16065b5b5415bba0b32371fdfe1c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66d4bf21b3d440a0a0e90a3a49da21fe","IPY_MODEL_68072d9827564899ac9b09aed4590109","IPY_MODEL_8f894b9e6e8f466e821e12ef3a659332"],"layout":"IPY_MODEL_0409946d94e0482d8243d968e28dfe47"}},"7f5562d176924d27adf7af3f3c962607":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84f2eb3c15d24a69971ee9e90228c38f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86a443f6f2094b919d0e3ad65c33e620":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16979a9ae8fe4f5a8b72de005bd430a3","IPY_MODEL_e95bf05893964b29b5b39bed57f9beb8","IPY_MODEL_58d460c295e9466ab846f5002507a6f5"],"layout":"IPY_MODEL_7f5562d176924d27adf7af3f3c962607"}},"86c0a8e93234405db7e39356cd653460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f894b9e6e8f466e821e12ef3a659332":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32b7065e4c244c22b7cd5b69e2305777","placeholder":"​","style":"IPY_MODEL_a1321906674f4f408dd6a58713dd7e9b","value":" 1800/1800 [01:04&lt;00:00, 28.50it/s, loss=0.21, D=5.04, accuracy=0.41]"}},"8fe3ac02462b4221b8d658bc6ec6c839":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"914e7b562c7141209f6ec82f362455e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"928908907613408294a95d8496d30d91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1321906674f4f408dd6a58713dd7e9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2bf7fb8b7e64342b66f488a6af0cb65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8c1f5b28f154c2493476a8acf13a883":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bce4caf6bb52457b88327fc6b44c2365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf0d02ce7a5b43b8b0f45d8cff7ec981":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc4e5930621440459849e2ed1b116ca7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_928908907613408294a95d8496d30d91","placeholder":"​","style":"IPY_MODEL_dca4117080474ff19fd1bbf3c313e7d4","value":" 400/400 [00:47&lt;00:00, 13.61it/s, loss=3.87, D=3.72, MSE=62.04, MAE=5.94, MdAE=7.00]"}},"d3268e24e96143a8aaae1e5e5bd9c50a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6faa11802014830b197875257918adb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dca4117080474ff19fd1bbf3c313e7d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4eab33aa8ce447d96f6ea840e496900":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50ed4edb9e8e463ea0fbc5fc4838f8bb","IPY_MODEL_1122936450c9426dad96d06758aa27d1","IPY_MODEL_51bddb1821dc4f87bfb3c0ec8915e0b0"],"layout":"IPY_MODEL_fb5871ea5d994703af85d71cd04df5d1"}},"e95bf05893964b29b5b39bed57f9beb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6faa11802014830b197875257918adb","max":2490,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53845fb5bcc94fc39255f0b5161876c7","value":2490}},"eee0d5ef5482493fb94c4448e3766429":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef0d354416714ad08e5ecb2a996b59fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2bf7fb8b7e64342b66f488a6af0cb65","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4312d5ff4e2a49ebbb9e1d2fd86226ea","value":400}},"fb5871ea5d994703af85d71cd04df5d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>