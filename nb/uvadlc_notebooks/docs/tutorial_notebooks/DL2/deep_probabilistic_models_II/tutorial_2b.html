<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DPM 2 - Variational Inference for Deep Continuous LVMs – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intended-learning-outcomes" id="toc-intended-learning-outcomes" class="nav-link active" data-scroll-target="#intended-learning-outcomes">0. Intended Learning Outcomes</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">1. Data</a></li>
  <li><a href="#latent-variable-models" id="toc-latent-variable-models" class="nav-link" data-scroll-target="#latent-variable-models">2. Latent variable models</a>
  <ul class="collapse">
  <li><a href="#prior-networks" id="toc-prior-networks" class="nav-link" data-scroll-target="#prior-networks">2.1 Prior networks</a></li>
  <li><a href="#conditional-probability-distributions" id="toc-conditional-probability-distributions" class="nav-link" data-scroll-target="#conditional-probability-distributions">2.2 Conditional probability distributions</a></li>
  <li><a href="#joint-distribution" id="toc-joint-distribution" class="nav-link" data-scroll-target="#joint-distribution">2.3 Joint distribution</a></li>
  </ul></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">3. Learning</a>
  <ul class="collapse">
  <li><a href="#intractable-lvms" id="toc-intractable-lvms" class="nav-link" data-scroll-target="#intractable-lvms">3.1 Intractable LVMs</a></li>
  <li><a href="#inference-model" id="toc-inference-model" class="nav-link" data-scroll-target="#inference-model">3.2 Inference model</a></li>
  <li><a href="#neural-variational-inference" id="toc-neural-variational-inference" class="nav-link" data-scroll-target="#neural-variational-inference">3.3 Neural Variational Inference</a></li>
  </ul></li>
  <li><a href="#beyond" id="toc-beyond" class="nav-link" data-scroll-target="#beyond">4. Beyond</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DPM 2 - Variational Inference for Deep Continuous LVMs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab"></a><br>
<strong>Authors</strong>: Wilker Aziz</p>
<section id="intended-learning-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="intended-learning-outcomes">0. Intended Learning Outcomes</h2>
<p>After this tutorial the student should be able to</p>
<ul>
<li>parameterise a latent variable model with continuous latent variables</li>
<li>estimate parameters using neural variational inference</li>
</ul>
<p><strong>Remark</strong> This tutorial builds upon the previous one and there is a lot of shared/unchanged code. The only changes are:</p>
<ul>
<li>additional prior nets (for continuous variables)</li>
<li>additional CPD nets (for continuous variables)</li>
<li>we changed DRL and forward in the NVIL class such that it can support LVMs trained via SFE or via reparameterisation</li>
</ul>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributions <span class="im">as</span> td</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, OrderedDict</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seed_all(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>seed_all()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">1. Data</h2>
<p>We are going to use toy image datsets for this notebook. These are fixed-dimensional observations for which encoder and decoders are relatively easy to design. This way we can focus on the aspects that are probabilistic in nature.</p>
<div id="cell-7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> FashionMNIST</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split, Dataset</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataloader <span class="im">import</span> DataLoader</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> make_grid</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> opt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A helper to binarize datasets:</p>
<div id="cell-9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Binarizer(Dataset):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ds, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._ds <span class="op">=</span> ds</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._threshold <span class="op">=</span> threshold</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Size of the corpus in number of sequence pairs"""</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._ds)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Return corpus_x[idx] and corpus_y[idx] converted to codes</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        the latter has the EOS code in the end</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> <span class="va">self</span>._ds[idx]</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (x <span class="op">&gt;=</span> <span class="va">self</span>._threshold).<span class="bu">float</span>(), y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>FashionMNIST</p>
<div id="cell-11" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> FashionMNIST(root<span class="op">=</span><span class="st">'data/'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>               transform<span class="op">=</span>transforms.Compose([transforms.Resize(<span class="dv">64</span>), transforms.ToTensor()]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-12" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>img_shape <span class="op">=</span> dataset[<span class="dv">0</span>][<span class="dv">0</span>].shape</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of an image:"</span>, img_shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of an image: torch.Size([1, 64, 64])</code></pre>
</div>
</div>
<p>Let’s make a dev set for ourselves:</p>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> val_size</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>train_ds, val_ds <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_ds), <span class="bu">len</span>(val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(59000, 1000)</code></pre>
</div>
</div>
<p>we suggest that you binarize the data in a first pass through this notebook, but as you will see, we can also model the continuous pixel intensities.</p>
<div id="cell-16" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>bin_data <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bin_data:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> Binarizer(train_ds)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    val_ds <span class="op">=</span> Binarizer(val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-18" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_ds, batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_ds, batch_size, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s visualise a few samples</p>
<div id="cell-20" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> images, y <span class="kw">in</span> train_loader:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'images.shape:'</span>, images.shape)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)    </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(make_grid(images, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>images.shape: torch.Size([64, 1, 64, 64])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="latent-variable-models" class="level2">
<h2 class="anchored" data-anchor-id="latent-variable-models">2. Latent variable models</h2>
<p>We will be using NNs to parameterise a latent variable model, that is, a joint distribution over a collection of random variables (rvs), some of which are observed, some of which are not.</p>
<p>We are interested in two random variables (rvs):</p>
<ul>
<li>a discrete latent code <span class="math inline">\(Z \in \mathcal Z\)</span></li>
<li>and an image <span class="math inline">\(X \in \mathcal X \subseteq \mathbb R^D\)</span></li>
</ul>
<p>In this tutorial, <span class="math inline">\(x\)</span> is has a number <span class="math inline">\(C\)</span> of channels, a certain width <span class="math inline">\(W\)</span> and a certain height <span class="math inline">\(H\)</span>, so <span class="math inline">\(\mathcal X \subseteq \mathbb R^{C \times W \times H}\)</span>. Because we have fixed <span class="math inline">\(D = C \times W \times H\)</span>, <span class="math inline">\(\mathcal X\)</span> is finite-dimensional, but this need not be the case in general (for example, in a different domain, <span class="math inline">\(\mathcal X\)</span> could be the unbounded space of all sentences of arbitrary lenght). We may treat the pixel intensities as discrete or continuous, as long as we choose an appropriate pmf/pdf for each case.</p>
<p>In this tutorial we will look into continuous latent codes. That is, <span class="math inline">\(z \in \mathcal Z \subseteq \mathbb R^K\)</span>.</p>
<p>We specify a joint distribution over <span class="math inline">\(\mathcal Z \times \mathcal X\)</span> by specifying a joint probability density function (pdf):</p>
<p><span class="math display">\[\begin{align}
p_{ZX}(z, x|\theta) &amp;= p_Z(z|\theta)p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>Here <span class="math inline">\(\theta\)</span> denotes the parameters of the NNs that parameterise the pdf <span class="math inline">\(p_Z\)</span> and the pdf <span class="math inline">\(p_{X|Z=z}\)</span> (for any given <span class="math inline">\(z\)</span>).</p>
<p>In this tutorial, the prior is fixed, but in general it need not be. We do not have additional predictors to condition on, but in some application domains you may have (e.g., in imagine captioning, we may be interested in a joint for a caption <span class="math inline">\(y\)</span> and a latent code <span class="math inline">\(z\)</span> given an image <span class="math inline">\(x\)</span>; in image generation, we may be interested in a joint distribution for an image <span class="math inline">\(x\)</span> and a latent code <span class="math inline">\(z\)</span> given a caption <span class="math inline">\(y\)</span>).</p>
<section id="prior-networks" class="level3">
<h3 class="anchored" data-anchor-id="prior-networks">2.1 Prior networks</h3>
<p>We begin by specifying the component that parameterises the prior <span class="math inline">\(p_Z\)</span>.</p>
<p>A prior network is an NN that parameterises a fixed prior distribution for the instances in a batch.</p>
<div id="cell-23" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PriorNet(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    An NN that parameterises a prior distribution.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    For this lab, our priors are fixed, so this NN's forward pass</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">     simply returns a fixed prior with a given batch_shape.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape: <span class="bu">tuple</span>):</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: this is the shape of a single outcome</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">            if you use a single integer k, we will turn it into (k,)            </span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(outcome_shape, <span class="bu">int</span>):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            outcome_shape <span class="op">=</span> (outcome_shape,)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outcome_shape <span class="op">=</span> outcome_shape</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns a td object for the batch.</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implement me!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s implement two priors.</p>
<p><strong>A standard Gaussian prior</strong></p>
<p>Here the latent code is a point in the <span class="math inline">\(K\)</span>-dimensional real coordinate space. We use a standard Gaussian per coordinate:</p>
<p><span class="math display">\[\begin{align}
p_Z(z) &amp;= \prod_{k=1}^K \mathcal N(z_k|0, 1)
\end{align}\]</span></p>
<p><strong>A mixture of Gaussians prior</strong></p>
<p>Here we learn a mixture of <span class="math inline">\(C\)</span> Gaussians, each a product of <span class="math inline">\(K\)</span> independent Gaussians:</p>
<p><span class="math display">\[\begin{align}
p_Z(z|\theta) &amp;= \sum_{c=1}^C \omega_c \prod_{k=1}^K \mathcal N(z_k|\mu_c, \sigma_c^2)
\end{align}\]</span></p>
<p>where the prior parameters are the mixing coefficients <span class="math inline">\(\omega_{1:C} \in \Delta_{C-1}\)</span>, the locations <span class="math inline">\(\mu_{1:C} \in \mathbb R^C\)</span> and the scales <span class="math inline">\(\sigma_{1:C} \in \mathbb R^C_{&gt;0}\)</span>.</p>
<div id="cell-25" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianPriorNet(PriorNet):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""    </span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    For z a K-dimensional code:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">        p(z) = prod_k Normal(z[k]|0, 1)</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the product of Bernoulli priors will have Bernoulli(0.5) factors</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"locs"</span>, torch.zeros(<span class="va">self</span>.outcome_shape, requires_grad<span class="op">=</span><span class="va">False</span>).detach())</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"scales"</span>, torch.ones(<span class="va">self</span>.outcome_shape, requires_grad<span class="op">=</span><span class="va">False</span>).detach())</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        shape <span class="op">=</span> batch_shape <span class="op">+</span> <span class="va">self</span>.outcome_shape</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we wrap around td.Independent to obtain a pdf over multivariate draws        </span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Normal(loc<span class="op">=</span><span class="va">self</span>.locs.expand(shape), scale<span class="op">=</span><span class="va">self</span>.scales.expand(shape)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MoGPriorNet(PriorNet):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""    </span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">    For z a K-dimensional code:</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">        p(z|w_1...w_C, u_1...u_C, s_1...s_C) </span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">         = \sum_c w_c prod_k Normal(z[k]|u[c], s[c]^2)</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape, num_components, lbound<span class="op">=-</span><span class="dv">10</span>, rbound<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [C]</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits <span class="op">=</span> nn.Parameter(torch.rand(num_components, requires_grad<span class="op">=</span><span class="va">True</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (C,) + outcome_shape</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        shape <span class="op">=</span> (num_components,) <span class="op">+</span> <span class="va">self</span>.outcome_shape        </span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.locs <span class="op">=</span> nn.Parameter(torch.rand(shape, requires_grad<span class="op">=</span><span class="va">True</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> nn.Parameter(<span class="dv">1</span> <span class="op">+</span> torch.rand(shape, requires_grad<span class="op">=</span><span class="va">True</span>), requires_grad<span class="op">=</span><span class="va">True</span>)        </span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_components <span class="op">=</span> num_components</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., with batch_shape (B,) and outcome_shape (K,) this is</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [B, C, K]</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        shape <span class="op">=</span> batch_shape <span class="op">+</span> (<span class="va">self</span>.num_components,) <span class="op">+</span> <span class="va">self</span>.outcome_shape</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we wrap around td.Independent to obtain a pdf over multivariate draws </span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (note that C is not part of the event_shape, thus td.Independent will</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  should not treat that dimension as part of the outcome)</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in our example, a draw from independent would return [B, C] draws of K-dimensional outcomes</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        comps <span class="op">=</span> td.Independent(td.Normal(loc<span class="op">=</span><span class="va">self</span>.locs.expand(shape), scale<span class="op">=</span><span class="va">self</span>.scales.expand(shape)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a batch of component selectors</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>        pc <span class="op">=</span> td.Categorical(logits<span class="op">=</span><span class="va">self</span>.logits.expand(batch_shape <span class="op">+</span> (<span class="va">self</span>.num_components,)))</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and finally, a mixture</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.MixtureSameFamily(pc, comps)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_priors(batch_size<span class="op">=</span><span class="dv">2</span>, latent_dim<span class="op">=</span><span class="dv">3</span>, num_comps<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    prior_net <span class="op">=</span> GaussianPriorNet(latent_dim)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Gaussian"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" trainable parameters"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(prior_net.parameters()))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>prior_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> prior_net(batch_shape<span class="op">=</span>(batch_size,))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    prior_net <span class="op">=</span> MoGPriorNet(latent_dim, num_comps)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Mixture of Gaussian"</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" trainable parameters"</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(prior_net.parameters()))</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>prior_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> prior_net(batch_shape<span class="op">=</span>(batch_size,))</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>test_priors()   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Gaussian
 trainable parameters
[]
 outcome_shape=(3,)
 distribution: Independent(Normal(loc: torch.Size([2, 3]), scale: torch.Size([2, 3])), 1)
 sample: tensor([[-0.3545, -0.5105, -0.7530],
        [ 0.6498, -0.0851, -1.2621]])
 shapes: sample=torch.Size([2, 3]) log_prob=torch.Size([2])

Mixture of Gaussian
 trainable parameters
[Parameter containing:
tensor([0.8617, 0.8520, 0.3585, 0.6196, 0.5566], requires_grad=True), Parameter containing:
tensor([[0.4819, 0.0711, 0.2805],
        [0.4312, 0.1763, 0.3839],
        [0.0172, 0.8007, 0.8341],
        [0.6358, 0.9348, 0.1698],
        [0.6220, 0.4291, 0.3030]], requires_grad=True), Parameter containing:
tensor([[1.5164, 1.3117, 1.3240],
        [1.5596, 1.3319, 1.5549],
        [1.1613, 1.6315, 1.0815],
        [1.9174, 1.9954, 1.5884],
        [1.8620, 1.4976, 1.3426]], requires_grad=True)]
 outcome_shape=(3,)
 distribution: MixtureSameFamily(
  Categorical(logits: torch.Size([2, 5])),
  Independent(Normal(loc: torch.Size([2, 5, 3]), scale: torch.Size([2, 5, 3])), 1))
 sample: tensor([[-1.5431, -0.7577,  0.3158],
        [ 3.9455, -0.1169,  0.7572]])
 shapes: sample=torch.Size([2, 3]) log_prob=torch.Size([2])</code></pre>
</div>
</div>
</section>
<section id="conditional-probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="conditional-probability-distributions">2.2 Conditional probability distributions</h3>
<p>Next, we create code to parameterise conditional probability distributions (cpds), which we do by having an NN parameterise a choice of pmf/pdf. This will be useful in parameterising the <span class="math inline">\(p_{X|Z=z}\)</span> component of our latent variable models (and, later on, it will also be useful for variational inference, when we develop <span class="math inline">\(q_{Z|X=x}\)</span>).</p>
<p>Our general strategy is to map from a number of inputs (which the user will choose) to the parameters of a pmf/pdf support by <code>torch.distributions</code>.</p>
<div id="cell-28" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CPDNet(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Let L be a choice of distribution</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">        and x ~ L is an outcome with shape outcome_shape</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This is an NN whose forward method maps from a number of inputs to the </span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">        parameters of L's pmf/pdf and returns a torch.distributions</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">        object representing L's pmf/pdf.</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: this is the shape of a single outcome</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">            if you use a single integer k, we will turn it into (k,)            </span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(outcome_shape, <span class="bu">int</span>):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            outcome_shape <span class="op">=</span> (outcome_shape,)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outcome_shape <span class="op">=</span> outcome_shape</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Return a torch.distribution object predicted from `inputs`.</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: a tensor with shape batch_shape + (num_inputs,)</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implemented me"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="observational-model" class="level4">
<h4 class="anchored" data-anchor-id="observational-model">2.2.1 Observational model</h4>
<p>The observational model prescribes the distribution of <span class="math inline">\(X|Z=z\)</span>.</p>
<p>If we assume our pixel intensities are binary, we can use a product of <span class="math inline">\(C\times W \times H\)</span> Bernoulli distributions, which we parameterise jointly using an NN:</p>
<p><span class="math display">\[\begin{align}
p_{X|Z}(x|z, \theta) &amp;= \prod_{c=1}^C\prod_{w=1}^W\prod_{h=1}^H \mathrm{Bernoulli}(x_{c,w,h} | f_{c,w,h}(z; \theta))
\end{align}\]</span></p>
<p>Here <span class="math inline">\(\mathbf f(z; \theta) \in (0,1)^{C}\times(0,1)^W \times (0,1)^H\)</span> is an NN architecture such as a feed-forward net or a stack of transposed convolution layers. In NN literature, such architectures are often called <em>decoders</em>.</p>
<p>If we assume our pixel intensities are real values in <span class="math inline">\([0, 1]\)</span> (0 and 1 included), we need to parameterise a pdf. A good choice of pdf is the <a href="https://arxiv.org/abs/1907.06845">ContinuousBernoulli distributions</a>, which is a single-parameter distribution (much like the Bernoulli) whose support is the set <span class="math inline">\([0, 1]\)</span>.</p>
<p>Let’s start by designing <span class="math inline">\(\mathbf f\)</span>.</p>
<p>A very basic design uses a FFNN:</p>
<div id="cell-31" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReshapeLast(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Helper layer to reshape the rightmost dimension of a tensor.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This can be used as a component of nn.Sequential.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, shape: <span class="bu">tuple</span>):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">        shape: desired rightmost shape</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._shape <span class="op">=</span> shape</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshapes the last dimension into self.shape</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span>.reshape(<span class="bu">input</span>.shape[:<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="va">self</span>._shape)        </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_ffnn_decoder(latent_size, num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, hidden_size<span class="op">=</span><span class="dv">512</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Map the latent code to a tensor with shape [num_channels, width, height] </span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">    using a FFNN with 2 hidden layers.</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">    latent_size: size of latent code</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">    num_channels: number of channels in the output</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co">    width: image shape</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co">    height: image shape</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co">    hidden_size: we first map from latent_size to hidden_size and </span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="co">        then use feed forward NNs to map it to [num_channels, width, height] </span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co">    p_drop: dropout rate before linear layers</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span>    </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> nn.Sequential(        </span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        nn.Linear(latent_size, hidden_size),</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        nn.Linear(hidden_size, hidden_size),</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        nn.Linear(hidden_size, num_channels <span class="op">*</span> width <span class="op">*</span> height),</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        ReshapeLast((num_channels, width, height)),        </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> decoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-32" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mapping from 10-dimensional latent code</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>build_ffnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.Size([5, 1, 64, 64])</code></pre>
</div>
</div>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can also have a structured batch shape (e.g., [3, 5])</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>build_ffnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>torch.Size([3, 5, 1, 64, 64])</code></pre>
</div>
</div>
<p>The downside is that the output layer is rather large.</p>
<p>An architecture with inductive biases that are more appropriate for our data type is a CNN, in particular, a transposed CNN. Here we design one such decoder:</p>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySequential(nn.Sequential):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This is a version of nn.Sequential that works with structured batches</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">     (i.e., batches that have multiple dimensions)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    even when some of the nn layers in it does not.</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    The idea is to just wrap nn.Sequential around two calls to reshape</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">     which remove and restore the batch dimensions.</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, event_dims<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._event_dims <span class="op">=</span> event_dims</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># memorise batch shape</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> <span class="bu">input</span>.shape[:<span class="op">-</span><span class="va">self</span>._event_dims]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># memorise latent shape</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        event_shape <span class="op">=</span> <span class="bu">input</span>.shape[<span class="op">-</span><span class="va">self</span>._event_dims:]</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten batch shape and obtain outputs</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="bu">super</span>().forward(<span class="bu">input</span>.reshape( (<span class="op">-</span><span class="dv">1</span>,) <span class="op">+</span> event_shape))</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># restore batch shape</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output.reshape(batch_shape <span class="op">+</span> output.shape[<span class="dv">1</span>:])</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_cnn_decoder(latent_size, num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, hidden_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Map the latent code to a tensor with shape [num_channels, width, height].</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="co">    latent_size: size of latent code</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="co">    num_channels: number of channels in the output</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co">    width: must be 64 (for now)</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="co">    height: must be 64 (for now)</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="co">    hidden_size: we first map from latent_size to hidden_size and </span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="co">        then use transposed 2d convolutions to [num_channels, width, height] </span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="co">    p_drop: dropout rate before linear layers</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The width is hardcoded"</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> height <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The height is hardcoded"</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: change the architecture so width and height are not hardcoded</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> MySequential(        </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>        nn.Linear(latent_size, hidden_size),</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        ReshapeLast((hidden_size, <span class="dv">1</span>, <span class="dv">1</span>)),</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(hidden_size, <span class="dv">128</span>, <span class="dv">5</span>, <span class="dv">2</span>),</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">64</span>, <span class="dv">5</span>, <span class="dv">2</span>),</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">6</span>, <span class="dv">2</span>),</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">32</span>, num_channels, <span class="dv">6</span>, <span class="dv">2</span>), </span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        event_dims<span class="op">=</span><span class="dv">1</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> decoder        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-36" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five 10-dimensional latent codes is transformed</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># into a batch of 5 images, each with shape [1,64,64]</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>build_cnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([5, 1, 64, 64])</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that because we use MySequential, </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we can have a batch of [3, 5] assignments</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (this is useful, for example, when we have multiple draws of the latent </span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># variable for each of the data points in the batch)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>build_cnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>torch.Size([3, 5, 1, 64, 64])</code></pre>
</div>
</div>
<p>Now we are in position to design a CPDNet for our image model, it simply combines a choice of decoder with a choice of distribution:</p>
<div id="cell-39" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinarizedImageModel(CPDNet):    </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels, width, height, latent_size, decoder_type<span class="op">=</span>build_ffnn_decoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>((num_channels, width, height))        </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span>  decoder_type(</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size, </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the cpd X|Z=z</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">        z: batch_shape + (latent_dim,)</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (num_channels, width, height)</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.decoder(z)        </span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Bernoulli(logits<span class="op">=</span>h), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape))</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ContinuousImageModel(CPDNet):  </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: this could be an exercise  </span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels, width, height, latent_size, decoder_type<span class="op">=</span>build_ffnn_decoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>((num_channels, width, height))        </span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span>  decoder_type(</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size, </span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width,</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the cpd X|Z=z</span></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="co">        z: batch_shape + (latent_dim,)</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (num_channels, width, height)</span></span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.decoder(z)        </span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.ContinuousBernoulli(logits<span class="op">=</span>h), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape))        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-40" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>obs_model <span class="op">=</span> BinarizedImageModel(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    latent_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five zs is mapped to 5 distributions over [1,64,64]-dimensional </span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># binary tensors</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model(torch.zeros([<span class="dv">5</span>, <span class="dv">10</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BinarizedImageModel(
  (decoder): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=10, out_features=512, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=512, out_features=512, bias=True)
    (5): ReLU()
    (6): Dropout(p=0.1, inplace=False)
    (7): Linear(in_features=512, out_features=4096, bias=True)
    (8): ReshapeLast()
  )
)
Independent(Bernoulli(logits: torch.Size([5, 1, 64, 64])), 3)</code></pre>
</div>
</div>
<p>We can also use a different decoder</p>
<div id="cell-42" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>obs_model <span class="op">=</span> BinarizedImageModel(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five zs is mapped to 5 distributions over [1,64,64]-dimensional </span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co"># binary tensors</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model(torch.zeros([<span class="dv">5</span>, <span class="dv">10</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BinarizedImageModel(
  (decoder): MySequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=10, out_features=1024, bias=True)
    (2): ReshapeLast()
    (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
    (4): ReLU()
    (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
    (6): ReLU()
    (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
    (8): ReLU()
    (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
  )
)
Independent(Bernoulli(logits: torch.Size([5, 1, 64, 64])), 3)</code></pre>
</div>
</div>
</section>
</section>
<section id="joint-distribution" class="level3">
<h3 class="anchored" data-anchor-id="joint-distribution">2.3 Joint distribution</h3>
<p>We can now combine a prior and an observational model into a joint distribution. A joint distribution supports a few important operations such as marginal and posterior pdf assessments, as well as sampling from the joint distribution. Marginal and posterior assessments require computations that may or may not be tractable, see below.</p>
<p>From a joint pdf, we can compute the marginal density of <span class="math inline">\(x\)</span> via</p>
<p><span class="math display">\[\begin{align}
p_X(x|\theta) &amp;= \int_{\mathcal Z} p_{ZX}(z,x|\theta) \mathrm{d}z\\
&amp;= \int_{\mathcal Z} p_Z(z|\theta)p_{X|Z}(x|z, \theta) \mathrm{d}z
\end{align}\]</span></p>
<p>For uncountable <span class="math inline">\(\mathcal Z\)</span> and a general enough parameterisation of <span class="math inline">\(p_{X|Z=z}\)</span>, this is intractable (a special case where this is tractable is that where <span class="math inline">\(p_{X|Z=z}\)</span> is itself a Gaussian whose mean depend linearly on <span class="math inline">\(z\)</span>, such a conditional is unreasonably simple for interesting datasets).</p>
<p>The posterior density of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span> depends on the intractable marginal:</p>
<p><span class="math display">\[\begin{align}
p_{Z|X}(z|x, \theta) &amp;= \frac{p_Z(z|\theta)p_{X|Z}(x|z, \theta)}{p_X(x|\theta)}
\end{align}\]</span></p>
<p>As marginalisation is intractable, we can obtain a naive lowerbound by direct application of Jensen’s inequality: <span class="math display">\[\begin{align}
\log p_X(x|\theta) &amp;= \log \int_{\mathcal Z} p_Z(z|\theta)p_{X|Z}(x|z, \theta) \mathrm{d} z\\
&amp;\overset{\text{JI}}{\ge} \int_{\mathcal Z} p_Z(z|\theta) \log p_{X|Z}(x|z, \theta) \mathrm{d}z \\
&amp;\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \log p_{X|Z}(x|z_s, \theta)  \quad \text{where } z_s \sim p_Z
\end{align}\]</span></p>
<p>A better lowerbound could be obtained via importance sampling, but it would require training an approximating distribution (as we will do in variational inference).</p>
<p>Recall that, given a dataset <span class="math inline">\(\mathcal D\)</span>, the log-likelihood function <span class="math inline">\(\mathcal L(\theta|\mathcal D)= \sum_{x \in \mathcal D} \log p_X(x|\theta)\)</span> requires performing marginal density assessments. Whenever exact marginalisation is intractable, we are unaible to assess <span class="math inline">\(\mathcal L(\theta|\mathcal D)\)</span> and its gradient with respect to <span class="math inline">\(\theta\)</span>. If the prior is fixed, we can use the naive lowerbound to obtain a gradient estimate, but, again, our naive application of JI leads to a generally rather loose bound.</p>
<div id="cell-44" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> JointDistribution(nn.Module):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A wrapper to combine a prior net and a cpd net into a joint distribution.</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prior_net: PriorNet, cpd_net: CPDNet):</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_net: object to parameterise p_Z</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">        cpd_net: object to parameterise p_{X|Z=z}</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_net <span class="op">=</span> prior_net</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cpd_net <span class="op">=</span> cpd_net</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prior(<span class="va">self</span>, shape):</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.prior_net(shape)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obs_model(<span class="va">self</span>, z):</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, shape):</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Return z via prior_net(shape).sample()</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="co">            and x via cpd_net(z).sample()</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(shape)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> pz.sample()</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> px_z.sample()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, x</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, z, x):</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Assess the log density of the joint outcome.            </span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> z.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.prior_net.outcome_shape)]        </span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x)</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_marginal(<span class="va">self</span>, x, enumerate_fn):</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Return log marginal density of x.</span></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a><span class="co">        enumerate_fn: function that enumerates the support of the prior</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a><span class="co">            (this is needed for marginalisation p(x) = \int p(z, x) dz)</span></span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a><span class="co">            This only really makes sense if the support is a </span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a><span class="co">            (small) countably finite set. In such cases, you can use</span></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a><span class="co">                enumerate=lambda p: p.enumerate_support()</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a><span class="co">            which is supported, for example, by Categorical and OneHotCategorical.</span></span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a><span class="co">            If the support is discrete (eg, bit vectors) you can still dare to </span></span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a><span class="co">            enumerate it explicitly, but you will need to write cutomised code, </span></span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a><span class="co">            as torch.distributions will not offer that functionality for you.</span></span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a><span class="co">            If the support is uncountable, countably infinite, or just large </span></span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a><span class="co">            anyway, you need approximate tools (such as VI, importance sampling, etc)</span></span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]</span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> []</span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enumerate_fn(pz)</span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.logsumexp(log_joint, <span class="dv">0</span>)</span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> posterior(<span class="va">self</span>, x, enumerate_fn):</span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the posterior distribution Z|X=x.</span></span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a><span class="co">        As the code is discrete, we return a discrete distribution over </span></span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a><span class="co">        the complete space of all possible latent codes. This is done via</span></span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a><span class="co">        exhaustive enumeration provided by `enumerate_fn`.</span></span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]        </span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enumerate_fn(pz)</span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-85"><a href="#cb35-85" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb35-86"><a href="#cb35-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (support_size,) </span></span>
<span id="cb35-87"><a href="#cb35-87" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> torch.swapaxes(log_joint, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb35-88"><a href="#cb35-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Categorical(logits<span class="op">=</span>log_joint)</span>
<span id="cb35-89"><a href="#cb35-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-90"><a href="#cb35-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> naive_lowerbound(<span class="va">self</span>, x, num_samples: <span class="bu">int</span>):</span>
<span id="cb35-91"><a href="#cb35-91" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-92"><a href="#cb35-92" aria-hidden="true" tabindex="-1"></a><span class="co">        Return an MC lowerbound on log marginal density of x:</span></span>
<span id="cb35-93"><a href="#cb35-93" aria-hidden="true" tabindex="-1"></a><span class="co">            log p(x) &gt;= 1/S \sum_s log p(x|z[s])</span></span>
<span id="cb35-94"><a href="#cb35-94" aria-hidden="true" tabindex="-1"></a><span class="co">                with z[s] ~ p_Z</span></span>
<span id="cb35-95"><a href="#cb35-95" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-96"><a href="#cb35-96" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]</span>
<span id="cb35-97"><a href="#cb35-97" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)</span>
<span id="cb35-98"><a href="#cb35-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (num_samples,) + batch_shape + prior_outcome_shape</span></span>
<span id="cb35-99"><a href="#cb35-99" aria-hidden="true" tabindex="-1"></a>        log_probs <span class="op">=</span> []</span>
<span id="cb35-100"><a href="#cb35-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I'm using a for loop, but note that with enough GPU memory </span></span>
<span id="cb35-101"><a href="#cb35-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># one could parallelise this step</span></span>
<span id="cb35-102"><a href="#cb35-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> z <span class="kw">in</span> pz.sample((num_samples,)): </span>
<span id="cb35-103"><a href="#cb35-103" aria-hidden="true" tabindex="-1"></a>            px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-104"><a href="#cb35-104" aria-hidden="true" tabindex="-1"></a>            log_probs.append(px_z.log_prob(x))</span>
<span id="cb35-105"><a href="#cb35-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (num_samples,) + batch_shape</span></span>
<span id="cb35-106"><a href="#cb35-106" aria-hidden="true" tabindex="-1"></a>        log_probs <span class="op">=</span> torch.stack(log_probs)</span>
<span id="cb35-107"><a href="#cb35-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb35-108"><a href="#cb35-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.mean(log_probs, <span class="dv">0</span>)        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-45" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_joint_dist(latent_size<span class="op">=</span><span class="dv">10</span>, num_comps<span class="op">=</span><span class="dv">3</span>, data_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>), batch_size<span class="op">=</span><span class="dv">2</span>, hidden_size<span class="op">=</span><span class="dv">32</span>):    </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> JointDistribution(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        prior_net<span class="op">=</span>GaussianPriorNet(latent_size),</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        cpd_net<span class="op">=</span>BinarizedImageModel(</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>data_shape[<span class="dv">0</span>],</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>data_shape[<span class="dv">1</span>],</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>data_shape[<span class="dv">2</span>],</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size,</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Model for binarized data"</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    z, x <span class="op">=</span> p.sample((batch_size,))</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled z"</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(z)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled x"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)    </span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"MC lowerbound"</span>)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 1:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 2:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))    </span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> JointDistribution(</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>        prior_net<span class="op">=</span>MoGPriorNet(latent_size, num_comps),</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>        cpd_net<span class="op">=</span>ContinuousImageModel(</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>data_shape[<span class="dv">0</span>],</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>data_shape[<span class="dv">1</span>],</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>data_shape[<span class="dv">2</span>],</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size,</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Model for continuous data"</span>)</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p)</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>    z, x <span class="op">=</span> p.sample((batch_size,))</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled z"</span>)</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(z)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled x"</span>)</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)    </span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"MC lowerbound"</span>)</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 1:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 2:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))    </span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>test_joint_dist(<span class="dv">10</span>)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model for binarized data
JointDistribution(
  (prior_net): GaussianPriorNet()
  (cpd_net): BinarizedImageModel(
    (decoder): MySequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=1024, bias=True)
      (2): ReshapeLast()
      (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
      (4): ReLU()
      (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
      (6): ReLU()
      (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
      (8): ReLU()
      (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
    )
  )
)
sampled z
tensor([[-0.8407,  0.2065, -1.2518, -0.9531,  0.1592, -0.1754, -1.6121,  0.1075,
         -0.7088, -0.3303],
        [-0.1712,  0.1613, -0.3089,  0.2564,  0.0636,  0.7447,  0.7566, -0.4789,
          1.1534, -0.8448]])
sampled x
tensor([[[[0., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 0.,  ..., 1., 0., 1.],
          [1., 0., 1.,  ..., 0., 0., 1.],
          ...,
          [0., 0., 1.,  ..., 0., 1., 0.],
          [1., 1., 0.,  ..., 1., 1., 1.],
          [1., 0., 0.,  ..., 0., 0., 1.]]],


        [[[0., 1., 0.,  ..., 1., 1., 1.],
          [1., 1., 0.,  ..., 1., 0., 0.],
          [0., 0., 0.,  ..., 1., 0., 1.],
          ...,
          [0., 1., 0.,  ..., 1., 1., 1.],
          [1., 1., 0.,  ..., 1., 0., 0.],
          [0., 0., 1.,  ..., 0., 1., 1.]]]])
MC lowerbound
 1: tensor([-2837.7363, -2837.9871], grad_fn=&lt;MeanBackward1&gt;)
 2: tensor([-2837.9243, -2838.0288], grad_fn=&lt;MeanBackward1&gt;)



Model for continuous data
JointDistribution(
  (prior_net): MoGPriorNet()
  (cpd_net): ContinuousImageModel(
    (decoder): MySequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=1024, bias=True)
      (2): ReshapeLast()
      (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
      (4): ReLU()
      (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
      (6): ReLU()
      (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
      (8): ReLU()
      (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
    )
  )
)
sampled z
tensor([[ 0.5024,  0.1232,  1.0614,  1.5447,  1.1263,  1.0789,  2.7709,  1.9042,
         -1.0329,  3.2964],
        [ 1.0950, -1.1369,  1.0845,  1.6282, -0.8367,  1.1774, -0.4392, -0.3845,
          1.5312, -1.0509]])
sampled x
tensor([[[[0.2456, 0.6908, 0.2022,  ..., 0.9125, 0.4140, 0.0154],
          [0.9211, 0.8082, 0.1273,  ..., 0.0145, 0.6978, 0.2584],
          [0.9423, 0.6667, 0.8885,  ..., 0.8678, 0.2550, 0.1817],
          ...,
          [0.9706, 0.4656, 0.9801,  ..., 0.3806, 0.5779, 0.3176],
          [0.8841, 0.9396, 0.1979,  ..., 0.4812, 0.1924, 0.1274],
          [0.9042, 0.7917, 0.8952,  ..., 0.4248, 0.3569, 0.6764]]],


        [[[0.2313, 0.6762, 0.1603,  ..., 0.8595, 0.0705, 0.6296],
          [0.6333, 0.8830, 0.3072,  ..., 0.0994, 0.7293, 0.3187],
          [0.2723, 0.6290, 0.7742,  ..., 0.1179, 0.6817, 0.0761],
          ...,
          [0.2667, 0.3814, 0.9479,  ..., 0.4457, 0.3651, 0.0825],
          [0.7468, 0.6061, 0.9087,  ..., 0.1951, 0.1604, 0.0321],
          [0.6896, 0.4760, 0.0178,  ..., 0.3982, 0.9280, 0.0480]]]])
MC lowerbound
 1: tensor([0.0437, 0.5014], grad_fn=&lt;MeanBackward1&gt;)
 2: tensor([0.1282, 0.4484], grad_fn=&lt;MeanBackward1&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="learning" class="level2">
<h2 class="anchored" data-anchor-id="learning">3. Learning</h2>
<p>We estimate <span class="math inline">\(\theta\)</span> using stochastic gradient-based maximum likelihood estimation. For a tractable model, we can assess the log-likelihood function</p>
<p><span class="math display">\[\begin{align}
\mathcal L(\theta|\mathcal D) &amp;= \sum_{x \in \mathcal D} \log p_X(x|\theta)
\end{align}\]</span></p>
<p>and estimate <span class="math inline">\(\nabla_{\theta} \mathcal L(\theta|\mathcal D)\)</span> using random mini-batches:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\theta}\mathcal L(\theta|\mathcal D) &amp;\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \nabla_{\theta}\log p_X(x^{(s)}|\theta) \\
&amp;\text{where }x^{(s)} \sim \mathcal D
\end{align}\]</span></p>
<p>An intractable model, such as the continuous LVM above requires approximate inference.</p>
<section id="intractable-lvms" class="level3">
<h3 class="anchored" data-anchor-id="intractable-lvms">3.1 Intractable LVMs</h3>
<p>When marginalisation is intractable, we resort to variational inference introducing a parametric approximation <span class="math inline">\(q_{Z|X=x}\)</span> to the model’s true posterior distribution <span class="math inline">\(p_{Z|X=x}\)</span> and estimate both the approximation and the joint distribution by maximising the evidence lowerbound (ELBO), shown below for a single observation <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{align}
\mathcal E(\lambda, \theta| \mathcal D) &amp;= \mathbb E_{x \sim \mathcal D}\left[ \mathbb E\left[\log \frac{ p_{ZX}(z, x|\theta)}{q_{Z|X}(z|x, \lambda)}\right] \right] \\
&amp;= \underbrace{\mathbb E_{x \sim \mathcal D}\left[ \mathbb E[ \log p_{Z|X}(x|z,\theta)] \right]}_{-D} - \underbrace{\mathbb E_{x \sim \mathcal D}\left[ \mathrm{KL}(q_{Z|X=x}||p_Z) \right]}_{R}
\end{align}\]</span></p>
<p>where the inner expectation is taken with respect to <span class="math inline">\(q_{Z|X}(z|x, \lambda)\)</span>.</p>
<p>When computed in expectation under the data distribution, the two components of the ELBO, namely, the expected log-likelihood <span class="math inline">\(\mathbb E[ \log p_{Z|X}(x|z,\theta)]\)</span> and the “KL term” <span class="math inline">\(\mathrm{KL}(q_{Z|X=x}||p_Z)\)</span>, are related to two information-theoretic quantities known as <em>distortion</em> and <em>rate</em>.</p>
<p>We choose our approximation to be such that: its support is embeded in the support of the prior, it is simple enough to sample from, and it is simple enough to assess the mass of a sample. If possible, we choose it such that other quantities are also tractable (e.g., entropy, relative entropy).</p>
<p>For a multivariate <span class="math inline">\(z\)</span>, we normally choose a factorised family, for example, if <span class="math inline">\(z\)</span> is a point in <span class="math inline">\(\mathbb R^K\)</span>:</p>
<p><span class="math display">\[\begin{align}
q_{Z|X}(z|x, \lambda) &amp;= \prod_{k=1}^K \mathcal N(z_k|\mu_k(x;\lambda), \sigma^2_k(x;\lambda))
\end{align}\]</span></p>
<p>with <span class="math inline">\(\boldsymbol\mu(x;\lambda) \in \mathbb R^K\)</span> and <span class="math inline">\(\boldsymbol\sigma(x;\lambda) \in \mathbb R^K_{&gt;0}\)</span>. This is called a <em>mean field assumption</em>.</p>
<p>We can obtain a more complex approximation by, for example, using a mixture of mean field families:</p>
<p><span class="math display">\[\begin{align}
q_{Z|X}(z|x, \lambda) &amp;= \sum_{c=1}^C \omega_c(x; \lambda) \prod_{k=1}^K \mathcal N(z_k|\mu_k(x;\lambda), \sigma^2_k(x;\lambda))
\end{align}\]</span></p>
<p>with <span class="math inline">\(\boldsymbol\mu(x;\lambda) \in \mathbb R^K\)</span>, <span class="math inline">\(\boldsymbol\sigma(x;\lambda) \in \mathbb R^K_{&gt;0}\)</span>, and <span class="math inline">\(\boldsymbol\omega(x; \lambda) \in \Delta_{C-1}\)</span>.</p>
<p>There are other ways to inject structure in the variational approximation, a common example is to use a normalising flow. When designing a structured approximation a few things must be kept in mind:</p>
<ul>
<li>sampling should remain tractable</li>
<li>assessing the density of a sample should remain tractable</li>
<li>it’s okay if we cannot compute entropy or KL in closed-form, we can always estimate the gradient of such terms (e.g., via score function estimation)</li>
</ul>
<p>As we shall see the two approximations above differ in a crucial way, the simple Gaussian mean field is amenable to a continuously differentiable reparameterisation which leads to a lower variance estimator (compared to, for example, the score function estimator).</p>
<section id="reparameterised-gradient" class="level4">
<h4 class="anchored" data-anchor-id="reparameterised-gradient">3.1.1 Reparameterised gradient</h4>
<p>For some distributions, it is possible to obtain a sample via a continuously differentiable transformation of a fixed random source. This enables a class for gradient estimators known as <em>reparameterised gradient</em> (or the “reparameterisation trick”). In these cases <span class="math inline">\(Z = \mathcal T(\epsilon, \lambda)\)</span> with <span class="math inline">\(\epsilon\)</span> drawn from a distribution whose parameters are independent of <span class="math inline">\(\lambda\)</span>. Moreover, <span class="math inline">\(\mathcal T\)</span> is differentiable and invertible.</p>
</section>
</section>
<section id="inference-model" class="level3">
<h3 class="anchored" data-anchor-id="inference-model">3.2 Inference model</h3>
<p>The inference model is a conditional model of the latent variable, for which we design CPD nets.</p>
<p>Before we go on, it is useful to design an “encoder” a function that maps an image <span class="math inline">\(x \in \mathcal X\)</span> to a fixed-size vector that we can use as a compact representation of <span class="math inline">\(x\)</span>. Next, we design one such encoder employing FFNNs and another employing CNNs.</p>
<div id="cell-52" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FlattenImage(nn.Module):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span>.reshape(<span class="bu">input</span>.shape[:<span class="op">-</span><span class="dv">3</span>] <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_ffnn_encoder(num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, output_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        FlattenImage(),</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>        nn.Linear(num_channels <span class="op">*</span> width <span class="op">*</span> height, output_size<span class="op">//</span><span class="dv">2</span>),</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>        nn.Linear(output_size<span class="op">//</span><span class="dv">2</span>, output_size<span class="op">//</span><span class="dv">2</span>),</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>        nn.Linear(output_size<span class="op">//</span><span class="dv">2</span>, output_size),        </span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoder</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_cnn_encoder(num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, output_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The width is hardcoded"</span>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> height <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The height is hardcoded"</span>)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> output_size <span class="op">!=</span> <span class="dv">1024</span>:</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The output_size is hardcoded"</span>)</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: change the architecture so width, height and output_size are not hardcoded</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> MySequential(</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(num_channels, <span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>        FlattenImage(),</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>        event_dims<span class="op">=</span><span class="dv">3</span></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-53" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five [1, 64, 64]-dimensional images is encoded into</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co"># five 1024-dimensional vectors</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>build_ffnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>torch.Size([5, 1024])</code></pre>
</div>
</div>
<div id="cell-54" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and, again, we can have structured batches</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (here trying with (3,5))</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>build_ffnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>torch.Size([3, 5, 1024])</code></pre>
</div>
</div>
<div id="cell-55" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five [1, 64, 64]-dimensional images is encoded into</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co"># five 1024-dimensional vectors</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>build_cnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([5, 1024])</code></pre>
</div>
</div>
<div id="cell-56" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and, again, since we use MySequential we can have structured batches</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (here trying with (3,5))</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>build_cnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.Size([3, 5, 1024])</code></pre>
</div>
</div>
<p>We can now design some CPD nets, assuming they map from an encoding of an image to a pmf over <span class="math inline">\(\mathcal Z\)</span>.</p>
<p><strong>Gaussian mean field</strong></p>
<p>This can be used to parameterise a cpd over real vectors of fixed dimensionality.</p>
<p><strong>Mixture of Gaussian mean fields</strong></p>
<p>This can also be used to parameterise a cpd over real vectors of fixed dimensionality, but it achieves a more complex density (e.g., multimodal).</p>
<div id="cell-58" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianCPDNet(CPDNet):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Output distribution is a product of Gaussian distributions</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape, num_inputs: <span class="bu">int</span>, hidden_size: <span class="bu">int</span><span class="op">=</span><span class="va">None</span>, p_drop: <span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: shape of the outcome (int or tuple)</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="co">            if int, we turn it into a singleton tuple</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co">        num_inputs: rightmost dimensionality of the inputs to forward</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: size of hidden layers for the CPDNet (use None to skip)        </span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co">        p_drop: configure dropout before every Linear layer</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        num_outputs <span class="op">=</span> np.prod(<span class="va">self</span>.outcome_shape)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hidden_size:    </span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>                nn.Dropout(p_drop),</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>                nn.Linear(num_inputs, hidden_size),</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>                nn.ReLU()</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Identity()</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> num_inputs</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.locs <span class="op">=</span> nn.Sequential(            </span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>            ReshapeLast(<span class="va">self</span>.outcome_shape)</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> nn.Sequential(            </span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a>            nn.Softplus(), <span class="co"># we use the softplus activations for the scales</span></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>            ReshapeLast(<span class="va">self</span>.outcome_shape)</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):        </span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(inputs)</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Normal(loc<span class="op">=</span><span class="va">self</span>.locs(h), scale<span class="op">=</span><span class="va">self</span>.scales(h)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MoGCPDNet(CPDNet):</span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Output distribution is a mixture of products of Gaussian distributions</span></span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape, num_inputs: <span class="bu">int</span>, hidden_size: <span class="bu">int</span><span class="op">=</span><span class="va">None</span>, p_drop: <span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>, num_components<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: shape of the outcome (int or tuple)</span></span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a><span class="co">            if int, we turn it into a singleton tuple</span></span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a><span class="co">        num_inputs: rightmost dimensionality of the inputs to forward</span></span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: size of hidden layers for the CPDNet (use None to skip)        </span></span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a><span class="co">        p_drop: configure dropout before every Linear layer</span></span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a><span class="co">        num_components: number of Gaussians to be mixed</span></span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_components <span class="op">=</span> num_components</span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a>        num_outputs <span class="op">=</span> num_components <span class="op">*</span> np.prod(<span class="va">self</span>.outcome_shape)</span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-62"><a href="#cb47-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hidden_size:    </span>
<span id="cb47-63"><a href="#cb47-63" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb47-64"><a href="#cb47-64" aria-hidden="true" tabindex="-1"></a>                nn.Dropout(p_drop),</span>
<span id="cb47-65"><a href="#cb47-65" aria-hidden="true" tabindex="-1"></a>                nn.Linear(num_inputs, hidden_size),</span>
<span id="cb47-66"><a href="#cb47-66" aria-hidden="true" tabindex="-1"></a>                nn.ReLU()</span>
<span id="cb47-67"><a href="#cb47-67" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb47-68"><a href="#cb47-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb47-69"><a href="#cb47-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Identity()</span>
<span id="cb47-70"><a href="#cb47-70" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> num_inputs</span>
<span id="cb47-71"><a href="#cb47-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-72"><a href="#cb47-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.locs <span class="op">=</span> nn.Sequential(            </span>
<span id="cb47-73"><a href="#cb47-73" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb47-74"><a href="#cb47-74" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb47-75"><a href="#cb47-75" aria-hidden="true" tabindex="-1"></a>            ReshapeLast((num_components,) <span class="op">+</span> <span class="va">self</span>.outcome_shape)</span>
<span id="cb47-76"><a href="#cb47-76" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-77"><a href="#cb47-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> nn.Sequential(            </span>
<span id="cb47-78"><a href="#cb47-78" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb47-79"><a href="#cb47-79" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb47-80"><a href="#cb47-80" aria-hidden="true" tabindex="-1"></a>            nn.Softplus(), <span class="co"># we use the softplus activations for the scales</span></span>
<span id="cb47-81"><a href="#cb47-81" aria-hidden="true" tabindex="-1"></a>            ReshapeLast((num_components,) <span class="op">+</span> <span class="va">self</span>.outcome_shape)</span>
<span id="cb47-82"><a href="#cb47-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-83"><a href="#cb47-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits <span class="op">=</span> nn.Sequential(            </span>
<span id="cb47-84"><a href="#cb47-84" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb47-85"><a href="#cb47-85" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_components),</span>
<span id="cb47-86"><a href="#cb47-86" aria-hidden="true" tabindex="-1"></a>            ReshapeLast((num_components,))</span>
<span id="cb47-87"><a href="#cb47-87" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-88"><a href="#cb47-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-89"><a href="#cb47-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):        </span>
<span id="cb47-90"><a href="#cb47-90" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(inputs)</span>
<span id="cb47-91"><a href="#cb47-91" aria-hidden="true" tabindex="-1"></a>        comps <span class="op">=</span> td.Independent(td.Normal(loc<span class="op">=</span><span class="va">self</span>.locs(h), scale<span class="op">=</span><span class="va">self</span>.scales(h)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb47-92"><a href="#cb47-92" aria-hidden="true" tabindex="-1"></a>        pc <span class="op">=</span> td.Categorical(logits<span class="op">=</span><span class="va">self</span>.logits(h))</span>
<span id="cb47-93"><a href="#cb47-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.MixtureSameFamily(pc, comps)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-59" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_cpds(outcome_shape, num_comps<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">3</span>, input_dim<span class="op">=</span><span class="dv">5</span>, hidden_size<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    cpd_net <span class="op">=</span> GaussianCPDNet(outcome_shape, num_inputs<span class="op">=</span>input_dim, hidden_size<span class="op">=</span>hidden_size) </span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Gaussian"</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cpd_net)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>cpd_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> torch.from_numpy(np.random.uniform(size<span class="op">=</span>(batch_size, input_dim))).<span class="bu">float</span>()</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shape of inputs: </span><span class="sc">{</span>inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> cpd_net(inputs)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    cpd_net <span class="op">=</span> MoGCPDNet(outcome_shape, num_inputs<span class="op">=</span>input_dim, hidden_size<span class="op">=</span>hidden_size, num_components<span class="op">=</span>num_comps) </span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Mixture of Gaussians"</span>)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cpd_net)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>cpd_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> torch.from_numpy(np.random.uniform(size<span class="op">=</span>(batch_size, input_dim))).<span class="bu">float</span>()</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shape of inputs: </span><span class="sc">{</span>inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> cpd_net(inputs)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Try a few </span></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>test_cpds(<span class="dv">12</span>)</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a><span class="co">#test_cpds(12, hidden_size=None)</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a><span class="co"># your latent code could be a metrix (we talk about it as a "vector" for convenience)</span></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="co">#test_cpds((4, 5))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Gaussian
GaussianCPDNet(
  (encoder): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=5, out_features=2, bias=True)
    (2): ReLU()
  )
  (locs): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=12, bias=True)
    (2): ReshapeLast()
  )
  (scales): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=12, bias=True)
    (2): Softplus(beta=1, threshold=20)
    (3): ReshapeLast()
  )
)
 outcome_shape=(12,)
 shape of inputs: torch.Size([3, 5])
 distribution: Independent(Normal(loc: torch.Size([3, 12]), scale: torch.Size([3, 12])), 1)
 sample: tensor([[-0.2740,  1.4641, -0.1492,  0.8619,  0.8987, -0.6384,  0.1898, -0.0084,
          0.0154, -0.0720,  0.5614, -0.7824],
        [ 1.4560, -0.2668, -0.2343, -0.8925,  0.0078, -0.1212,  0.3651, -0.8519,
          1.0650, -1.2128, -0.4736, -0.1115],
        [ 0.1504, -0.5747,  0.1681,  0.4921,  1.2217, -0.4474, -0.9136, -1.1818,
          0.9822, -0.6862, -0.0885, -0.3536]])
 shapes: sample=torch.Size([3, 12]) log_prob=torch.Size([3])

Mixture of Gaussians
MoGCPDNet(
  (encoder): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=5, out_features=2, bias=True)
    (2): ReLU()
  )
  (locs): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=24, bias=True)
    (2): ReshapeLast()
  )
  (scales): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=24, bias=True)
    (2): Softplus(beta=1, threshold=20)
    (3): ReshapeLast()
  )
  (logits): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=2, bias=True)
    (2): ReshapeLast()
  )
)
 outcome_shape=(12,)
 shape of inputs: torch.Size([3, 5])
 distribution: MixtureSameFamily(
  Categorical(logits: torch.Size([3, 2])),
  Independent(Normal(loc: torch.Size([3, 2, 12]), scale: torch.Size([3, 2, 12])), 1))
 sample: tensor([[ 2.2989,  0.6416, -0.7611, -0.2090, -0.4584,  0.5179, -0.1198,  0.0880,
          0.7712, -0.4315, -0.3756,  0.2588],
        [-0.1332, -0.0895, -0.0302,  1.7607, -0.6219,  0.5210,  1.3052,  0.4076,
          0.2439, -0.4406,  0.0756,  0.6263],
        [-0.1179,  1.3194, -0.2864,  1.0093,  1.1053,  0.9005,  0.6088,  0.2438,
          0.7631, -0.8659,  0.4035,  0.1295]])
 shapes: sample=torch.Size([3, 12]) log_prob=torch.Size([3])</code></pre>
</div>
</div>
<p>Last, but certainly not least, we can combine our encoder and a choice of CPD net.</p>
<div id="cell-61" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InferenceModel(CPDNet):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, cpd_net_type, </span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        latent_size, num_channels<span class="op">=</span><span class="dv">1</span>, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, </span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        encoder_type<span class="op">=</span>build_ffnn_encoder):        </span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(latent_size)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latent_size <span class="op">=</span> latent_size    </span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encodes an image to a hidden_size-dimensional vector    </span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder_type(</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width, </span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>hidden_size,</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># maps from a hidden_size-dimensional encoding</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to a cpd for Z|X=x</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cpd_net <span class="op">=</span> cpd_net_type(</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>             latent_size, </span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>             num_inputs<span class="op">=</span>hidden_size, </span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>             hidden_size<span class="op">=</span><span class="dv">2</span><span class="op">*</span>latent_size,</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>             p_drop<span class="op">=</span>p_drop</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x)        </span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cpd_net(h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-62" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>InferenceModel(GaussianCPDNet, latent_size<span class="op">=</span><span class="dv">10</span>)(torch.zeros(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>Independent(Normal(loc: torch.Size([5, 10]), scale: torch.Size([5, 10])), 1)</code></pre>
</div>
</div>
<div id="cell-63" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>InferenceModel(partial(MoGCPDNet, num_components<span class="op">=</span><span class="dv">3</span>), latent_size<span class="op">=</span><span class="dv">10</span>)(torch.zeros(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>MixtureSameFamily(
  Categorical(logits: torch.Size([5, 3])),
  Independent(Normal(loc: torch.Size([5, 3, 10]), scale: torch.Size([5, 3, 10])), 1))</code></pre>
</div>
</div>
<p>We now have everything in place to use variational inference.</p>
</section>
<section id="neural-variational-inference" class="level3">
<h3 class="anchored" data-anchor-id="neural-variational-inference">3.3 Neural Variational Inference</h3>
<p>We will train our generative model via variational inference, for which we need to train an inference model along with it. We will use the ELBO objective, and gradient estimators based on score function estimation and differentiable reparameterisation.</p>
<p>It’s common to refer to any one such model as a variational auto-encoder (VAE), especially so when using reparameterised gradients.</p>
<p>Let’s start with score function estimation.</p>
<p>Given a data point <span class="math inline">\(x\)</span>, we estimate the gradient of the ELBO with respect to <span class="math inline">\(\lambda\)</span> by MC estimating the following expressions:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\lambda} \mathcal E(\lambda, \theta|x) &amp;= \mathbb E\left[ r(z, x; \theta, \lambda) \nabla_{\lambda}\log \frac{p_{ZX}(z, x|\theta)}{q_{Z|X}(z|x, \lambda)} \right]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nabla_{\theta} \mathcal E(\lambda, \theta|x) &amp;= \mathbb E\left[ \nabla_{\theta} \log p_{ZX}(z, x|\theta) \right]
\end{align}\]</span></p>
<p>where the “reward” function in the gradient estimator for <span class="math inline">\(\lambda\)</span> is</p>
<p><span class="math display">\[\begin{align}
r(z, x; \theta, \lambda) &amp;= \log p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>And, because this gradient estimator is rather noisy, it’s commong to transform the reward function by further employing control variates. The simplest control variates are functions of <span class="math inline">\(x\)</span> and possibly of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>, but not a function of the action <span class="math inline">\(z\)</span> with respect to which we evaluate the reward function. We will implement those as wrappers around the reward function. So, let’s start by agreeing on the API of our control variates.</p>
<div id="cell-66" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VarianceReduction(nn.Module):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co">    We will be using simple forms of control variates for variance reduction.</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co">    These are transformations of the reward that are independent of the sampled</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="co">    latent variable, but they can, in principle, depend on x, and on the </span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters of the generative and inference model.</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Some of these are trainable components, thus they also contribute to the loss.</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the transformed reward and a contribution to the loss.</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co">        r: a batch of rewards</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a batch of observations</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a><span class="co">        q: policy</span></span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a><span class="co">        r_fn: reward function</span></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r, torch.zeros_like(r)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In case a reparameterisation trick is available for the approximate posterior, we can MC estimate the following expressions:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\lambda}\mathcal E(\lambda, \theta|x) &amp;= \mathbb E_{\epsilon \sim s(\cdot)}\left[\nabla_{\lambda}\log \frac{p_{XZ}(x, Z=\mathcal T(\epsilon, \lambda)|\theta)}{q_{Z|X}(Z=\mathcal T(\epsilon, \lambda)|x, \lambda)}\right]
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
\nabla_{\theta}\mathcal E(\lambda, \theta|x) &amp;= \mathbb E_{\epsilon \sim s(\cdot)}\left[\nabla_{\theta}\log \frac{p_{XZ}(x, Z=\mathcal T(\epsilon, \lambda)|\theta)}{q_{Z|X}(Z=\mathcal T(\epsilon, \lambda)|x, \lambda)}\right]
\end{align}\]</span></p>
<p>Gradients of these kind are commonly referred to as <em>path derivatives</em>. We don’t need to implement the path derivatives ourselves, nor the transformations, rather we use a distribution object which supports an <code>rsample</code> method (for “reparameterised sample”), this distribution will be able to assess the density of the sample should we need it and if we obtained the sample via <code>rsample</code> the path derivative will be automatically available to backprop.</p>
<p>If KL divergence from the prior to the approximate posterior is computable, we use a different gradient estimator for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\theta\)</span>, namely:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\lambda}\mathcal E(\lambda, \theta|x) &amp;= \mathbb E_{\epsilon \sim s(\cdot)}\left[\nabla_{\lambda}\log p_{X|Z}(x|Z=\mathcal T(\epsilon, \lambda), \theta)\right] - \nabla_{\lambda}\mathrm{KL}(q_{Z|X} || p_Z)
\end{align}\]</span> and <span class="math display">\[\begin{align}
\nabla_{\theta}\mathcal E(\lambda, \theta|x) &amp;= \mathbb E_{\epsilon \sim s(\cdot)}\left[\nabla_{\theta}\log p_{X|Z}(x|Z=\mathcal T(\epsilon, \lambda), \theta)\right] - \nabla_{\theta}\mathrm{KL}(q_{Z|X} || p_Z)
\end{align}\]</span></p>
<p>Recall that, in practice, we will need to design a surrogate loss: a node in the computation graph whose backward corresponds to the gradient estimator we want. Check carefully the changes we made to the forward method of the NVIL class.</p>
<p>Now we can work on our general NVIL model. The following class implements the NVIL objective as well as a lot of helper code to manipulate the model components in interesting ways (e.g., sampling, sampling conditionally, estimating marginal density, etc.)</p>
<div id="cell-70" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NVIL(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A generative model p(z)p(x|z) and an approximation q(z|x) to that </span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co">     model's true posterior.</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co">    The approximation is estimated to maximise the ELBO, and so is the joint</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="co">     distribution.     </span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, gen_model: JointDistribution, inf_model: InferenceModel, cv_model: VarianceReduction):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co">        gen_model: p(z)p(x|z)</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co">        inf_model: q(z|x) which approximates p(z|x)</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co">        cv_model: optional transformations of the reward</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen_model <span class="op">=</span> gen_model        </span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf_model <span class="op">=</span> inf_model</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cv_model <span class="op">=</span> cv_model</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gen_params(<span class="va">self</span>):</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gen_model.parameters()</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inf_params(<span class="va">self</span>):</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.inf_model.parameters()</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cv_params(<span class="va">self</span>):</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cv_model.parameters()</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, batch_size, sample_size<span class="op">=</span><span class="va">None</span>, oversample<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a><span class="co">        A sample from the joint distribution:</span></span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a><span class="co">            z ~ prior</span></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a><span class="co">            x|z ~ obs model</span></span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size: number of samples in a batch</span></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if None, the output tensor has shape [batch_size] + data_shape</span></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a><span class="co">            if 1 or more, the output tensor has shape [sample_size, batch_size] + data_shape</span></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a><span class="co">            while batch_size controls a parallel computation, </span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a><span class="co">        oversample: if True, samples z (batch_size times), hold it fixed, </span></span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a><span class="co">            and sample x (sample_size times)</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior((batch_size,))</span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> (sample_size <span class="kw">or</span> <span class="dv">1</span>)        </span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(pz.sample()) <span class="cf">if</span> oversample <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size <span class="kw">or</span> <span class="dv">1</span>):</span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> oversample:</span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(pz.sample())</span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>            samples[k] <span class="op">=</span> px_z.sample()</span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.stack(samples) </span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="cf">if</span> sample_size <span class="cf">else</span> x.squeeze(<span class="dv">0</span>)</span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cond_sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>, oversample<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Condition on x and draw a sample:</span></span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a><span class="co">            z|x ~ inf model</span></span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a><span class="co">            x'|z ~ obs model</span></span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a batch of seed data samples</span></span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if None, the output tensor has shape [batch_size] + data_shape</span></span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a><span class="co">            if 1 or more, the output tensor has shape [sample_size, batch_size] + data_shape            </span></span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a><span class="co">        oversample: if True, samples z (batch_size times), hold it fixed, </span></span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a><span class="co">            and sample x' (sample_size times)</span></span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a>        qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> (sample_size <span class="kw">or</span> <span class="dv">1</span>)        </span>
<span id="cb56-68"><a href="#cb56-68" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(qz.sample()) <span class="cf">if</span> oversample <span class="cf">else</span> <span class="va">None</span>        </span>
<span id="cb56-69"><a href="#cb56-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size <span class="kw">or</span> <span class="dv">1</span>):</span>
<span id="cb56-70"><a href="#cb56-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> oversample:</span>
<span id="cb56-71"><a href="#cb56-71" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(qz.sample())</span>
<span id="cb56-72"><a href="#cb56-72" aria-hidden="true" tabindex="-1"></a>            samples[k] <span class="op">=</span> px_z.sample()        </span>
<span id="cb56-73"><a href="#cb56-73" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.stack(samples) </span>
<span id="cb56-74"><a href="#cb56-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="cf">if</span> sample_size <span class="cf">else</span> x.squeeze(<span class="dv">0</span>)</span>
<span id="cb56-75"><a href="#cb56-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-76"><a href="#cb56-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, z, x):</span>
<span id="cb56-77"><a href="#cb56-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-78"><a href="#cb56-78" aria-hidden="true" tabindex="-1"></a><span class="co">        The log density of the joint outcome under the generative model</span></span>
<span id="cb56-79"><a href="#cb56-79" aria-hidden="true" tabindex="-1"></a><span class="co">        z: [batch_size, latent_dim]</span></span>
<span id="cb56-80"><a href="#cb56-80" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb56-81"><a href="#cb56-81" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb56-82"><a href="#cb56-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gen_model.log_prob(z<span class="op">=</span>z, x<span class="op">=</span>x)</span>
<span id="cb56-83"><a href="#cb56-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-84"><a href="#cb56-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> DRL(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb56-85"><a href="#cb56-85" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-86"><a href="#cb56-86" aria-hidden="true" tabindex="-1"></a><span class="co">        MC estimates of a model's</span></span>
<span id="cb56-87"><a href="#cb56-87" aria-hidden="true" tabindex="-1"></a><span class="co">            * distortion D</span></span>
<span id="cb56-88"><a href="#cb56-88" aria-hidden="true" tabindex="-1"></a><span class="co">            * rate R</span></span>
<span id="cb56-89"><a href="#cb56-89" aria-hidden="true" tabindex="-1"></a><span class="co">            * and log-likelihood L</span></span>
<span id="cb56-90"><a href="#cb56-90" aria-hidden="true" tabindex="-1"></a><span class="co">        The estimates are based on single data points </span></span>
<span id="cb56-91"><a href="#cb56-91" aria-hidden="true" tabindex="-1"></a><span class="co">         but multiple latent samples.</span></span>
<span id="cb56-92"><a href="#cb56-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-93"><a href="#cb56-93" aria-hidden="true" tabindex="-1"></a><span class="co">        x: batch_shape + data_shape</span></span>
<span id="cb56-94"><a href="#cb56-94" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb56-95"><a href="#cb56-95" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-96"><a href="#cb56-96" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-97"><a href="#cb56-97" aria-hidden="true" tabindex="-1"></a>        sample_size <span class="op">=</span> sample_size <span class="kw">or</span> <span class="dv">1</span></span>
<span id="cb56-98"><a href="#cb56-98" aria-hidden="true" tabindex="-1"></a>        obs_dims <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.gen_model.cpd_net.outcome_shape)</span>
<span id="cb56-99"><a href="#cb56-99" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span>obs_dims]</span>
<span id="cb56-100"><a href="#cb56-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():        </span>
<span id="cb56-101"><a href="#cb56-101" aria-hidden="true" tabindex="-1"></a>            qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb56-102"><a href="#cb56-102" aria-hidden="true" tabindex="-1"></a>            pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior(batch_shape)</span>
<span id="cb56-103"><a href="#cb56-103" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:  <span class="co"># not every design admits tractable KL</span></span>
<span id="cb56-104"><a href="#cb56-104" aria-hidden="true" tabindex="-1"></a>                R <span class="op">=</span> td.kl_divergence(qz, pz)</span>
<span id="cb56-105"><a href="#cb56-105" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">NotImplementedError</span>:</span>
<span id="cb56-106"><a href="#cb56-106" aria-hidden="true" tabindex="-1"></a>                <span class="co"># MC estimation of KL(q(z|x)||p(z))</span></span>
<span id="cb56-107"><a href="#cb56-107" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> qz.sample((sample_size,))</span>
<span id="cb56-108"><a href="#cb56-108" aria-hidden="true" tabindex="-1"></a>                R <span class="op">=</span> (qz.log_prob(z) <span class="op">-</span> pz.log_prob(z)).mean(<span class="dv">0</span>)</span>
<span id="cb56-109"><a href="#cb56-109" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-110"><a href="#cb56-110" aria-hidden="true" tabindex="-1"></a>            ratios <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> sample_size</span>
<span id="cb56-111"><a href="#cb56-111" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size):</span>
<span id="cb56-112"><a href="#cb56-112" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> qz.sample()</span>
<span id="cb56-113"><a href="#cb56-113" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(z)</span>
<span id="cb56-114"><a href="#cb56-114" aria-hidden="true" tabindex="-1"></a>                ratios[k] <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x) <span class="op">-</span> qz.log_prob(z)</span>
<span id="cb56-115"><a href="#cb56-115" aria-hidden="true" tabindex="-1"></a>                D <span class="op">=</span> D <span class="op">-</span> px_z.log_prob(x)</span>
<span id="cb56-116"><a href="#cb56-116" aria-hidden="true" tabindex="-1"></a>            ratios <span class="op">=</span> torch.stack(ratios, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb56-117"><a href="#cb56-117" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> torch.logsumexp(ratios, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">-</span> np.log(sample_size)            </span>
<span id="cb56-118"><a href="#cb56-118" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> D <span class="op">/</span> sample_size</span>
<span id="cb56-119"><a href="#cb56-119" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> D, R, L</span>
<span id="cb56-120"><a href="#cb56-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-121"><a href="#cb56-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> elbo(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb56-122"><a href="#cb56-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-123"><a href="#cb56-123" aria-hidden="true" tabindex="-1"></a><span class="co">        An MC estimate of ELBO = -D -R</span></span>
<span id="cb56-124"><a href="#cb56-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-125"><a href="#cb56-125" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb56-126"><a href="#cb56-126" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb56-127"><a href="#cb56-127" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-128"><a href="#cb56-128" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-129"><a href="#cb56-129" aria-hidden="true" tabindex="-1"></a>        D, R, _ <span class="op">=</span> <span class="va">self</span>.DRL(x, sample_size<span class="op">=</span>sample_size)</span>
<span id="cb56-130"><a href="#cb56-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>D <span class="op">-</span>R</span>
<span id="cb56-131"><a href="#cb56-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-132"><a href="#cb56-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob_estimate(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb56-133"><a href="#cb56-133" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-134"><a href="#cb56-134" aria-hidden="true" tabindex="-1"></a><span class="co">        An importance sampling estimate of log p(x)</span></span>
<span id="cb56-135"><a href="#cb56-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-136"><a href="#cb56-136" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb56-137"><a href="#cb56-137" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb56-138"><a href="#cb56-138" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-139"><a href="#cb56-139" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-140"><a href="#cb56-140" aria-hidden="true" tabindex="-1"></a>        _, _, L <span class="op">=</span> <span class="va">self</span>.DRL(x, sample_size<span class="op">=</span>sample_size)</span>
<span id="cb56-141"><a href="#cb56-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> L</span>
<span id="cb56-142"><a href="#cb56-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-143"><a href="#cb56-143" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>, rate_weight<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb56-144"><a href="#cb56-144" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-145"><a href="#cb56-145" aria-hidden="true" tabindex="-1"></a><span class="co">        A surrogate for an MC estimate of - grad ELBO </span></span>
<span id="cb56-146"><a href="#cb56-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-147"><a href="#cb56-147" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb56-148"><a href="#cb56-148" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb56-149"><a href="#cb56-149" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb56-150"><a href="#cb56-150" aria-hidden="true" tabindex="-1"></a><span class="co">        cv: optional module for variance reduction</span></span>
<span id="cb56-151"><a href="#cb56-151" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-152"><a href="#cb56-152" aria-hidden="true" tabindex="-1"></a>        sample_size <span class="op">=</span> sample_size <span class="kw">or</span> <span class="dv">1</span></span>
<span id="cb56-153"><a href="#cb56-153" aria-hidden="true" tabindex="-1"></a>        obs_dims <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.gen_model.cpd_net.outcome_shape)</span>
<span id="cb56-154"><a href="#cb56-154" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span>obs_dims]</span>
<span id="cb56-155"><a href="#cb56-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-156"><a href="#cb56-156" aria-hidden="true" tabindex="-1"></a>        qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb56-157"><a href="#cb56-157" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior(batch_shape)</span>
<span id="cb56-158"><a href="#cb56-158" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-159"><a href="#cb56-159" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we can *always* make use of the score function estimator (SFE)</span></span>
<span id="cb56-160"><a href="#cb56-160" aria-hidden="true" tabindex="-1"></a>        use_sfe <span class="op">=</span> <span class="va">True</span> </span>
<span id="cb56-161"><a href="#cb56-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-162"><a href="#cb56-162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># these 3 log densities will contribute to the different parts of the objective</span></span>
<span id="cb56-163"><a href="#cb56-163" aria-hidden="true" tabindex="-1"></a>        log_p_x_z <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb56-164"><a href="#cb56-164" aria-hidden="true" tabindex="-1"></a>        log_p_z <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb56-165"><a href="#cb56-165" aria-hidden="true" tabindex="-1"></a>        log_q_z_x <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb56-166"><a href="#cb56-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-167"><a href="#cb56-167" aria-hidden="true" tabindex="-1"></a>        <span class="co"># these quantities will help us compute the SFE part of the objective</span></span>
<span id="cb56-168"><a href="#cb56-168" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (if needed)</span></span>
<span id="cb56-169"><a href="#cb56-169" aria-hidden="true" tabindex="-1"></a>        sfe <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-170"><a href="#cb56-170" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-171"><a href="#cb56-171" aria-hidden="true" tabindex="-1"></a>        cv_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-172"><a href="#cb56-172" aria-hidden="true" tabindex="-1"></a>        raw_r <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-173"><a href="#cb56-173" aria-hidden="true" tabindex="-1"></a>        cv_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-174"><a href="#cb56-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-175"><a href="#cb56-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(sample_size):            </span>
<span id="cb56-176"><a href="#cb56-176" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-177"><a href="#cb56-177" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Obtain a sample</span></span>
<span id="cb56-178"><a href="#cb56-178" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> qz.has_rsample:  <span class="co"># this is how td objects tell us whether they are continuously reparameterisable</span></span>
<span id="cb56-179"><a href="#cb56-179" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> qz.rsample()</span>
<span id="cb56-180"><a href="#cb56-180" aria-hidden="true" tabindex="-1"></a>                use_sfe <span class="op">=</span> <span class="va">False</span>  <span class="co"># with path derivatives, we do not need SFE</span></span>
<span id="cb56-181"><a href="#cb56-181" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb56-182"><a href="#cb56-182" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> qz.sample()</span>
<span id="cb56-183"><a href="#cb56-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-184"><a href="#cb56-184" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Parameterise the observational model</span></span>
<span id="cb56-185"><a href="#cb56-185" aria-hidden="true" tabindex="-1"></a>            px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(z)</span>
<span id="cb56-186"><a href="#cb56-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-187"><a href="#cb56-187" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute all three relevant densities:</span></span>
<span id="cb56-188"><a href="#cb56-188" aria-hidden="true" tabindex="-1"></a>            <span class="co"># p(x|z,theta)</span></span>
<span id="cb56-189"><a href="#cb56-189" aria-hidden="true" tabindex="-1"></a>            log_p_x_z <span class="op">=</span> log_p_x_z <span class="op">+</span> px_z.log_prob(x)</span>
<span id="cb56-190"><a href="#cb56-190" aria-hidden="true" tabindex="-1"></a>            <span class="co"># q(z|x,lambda)</span></span>
<span id="cb56-191"><a href="#cb56-191" aria-hidden="true" tabindex="-1"></a>            log_q_z_x <span class="op">=</span> log_q_z_x <span class="op">+</span> qz.log_prob(z)</span>
<span id="cb56-192"><a href="#cb56-192" aria-hidden="true" tabindex="-1"></a>            <span class="co"># p(z|theta)</span></span>
<span id="cb56-193"><a href="#cb56-193" aria-hidden="true" tabindex="-1"></a>            log_p_z <span class="op">=</span> log_p_z <span class="op">+</span> pz.log_prob(z)</span>
<span id="cb56-194"><a href="#cb56-194" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-195"><a href="#cb56-195" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the "reward" for SFE</span></span>
<span id="cb56-196"><a href="#cb56-196" aria-hidden="true" tabindex="-1"></a>            raw_r <span class="op">=</span> log_p_x_z <span class="op">+</span> log_p_z <span class="op">-</span> log_q_z_x</span>
<span id="cb56-197"><a href="#cb56-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-198"><a href="#cb56-198" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply variance reduction techniques</span></span>
<span id="cb56-199"><a href="#cb56-199" aria-hidden="true" tabindex="-1"></a>            r, l <span class="op">=</span> <span class="va">self</span>.cv_model(raw_r.detach(), x<span class="op">=</span>x, q<span class="op">=</span>qz, r_fn<span class="op">=</span><span class="kw">lambda</span> a: <span class="va">self</span>.gen_model(a).log_prob(x))</span>
<span id="cb56-200"><a href="#cb56-200" aria-hidden="true" tabindex="-1"></a>            cv_loss <span class="op">=</span> cv_loss <span class="op">+</span> l</span>
<span id="cb56-201"><a href="#cb56-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-202"><a href="#cb56-202" aria-hidden="true" tabindex="-1"></a>            <span class="co"># SFE part for updating lambda</span></span>
<span id="cb56-203"><a href="#cb56-203" aria-hidden="true" tabindex="-1"></a>            sfe <span class="op">=</span> sfe <span class="op">+</span> r.detach() <span class="op">*</span> qz.log_prob(z) </span>
<span id="cb56-204"><a href="#cb56-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-205"><a href="#cb56-205" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the sample mean for the different terms        </span></span>
<span id="cb56-206"><a href="#cb56-206" aria-hidden="true" tabindex="-1"></a>        sfe <span class="op">=</span> (sfe <span class="op">/</span> sample_size)</span>
<span id="cb56-207"><a href="#cb56-207" aria-hidden="true" tabindex="-1"></a>        cv_loss <span class="op">=</span> cv_loss <span class="op">/</span> sample_size        </span>
<span id="cb56-208"><a href="#cb56-208" aria-hidden="true" tabindex="-1"></a>        log_p_x_z <span class="op">=</span> log_p_x_z <span class="op">/</span> sample_size</span>
<span id="cb56-209"><a href="#cb56-209" aria-hidden="true" tabindex="-1"></a>        log_p_z <span class="op">=</span> log_p_z <span class="op">/</span> sample_size</span>
<span id="cb56-210"><a href="#cb56-210" aria-hidden="true" tabindex="-1"></a>        log_q_z_x <span class="op">=</span> log_q_z_x <span class="op">/</span> sample_size</span>
<span id="cb56-211"><a href="#cb56-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-212"><a href="#cb56-212" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> <span class="op">-</span> log_p_x_z</span>
<span id="cb56-213"><a href="#cb56-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:  <span class="co"># not every design admits tractable KL</span></span>
<span id="cb56-214"><a href="#cb56-214" aria-hidden="true" tabindex="-1"></a>            R <span class="op">=</span> td.kl_divergence(qz, pz)</span>
<span id="cb56-215"><a href="#cb56-215" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">NotImplementedError</span>:         </span>
<span id="cb56-216"><a href="#cb56-216" aria-hidden="true" tabindex="-1"></a>            R <span class="op">=</span> log_q_z_x <span class="op">-</span> log_p_z</span>
<span id="cb56-217"><a href="#cb56-217" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-218"><a href="#cb56-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_sfe:</span>
<span id="cb56-219"><a href="#cb56-219" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the first two terms update theta</span></span>
<span id="cb56-220"><a href="#cb56-220" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the last term updates lambda </span></span>
<span id="cb56-221"><a href="#cb56-221" aria-hidden="true" tabindex="-1"></a>            elbo_grad_surrogate <span class="op">=</span> log_p_x_z <span class="op">+</span> log_p_z <span class="op">+</span> sfe</span>
<span id="cb56-222"><a href="#cb56-222" aria-hidden="true" tabindex="-1"></a>            <span class="co"># note that the term (log_p_x_z + log_p_z) is also part of sfe</span></span>
<span id="cb56-223"><a href="#cb56-223" aria-hidden="true" tabindex="-1"></a>            <span class="co"># but there it is detached, meaning that it won't contribute to </span></span>
<span id="cb56-224"><a href="#cb56-224" aria-hidden="true" tabindex="-1"></a>            <span class="co"># grad theta</span></span>
<span id="cb56-225"><a href="#cb56-225" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb56-226"><a href="#cb56-226" aria-hidden="true" tabindex="-1"></a>            <span class="co"># without SFE, we can use the classic form of the ELBO</span></span>
<span id="cb56-227"><a href="#cb56-227" aria-hidden="true" tabindex="-1"></a>            elbo_grad_surrogate <span class="op">=</span> <span class="op">-</span>D <span class="op">-</span> R</span>
<span id="cb56-228"><a href="#cb56-228" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-229"><a href="#cb56-229" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>elbo_grad_surrogate <span class="op">+</span> cv_loss</span>
<span id="cb56-230"><a href="#cb56-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-231"><a href="#cb56-231" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">'loss'</span>: loss.mean(<span class="dv">0</span>), <span class="st">'ELBO'</span>: (<span class="op">-</span>D <span class="op">-</span>R).mean(<span class="dv">0</span>).item(), <span class="st">'D'</span>: D.mean(<span class="dv">0</span>).item(), <span class="st">'R'</span>: R.mean(<span class="dv">0</span>).item(), <span class="st">'cv_loss'</span>: cv_loss.mean(<span class="dv">0</span>).item()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s an example</p>
<div id="cell-72" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> NVIL(</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    JointDistribution(</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        GaussianPriorNet(<span class="dv">10</span>),</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        BinarizedImageModel(</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_ffnn_decoder</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    InferenceModel(</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        cpd_net_type<span class="op">=</span>GaussianCPDNet,</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>], </span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>], </span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],        </span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>        encoder_type<span class="op">=</span>build_ffnn_encoder</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>    VarianceReduction()</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>vae</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>NVIL(
  (gen_model): JointDistribution(
    (prior_net): GaussianPriorNet()
    (cpd_net): BinarizedImageModel(
      (decoder): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=10, out_features=512, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): ReLU()
        (6): Dropout(p=0.1, inplace=False)
        (7): Linear(in_features=512, out_features=4096, bias=True)
        (8): ReshapeLast()
      )
    )
  )
  (inf_model): InferenceModel(
    (encoder): Sequential(
      (0): FlattenImage()
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=4096, out_features=512, bias=True)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=512, out_features=512, bias=True)
      (6): ReLU()
      (7): Dropout(p=0.0, inplace=False)
      (8): Linear(in_features=512, out_features=1024, bias=True)
    )
    (cpd_net): GaussianCPDNet(
      (encoder): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=1024, out_features=20, bias=True)
        (2): ReLU()
      )
      (locs): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): ReshapeLast()
      )
      (scales): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): Softplus(beta=1, threshold=20)
        (3): ReshapeLast()
      )
    )
  )
  (cv_model): VarianceReduction()
)</code></pre>
</div>
</div>
<div id="cell-73" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, y <span class="kw">in</span> train_loader:</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'x.shape:'</span>, x.shape)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(vae(x))</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.shape: torch.Size([64, 1, 64, 64])
{'loss': tensor(2845.1970, grad_fn=&lt;MeanBackward1&gt;), 'ELBO': -2845.197021484375, 'D': 2843.9189453125, 'R': 1.27774977684021, 'cv_loss': 0.0}</code></pre>
</div>
</div>
<section id="training-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="training-algorithm">3.3.1 Training algorithm</h4>
<p>We have up to three components (recall that some control variates can have their own parameters), so we will be manipulating up to three optimisers:</p>
<div id="cell-75" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptCollection:</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, gen, inf, cv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen <span class="op">=</span> gen</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf <span class="op">=</span> inf</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cv <span class="op">=</span> cv</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen.zero_grad()</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf.zero_grad()</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cv:</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cv.zero_grad()</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen.step()</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf.step()</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cv:</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cv.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s some helper code to assess and train the model</p>
<div id="cell-77" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, OrderedDict</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assess(model, sample_size, dl, device):</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrapper for estimating a model's ELBO, distortion, rate, and log-likelihood</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="co">     using all data points in a data loader.</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    data_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> dl:</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>            Dx, Rx, Lx <span class="op">=</span> model.DRL(batch_x.to(device), sample_size<span class="op">=</span>sample_size)</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> D <span class="op">+</span> Dx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>            R <span class="op">=</span> R <span class="op">+</span> Rx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">+</span> Lx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>            data_size <span class="op">+=</span> batch_x.shape[<span class="dv">0</span>]</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">/</span> data_size</span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> R <span class="op">/</span> data_size</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L <span class="op">/</span> data_size</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'ELBO'</span>: (<span class="op">-</span>D <span class="op">-</span>R).item(), <span class="st">'D'</span>: D.item(), <span class="st">'R'</span>: R.item(), <span class="st">'L'</span>: L.item()}</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_vae(model: NVIL, opts: OptCollection, </span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>    training_data, dev_data,</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>    sample_size_training<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>    sample_size_eval<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a><span class="co">    model: pytorch model</span></span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a><span class="co">    optimiser: pytorch optimiser</span></span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a><span class="co">    training_corpus: a TaggedCorpus for trianing</span></span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a><span class="co">    dev_corpus: a TaggedCorpus for dev</span></span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a><span class="co">    batch_size: use more if you have more memory</span></span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a><span class="co">    num_epochs: use more for improved convergence</span></span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a><span class="co">    check_every: use less to check performance on dev set more often</span></span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a><span class="co">    device: where we run the experiment</span></span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-46"><a href="#cb62-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a log of quantities computed during training (for plotting)</span></span>
<span id="cb62-47"><a href="#cb62-47" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb62-48"><a href="#cb62-48" aria-hidden="true" tabindex="-1"></a>    batcher <span class="op">=</span> DataLoader(training_data, batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-49"><a href="#cb62-49" aria-hidden="true" tabindex="-1"></a>    dev_batcher <span class="op">=</span> DataLoader(dev_data, batch_size, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-50"><a href="#cb62-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-51"><a href="#cb62-51" aria-hidden="true" tabindex="-1"></a>    total_steps <span class="op">=</span> num_epochs <span class="op">*</span> <span class="bu">len</span>(batcher)</span>
<span id="cb62-52"><a href="#cb62-52" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb62-53"><a href="#cb62-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-54"><a href="#cb62-54" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-55"><a href="#cb62-55" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb62-56"><a href="#cb62-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb62-57"><a href="#cb62-57" aria-hidden="true" tabindex="-1"></a>        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb62-58"><a href="#cb62-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-59"><a href="#cb62-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tqdm(<span class="bu">range</span>(total_steps)) <span class="im">as</span> bar:</span>
<span id="cb62-60"><a href="#cb62-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb62-61"><a href="#cb62-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb62-62"><a href="#cb62-62" aria-hidden="true" tabindex="-1"></a>                model.train()</span>
<span id="cb62-63"><a href="#cb62-63" aria-hidden="true" tabindex="-1"></a>                opts.zero_grad()</span>
<span id="cb62-64"><a href="#cb62-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-65"><a href="#cb62-65" aria-hidden="true" tabindex="-1"></a>                loss_dict <span class="op">=</span> model(</span>
<span id="cb62-66"><a href="#cb62-66" aria-hidden="true" tabindex="-1"></a>                    batch_x.to(device), </span>
<span id="cb62-67"><a href="#cb62-67" aria-hidden="true" tabindex="-1"></a>                    sample_size<span class="op">=</span>sample_size_training,</span>
<span id="cb62-68"><a href="#cb62-68" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb62-69"><a href="#cb62-69" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric, value <span class="kw">in</span> loss_dict.items():</span>
<span id="cb62-70"><a href="#cb62-70" aria-hidden="true" tabindex="-1"></a>                    log[<span class="ss">f'training.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">'</span>].append((step, value))</span>
<span id="cb62-71"><a href="#cb62-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-72"><a href="#cb62-72" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">'loss'</span>].backward()</span>
<span id="cb62-73"><a href="#cb62-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-74"><a href="#cb62-74" aria-hidden="true" tabindex="-1"></a>                nn.utils.clip_grad_norm_(</span>
<span id="cb62-75"><a href="#cb62-75" aria-hidden="true" tabindex="-1"></a>                    model.parameters(), </span>
<span id="cb62-76"><a href="#cb62-76" aria-hidden="true" tabindex="-1"></a>                    grad_clip</span>
<span id="cb62-77"><a href="#cb62-77" aria-hidden="true" tabindex="-1"></a>                )    </span>
<span id="cb62-78"><a href="#cb62-78" aria-hidden="true" tabindex="-1"></a>                opts.step()</span>
<span id="cb62-79"><a href="#cb62-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-80"><a href="#cb62-80" aria-hidden="true" tabindex="-1"></a>                bar_dict <span class="op">=</span> OrderedDict()</span>
<span id="cb62-81"><a href="#cb62-81" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric, value <span class="kw">in</span> loss_dict.items():</span>
<span id="cb62-82"><a href="#cb62-82" aria-hidden="true" tabindex="-1"></a>                    bar_dict[<span class="ss">f'training.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>loss_dict[metric]<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb62-83"><a href="#cb62-83" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric <span class="kw">in</span> [<span class="st">'ELBO'</span>, <span class="st">'D'</span>, <span class="st">'R'</span>, <span class="st">'L'</span>]:</span>
<span id="cb62-84"><a href="#cb62-84" aria-hidden="true" tabindex="-1"></a>                    bar_dict[<span class="ss">f"dev.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span>  <span class="st">"</span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(log[<span class="ss">f"dev.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">"</span>][<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>])</span>
<span id="cb62-85"><a href="#cb62-85" aria-hidden="true" tabindex="-1"></a>                bar.set_postfix(bar_dict)</span>
<span id="cb62-86"><a href="#cb62-86" aria-hidden="true" tabindex="-1"></a>                bar.update()</span>
<span id="cb62-87"><a href="#cb62-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-88"><a href="#cb62-88" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> step <span class="op">%</span> check_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb62-89"><a href="#cb62-89" aria-hidden="true" tabindex="-1"></a>                    model.<span class="bu">eval</span>()</span>
<span id="cb62-90"><a href="#cb62-90" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb62-91"><a href="#cb62-91" aria-hidden="true" tabindex="-1"></a>                        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb62-92"><a href="#cb62-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-93"><a href="#cb62-93" aria-hidden="true" tabindex="-1"></a>                step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb62-94"><a href="#cb62-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-95"><a href="#cb62-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-96"><a href="#cb62-96" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb62-97"><a href="#cb62-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb62-98"><a href="#cb62-98" aria-hidden="true" tabindex="-1"></a>        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb62-99"><a href="#cb62-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-100"><a href="#cb62-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And, finally, some code to help inspect samples</p>
<div id="cell-79" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inspect_lvm(model, dl, device):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> dl:    </span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>        x_ <span class="op">=</span> model.sample(<span class="dv">16</span>, <span class="dv">4</span>, oversample<span class="op">=</span><span class="va">True</span>).cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x_, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Prior samples"</span>)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Observations"</span>)</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        x_ <span class="op">=</span> model.cond_sample(x.to(device)).cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x_, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Conditional samples"</span>)</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="variance-reduction" class="level4">
<h4 class="anchored" data-anchor-id="variance-reduction">3.3.2 Variance reduction</h4>
<p>Here are some concrete strategies for variance reduction. You can skip those in a first pass.</p>
<div id="cell-81" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CentredReward(VarianceReduction):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co">     it maintains a running estimate of the average reward and updates</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co">     a batch of rewards by computing reward - avg.</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._alpha <span class="op">=</span> alpha</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._r_mean <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x<span class="op">=</span><span class="va">None</span>, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Centre the reward and update running estimates of mean.</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sufficient statistics for next updates</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>            r_mean <span class="op">=</span> torch.mean(r, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># centre the signal</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">-</span> <span class="va">self</span>._r_mean</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update running estimate of mean</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_mean <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>._alpha) <span class="op">*</span> <span class="va">self</span>._r_mean <span class="op">+</span> <span class="va">self</span>._alpha <span class="op">*</span> r_mean.item()                        </span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledReward(VarianceReduction):</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="co">     it maintains a running estimate of the reward's standard deviation and </span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a><span class="co">     updates a batch of rewards by computing reward / maximum(stddev, 1).</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._alpha <span class="op">=</span> alpha</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._r_std <span class="op">=</span> <span class="fl">1.0</span>                   </span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x<span class="op">=</span><span class="va">None</span>, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Scale the reward by a running estimate of std, and also update the estimate.</span></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sufficient statistics for next updates</span></span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>            r_std <span class="op">=</span> torch.std(r, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># standardise the signal</span></span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">/</span> <span class="va">self</span>._r_std            </span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update running estimate of std</span></span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_std <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>._alpha) <span class="op">*</span> <span class="va">self</span>._r_std <span class="op">+</span> <span class="va">self</span>._alpha <span class="op">*</span> r_std.item()</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># it's not safe to standardise with scales less than 1</span></span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_std <span class="op">=</span> np.maximum(<span class="va">self</span>._r_std, <span class="fl">1.</span>)</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfCritic(VarianceReduction):</span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a><span class="co">     it updates a batch of rewards by computing reward - reward', where</span></span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a><span class="co">     reward' is (log p(X=x|Z=z')).detach() assessed for a novel sample</span></span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a><span class="co">     z' ~ Z|X=x.</span></span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb64-64"><a href="#cb64-64" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb64-65"><a href="#cb64-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-66"><a href="#cb64-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):</span>
<span id="cb64-67"><a href="#cb64-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-68"><a href="#cb64-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Standardise the reward and update running estimates of mean/std.</span></span>
<span id="cb64-69"><a href="#cb64-69" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-70"><a href="#cb64-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-71"><a href="#cb64-71" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> q.sample()</span>
<span id="cb64-72"><a href="#cb64-72" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">-</span> r_fn(z, x)</span>
<span id="cb64-73"><a href="#cb64-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb64-74"><a href="#cb64-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-75"><a href="#cb64-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-76"><a href="#cb64-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Baseline(VarianceReduction):</span>
<span id="cb64-77"><a href="#cb64-77" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb64-78"><a href="#cb64-78" aria-hidden="true" tabindex="-1"></a><span class="co">    An input-dependent baseline implemented as an MLP.</span></span>
<span id="cb64-79"><a href="#cb64-79" aria-hidden="true" tabindex="-1"></a><span class="co">    The trainable parameters are adjusted via MSE.</span></span>
<span id="cb64-80"><a href="#cb64-80" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb64-81"><a href="#cb64-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-82"><a href="#cb64-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, hidden_size, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb64-83"><a href="#cb64-83" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb64-84"><a href="#cb64-84" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.baseline <span class="op">=</span> nn.Sequential(</span>
<span id="cb64-85"><a href="#cb64-85" aria-hidden="true" tabindex="-1"></a>            FlattenImage(),</span>
<span id="cb64-86"><a href="#cb64-86" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb64-87"><a href="#cb64-87" aria-hidden="true" tabindex="-1"></a>            nn.Linear(num_inputs, hidden_size),</span>
<span id="cb64-88"><a href="#cb64-88" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb64-89"><a href="#cb64-89" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb64-90"><a href="#cb64-90" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb64-91"><a href="#cb64-91" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb64-92"><a href="#cb64-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-93"><a href="#cb64-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-94"><a href="#cb64-94" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-95"><a href="#cb64-95" aria-hidden="true" tabindex="-1"></a><span class="co">        Return r - baseline(x) and Baseline's loss.</span></span>
<span id="cb64-96"><a href="#cb64-96" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-97"><a href="#cb64-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (1,)</span></span>
<span id="cb64-98"><a href="#cb64-98" aria-hidden="true" tabindex="-1"></a>        r_hat <span class="op">=</span> <span class="va">self</span>.baseline(x)</span>
<span id="cb64-99"><a href="#cb64-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb64-100"><a href="#cb64-100" aria-hidden="true" tabindex="-1"></a>        r_hat <span class="op">=</span> r_hat.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb64-101"><a href="#cb64-101" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (r <span class="op">-</span> r_hat)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb64-102"><a href="#cb64-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r <span class="op">-</span> r_hat.detach(), loss</span>
<span id="cb64-103"><a href="#cb64-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-104"><a href="#cb64-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-105"><a href="#cb64-105" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CVChain(VarianceReduction):</span>
<span id="cb64-106"><a href="#cb64-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-107"><a href="#cb64-107" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb64-108"><a href="#cb64-108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb64-109"><a href="#cb64-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(args) <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> <span class="bu">isinstance</span>(args[<span class="dv">0</span>], OrderedDict):</span>
<span id="cb64-110"><a href="#cb64-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> key, module <span class="kw">in</span> args[<span class="dv">0</span>].items():</span>
<span id="cb64-111"><a href="#cb64-111" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.add_module(key, module)</span>
<span id="cb64-112"><a href="#cb64-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb64-113"><a href="#cb64-113" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, module <span class="kw">in</span> <span class="bu">enumerate</span>(args):</span>
<span id="cb64-114"><a href="#cb64-114" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.add_module(<span class="bu">str</span>(idx), module)</span>
<span id="cb64-115"><a href="#cb64-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-116"><a href="#cb64-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):         </span>
<span id="cb64-117"><a href="#cb64-117" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-118"><a href="#cb64-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> cv <span class="kw">in</span> <span class="va">self</span>._modules.values():</span>
<span id="cb64-119"><a href="#cb64-119" aria-hidden="true" tabindex="-1"></a>            r, l <span class="op">=</span> cv(r, x<span class="op">=</span>x, q<span class="op">=</span>q, r_fn<span class="op">=</span>r_fn)</span>
<span id="cb64-120"><a href="#cb64-120" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">+</span> l</span>
<span id="cb64-121"><a href="#cb64-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r, loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="experiment" class="level4">
<h4 class="anchored" data-anchor-id="experiment">3.3.3 Experiment</h4>
<div id="cell-83" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>seed_all()</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NVIL(</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    JointDistribution(</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        GaussianPriorNet(<span class="dv">10</span>),</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        BinarizedImageModel(</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    InferenceModel(</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>], </span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>], </span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>        cpd_net_type<span class="op">=</span>GaussianCPDNet <span class="co"># Gaussian prior and Gaussian posterior: this is a classic VAE</span></span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>    VarianceReduction(), <span class="co"># no variance reduction is needed for a VAE</span></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#CVChain(  # variance reduction helps SFE</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    CentredReward(),        </span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    #Baseline(np.prod(img_shape), 512), # this is how you would use a trained baselined</span></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    #ScaledReward()</span></span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#)</span></span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> OptCollection(</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tips based on empirical practice:</span></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adam is the go-to choice for (reparameterised) VAEs</span></span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>    opt.Adam(model.gen_params(), lr<span class="op">=</span><span class="fl">5e-4</span>, weight_decay<span class="op">=</span><span class="fl">1e-6</span>),</span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    opt.Adam(model.inf_params(), lr<span class="op">=</span><span class="fl">1e-4</span>),</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adam is not often a good choice for SFE-based optimisation</span></span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  a possible reason: SFE is too noisy and the design choices behind Adam</span></span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  were made having reparameterised gradients in mind</span></span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">#opt.RMSprop(model.gen_params(), lr=5e-4, weight_decay=1e-6),</span></span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">#opt.RMSprop(model.inf_params(), lr=1e-4),</span></span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">#opt.RMSprop(model.cv_params(), lr=1e-4, weight_decay=1e-6) # you need this if your baseline has trainable parameters</span></span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>NVIL(
  (gen_model): JointDistribution(
    (prior_net): GaussianPriorNet()
    (cpd_net): BinarizedImageModel(
      (decoder): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=10, out_features=512, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): ReLU()
        (6): Dropout(p=0.1, inplace=False)
        (7): Linear(in_features=512, out_features=4096, bias=True)
        (8): ReshapeLast()
      )
    )
  )
  (inf_model): InferenceModel(
    (encoder): Sequential(
      (0): FlattenImage()
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=4096, out_features=512, bias=True)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=512, out_features=512, bias=True)
      (6): ReLU()
      (7): Dropout(p=0.0, inplace=False)
      (8): Linear(in_features=512, out_features=1024, bias=True)
    )
    (cpd_net): GaussianCPDNet(
      (encoder): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=1024, out_features=20, bias=True)
        (2): ReLU()
      )
      (locs): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): ReshapeLast()
      )
      (scales): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): Softplus(beta=1, threshold=20)
        (3): ReshapeLast()
      )
    )
  )
  (cv_model): VarianceReduction()
)</code></pre>
</div>
</div>
<div id="cell-84" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_vae(</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    opts<span class="op">=</span>opts,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>train_ds, </span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>val_ds,</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">256</span>, </span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span><span class="dv">5</span>,  <span class="co"># use more for better models</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    sample_size_training<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    sample_size_eval<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"079dc0a1ab5843ac8756d93dd2f98aad","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="cell-85" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>log.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>dict_keys(['dev.ELBO', 'dev.D', 'dev.R', 'dev.L', 'training.loss', 'training.ELBO', 'training.D', 'training.R', 'training.cv_loss'])</code></pre>
</div>
</div>
<div id="cell-86" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span> <span class="op">+</span> <span class="bu">int</span>(<span class="st">'training.cv_loss'</span> <span class="kw">in</span> log), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.array(log[<span class="st">'training.ELBO'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.ELBO'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">"training ELBO"</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.array(log[<span class="st">'training.D'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.D'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">"training D"</span>)</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.array(log[<span class="st">'training.R'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.R'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">"training R"</span>)</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'training.cv_loss'</span> <span class="kw">in</span> log:</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].plot(np.array(log[<span class="st">'training.cv_loss'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.cv_loss'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].set_ylabel(<span class="st">"cv loss"</span>)</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-50-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-87" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.array(log[<span class="st">'dev.ELBO'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.ELBO'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">"dev ELBO"</span>)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.array(log[<span class="st">'dev.D'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.D'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">"dev D"</span>)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.array(log[<span class="st">'dev.R'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.R'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">"dev R"</span>)</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].plot(np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_ylabel(<span class="st">"dev L"</span>)</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>fig.tight_layout(h_pad<span class="op">=</span><span class="dv">2</span>, w_pad<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-51-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-88" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>inspect_lvm(model, DataLoader(val_ds, <span class="dv">64</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>), my_device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-52-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2b_files/figure-html/cell-52-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="beyond" class="level2">
<h2 class="anchored" data-anchor-id="beyond">4. Beyond</h2>
<p>There are various things you can try.</p>
<p>You can try to use a <strong>trainable prior</strong>. If you do, you will probably note that it is not trivial how to get the prior to be used in an interesting way. In fact, trained priors are completely data-driven, and there’s no reason to believe that an NN will find a “data-driven” explanation of the data that is anything like what you would like it to. If you want the different components of your prior to specialise to certain types of output, you will need to design stronger pressures. For example, you may use some degree of annotation to inform what each component should typically be responsible for. Ideas that dispense with the need for annotation will have to focus on the architecture of the decoder or on other penalties in the loss. For example, a decoder that is built with some geometric properties.</p>
<p>If you use a trainable prior, it is a good idea to try and visualise what the final prior looks like. You can try sampling from it and plotting histograms of the different coordinates of the samples. You can flatten the samples and inspect the coordinates marginally, you can also use other plotting tools (see <a href="https://seaborn.pydata.org/examples/scatterplot_matrix.html">some examples from seaborn</a>) to spot dependency, for example. And, of course, you can always use tools for dimensionality reduction (eg, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>).</p>
<p>You can try to use a <strong>stronger posterior approximation</strong> that is reparameterisable. A good idea is to build a bijective transformation (ie., a normalising flow).</p>
<p>You can try to <strong>improve the gradient estimator</strong> of the mixture of mean field families. Within a mixture of C components that are each reparameterisable, only the component assigmment is a discrete operation, so with access to the sampling procedure internal to mixture, one can design a customised gradient estimator that updates <span class="math inline">\(\omega(x; \lambda)\)</span> through SFE, but updates <span class="math inline">\(\mu(x; \lambda)\)</span> and <span class="math inline">\(\sigma(x;\lambda)\)</span> through reparameterisation.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>