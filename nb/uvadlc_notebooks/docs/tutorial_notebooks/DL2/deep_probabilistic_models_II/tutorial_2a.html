<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DPM2 - Variational inference for deep discrete latent variable models – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intended-learning-outcomes" id="toc-intended-learning-outcomes" class="nav-link active" data-scroll-target="#intended-learning-outcomes">0. Intended Learning Outcomes</a>
  <ul class="collapse">
  <li><a href="#setting-up" id="toc-setting-up" class="nav-link" data-scroll-target="#setting-up">0.1 Setting up</a></li>
  </ul></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">1. Data</a></li>
  <li><a href="#latent-variable-models" id="toc-latent-variable-models" class="nav-link" data-scroll-target="#latent-variable-models">2. Latent variable models</a>
  <ul class="collapse">
  <li><a href="#prior-networks" id="toc-prior-networks" class="nav-link" data-scroll-target="#prior-networks">2.1 Prior networks</a></li>
  <li><a href="#conditional-probability-distributions" id="toc-conditional-probability-distributions" class="nav-link" data-scroll-target="#conditional-probability-distributions">2.2 Conditional probability distributions</a></li>
  <li><a href="#joint-distribution" id="toc-joint-distribution" class="nav-link" data-scroll-target="#joint-distribution">2.3 Joint distribution</a></li>
  </ul></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">3. Learning</a>
  <ul class="collapse">
  <li><a href="#tractable-lvms" id="toc-tractable-lvms" class="nav-link" data-scroll-target="#tractable-lvms">3.1 Tractable LVMs</a></li>
  <li><a href="#training-algorithm" id="toc-training-algorithm" class="nav-link" data-scroll-target="#training-algorithm">3.2 Training algorithm</a></li>
  <li><a href="#intractable-lvms" id="toc-intractable-lvms" class="nav-link" data-scroll-target="#intractable-lvms">3.3 Intractable LVMs</a></li>
  <li><a href="#inference-model" id="toc-inference-model" class="nav-link" data-scroll-target="#inference-model">3.4 Inference model</a></li>
  <li><a href="#neural-variational-inference" id="toc-neural-variational-inference" class="nav-link" data-scroll-target="#neural-variational-inference">3.5 Neural Variational Inference</a></li>
  </ul></li>
  <li><a href="#beyond" id="toc-beyond" class="nav-link" data-scroll-target="#beyond">4. Beyond</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DPM2 - Variational inference for deep discrete latent variable models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab"></a><br>
<strong>Recordings:</strong> <a href="https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/aa13f3fd4c064f29b0fcb4e43c05c0cc1d">Lecture 1.3</a>, <a href="https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/1846886c57834477adc2eb403b64b3481d">Lecture 1.4</a> &amp; <a href="https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/6000b98c8a1944c5979cc0ae11690bd61d">Lecture 1.5</a><br>
<strong>Authors</strong>: Wilker Aziz</p>
<section id="intended-learning-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="intended-learning-outcomes">0. Intended Learning Outcomes</h2>
<p>After this tutorial the student should be able to</p>
<ul>
<li>parameterise a latent variable model with discrete latent variables</li>
<li>estimate parameters using exact log-likelihood function (when tractable)</li>
<li>estimate parameters using neural variational inference</li>
</ul>
<section id="setting-up" class="level3">
<h3 class="anchored" data-anchor-id="setting-up">0.1 Setting up</h3>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributions <span class="im">as</span> td</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, OrderedDict</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For having SVG graphics</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seed_all(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>seed_all()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">1. Data</h2>
<p>We are going to use toy image datasets for this notebook. These are fixed-dimensional observations for which encoder and decoders are relatively easy to design. This way we can focus on the aspects that are probabilistic in nature.</p>
<div id="cell-8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> FashionMNIST</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split, Dataset</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataloader <span class="im">import</span> DataLoader</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> make_grid</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> opt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>DATASET_PATH <span class="op">=</span> <span class="st">"../../data"</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>my_device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A helper to binarize datasets:</p>
<div id="cell-10" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Binarizer(Dataset):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ds, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._ds <span class="op">=</span> ds</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._threshold <span class="op">=</span> threshold</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Size of the corpus in number of sequence pairs"""</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._ds)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Return corpus_x[idx] and corpus_y[idx] converted to codes</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        the latter has the EOS code in the end</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> <span class="va">self</span>._ds[idx]</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (x <span class="op">&gt;=</span> <span class="va">self</span>._threshold).<span class="bu">float</span>(), y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>FashionMNIST</p>
<div id="cell-12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> FashionMNIST(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>               transform<span class="op">=</span>transforms.Compose([transforms.Resize(<span class="dv">64</span>), transforms.ToTensor()]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>img_shape <span class="op">=</span> dataset[<span class="dv">0</span>][<span class="dv">0</span>].shape</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of an image:"</span>, img_shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of an image: torch.Size([1, 64, 64])</code></pre>
</div>
</div>
<p>Let’s make a dev set for ourselves:</p>
<div id="cell-15" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> val_size</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>train_ds, val_ds <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_ds), <span class="bu">len</span>(val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(59000, 1000)</code></pre>
</div>
</div>
<p>we suggest that you binarize the data in a first pass through this notebook, but as you will see, we can also model the continuous pixel intensities.</p>
<div id="cell-17" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>bin_data <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bin_data:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> Binarizer(train_ds)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    val_ds <span class="op">=</span> Binarizer(val_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_ds, batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_ds, batch_size, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s visualise a few samples</p>
<div id="cell-21" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> images, y <span class="kw">in</span> train_loader:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'images.shape:'</span>, images.shape)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)    </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(make_grid(images, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>images.shape: torch.Size([64, 1, 64, 64])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-12-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="latent-variable-models" class="level2">
<h2 class="anchored" data-anchor-id="latent-variable-models">2. Latent variable models</h2>
<p>We will be using NNs to parameterise a latent variable model, that is, a joint distribution over a collection of random variables (rvs), some of which are observed, some of which are not.</p>
<p>We are interested in two random variables (rvs):</p>
<ul>
<li>a discrete latent code <span class="math inline">\(Z \in \mathcal Z\)</span></li>
<li>and an image <span class="math inline">\(X \in \mathcal X \subseteq \mathbb R^D\)</span></li>
</ul>
<p>In this tutorial, <span class="math inline">\(x\)</span> is has a number <span class="math inline">\(C\)</span> of channels, a certain width <span class="math inline">\(W\)</span> and a certain height <span class="math inline">\(H\)</span>, so <span class="math inline">\(\mathcal X \subseteq \mathbb R^{C \times W \times H}\)</span>. Because we have fixed <span class="math inline">\(D = C \times W \times H\)</span>, <span class="math inline">\(\mathcal X\)</span> is finite-dimensional, but this need not be the case in general (for example, in a different domain, <span class="math inline">\(\mathcal X\)</span> could be the unbounded space of all sentences of arbitrary lenght). We may treat the pixel intensities as discrete or continuous, as long as we choose an appropriate pmf/pdf for each case.</p>
<p>In this tutorial we will look into two types of latent code. A categorical code <span class="math inline">\(z \in \{1, \ldots, K\}\)</span>, and a combinatorial code <span class="math inline">\(z \in \{0, 1\}^K\)</span>, in both cases <span class="math inline">\(\mathcal Z\)</span> is countably finite, but in general this need not be the case (for example, we could have <span class="math inline">\(z \in \mathbb N\)</span> or <span class="math inline">\(z\)</span> be a latent sequence of arbritrary length).</p>
<p>In this tutorial, we specify a joint distribution over <span class="math inline">\(\mathcal Z \times \mathcal X\)</span> by specifying a joint probability density function (pdf):</p>
<p><span class="math display">\[\begin{align}
p_{ZX}(z, x|\theta) &amp;= p_Z(z|\theta)p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>Here <span class="math inline">\(\theta\)</span> denotes the parameters of the NNs that parameterise the pmf <span class="math inline">\(p_Z\)</span> and the pdf <span class="math inline">\(p_{X|Z=z}\)</span> (for any given <span class="math inline">\(z\)</span>).</p>
<p>In this tutorial, the prior is fixed, but in general it need not be. We do not have additional predictors to condition on, but in some application domains you may have (e.g., in imagine captioning, we may be interested in a joint for a caption <span class="math inline">\(y\)</span> and a latent code <span class="math inline">\(z\)</span> given an image <span class="math inline">\(x\)</span>; in image generation, we may be interested in a joint distribution for an image <span class="math inline">\(x\)</span> and a latent code <span class="math inline">\(z\)</span> given a caption <span class="math inline">\(y\)</span>).</p>
<section id="prior-networks" class="level3">
<h3 class="anchored" data-anchor-id="prior-networks">2.1 Prior networks</h3>
<p>We begin by specifying the component that parameterises the prior <span class="math inline">\(p_Z\)</span>.</p>
<p>A prior network is an NN that parameterises a fixed prior distribution for the instances in a batch.</p>
<div id="cell-24" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PriorNet(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    An NN that parameterises a prior distribution.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    For this lab, our priors are fixed, so this NN's forward pass</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">     simply returns a fixed prior with a given batch_shape.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape: <span class="bu">tuple</span>):</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: this is the shape of a single outcome</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">            if you use a single integer k, we will turn it into (k,)            </span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(outcome_shape, <span class="bu">int</span>):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            outcome_shape <span class="op">=</span> (outcome_shape,)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outcome_shape <span class="op">=</span> outcome_shape</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns a td object for the batch.</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implement me!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s implement two priors.</p>
<p><strong>A product of (uniform) Bernoulli distributions</strong></p>
<p>Here the latent code is a <span class="math inline">\(K\)</span>-dimensinoal bit vector, you can think of each coordinate of the code as an attribute of the data point. We use a uniform prior per coordinate:</p>
<p><span class="math display">\[\begin{align}
p_Z(z) &amp;= \prod_{k=1}^K \mathrm{Bernoulli}(z_k|0.5)
\end{align}\]</span></p>
<p><strong>Uniform (one-hot) Categorical distribution</strong></p>
<p>Here the latent code is the a class from a discrete set <span class="math inline">\(\{1, \ldots, K\}\)</span>, we use a uniform prior over classes:</p>
<p><span class="math display">\[\begin{align}
p_Z(z) &amp;= \mathrm{Categorical}(z|K^{-1} \mathbf 1_K )
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf 1_K\)</span> is the <span class="math inline">\(K\)</span>-dimensional vector of ones.</p>
<p>In practice, we can use the “OneHotCategorical” distribution, which wraps Categorical samples around a call to <code>onehot(sample, K)</code>.</p>
<div id="cell-26" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BernoulliPriorNet(PriorNet):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""    </span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    For z a D-dimensional bit vector:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">        p(z) = prod_d Bernoulli(z[d]|0.5)</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the product of Bernoulli priors will have Bernoulli(0.5) factors</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"logits"</span>, torch.zeros(<span class="va">self</span>.outcome_shape, requires_grad<span class="op">=</span><span class="va">False</span>).detach())</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        shape <span class="op">=</span> batch_shape <span class="op">+</span> <span class="va">self</span>.outcome_shape</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we wrap around td.Independent to obtain a pmf over multivariate draws</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># without td.Independent, we would have multiple pmfs, rather than one</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pmf over a multivariate outcome space</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># td.Independent will interpret the rightmost dimensions as part of</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the shape of the outcome</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Bernoulli(logits<span class="op">=</span><span class="va">self</span>.logits.expand(shape)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CategoricalPriorNet(PriorNet):</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""    </span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">    For z the one-hot encoding of a category in a set of K categories:</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">        p(z) = OneHotCategorical(z|torch.ones(K) / K)</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape):</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"logits"</span>, torch.zeros(<span class="va">self</span>.outcome_shape, requires_grad<span class="op">=</span><span class="va">False</span>).detach())</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_shape):</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        shape <span class="op">=</span> batch_shape <span class="op">+</span> <span class="va">self</span>.outcome_shape</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># OneHotCategorical is a wrapper around Categorical,</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># after drawing a Categorical sample, td.OneHotCategorical</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encodes it using onehot(sample, support_size)</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Here we do not need td.Independent, because OneHotCategorical</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># is a distribution over a multivariate draw (the one-hot </span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encoding of a category), which is different from the product of </span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bernoulli prior</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.OneHotCategorical(logits<span class="op">=</span><span class="va">self</span>.logits.expand(shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_priors(batch_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    prior_net <span class="op">=</span> BernoulliPriorNet(<span class="dv">7</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Bernoulli"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>prior_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> prior_net(batch_shape<span class="op">=</span>(batch_size,))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    prior_net <span class="op">=</span> CategoricalPriorNet(<span class="dv">7</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Categorical"</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>prior_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> prior_net(batch_shape<span class="op">=</span>(batch_size,))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>test_priors()    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Bernoulli
 outcome_shape=(7,)
 distribution: Independent(Bernoulli(logits: torch.Size([3, 7])), 1)
 sample: tensor([[0., 0., 0., 1., 1., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0.],
        [1., 0., 0., 1., 1., 1., 1.]])
 shapes: sample=torch.Size([3, 7]) log_prob=torch.Size([3])

Categorical
 outcome_shape=(7,)
 distribution: OneHotCategorical()
 sample: tensor([[0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0.]])
 shapes: sample=torch.Size([3, 7]) log_prob=torch.Size([3])</code></pre>
</div>
</div>
</section>
<section id="conditional-probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="conditional-probability-distributions">2.2 Conditional probability distributions</h3>
<p>Next, we create code to parameterise conditional probability distributions (cpds), which we do by having an NN parameterise a choice of pmf/pdf. This will be useful in parameterising the <span class="math inline">\(p_{X|Z=z}\)</span> component of our latent variable models (and, later on, it will also be useful for variational inference, when we develop <span class="math inline">\(q_{Z|X=x}\)</span>).</p>
<p>Our general strategy is to map from a number of inputs (which the user will choose) to the parameters of a pmf/pdf support by <code>torch.distributions</code>.</p>
<div id="cell-29" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CPDNet(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Let L be a choice of distribution</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">        and x ~ L is an outcome with shape outcome_shape</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This is an NN whose forward method maps from a number of inputs to the </span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">        parameters of L's pmf/pdf and returns a torch.distributions</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">        object representing L's pmf/pdf.</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: this is the shape of a single outcome</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">            if you use a single integer k, we will turn it into (k,)            </span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(outcome_shape, <span class="bu">int</span>):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            outcome_shape <span class="op">=</span> (outcome_shape,)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outcome_shape <span class="op">=</span> outcome_shape</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Return a torch.distribution object predicted from `inputs`.</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: a tensor with shape batch_shape + (num_inputs,)</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Implemented me"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="observational-model" class="level4">
<h4 class="anchored" data-anchor-id="observational-model">2.2.1 Observational model</h4>
<p>The observational model prescribes the distribution of <span class="math inline">\(X|Z=z\)</span>.</p>
<p>If we assume our pixel intensities are binary, we can use a product of <span class="math inline">\(C\times W \times H\)</span> Bernoulli distributions, which we parameterise jointly using an NN:</p>
<p><span class="math display">\[\begin{align}
p_{X|Z}(x|z, \theta) &amp;= \prod_{c=1}^C\prod_{w=1}^W\prod_{h=1}^H \mathrm{Bernoulli}(x_{c,w,h} | f_{c,w,h}(z; \theta))
\end{align}\]</span></p>
<p>Here <span class="math inline">\(\mathbf f(z; \theta) \in (0,1)^{C}\times(0,1)^W \times (0,1)^H\)</span> is an NN architecture such as a feed-forward net or a stack of transposed convolution layers. In NN literature, such architectures are often called <em>decoders</em>.</p>
<p>If we assume our pixel intensities are real values in <span class="math inline">\([0, 1]\)</span> (0 and 1 included), we need to parameterise a pdf. A good choice of pdf is the <a href="https://arxiv.org/abs/1907.06845">ContinuousBernoulli distributions</a>, which is a single-parameter distribution (much like the Bernoulli) whose support is the set <span class="math inline">\([0, 1]\)</span>.</p>
<p>Let’s start by designing <span class="math inline">\(\mathbf f\)</span>.</p>
<p>A very basic design uses a FFNN:</p>
<div id="cell-32" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReshapeLast(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Helper layer to reshape the rightmost dimension of a tensor.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This can be used as a component of nn.Sequential.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, shape: <span class="bu">tuple</span>):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">        shape: desired rightmost shape</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._shape <span class="op">=</span> shape</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshapes the last dimension into self.shape</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span>.reshape(<span class="bu">input</span>.shape[:<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="va">self</span>._shape)        </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_ffnn_decoder(latent_size, num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, hidden_size<span class="op">=</span><span class="dv">512</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Map the latent code to a tensor with shape [num_channels, width, height] </span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">    using a FFNN with 2 hidden layers.</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">    latent_size: size of latent code</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">    num_channels: number of channels in the output</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co">    width: image shape</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co">    height: image shape</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co">    hidden_size: we first map from latent_size to hidden_size and </span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="co">        then use feed forward NNs to map it to [num_channels, width, height] </span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co">    p_drop: dropout rate before linear layers</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span>    </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> nn.Sequential(        </span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        nn.Linear(latent_size, hidden_size),</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        nn.Linear(hidden_size, hidden_size),</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        nn.Linear(hidden_size, num_channels <span class="op">*</span> width <span class="op">*</span> height),</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        ReshapeLast((num_channels, width, height)),        </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> decoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-33" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mapping from 10-dimensional latent code</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>build_ffnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.Size([5, 1, 64, 64])</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can also have a structured batch shape (e.g., [3, 5])</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>build_ffnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>torch.Size([3, 5, 1, 64, 64])</code></pre>
</div>
</div>
<p>The downside is that the output layer is rather large.</p>
<p>An architecture with inductive biases that are more appropriate for our data type is a CNN, in particular, a transposed CNN. Here we design one such decoder:</p>
<div id="cell-36" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySequential(nn.Sequential):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This is a version of nn.Sequential that works with structured batches</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">     (i.e., batches that have multiple dimensions)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    even when some of the nn layers in it does not.</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    The idea is to just wrap nn.Sequential around two calls to reshape</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">     which remove and restore the batch dimensions.</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, event_dims<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._event_dims <span class="op">=</span> event_dims</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># memorise batch shape</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> <span class="bu">input</span>.shape[:<span class="op">-</span><span class="va">self</span>._event_dims]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># memorise latent shape</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        event_shape <span class="op">=</span> <span class="bu">input</span>.shape[<span class="op">-</span><span class="va">self</span>._event_dims:]</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten batch shape and obtain outputs</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="bu">super</span>().forward(<span class="bu">input</span>.reshape( (<span class="op">-</span><span class="dv">1</span>,) <span class="op">+</span> event_shape))</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># restore batch shape</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output.reshape(batch_shape <span class="op">+</span> output.shape[<span class="dv">1</span>:])</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_cnn_decoder(latent_size, num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, hidden_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Map the latent code to a tensor with shape [num_channels, width, height].</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="co">    latent_size: size of latent code</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="co">    num_channels: number of channels in the output</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co">    width: must be 64 (for now)</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="co">    height: must be 64 (for now)</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="co">    hidden_size: we first map from latent_size to hidden_size and </span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="co">        then use transposed 2d convolutions to [num_channels, width, height] </span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="co">    p_drop: dropout rate before linear layers</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The width is hardcoded"</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> height <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The height is hardcoded"</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: change the architecture so width and height are not hardcoded</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> MySequential(        </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>        nn.Linear(latent_size, hidden_size),</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        ReshapeLast((hidden_size, <span class="dv">1</span>, <span class="dv">1</span>)),</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(hidden_size, <span class="dv">128</span>, <span class="dv">5</span>, <span class="dv">2</span>),</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">64</span>, <span class="dv">5</span>, <span class="dv">2</span>),</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">6</span>, <span class="dv">2</span>),</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(<span class="dv">32</span>, num_channels, <span class="dv">6</span>, <span class="dv">2</span>), </span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        event_dims<span class="op">=</span><span class="dv">1</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> decoder        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five 10-dimensional latent codes is transformed</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># into a batch of 5 images, each with shape [1,64,64]</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>build_cnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([5, 1, 64, 64])</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that because we use MySequential, </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we can have a batch of [3, 5] assignments</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (this is useful, for example, when we have multiple draws of the latent </span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># variable for each of the data points in the batch)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>build_cnn_decoder(latent_size<span class="op">=</span><span class="dv">10</span>, num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>torch.Size([3, 5, 1, 64, 64])</code></pre>
</div>
</div>
<p>Now we are in position to design a CPDNet for our image model, it simply combines a choice of decoder with a choice of distribution:</p>
<div id="cell-40" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinarizedImageModel(CPDNet):    </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels, width, height, latent_size, decoder_type<span class="op">=</span>build_ffnn_decoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>((num_channels, width, height))        </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span>  decoder_type(</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size, </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the cpd X|Z=z</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">        z: batch_shape + (latent_dim,)</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (num_channels, width, height)</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.decoder(z)        </span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Bernoulli(logits<span class="op">=</span>h), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape))</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ContinuousImageModel(CPDNet):    </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels, width, height, latent_size, decoder_type<span class="op">=</span>build_ffnn_decoder, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>((num_channels, width, height))        </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span>  decoder_type(</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size, </span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width,</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>        ) </span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the cpd X|Z=z</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a><span class="co">        z: batch_shape + (latent_dim,)</span></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (num_channels, width, height)</span></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.decoder(z)        </span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.ContinuousBernoulli(logits<span class="op">=</span>h), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape))        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-41" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>obs_model <span class="op">=</span> BinarizedImageModel(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    latent_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five zs is mapped to 5 distributions over [1,64,64]-dimensional </span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># binary tensors</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model(torch.zeros([<span class="dv">5</span>, <span class="dv">10</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BinarizedImageModel(
  (decoder): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=10, out_features=512, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=512, out_features=512, bias=True)
    (5): ReLU()
    (6): Dropout(p=0.1, inplace=False)
    (7): Linear(in_features=512, out_features=4096, bias=True)
    (8): ReshapeLast()
  )
)
Independent(Bernoulli(logits: torch.Size([5, 1, 64, 64])), 3)</code></pre>
</div>
</div>
<p>We can also use a different decoder</p>
<div id="cell-43" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>obs_model <span class="op">=</span> BinarizedImageModel(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    p_drop<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five zs is mapped to 5 distributions over [1,64,64]-dimensional </span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co"># binary tensors</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(obs_model(torch.zeros([<span class="dv">5</span>, <span class="dv">10</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BinarizedImageModel(
  (decoder): MySequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=10, out_features=1024, bias=True)
    (2): ReshapeLast()
    (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
    (4): ReLU()
    (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
    (6): ReLU()
    (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
    (8): ReLU()
    (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
  )
)
Independent(Bernoulli(logits: torch.Size([5, 1, 64, 64])), 3)</code></pre>
</div>
</div>
</section>
</section>
<section id="joint-distribution" class="level3">
<h3 class="anchored" data-anchor-id="joint-distribution">2.3 Joint distribution</h3>
<p>We can now combine a prior and an observational model into a joint distribution. A joint distribution supports a few important operations such as marginal and posterior pdf assessments, as well as sampling from the joint distribution. Marginal and posterior assessments require computations that may or may not be tractable, see below.</p>
<p>From a joint pdf, we can compute the marginal density of <span class="math inline">\(x\)</span> via</p>
<p><span class="math display">\[\begin{align}
p_X(x|\theta) &amp;= \sum_{z \in \mathcal Z} p_{ZX}(z,x|\theta) \\
&amp;=\sum_{z \in \mathcal Z}p_Z(z|\theta)p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>Under the assumption that <span class="math inline">\(\mathcal Z\)</span> is countably finite, this can be done via enumeration in time proportional to <span class="math inline">\(\mathcal O(|\mathcal Z|)\)</span>. This marginalisation is therefore intractable for countably infinite <span class="math inline">\(\mathcal Z\)</span> or for <span class="math inline">\(\mathcal Z\)</span> that is combinatorially large (e.g., the space of <span class="math inline">\(K\)</span>-dimensinoal bit vectors).</p>
<p>Assuming enumeration is tractable, we can also assess the posterior mass of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span> via</p>
<p><span class="math display">\[\begin{align}
p_{Z|X}(z|x, \theta) &amp;= \frac{p_Z(z|\theta)p_{X|Z}(x|z, \theta)}{p_X(x|\theta)}
\end{align}\]</span></p>
<p>When marginalisation is intractable, we can obtain a naive lowerbound by direct application of Jensen’s inequality: <span class="math display">\[\begin{align}
\log p_X(x|\theta) &amp;= \log \sum_{z \in \mathcal Z} p_Z(z|\theta)p_{X|Z}(x|z, \theta) \\
&amp;\overset{\text{JI}}{\ge} \sum_{z \in \mathcal Z} p_Z(z|\theta) \log p_{X|Z}(x|z, \theta) \\
&amp;\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \log p_{X|Z}(x|z_s, \theta)  \quad \text{where } z_s \sim p_Z
\end{align}\]</span></p>
<p>A better lowerbound could be obtained via importance sampling, but it would require training an approximating distribution (as we will do in variational inference).</p>
<p>Recall that, given a dataset <span class="math inline">\(\mathcal D\)</span>, the log-likelihood function <span class="math inline">\(\mathcal L(\theta|\mathcal D)= \sum_{x \in \mathcal D} \log p_X(x|\theta)\)</span> requires performing marginal density assessments. Whenever exact marginalisation is intractable, we are unaible to assess <span class="math inline">\(\mathcal L(\theta|\mathcal D)\)</span> and its gradient with respect to <span class="math inline">\(\theta\)</span>. If the prior is fixed, we can use the naive lowerbound to obtain a gradient estimate, but, again, our naive application of JI leads to a generally rather loose bound.</p>
<div id="cell-45" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> JointDistribution(nn.Module):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A wrapper to combine a prior net and a cpd net into a joint distribution.</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prior_net: PriorNet, cpd_net: CPDNet):</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_net: object to parameterise p_Z</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">        cpd_net: object to parameterise p_{X|Z=z}</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_net <span class="op">=</span> prior_net</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cpd_net <span class="op">=</span> cpd_net</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prior(<span class="va">self</span>, shape):</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.prior_net(shape)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obs_model(<span class="va">self</span>, z):</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, shape):</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Return z via prior_net(shape).sample()</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="co">            and x via cpd_net(z).sample()</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(shape)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> pz.sample()</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> px_z.sample()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, x</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, z, x):</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Assess the log density of the joint outcome.            </span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> z.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.prior_net.outcome_shape)]        </span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x)</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_marginal(<span class="va">self</span>, x, enumerate_fn):</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Return log marginal density of x.</span></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a><span class="co">        enumerate_fn: function that enumerates the support of the prior</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a><span class="co">            (this is needed for marginalisation p(x) = \int p(z, x) dz)</span></span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a><span class="co">            This only really makes sense if the support is a </span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a><span class="co">            (small) countably finite set. In such cases, you can use</span></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a><span class="co">                enumerate=lambda p: p.enumerate_support()</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a><span class="co">            which is supported, for example, by Categorical and OneHotCategorical.</span></span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a><span class="co">            If the support is discrete (eg, bit vectors) you can still dare to </span></span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a><span class="co">            enumerate it explicitly, but you will need to write cutomised code, </span></span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a><span class="co">            as torch.distributions will not offer that functionality for you.</span></span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a><span class="co">            If the support is uncountable, countably infinite, or just large </span></span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a><span class="co">            anyway, you need approximate tools (such as VI, importance sampling, etc)</span></span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]</span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> []</span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enumerate_fn(pz)</span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.logsumexp(log_joint, <span class="dv">0</span>)</span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> posterior(<span class="va">self</span>, x, enumerate_fn):</span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the posterior distribution Z|X=x.</span></span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a><span class="co">        As the code is discrete, we return a discrete distribution over </span></span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a><span class="co">        the complete space of all possible latent codes. This is done via</span></span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a><span class="co">        exhaustive enumeration provided by `enumerate_fn`.</span></span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]        </span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)        </span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enumerate_fn(pz)</span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (support_size,) + batch_shape</span></span>
<span id="cb35-85"><a href="#cb35-85" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb35-86"><a href="#cb35-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (support_size,) </span></span>
<span id="cb35-87"><a href="#cb35-87" aria-hidden="true" tabindex="-1"></a>        log_joint <span class="op">=</span> torch.swapaxes(log_joint, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb35-88"><a href="#cb35-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Categorical(logits<span class="op">=</span>log_joint)</span>
<span id="cb35-89"><a href="#cb35-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-90"><a href="#cb35-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> naive_lowerbound(<span class="va">self</span>, x, num_samples: <span class="bu">int</span>):</span>
<span id="cb35-91"><a href="#cb35-91" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb35-92"><a href="#cb35-92" aria-hidden="true" tabindex="-1"></a><span class="co">        Return an MC lowerbound on log marginal density of x:</span></span>
<span id="cb35-93"><a href="#cb35-93" aria-hidden="true" tabindex="-1"></a><span class="co">            log p(x) &gt;= 1/S \sum_s log p(x|z[s])</span></span>
<span id="cb35-94"><a href="#cb35-94" aria-hidden="true" tabindex="-1"></a><span class="co">                with z[s] ~ p_Z</span></span>
<span id="cb35-95"><a href="#cb35-95" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-96"><a href="#cb35-96" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span><span class="bu">len</span>(<span class="va">self</span>.cpd_net.outcome_shape)]</span>
<span id="cb35-97"><a href="#cb35-97" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.prior_net(batch_shape)</span>
<span id="cb35-98"><a href="#cb35-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (num_samples,) + batch_shape + prior_outcome_shape</span></span>
<span id="cb35-99"><a href="#cb35-99" aria-hidden="true" tabindex="-1"></a>        log_probs <span class="op">=</span> []</span>
<span id="cb35-100"><a href="#cb35-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I'm using a for loop, but note that with enough GPU memory </span></span>
<span id="cb35-101"><a href="#cb35-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># one could parallelise this step</span></span>
<span id="cb35-102"><a href="#cb35-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> z <span class="kw">in</span> pz.sample((num_samples,)): </span>
<span id="cb35-103"><a href="#cb35-103" aria-hidden="true" tabindex="-1"></a>            px_z <span class="op">=</span> <span class="va">self</span>.cpd_net(z)</span>
<span id="cb35-104"><a href="#cb35-104" aria-hidden="true" tabindex="-1"></a>            log_probs.append(px_z.log_prob(x))</span>
<span id="cb35-105"><a href="#cb35-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (num_samples,) + batch_shape</span></span>
<span id="cb35-106"><a href="#cb35-106" aria-hidden="true" tabindex="-1"></a>        log_probs <span class="op">=</span> torch.stack(log_probs)</span>
<span id="cb35-107"><a href="#cb35-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb35-108"><a href="#cb35-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.mean(log_probs, <span class="dv">0</span>)        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_joint_dist(latent_size<span class="op">=</span><span class="dv">10</span>, data_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>), batch_size<span class="op">=</span><span class="dv">2</span>, hidden_size<span class="op">=</span><span class="dv">32</span>):    </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> JointDistribution(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        prior_net<span class="op">=</span>CategoricalPriorNet(latent_size),</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        cpd_net<span class="op">=</span>BinarizedImageModel(</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>data_shape[<span class="dv">0</span>],</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>data_shape[<span class="dv">1</span>],</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>data_shape[<span class="dv">2</span>],</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size,</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Model for binarized data"</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    z, x <span class="op">=</span> p.sample((batch_size,))</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled z"</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(z)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled x"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    enumerate_fn <span class="op">=</span> <span class="kw">lambda</span> p: p.enumerate_support()</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"marginal"</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p.log_marginal(x, enumerate_fn))</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"MC lowerbound"</span>)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 1:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 2:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"posterior distribution"</span>)</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p.posterior(x, enumerate_fn).probs)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> JointDistribution(</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        prior_net<span class="op">=</span>CategoricalPriorNet(latent_size),</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        cpd_net<span class="op">=</span>ContinuousImageModel(</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>data_shape[<span class="dv">0</span>],</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>data_shape[<span class="dv">1</span>],</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>data_shape[<span class="dv">2</span>],</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span>latent_size,</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_cnn_decoder</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Model for continuous data"</span>)</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p)</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    z, x <span class="op">=</span> p.sample((batch_size,))</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled z"</span>)</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(z)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"sampled x"</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>    enumerate_fn <span class="op">=</span> <span class="kw">lambda</span> p: p.enumerate_support()</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"marginal"</span>)</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p.log_marginal(x, enumerate_fn))</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"MC lowerbound"</span>)</span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 1:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" 2:"</span>, p.naive_lowerbound(x, <span class="dv">10</span>))</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"posterior distribution"</span>)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p.posterior(x, enumerate_fn).probs)</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a>test_joint_dist(<span class="dv">10</span>)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model for binarized data
JointDistribution(
  (prior_net): CategoricalPriorNet()
  (cpd_net): BinarizedImageModel(
    (decoder): MySequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=1024, bias=True)
      (2): ReshapeLast()
      (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
      (4): ReLU()
      (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
      (6): ReLU()
      (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
      (8): ReLU()
      (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
    )
  )
)
sampled z
tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])
sampled x
tensor([[[[0., 0., 1.,  ..., 0., 1., 1.],
          [1., 0., 1.,  ..., 1., 0., 0.],
          [0., 1., 1.,  ..., 0., 1., 0.],
          ...,
          [1., 1., 0.,  ..., 1., 1., 0.],
          [0., 0., 1.,  ..., 0., 0., 1.],
          [1., 0., 1.,  ..., 1., 0., 0.]]],


        [[[0., 0., 0.,  ..., 1., 1., 1.],
          [1., 0., 1.,  ..., 0., 0., 1.],
          [0., 1., 1.,  ..., 1., 1., 0.],
          ...,
          [0., 0., 0.,  ..., 1., 1., 1.],
          [1., 0., 0.,  ..., 0., 0., 0.],
          [1., 0., 1.,  ..., 1., 0., 0.]]]])
marginal
tensor([-2833.0503, -2840.0588], grad_fn=&lt;LogsumexpBackward0&gt;)
MC lowerbound
 1: tensor([-2833.0542, -2840.0547], grad_fn=&lt;MeanBackward1&gt;)
 2: tensor([-2833.0972, -2840.0427], grad_fn=&lt;MeanBackward1&gt;)
posterior distribution
tensor([[0.0848, 0.1013, 0.1000, 0.1056, 0.0816, 0.0984, 0.1063, 0.1045, 0.1015,
         0.1161],
        [0.0965, 0.1006, 0.0892, 0.0886, 0.1072, 0.1042, 0.1156, 0.1104, 0.1010,
         0.0867]], grad_fn=&lt;SoftmaxBackward0&gt;)



Model for continuous data
JointDistribution(
  (prior_net): CategoricalPriorNet()
  (cpd_net): ContinuousImageModel(
    (decoder): MySequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=1024, bias=True)
      (2): ReshapeLast()
      (3): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))
      (4): ReLU()
      (5): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))
      (6): ReLU()
      (7): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))
      (8): ReLU()
      (9): ConvTranspose2d(32, 1, kernel_size=(6, 6), stride=(2, 2))
    )
  )
)
sampled z
tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])
sampled x
tensor([[[[0.0177, 0.4419, 0.8944,  ..., 0.1530, 0.0298, 0.2958],
          [0.3006, 0.2818, 0.9756,  ..., 0.3906, 0.3666, 0.5736],
          [0.5353, 0.1523, 0.1889,  ..., 0.1321, 0.3172, 0.1197],
          ...,
          [0.5384, 0.4939, 0.3950,  ..., 0.1797, 0.4078, 0.8856],
          [0.8161, 0.1738, 0.2747,  ..., 0.3477, 0.0262, 0.0069],
          [0.5498, 0.7573, 0.8136,  ..., 0.1438, 0.7143, 0.1651]]],


        [[[0.3061, 0.7684, 0.9373,  ..., 0.3133, 0.9556, 0.9470],
          [0.6662, 0.7195, 0.2499,  ..., 0.1698, 0.7220, 0.4024],
          [0.9018, 0.6794, 0.7495,  ..., 0.5036, 0.7543, 0.2213],
          ...,
          [0.9679, 0.9352, 0.9314,  ..., 0.1054, 0.8021, 0.6079],
          [0.4803, 0.2586, 0.0312,  ..., 0.7962, 0.8062, 0.9996],
          [0.2138, 0.5723, 0.5005,  ..., 0.7090, 0.4256, 0.8469]]]])
marginal
tensor([2.4405, 1.3252], grad_fn=&lt;LogsumexpBackward0&gt;)
MC lowerbound
 1: tensor([2.4240, 1.3365], grad_fn=&lt;MeanBackward1&gt;)
 2: tensor([2.4259, 1.3594], grad_fn=&lt;MeanBackward1&gt;)
posterior distribution
tensor([[0.1080, 0.1020, 0.0954, 0.1017, 0.0990, 0.1036, 0.0978, 0.0957, 0.0963,
         0.1006],
        [0.0995, 0.1146, 0.0970, 0.1074, 0.1070, 0.0973, 0.0960, 0.0972, 0.0957,
         0.0882]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="learning" class="level2">
<h2 class="anchored" data-anchor-id="learning">3. Learning</h2>
<p>We estimate <span class="math inline">\(\theta\)</span> using stochastic gradient-based maximum likelihood estimation. For a tractable model, we can assess the log-likelihood function</p>
<p><span class="math display">\[\begin{align}
\mathcal L(\theta|\mathcal D) &amp;= \sum_{x \in \mathcal D} \log p_X(x|\theta)
\end{align}\]</span></p>
<p>and estimate <span class="math inline">\(\nabla_{\theta} \mathcal L(\theta|\mathcal D)\)</span> using random mini-batches:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\theta}\mathcal L(\theta|\mathcal D) &amp;\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \nabla_{\theta}\log p_X(x^{(s)}|\theta) \\
&amp;\text{where }x^{(s)} \sim \mathcal D
\end{align}\]</span></p>
<section id="tractable-lvms" class="level3">
<h3 class="anchored" data-anchor-id="tractable-lvms">3.1 Tractable LVMs</h3>
<p>A mixture model with <span class="math inline">\(K\)</span> components is in principle a tractable LVM:</p>
<p><span class="math display">\[\begin{align}
p_X(x|\theta) &amp;= \sum_{z=1}^K\frac{1}{K} p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>For <span class="math inline">\(p_{X|Z=z}\)</span> we use the image model above, which conditions on the one-hot encoding of <span class="math inline">\(z\)</span>.</p>
<div id="cell-49" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(JointDistribution(</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    prior_net<span class="op">=</span>CategoricalPriorNet(<span class="dv">10</span>),</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    cpd_net<span class="op">=</span>BinarizedImageModel(</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        decoder_type<span class="op">=</span>build_ffnn_decoder</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>JointDistribution(
  (prior_net): CategoricalPriorNet()
  (cpd_net): BinarizedImageModel(
    (decoder): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=10, out_features=512, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.0, inplace=False)
      (4): Linear(in_features=512, out_features=512, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.0, inplace=False)
      (7): Linear(in_features=512, out_features=4096, bias=True)
      (8): ReshapeLast()
    )
  )
)</code></pre>
</div>
</div>
</section>
<section id="training-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="training-algorithm">3.2 Training algorithm</h3>
<p>Here we have some helper code to assess and train an LVM using its exact log-likelihood function.</p>
<div id="cell-51" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assess_exact(model: JointDistribution, enumerate_fn, dl, device):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrapper for estimating the model's log-likelihood.</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">    As data samples, we use all data points in a data loader.</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co">    model: a joint distribution for which Z can be exactly marginalised</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co">    enumerate_fn: algorithm to enumerate the support of Z for a batch</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co">        this will be used to assess `model.log_prob(batch, enumerate_fn)`</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co">    dl: torch data loader</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co">    device: torch device</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    data_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> dl:</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">+</span> model.log_marginal(batch_x.to(device), enumerate_fn).<span class="bu">sum</span>(<span class="dv">0</span>) </span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>            data_size <span class="op">+=</span> batch_x.shape[<span class="dv">0</span>]</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L <span class="op">/</span> data_size</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_exact(model: JointDistribution, enumerate_fn, optimiser, </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    training_data, dev_data,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">10</span>,    </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    report_metrics<span class="op">=</span>[<span class="st">'L'</span>],</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co">    model: a joint distribution where Z can be exactly marginalised</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co">    enumerate_fn: function to enumerate the support of Z</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co">        (used in combintation with model.log_prob)</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co">    optimiser: pytorch optimiser</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co">    training_corpus: a torchvision corpus for training</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co">    dev_corpus: a torchvision corpus for validation</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="co">    batch_size: use more if you have more memory</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co">    num_epochs: use more for improved convergence</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co">    check_every: use less to check performance on dev set more often</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co">    device: where we run the experiment</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a log of quantities computed during training (for plotting)</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    batcher <span class="op">=</span> DataLoader(training_data, batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    dev_batcher <span class="op">=</span> DataLoader(dev_data, batch_size, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    total_steps <span class="op">=</span> num_epochs <span class="op">*</span> <span class="bu">len</span>(batcher)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    log[<span class="st">'dev.L'</span>].append((step, assess_exact(model, enumerate_fn, dev_batcher, device<span class="op">=</span>device).item()))</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tqdm(<span class="bu">range</span>(total_steps)) <span class="im">as</span> bar:</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>                model.train()</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>                optimiser.zero_grad()</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> <span class="op">-</span> model.log_marginal(</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>                    batch_x.to(device), </span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>                    enumerate_fn</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>                ).mean(<span class="dv">0</span>)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>                log[<span class="st">'loss'</span>].append((step, loss.item()))</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>                nn.utils.clip_grad_norm_(</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>                    model.parameters(), </span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>                    grad_clip</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>                )    </span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>                optimiser.step()</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>                bar_dict <span class="op">=</span> OrderedDict()</span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>                bar_dict[<span class="st">'loss'</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a>                bar_dict[<span class="ss">f"dev.L"</span>] <span class="op">=</span>  <span class="st">"</span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(log[<span class="ss">f"dev.L"</span>][<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>])</span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a>                bar.set_postfix(bar_dict)</span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>                bar.update()</span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> step <span class="op">%</span> check_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>                    model.<span class="bu">eval</span>()</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>                    log[<span class="st">'dev.L'</span>].append((step, assess_exact(model, enumerate_fn, dev_batcher, device<span class="op">=</span>device).item()))</span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a>                step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a>    log[<span class="st">'dev.L'</span>].append((step, assess_exact(model, enumerate_fn, dev_batcher, device<span class="op">=</span>device).item()))</span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-71"><a href="#cb41-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s some helper code to inspect samples from a mixture model</p>
<div id="cell-54" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inspect_mixture_model(model: JointDistribution, support_size):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display some samples from the prior</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    _, x_ <span class="op">=</span> model.sample((support_size, <span class="dv">5</span>))</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    x_ <span class="op">=</span> x_.cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    plt.imshow(make_grid(x_, nrow<span class="op">=</span>support_size).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Prior samples"</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display samples from each of the 10 components</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [support_size, latent_dim]</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    support <span class="op">=</span> model.prior(<span class="bu">tuple</span>()).enumerate_support()        </span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [support_size, 1, 64, 64]</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    x_ <span class="op">=</span> model.obs_model(support).sample((<span class="dv">5</span>,)).cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    plt.imshow(make_grid(x_, nrow<span class="op">=</span>support_size).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Component samples"</span>)</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="experiment" class="level4">
<h4 class="anchored" data-anchor-id="experiment">3.2.1 Experiment</h4>
<div id="cell-56" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>seed_all()</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>mixture_model <span class="op">=</span> JointDistribution(</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    prior_net<span class="op">=</span>CategoricalPriorNet(<span class="dv">10</span>),</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    cpd_net<span class="op">=</span>BinarizedImageModel(  <span class="co"># could also be continuous (but then change the preprocessing in the beginning)</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        decoder_type<span class="op">=</span>build_ffnn_decoder, <span class="co"># could also be CNN (but consider less latent classes, as the transposed CNN decoder is a bit too slow for exact enumeration)</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        p_drop<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mixture_model)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>mm_optim <span class="op">=</span> opt.Adam(mixture_model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>JointDistribution(
  (prior_net): CategoricalPriorNet()
  (cpd_net): BinarizedImageModel(
    (decoder): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=10, out_features=512, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=512, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.1, inplace=False)
      (7): Linear(in_features=512, out_features=4096, bias=True)
      (8): ReshapeLast()
    )
  )
)</code></pre>
</div>
</div>
<div id="cell-57" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_exact(</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>mixture_model,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    optimiser<span class="op">=</span>mm_optim,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    enumerate_fn<span class="op">=</span><span class="kw">lambda</span> p: p.enumerate_support(),</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>train_ds, </span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>val_ds,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">256</span>, </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span><span class="dv">1</span>,  <span class="co"># use more for a better model</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    check_every<span class="op">=</span><span class="dv">10</span>,    </span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"47c2ee42660e4b7cb681102f62886046","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="cell-58" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.array(log[<span class="st">'loss'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'loss'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">"training loss"</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">"dev L"</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">"steps"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-34-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-59" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>inspect_mixture_model(mixture_model, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-35-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-35-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>A mixture model offers very limited capacity to the observational model, it can at best partition the data space into <span class="math inline">\(K\)</span> groups.</p>
<p>A factor model instead offers up <span class="math inline">\(2^K\)</span> codes to the observational model, next we will design a factor model.</p>
</section>
</section>
<section id="intractable-lvms" class="level3">
<h3 class="anchored" data-anchor-id="intractable-lvms">3.3 Intractable LVMs</h3>
<p>When marginalisation is intractable (e.g., <span class="math inline">\(\mathcal Z = \{0, 1\}^K\)</span> for moderate <span class="math inline">\(K\)</span>) or just plain incovenient (e.g., <span class="math inline">\(\mathcal Z = \{1, \ldots, K\}\)</span> for large <span class="math inline">\(K\)</span>, e.g., <span class="math inline">\(K &gt; 10\)</span>), we resort to variational inference introducing a parametric approximation <span class="math inline">\(q_{Z|X=x}\)</span> to the model’s true posterior distribution <span class="math inline">\(p_{Z|X=x}\)</span> and estimate both the approximation and the joint distribution by maximising the evidence lowerbound (ELBO), show below for a single observation <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{align}
\mathcal E(\lambda, \theta| \mathcal D) &amp;= \mathbb E_{x \sim \mathcal D}\left[ \mathbb E\left[\log \frac{ p_{ZX}(z, x|\theta)}{q_{Z|X}(z|x, \lambda)}\right] \right] \\
&amp;= \underbrace{\mathbb E_{x \sim \mathcal D}\left[ \mathbb E[ \log p_{ZX}(x|z,\theta)] \right]}_{-D} - \underbrace{\mathbb E_{x \sim \mathcal D}\left[ \mathrm{KL}(q_{Z|X=x}||p_Z) \right]}_{R}
\end{align}\]</span></p>
<p>where the inner expectation is taken with respect to <span class="math inline">\(q_{Z|X}(z|x, \lambda)\)</span>.</p>
<p>When computed in expectation under the data distribution, the two components of the ELBO, namely, the expected log-likelihood <span class="math inline">\(\mathbb E[ \log p_{ZX}(x|z,\theta)]\)</span> and the “KL term” <span class="math inline">\(\mathrm{KL}(q_{Z|X=x}||p_Z)\)</span>, are related to two information-theoretic quantities known as <em>distortion</em> and <em>rate</em>.</p>
<p>We choose our approximation to be such that: its support is embeded in the support of the prior, it is simple enough to sample from, and it is simple enough to assess the mass of a sample. If possible, we choose it such that other quantities are also tractable (e.g., entropy, relative entropy).</p>
<p>For a multivariate <span class="math inline">\(z\)</span>, we normally choose a factorised family, for example, if <span class="math inline">\(z\)</span> is a <span class="math inline">\(D\)</span>-dimensional bit vector:</p>
<p><span class="math display">\[\begin{align}
q_{Z|X}(z|x, \lambda) &amp;= \prod_{k=1}^K \mathrm{Bernoulli}(z_k|g_k(x;\lambda))
\end{align}\]</span></p>
<p>with <span class="math inline">\(\mathbf g(x;\lambda) \in (0, 1)^K\)</span>. This is called a <em>mean field assumption</em>.</p>
<p>For a Categorical <span class="math inline">\(z\)</span>, the marginalisation is tractable in principle (it takes time linear in the number <span class="math inline">\(K\)</span> of possible assignments), but for certain decoders, even <span class="math inline">\(K\)</span> forward passes might be too much, so we use an independently parameterised Categorical distribution:</p>
<p><span class="math display">\[\begin{align}
q_{Z|X}(z|x, \lambda) &amp;= \mathrm{Categorical}(z|\mathbf g(x;\lambda))
\end{align}\]</span></p>
<p>with <span class="math inline">\(\mathbf g(x;\lambda) \in \Delta_{K-1}\)</span>.</p>
</section>
<section id="inference-model" class="level3">
<h3 class="anchored" data-anchor-id="inference-model">3.4 Inference model</h3>
<p>The inference model is a conditional model of the latent variable, for which we design CPD nets.</p>
<p>Before we go on, it is useful to design an “encoder” a function that maps an image <span class="math inline">\(x \in \mathcal X\)</span> to a fixed-size vector that we can use as a compact representation of <span class="math inline">\(x\)</span>. Next, we design one such encoder employing FFNNs and another employing CNNs.</p>
<div id="cell-63" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FlattenImage(nn.Module):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span>.reshape(<span class="bu">input</span>.shape[:<span class="op">-</span><span class="dv">3</span>] <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_ffnn_encoder(num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, output_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        FlattenImage(),</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        nn.Linear(num_channels <span class="op">*</span> width <span class="op">*</span> height, output_size<span class="op">//</span><span class="dv">2</span>),</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        nn.Linear(output_size<span class="op">//</span><span class="dv">2</span>, output_size<span class="op">//</span><span class="dv">2</span>),</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(p_drop),</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>        nn.Linear(output_size<span class="op">//</span><span class="dv">2</span>, output_size),        </span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoder</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_cnn_encoder(num_channels, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, output_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>):        </span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The width is hardcoded"</span>)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> height <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The height is hardcoded"</span>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> output_size <span class="op">!=</span> <span class="dv">1024</span>:</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"The output_size is hardcoded"</span>)</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: change the architecture so width, height and output_size are not hardcoded</span></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> MySequential(</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(num_channels, <span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>        nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>        FlattenImage(),</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a>        event_dims<span class="op">=</span><span class="dv">3</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five [1, 64, 64]-dimensional images is encoded into</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co"># five 1024-dimensional vectors</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>build_ffnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>torch.Size([5, 1024])</code></pre>
</div>
</div>
<div id="cell-65" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and, again, we can have structured batches</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (here trying with (3,5))</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>build_ffnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>torch.Size([3, 5, 1024])</code></pre>
</div>
</div>
<div id="cell-66" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of five [1, 64, 64]-dimensional images is encoded into</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># five 1024-dimensional vectors</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>build_cnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>torch.Size([5, 1024])</code></pre>
</div>
</div>
<div id="cell-67" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and, again, since we use MySequential we can have structured batches</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (here trying with (3,5))</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>build_cnn_encoder(num_channels<span class="op">=</span><span class="dv">1</span>)(torch.zeros((<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([3, 5, 1024])</code></pre>
</div>
</div>
<p>We can now design some CPD nets, assuming they map from an encoding of an image to a pmf over <span class="math inline">\(\mathcal Z\)</span>.</p>
<p><strong>Product of Bernoullis</strong></p>
<p>This can be used to parameterise a cpd over binary vectors of fixed dimensionality.</p>
<p><strong>OneHotCategorical</strong></p>
<p>This can be used to parameterise a cpd over the one-hot encoding of categories from a finite set.</p>
<div id="cell-69" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BernoulliCPDNet(CPDNet):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Output distribution is a product of Bernoulli distributions</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape, num_inputs: <span class="bu">int</span>, hidden_size: <span class="bu">int</span><span class="op">=</span><span class="va">None</span>, p_drop: <span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: shape of the outcome (int or tuple)</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="co">            if int, we turn it into a singleton tuple</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co">        num_inputs: rightmost dimensionality of the inputs to forward</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: size of hidden layers for the CPDNet (use None to skip)        </span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="co">        p_drop: configure dropout before every Linear layer</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        num_outputs <span class="op">=</span> np.prod(<span class="va">self</span>.outcome_shape)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hidden_size:    </span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>                nn.Dropout(p_drop),</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>                nn.Linear(num_inputs, hidden_size),</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>                nn.ReLU()</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Identity()</span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> num_inputs</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits <span class="op">=</span> nn.Sequential(            </span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>            ReshapeLast(<span class="va">self</span>.outcome_shape)</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):        </span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(inputs)</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.Independent(td.Bernoulli(logits<span class="op">=</span><span class="va">self</span>.logits(h)), <span class="bu">len</span>(<span class="va">self</span>.outcome_shape)) </span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CategoricalCPDNet(CPDNet):</span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Output distribution is a product of Bernoulli distributions</span></span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, outcome_shape, num_inputs: <span class="bu">int</span>, hidden_size: <span class="bu">int</span><span class="op">=</span><span class="va">None</span>, p_drop: <span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a><span class="co">        outcome_shape: shape of the outcome (int or tuple)</span></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a><span class="co">            if int, we turn it into a singleton tuple</span></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a><span class="co">        num_inputs: rightmost dimensionality of the inputs to forward</span></span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden_size: size of hidden layers for the CPDNet (use None to skip)        </span></span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a><span class="co">        p_drop: configure dropout before every Linear layer</span></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(outcome_shape)</span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>        num_outputs <span class="op">=</span> np.prod(<span class="va">self</span>.outcome_shape)</span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hidden_size:    </span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>                nn.Dropout(p_drop),</span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a>                nn.Linear(num_inputs, hidden_size),</span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a>                nn.ReLU()</span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder <span class="op">=</span> nn.Identity()</span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> num_inputs</span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logits <span class="op">=</span> nn.Sequential(            </span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb57-66"><a href="#cb57-66" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, num_outputs),</span>
<span id="cb57-67"><a href="#cb57-67" aria-hidden="true" tabindex="-1"></a>            ReshapeLast(<span class="va">self</span>.outcome_shape)</span>
<span id="cb57-68"><a href="#cb57-68" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb57-69"><a href="#cb57-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-70"><a href="#cb57-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):        </span>
<span id="cb57-71"><a href="#cb57-71" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(inputs)</span>
<span id="cb57-72"><a href="#cb57-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> td.OneHotCategorical(logits<span class="op">=</span><span class="va">self</span>.logits(h))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-70" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_cpds(outcome_shape, batch_size<span class="op">=</span><span class="dv">3</span>, input_dim<span class="op">=</span><span class="dv">5</span>, hidden_size<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    cpd_net <span class="op">=</span> BernoulliCPDNet(outcome_shape, num_inputs<span class="op">=</span>input_dim, hidden_size<span class="op">=</span>hidden_size) </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Bernoulli"</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(cpd_net)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" outcome_shape=</span><span class="sc">{</span>cpd_net<span class="sc">.</span>outcome_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> torch.from_numpy(np.random.uniform(size<span class="op">=</span>(batch_size, input_dim))).<span class="bu">float</span>()</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shape of inputs: </span><span class="sc">{</span>inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> cpd_net(inputs)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" distribution: </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> p.sample()</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" sample: </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" shapes: sample=</span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> log_prob=</span><span class="sc">{</span>p<span class="sc">.</span>log_prob(z)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Try a few </span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>test_cpds(<span class="dv">12</span>)</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>test_cpds(<span class="dv">12</span>, hidden_size<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>test_cpds((<span class="dv">4</span>, <span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Bernoulli
BernoulliCPDNet(
  (encoder): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=5, out_features=2, bias=True)
    (2): ReLU()
  )
  (logits): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=12, bias=True)
    (2): ReshapeLast()
  )
)
 outcome_shape=(12,)
 shape of inputs: torch.Size([3, 5])
 distribution: Independent(Bernoulli(logits: torch.Size([3, 12])), 1)
 sample: tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.],
        [1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.]])
 shapes: sample=torch.Size([3, 12]) log_prob=torch.Size([3])

Bernoulli
BernoulliCPDNet(
  (encoder): Identity()
  (logits): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=5, out_features=12, bias=True)
    (2): ReshapeLast()
  )
)
 outcome_shape=(12,)
 shape of inputs: torch.Size([3, 5])
 distribution: Independent(Bernoulli(logits: torch.Size([3, 12])), 1)
 sample: tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.],
        [0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.],
        [1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.]])
 shapes: sample=torch.Size([3, 12]) log_prob=torch.Size([3])

Bernoulli
BernoulliCPDNet(
  (encoder): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=5, out_features=2, bias=True)
    (2): ReLU()
  )
  (logits): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=2, out_features=20, bias=True)
    (2): ReshapeLast()
  )
)
 outcome_shape=(4, 5)
 shape of inputs: torch.Size([3, 5])
 distribution: Independent(Bernoulli(logits: torch.Size([3, 4, 5])), 2)
 sample: tensor([[[1., 1., 0., 0., 1.],
         [0., 1., 1., 0., 0.],
         [0., 0., 0., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 0., 0., 0.],
         [0., 1., 1., 0., 1.],
         [0., 0., 1., 0., 1.],
         [1., 0., 0., 0., 1.]],

        [[1., 1., 1., 0., 1.],
         [0., 0., 0., 1., 0.],
         [1., 0., 1., 1., 1.],
         [1., 0., 0., 1., 0.]]])
 shapes: sample=torch.Size([3, 4, 5]) log_prob=torch.Size([3])</code></pre>
</div>
</div>
<p>Last, but certainly not least, we can combine our encoder and a choice of CPD net.</p>
<div id="cell-72" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InferenceModel(CPDNet):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, cpd_net_type, </span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        latent_size, num_channels<span class="op">=</span><span class="dv">1</span>, width<span class="op">=</span><span class="dv">64</span>, height<span class="op">=</span><span class="dv">64</span>, </span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        hidden_size<span class="op">=</span><span class="dv">1024</span>, p_drop<span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        encoder_type<span class="op">=</span>build_ffnn_encoder):        </span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(latent_size)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latent_size <span class="op">=</span> latent_size    </span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encodes an image to a hidden_size-dimensional vector    </span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder_type(</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>num_channels,</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>width, </span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>height,</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>hidden_size,</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span>p_drop</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># maps from a hidden_size-dimensional encoding</span></span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to a cpd for Z|X=x</span></span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cpd_net <span class="op">=</span> cpd_net_type(</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>             latent_size, </span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>             num_inputs<span class="op">=</span>hidden_size, </span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>             hidden_size<span class="op">=</span><span class="dv">2</span><span class="op">*</span>latent_size,</span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>             p_drop<span class="op">=</span>p_drop</span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x)        </span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cpd_net(h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-73" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>InferenceModel(BernoulliCPDNet, latent_size<span class="op">=</span><span class="dv">10</span>)(torch.zeros(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>Independent(Bernoulli(logits: torch.Size([5, 10])), 1)</code></pre>
</div>
</div>
<p>We now have everything in place to use variational inference.</p>
</section>
<section id="neural-variational-inference" class="level3">
<h3 class="anchored" data-anchor-id="neural-variational-inference">3.5 Neural Variational Inference</h3>
<p>We will train our generative model via variational inference, for which we need to train an inference model along with it. We will se the ELBO objective, and gradient estimator based on score function estimation.</p>
<p>It’s common to refer to any one such model as a variational auto-encoder (VAE), even though the original VAE was a very different generative model and employed a different gradient estimator.</p>
<p>Given a data point <span class="math inline">\(x\)</span>, we estimate the gradient of the ELBO with respect to <span class="math inline">\(\lambda\)</span> by MC estimating the following expression:</p>
<p><span class="math display">\[\begin{align}
\nabla_{\lambda} \mathcal E(\lambda, \theta|x) &amp;= \mathbb E\left[ r(z, x; \theta, \lambda) \nabla_{\lambda}\log \frac{p_{ZX}(z, x|\theta)}{q_{Z|X}(z|x, \lambda)} \right]
\end{align}\]</span></p>
<p>where the “reward” function is</p>
<p><span class="math display">\[\begin{align}
r(z, x; \theta, \lambda) &amp;= \log p_{X|Z}(x|z, \theta)
\end{align}\]</span></p>
<p>And, because this gradient estimator is rather noisy, it’s commong to transform the reward function by further employing control variates. The simplest control variates are functions of <span class="math inline">\(x\)</span> and possibly of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>, but not a function of the action <span class="math inline">\(z\)</span> with respect to which we evaluate the reward function. We will implement those as wrappers around the reward function. So, let’s start by agreeing on the API of our control variates.</p>
<div id="cell-76" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VarianceReduction(nn.Module):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co">    We will be using simple forms of control variates for variance reduction.</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co">    These are transformations of the reward that are independent of the sampled</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="co">    latent variable, but they can, in principle, depend on x, and on the </span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters of the generative and inference model.</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Some of these are trainable components, thus they also contribute to the loss.</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the transformed reward and a contribution to the loss.</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="co">        r: a batch of rewards</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a batch of observations</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="co">        q: policy</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a><span class="co">        r_fn: reward function</span></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r, torch.zeros_like(r)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can work on our general NVIL model. The following class implements the NVIL objective as well as a lot of helper code to manipulate the model components in interesting ways (e.g., sampling, sampling conditionally, estimating marginal density, etc.)</p>
<div id="cell-78" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NVIL(nn.Module):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A generative model p(z)p(x|z) and an approximation q(z|x) to that </span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co">     model's true posterior.</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co">    The approximation is estimated to maximise the ELBO, and so is the joint</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co">     distribution.     </span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, gen_model: JointDistribution, inf_model: InferenceModel, cv_model: VarianceReduction):</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co">        gen_model: p(z)p(x|z)</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="co">        inf_model: q(z|x) which approximates p(z|x)</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="co">        cv_model: optional transformations of the reward</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen_model <span class="op">=</span> gen_model        </span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf_model <span class="op">=</span> inf_model</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cv_model <span class="op">=</span> cv_model</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gen_params(<span class="va">self</span>):</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gen_model.parameters()</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inf_params(<span class="va">self</span>):</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.inf_model.parameters()</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cv_params(<span class="va">self</span>):</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cv_model.parameters()</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, batch_size, sample_size<span class="op">=</span><span class="va">None</span>, oversample<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a><span class="co">        A sample from the joint distribution:</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a><span class="co">            z ~ prior</span></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a><span class="co">            x|z ~ obs model</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size: number of samples in a batch</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if None, the output tensor has shape [batch_size] + data_shape</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a><span class="co">            if 1 or more, the output tensor has shape [sample_size, batch_size] + data_shape</span></span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a><span class="co">            while batch_size controls a parallel computation, </span></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a><span class="co">        oversample: if True, samples z (batch_size times), hold it fixed, </span></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a><span class="co">            and sample x (sample_size times)</span></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior((batch_size,))</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> (sample_size <span class="kw">or</span> <span class="dv">1</span>)        </span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(pz.sample()) <span class="cf">if</span> oversample <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size <span class="kw">or</span> <span class="dv">1</span>):</span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> oversample:</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(pz.sample())</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>            samples[k] <span class="op">=</span> px_z.sample()</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.stack(samples) </span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="cf">if</span> sample_size <span class="cf">else</span> x.squeeze(<span class="dv">0</span>)</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cond_sample(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>, oversample<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Condition on x and draw a sample:</span></span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a><span class="co">            z|x ~ inf model</span></span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a><span class="co">            x'|z ~ obs model</span></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a batch of seed data samples</span></span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if None, the output tensor has shape [batch_size] + data_shape</span></span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a><span class="co">            if 1 or more, the output tensor has shape [sample_size, batch_size] + data_shape            </span></span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a><span class="co">        oversample: if True, samples z (batch_size times), hold it fixed, </span></span>
<span id="cb64-64"><a href="#cb64-64" aria-hidden="true" tabindex="-1"></a><span class="co">            and sample x' (sample_size times)</span></span>
<span id="cb64-65"><a href="#cb64-65" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-66"><a href="#cb64-66" aria-hidden="true" tabindex="-1"></a>        qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb64-67"><a href="#cb64-67" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> (sample_size <span class="kw">or</span> <span class="dv">1</span>)        </span>
<span id="cb64-68"><a href="#cb64-68" aria-hidden="true" tabindex="-1"></a>        px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(qz.sample()) <span class="cf">if</span> oversample <span class="cf">else</span> <span class="va">None</span>        </span>
<span id="cb64-69"><a href="#cb64-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size <span class="kw">or</span> <span class="dv">1</span>):</span>
<span id="cb64-70"><a href="#cb64-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> oversample:</span>
<span id="cb64-71"><a href="#cb64-71" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(qz.sample())</span>
<span id="cb64-72"><a href="#cb64-72" aria-hidden="true" tabindex="-1"></a>            samples[k] <span class="op">=</span> px_z.sample()        </span>
<span id="cb64-73"><a href="#cb64-73" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.stack(samples) </span>
<span id="cb64-74"><a href="#cb64-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="cf">if</span> sample_size <span class="cf">else</span> x.squeeze(<span class="dv">0</span>)</span>
<span id="cb64-75"><a href="#cb64-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-76"><a href="#cb64-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, z, x):</span>
<span id="cb64-77"><a href="#cb64-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-78"><a href="#cb64-78" aria-hidden="true" tabindex="-1"></a><span class="co">        The log density of the joint outcome under the generative model</span></span>
<span id="cb64-79"><a href="#cb64-79" aria-hidden="true" tabindex="-1"></a><span class="co">        z: [batch_size, latent_dim]</span></span>
<span id="cb64-80"><a href="#cb64-80" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb64-81"><a href="#cb64-81" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb64-82"><a href="#cb64-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gen_model.log_prob(z<span class="op">=</span>z, x<span class="op">=</span>x)</span>
<span id="cb64-83"><a href="#cb64-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-84"><a href="#cb64-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> DRL(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-85"><a href="#cb64-85" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-86"><a href="#cb64-86" aria-hidden="true" tabindex="-1"></a><span class="co">        MC estimates of a model's</span></span>
<span id="cb64-87"><a href="#cb64-87" aria-hidden="true" tabindex="-1"></a><span class="co">            * distortion D</span></span>
<span id="cb64-88"><a href="#cb64-88" aria-hidden="true" tabindex="-1"></a><span class="co">            * rate R</span></span>
<span id="cb64-89"><a href="#cb64-89" aria-hidden="true" tabindex="-1"></a><span class="co">            * and log-likelihood L</span></span>
<span id="cb64-90"><a href="#cb64-90" aria-hidden="true" tabindex="-1"></a><span class="co">        The estimates are based on single data points </span></span>
<span id="cb64-91"><a href="#cb64-91" aria-hidden="true" tabindex="-1"></a><span class="co">         but multiple latent samples.</span></span>
<span id="cb64-92"><a href="#cb64-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-93"><a href="#cb64-93" aria-hidden="true" tabindex="-1"></a><span class="co">        x: batch_shape + data_shape</span></span>
<span id="cb64-94"><a href="#cb64-94" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb64-95"><a href="#cb64-95" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-96"><a href="#cb64-96" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-97"><a href="#cb64-97" aria-hidden="true" tabindex="-1"></a>        sample_size <span class="op">=</span> sample_size <span class="kw">or</span> <span class="dv">1</span></span>
<span id="cb64-98"><a href="#cb64-98" aria-hidden="true" tabindex="-1"></a>        obs_dims <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.gen_model.cpd_net.outcome_shape)</span>
<span id="cb64-99"><a href="#cb64-99" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span>obs_dims]</span>
<span id="cb64-100"><a href="#cb64-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():        </span>
<span id="cb64-101"><a href="#cb64-101" aria-hidden="true" tabindex="-1"></a>            qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb64-102"><a href="#cb64-102" aria-hidden="true" tabindex="-1"></a>            pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior(batch_shape)</span>
<span id="cb64-103"><a href="#cb64-103" aria-hidden="true" tabindex="-1"></a>            R <span class="op">=</span> td.kl_divergence(qz, pz)</span>
<span id="cb64-104"><a href="#cb64-104" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-105"><a href="#cb64-105" aria-hidden="true" tabindex="-1"></a>            ratios <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> sample_size</span>
<span id="cb64-106"><a href="#cb64-106" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(sample_size):</span>
<span id="cb64-107"><a href="#cb64-107" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> qz.sample()</span>
<span id="cb64-108"><a href="#cb64-108" aria-hidden="true" tabindex="-1"></a>                px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(z)</span>
<span id="cb64-109"><a href="#cb64-109" aria-hidden="true" tabindex="-1"></a>                ratios[k] <span class="op">=</span> pz.log_prob(z) <span class="op">+</span> px_z.log_prob(x) <span class="op">-</span> qz.log_prob(z)</span>
<span id="cb64-110"><a href="#cb64-110" aria-hidden="true" tabindex="-1"></a>                D <span class="op">=</span> D <span class="op">-</span> px_z.log_prob(x)</span>
<span id="cb64-111"><a href="#cb64-111" aria-hidden="true" tabindex="-1"></a>            ratios <span class="op">=</span> torch.stack(ratios, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb64-112"><a href="#cb64-112" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> torch.logsumexp(ratios, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">-</span> np.log(sample_size)            </span>
<span id="cb64-113"><a href="#cb64-113" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> D <span class="op">/</span> sample_size</span>
<span id="cb64-114"><a href="#cb64-114" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> D, R, L</span>
<span id="cb64-115"><a href="#cb64-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-116"><a href="#cb64-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> elbo(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-117"><a href="#cb64-117" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-118"><a href="#cb64-118" aria-hidden="true" tabindex="-1"></a><span class="co">        An MC estimate of ELBO = -D -R</span></span>
<span id="cb64-119"><a href="#cb64-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-120"><a href="#cb64-120" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb64-121"><a href="#cb64-121" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb64-122"><a href="#cb64-122" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-123"><a href="#cb64-123" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-124"><a href="#cb64-124" aria-hidden="true" tabindex="-1"></a>        D, R, _ <span class="op">=</span> <span class="va">self</span>.DRL(x, sample_size<span class="op">=</span>sample_size)</span>
<span id="cb64-125"><a href="#cb64-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>D <span class="op">-</span>R</span>
<span id="cb64-126"><a href="#cb64-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-127"><a href="#cb64-127" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob_estimate(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-128"><a href="#cb64-128" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-129"><a href="#cb64-129" aria-hidden="true" tabindex="-1"></a><span class="co">        An importance sampling estimate of log p(x)</span></span>
<span id="cb64-130"><a href="#cb64-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-131"><a href="#cb64-131" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb64-132"><a href="#cb64-132" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb64-133"><a href="#cb64-133" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-134"><a href="#cb64-134" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-135"><a href="#cb64-135" aria-hidden="true" tabindex="-1"></a>        _, _, L <span class="op">=</span> <span class="va">self</span>.DRL(x, sample_size<span class="op">=</span>sample_size)</span>
<span id="cb64-136"><a href="#cb64-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> L</span>
<span id="cb64-137"><a href="#cb64-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-138"><a href="#cb64-138" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sample_size<span class="op">=</span><span class="va">None</span>, rate_weight<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb64-139"><a href="#cb64-139" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb64-140"><a href="#cb64-140" aria-hidden="true" tabindex="-1"></a><span class="co">        A surrogate for an MC estimate of - grad ELBO </span></span>
<span id="cb64-141"><a href="#cb64-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-142"><a href="#cb64-142" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size] + data_shape</span></span>
<span id="cb64-143"><a href="#cb64-143" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size: if 1 or more, we use multiple samples</span></span>
<span id="cb64-144"><a href="#cb64-144" aria-hidden="true" tabindex="-1"></a><span class="co">            sample_size controls a sequential computation (a for loop)</span></span>
<span id="cb64-145"><a href="#cb64-145" aria-hidden="true" tabindex="-1"></a><span class="co">        cv: optional module for variance reduction</span></span>
<span id="cb64-146"><a href="#cb64-146" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb64-147"><a href="#cb64-147" aria-hidden="true" tabindex="-1"></a>        sample_size <span class="op">=</span> sample_size <span class="kw">or</span> <span class="dv">1</span></span>
<span id="cb64-148"><a href="#cb64-148" aria-hidden="true" tabindex="-1"></a>        obs_dims <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.gen_model.cpd_net.outcome_shape)</span>
<span id="cb64-149"><a href="#cb64-149" aria-hidden="true" tabindex="-1"></a>        batch_shape <span class="op">=</span> x.shape[:<span class="op">-</span>obs_dims]</span>
<span id="cb64-150"><a href="#cb64-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-151"><a href="#cb64-151" aria-hidden="true" tabindex="-1"></a>        qz <span class="op">=</span> <span class="va">self</span>.inf_model(x)</span>
<span id="cb64-152"><a href="#cb64-152" aria-hidden="true" tabindex="-1"></a>        pz <span class="op">=</span> <span class="va">self</span>.gen_model.prior(batch_shape)</span>
<span id="cb64-153"><a href="#cb64-153" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-154"><a href="#cb64-154" aria-hidden="true" tabindex="-1"></a>        sfe <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-155"><a href="#cb64-155" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-156"><a href="#cb64-156" aria-hidden="true" tabindex="-1"></a>        cv_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-157"><a href="#cb64-157" aria-hidden="true" tabindex="-1"></a>        raw_r <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-158"><a href="#cb64-158" aria-hidden="true" tabindex="-1"></a>        cv_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-159"><a href="#cb64-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(sample_size):            </span>
<span id="cb64-160"><a href="#cb64-160" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> qz.sample()                </span>
<span id="cb64-161"><a href="#cb64-161" aria-hidden="true" tabindex="-1"></a>            px_z <span class="op">=</span> <span class="va">self</span>.gen_model.obs_model(z)</span>
<span id="cb64-162"><a href="#cb64-162" aria-hidden="true" tabindex="-1"></a>            raw_r <span class="op">=</span> px_z.log_prob(x) <span class="op">+</span> pz.log_prob(z) <span class="op">-</span> qz.log_prob(z)                </span>
<span id="cb64-163"><a href="#cb64-163" aria-hidden="true" tabindex="-1"></a>            r, l <span class="op">=</span> <span class="va">self</span>.cv_model(raw_r.detach(), x<span class="op">=</span>x, q<span class="op">=</span>qz, r_fn<span class="op">=</span><span class="kw">lambda</span> a: <span class="va">self</span>.gen_model(a).log_prob(x))</span>
<span id="cb64-164"><a href="#cb64-164" aria-hidden="true" tabindex="-1"></a>            cv_loss <span class="op">=</span> cv_loss <span class="op">+</span> l</span>
<span id="cb64-165"><a href="#cb64-165" aria-hidden="true" tabindex="-1"></a>            sfe <span class="op">=</span> sfe <span class="op">+</span> r.detach() <span class="op">*</span> qz.log_prob(z) </span>
<span id="cb64-166"><a href="#cb64-166" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> D <span class="op">-</span> px_z.log_prob(x)</span>
<span id="cb64-167"><a href="#cb64-167" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> (D <span class="op">/</span> sample_size)</span>
<span id="cb64-168"><a href="#cb64-168" aria-hidden="true" tabindex="-1"></a>        sfe <span class="op">=</span> (sfe <span class="op">/</span> sample_size)</span>
<span id="cb64-169"><a href="#cb64-169" aria-hidden="true" tabindex="-1"></a>        R <span class="op">=</span> td.kl_divergence(qz, pz)</span>
<span id="cb64-170"><a href="#cb64-170" aria-hidden="true" tabindex="-1"></a>        cv_loss <span class="op">=</span> cv_loss <span class="op">/</span> sample_size</span>
<span id="cb64-171"><a href="#cb64-171" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-172"><a href="#cb64-172" aria-hidden="true" tabindex="-1"></a>        elbo_grad_surrogate <span class="op">=</span> (<span class="op">-</span>D <span class="op">+</span> sfe)        </span>
<span id="cb64-173"><a href="#cb64-173" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>elbo_grad_surrogate <span class="op">+</span> cv_loss</span>
<span id="cb64-174"><a href="#cb64-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-175"><a href="#cb64-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">'loss'</span>: loss.mean(<span class="dv">0</span>), <span class="st">'ELBO'</span>: (<span class="op">-</span>D <span class="op">-</span>R).mean(<span class="dv">0</span>).item(), <span class="st">'D'</span>: D.mean(<span class="dv">0</span>).item(), <span class="st">'R'</span>: R.mean(<span class="dv">0</span>).item(), <span class="st">'cv_loss'</span>: cv_loss.mean(<span class="dv">0</span>).item()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s an example</p>
<div id="cell-80" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>nvil <span class="op">=</span> NVIL(</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    JointDistribution(</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        BernoulliPriorNet(<span class="dv">10</span>),</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        BinarizedImageModel(</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>            decoder_type<span class="op">=</span>build_ffnn_decoder</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    InferenceModel(</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>        cpd_net_type<span class="op">=</span>BernoulliCPDNet,</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>], </span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>], </span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],        </span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>        encoder_type<span class="op">=</span>build_ffnn_encoder</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>    VarianceReduction()</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>nvil</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>NVIL(
  (gen_model): JointDistribution(
    (prior_net): BernoulliPriorNet()
    (cpd_net): BinarizedImageModel(
      (decoder): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=10, out_features=512, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): ReLU()
        (6): Dropout(p=0.1, inplace=False)
        (7): Linear(in_features=512, out_features=4096, bias=True)
        (8): ReshapeLast()
      )
    )
  )
  (inf_model): InferenceModel(
    (encoder): Sequential(
      (0): FlattenImage()
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=4096, out_features=512, bias=True)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=512, out_features=512, bias=True)
      (6): ReLU()
      (7): Dropout(p=0.0, inplace=False)
      (8): Linear(in_features=512, out_features=1024, bias=True)
    )
    (cpd_net): BernoulliCPDNet(
      (encoder): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=1024, out_features=20, bias=True)
        (2): ReLU()
      )
      (logits): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): ReshapeLast()
      )
    )
  )
  (cv_model): VarianceReduction()
)</code></pre>
</div>
</div>
<div id="cell-81" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, y <span class="kw">in</span> train_loader:</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'x.shape:'</span>, x.shape)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(nvil(x))</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.shape: torch.Size([64, 1, 64, 64])
{'loss': tensor(-16754.9395, grad_fn=&lt;MeanBackward1&gt;), 'ELBO': -2844.470947265625, 'D': 2844.458740234375, 'R': 0.012489881366491318, 'cv_loss': 0.0}</code></pre>
</div>
</div>
<section id="training-algorithm-1" class="level4">
<h4 class="anchored" data-anchor-id="training-algorithm-1">3.5.1 Training algorithm</h4>
<p>We have up to three components (recall that some control variates can have their own parameters), so we will be manipulating up to three optimisers:</p>
<div id="cell-83" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptCollection:</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, gen, inf, cv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen <span class="op">=</span> gen</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf <span class="op">=</span> inf</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cv <span class="op">=</span> cv</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen.zero_grad()</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf.zero_grad()</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cv:</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cv.zero_grad()</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gen.step()</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inf.step()</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cv:</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cv.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s some helper code to assess and train the model</p>
<div id="cell-85" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, OrderedDict</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assess(model, sample_size, dl, device):</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrapper for estimating a model's ELBO, distortion, rate, and log-likelihood</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="co">     using all data points in a data loader.</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>    data_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> dl:</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>            Dx, Rx, Lx <span class="op">=</span> model.DRL(batch_x.to(device), sample_size<span class="op">=</span>sample_size)</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>            D <span class="op">=</span> D <span class="op">+</span> Dx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>            R <span class="op">=</span> R <span class="op">+</span> Rx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">+</span> Lx.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a>            data_size <span class="op">+=</span> batch_x.shape[<span class="dv">0</span>]</span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">/</span> data_size</span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> R <span class="op">/</span> data_size</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L <span class="op">/</span> data_size</span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'ELBO'</span>: (<span class="op">-</span>D <span class="op">-</span>R).item(), <span class="st">'D'</span>: D.item(), <span class="st">'R'</span>: R.item(), <span class="st">'L'</span>: L.item()}</span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_vae(model: NVIL, opts: OptCollection, </span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a>    training_data, dev_data,</span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>, num_epochs<span class="op">=</span><span class="dv">10</span>, check_every<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a>    sample_size_training<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>    sample_size_eval<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>torch.device(<span class="st">'cuda:0'</span>)</span>
<span id="cb70-35"><a href="#cb70-35" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb70-36"><a href="#cb70-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb70-37"><a href="#cb70-37" aria-hidden="true" tabindex="-1"></a><span class="co">    model: pytorch model</span></span>
<span id="cb70-38"><a href="#cb70-38" aria-hidden="true" tabindex="-1"></a><span class="co">    optimiser: pytorch optimiser</span></span>
<span id="cb70-39"><a href="#cb70-39" aria-hidden="true" tabindex="-1"></a><span class="co">    training_corpus: a TaggedCorpus for trianing</span></span>
<span id="cb70-40"><a href="#cb70-40" aria-hidden="true" tabindex="-1"></a><span class="co">    dev_corpus: a TaggedCorpus for dev</span></span>
<span id="cb70-41"><a href="#cb70-41" aria-hidden="true" tabindex="-1"></a><span class="co">    batch_size: use more if you have more memory</span></span>
<span id="cb70-42"><a href="#cb70-42" aria-hidden="true" tabindex="-1"></a><span class="co">    num_epochs: use more for improved convergence</span></span>
<span id="cb70-43"><a href="#cb70-43" aria-hidden="true" tabindex="-1"></a><span class="co">    check_every: use less to check performance on dev set more often</span></span>
<span id="cb70-44"><a href="#cb70-44" aria-hidden="true" tabindex="-1"></a><span class="co">    device: where we run the experiment</span></span>
<span id="cb70-45"><a href="#cb70-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-46"><a href="#cb70-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a log of quantities computed during training (for plotting)</span></span>
<span id="cb70-47"><a href="#cb70-47" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb70-48"><a href="#cb70-48" aria-hidden="true" tabindex="-1"></a>    batcher <span class="op">=</span> DataLoader(training_data, batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb70-49"><a href="#cb70-49" aria-hidden="true" tabindex="-1"></a>    dev_batcher <span class="op">=</span> DataLoader(dev_data, batch_size, num_workers<span class="op">=</span>num_workers, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb70-50"><a href="#cb70-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-51"><a href="#cb70-51" aria-hidden="true" tabindex="-1"></a>    total_steps <span class="op">=</span> num_epochs <span class="op">*</span> <span class="bu">len</span>(batcher)</span>
<span id="cb70-52"><a href="#cb70-52" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb70-53"><a href="#cb70-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-54"><a href="#cb70-54" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-55"><a href="#cb70-55" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb70-56"><a href="#cb70-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb70-57"><a href="#cb70-57" aria-hidden="true" tabindex="-1"></a>        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb70-58"><a href="#cb70-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-59"><a href="#cb70-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tqdm(<span class="bu">range</span>(total_steps)) <span class="im">as</span> bar:</span>
<span id="cb70-60"><a href="#cb70-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb70-61"><a href="#cb70-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> batcher:</span>
<span id="cb70-62"><a href="#cb70-62" aria-hidden="true" tabindex="-1"></a>                model.train()</span>
<span id="cb70-63"><a href="#cb70-63" aria-hidden="true" tabindex="-1"></a>                opts.zero_grad()</span>
<span id="cb70-64"><a href="#cb70-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-65"><a href="#cb70-65" aria-hidden="true" tabindex="-1"></a>                loss_dict <span class="op">=</span> model(</span>
<span id="cb70-66"><a href="#cb70-66" aria-hidden="true" tabindex="-1"></a>                    batch_x.to(device), </span>
<span id="cb70-67"><a href="#cb70-67" aria-hidden="true" tabindex="-1"></a>                    sample_size<span class="op">=</span>sample_size_training,</span>
<span id="cb70-68"><a href="#cb70-68" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb70-69"><a href="#cb70-69" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric, value <span class="kw">in</span> loss_dict.items():</span>
<span id="cb70-70"><a href="#cb70-70" aria-hidden="true" tabindex="-1"></a>                    log[<span class="ss">f'training.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">'</span>].append((step, value))</span>
<span id="cb70-71"><a href="#cb70-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-72"><a href="#cb70-72" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">'loss'</span>].backward()</span>
<span id="cb70-73"><a href="#cb70-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-74"><a href="#cb70-74" aria-hidden="true" tabindex="-1"></a>                nn.utils.clip_grad_norm_(</span>
<span id="cb70-75"><a href="#cb70-75" aria-hidden="true" tabindex="-1"></a>                    model.parameters(), </span>
<span id="cb70-76"><a href="#cb70-76" aria-hidden="true" tabindex="-1"></a>                    grad_clip</span>
<span id="cb70-77"><a href="#cb70-77" aria-hidden="true" tabindex="-1"></a>                )    </span>
<span id="cb70-78"><a href="#cb70-78" aria-hidden="true" tabindex="-1"></a>                opts.step()</span>
<span id="cb70-79"><a href="#cb70-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-80"><a href="#cb70-80" aria-hidden="true" tabindex="-1"></a>                bar_dict <span class="op">=</span> OrderedDict()</span>
<span id="cb70-81"><a href="#cb70-81" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric, value <span class="kw">in</span> loss_dict.items():</span>
<span id="cb70-82"><a href="#cb70-82" aria-hidden="true" tabindex="-1"></a>                    bar_dict[<span class="ss">f'training.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>loss_dict[metric]<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb70-83"><a href="#cb70-83" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> metric <span class="kw">in</span> [<span class="st">'ELBO'</span>, <span class="st">'D'</span>, <span class="st">'R'</span>, <span class="st">'L'</span>]:</span>
<span id="cb70-84"><a href="#cb70-84" aria-hidden="true" tabindex="-1"></a>                    bar_dict[<span class="ss">f"dev.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span>  <span class="st">"</span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(log[<span class="ss">f"dev.</span><span class="sc">{</span>metric<span class="sc">}</span><span class="ss">"</span>][<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>])</span>
<span id="cb70-85"><a href="#cb70-85" aria-hidden="true" tabindex="-1"></a>                bar.set_postfix(bar_dict)</span>
<span id="cb70-86"><a href="#cb70-86" aria-hidden="true" tabindex="-1"></a>                bar.update()</span>
<span id="cb70-87"><a href="#cb70-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-88"><a href="#cb70-88" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> step <span class="op">%</span> check_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb70-89"><a href="#cb70-89" aria-hidden="true" tabindex="-1"></a>                    model.<span class="bu">eval</span>()</span>
<span id="cb70-90"><a href="#cb70-90" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb70-91"><a href="#cb70-91" aria-hidden="true" tabindex="-1"></a>                        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb70-92"><a href="#cb70-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-93"><a href="#cb70-93" aria-hidden="true" tabindex="-1"></a>                step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb70-94"><a href="#cb70-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-95"><a href="#cb70-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-96"><a href="#cb70-96" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb70-97"><a href="#cb70-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> assess(model, sample_size_eval, dev_batcher, device<span class="op">=</span>device).items():</span>
<span id="cb70-98"><a href="#cb70-98" aria-hidden="true" tabindex="-1"></a>        log[<span class="ss">f"dev.</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>].append((step, v))</span>
<span id="cb70-99"><a href="#cb70-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-100"><a href="#cb70-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And, finally, some code to help inspect samples</p>
<div id="cell-87" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inspect_discrete_lvm(model, dl, device):</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> dl:    </span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>        x_ <span class="op">=</span> model.sample(<span class="dv">16</span>, <span class="dv">4</span>, oversample<span class="op">=</span><span class="va">True</span>).cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x_, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Prior samples"</span>)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Observations"</span>)</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>        x_ <span class="op">=</span> model.cond_sample(x.to(device)).cpu().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        plt.imshow(make_grid(x_, nrow<span class="op">=</span><span class="dv">16</span>).permute((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))    </span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Conditional samples"</span>)</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="variance-reduction" class="level4">
<h4 class="anchored" data-anchor-id="variance-reduction">3.5.2 Variance reduction</h4>
<p>Here are some concrete strategies for variance reduction. You can skip those in a first pass.</p>
<div id="cell-89" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CentredReward(VarianceReduction):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co">     it maintains a running estimate of the average reward and updates</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co">     a batch of rewards by computing reward - avg.</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._alpha <span class="op">=</span> alpha</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._r_mean <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x<span class="op">=</span><span class="va">None</span>, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Centre the reward and update running estimates of mean.</span></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sufficient statistics for next updates</span></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>            r_mean <span class="op">=</span> torch.mean(r, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># centre the signal</span></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">-</span> <span class="va">self</span>._r_mean</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update running estimate of mean</span></span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_mean <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>._alpha) <span class="op">*</span> <span class="va">self</span>._r_mean <span class="op">+</span> <span class="va">self</span>._alpha <span class="op">*</span> r_mean.item()                        </span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledReward(VarianceReduction):</span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a><span class="co">     it maintains a running estimate of the reward's standard deviation and </span></span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a><span class="co">     updates a batch of rewards by computing reward / maximum(stddev, 1).</span></span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-36"><a href="#cb72-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._alpha <span class="op">=</span> alpha</span>
<span id="cb72-37"><a href="#cb72-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._r_std <span class="op">=</span> <span class="fl">1.0</span>                   </span>
<span id="cb72-38"><a href="#cb72-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-39"><a href="#cb72-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x<span class="op">=</span><span class="va">None</span>, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb72-40"><a href="#cb72-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb72-41"><a href="#cb72-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Scale the reward by a running estimate of std, and also update the estimate.</span></span>
<span id="cb72-42"><a href="#cb72-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span>        </span>
<span id="cb72-43"><a href="#cb72-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb72-44"><a href="#cb72-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sufficient statistics for next updates</span></span>
<span id="cb72-45"><a href="#cb72-45" aria-hidden="true" tabindex="-1"></a>            r_std <span class="op">=</span> torch.std(r, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-46"><a href="#cb72-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># standardise the signal</span></span>
<span id="cb72-47"><a href="#cb72-47" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">/</span> <span class="va">self</span>._r_std            </span>
<span id="cb72-48"><a href="#cb72-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update running estimate of std</span></span>
<span id="cb72-49"><a href="#cb72-49" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_std <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>._alpha) <span class="op">*</span> <span class="va">self</span>._r_std <span class="op">+</span> <span class="va">self</span>._alpha <span class="op">*</span> r_std.item()</span>
<span id="cb72-50"><a href="#cb72-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># it's not safe to standardise with scales less than 1</span></span>
<span id="cb72-51"><a href="#cb72-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._r_std <span class="op">=</span> np.maximum(<span class="va">self</span>._r_std, <span class="fl">1.</span>)</span>
<span id="cb72-52"><a href="#cb72-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb72-53"><a href="#cb72-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-54"><a href="#cb72-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-55"><a href="#cb72-55" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfCritic(VarianceReduction):</span>
<span id="cb72-56"><a href="#cb72-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb72-57"><a href="#cb72-57" aria-hidden="true" tabindex="-1"></a><span class="co">    This control variate does not have trainable parameters, </span></span>
<span id="cb72-58"><a href="#cb72-58" aria-hidden="true" tabindex="-1"></a><span class="co">     it updates a batch of rewards by computing reward - reward', where</span></span>
<span id="cb72-59"><a href="#cb72-59" aria-hidden="true" tabindex="-1"></a><span class="co">     reward' is (log p(X=x|Z=z')).detach() assessed for a novel sample</span></span>
<span id="cb72-60"><a href="#cb72-60" aria-hidden="true" tabindex="-1"></a><span class="co">     z' ~ Z|X=x.</span></span>
<span id="cb72-61"><a href="#cb72-61" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb72-62"><a href="#cb72-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-63"><a href="#cb72-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb72-64"><a href="#cb72-64" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb72-65"><a href="#cb72-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-66"><a href="#cb72-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):</span>
<span id="cb72-67"><a href="#cb72-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb72-68"><a href="#cb72-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Standardise the reward and update running estimates of mean/std.</span></span>
<span id="cb72-69"><a href="#cb72-69" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb72-70"><a href="#cb72-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb72-71"><a href="#cb72-71" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> q.sample()</span>
<span id="cb72-72"><a href="#cb72-72" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> r <span class="op">-</span> r_fn(z, x)</span>
<span id="cb72-73"><a href="#cb72-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r, torch.zeros_like(r)</span>
<span id="cb72-74"><a href="#cb72-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-75"><a href="#cb72-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-76"><a href="#cb72-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Baseline(VarianceReduction):</span>
<span id="cb72-77"><a href="#cb72-77" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb72-78"><a href="#cb72-78" aria-hidden="true" tabindex="-1"></a><span class="co">    An input-dependent baseline implemented as an MLP.</span></span>
<span id="cb72-79"><a href="#cb72-79" aria-hidden="true" tabindex="-1"></a><span class="co">    The trainable parameters are adjusted via MSE.</span></span>
<span id="cb72-80"><a href="#cb72-80" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb72-81"><a href="#cb72-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-82"><a href="#cb72-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, hidden_size, p_drop<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb72-83"><a href="#cb72-83" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb72-84"><a href="#cb72-84" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.baseline <span class="op">=</span> nn.Sequential(</span>
<span id="cb72-85"><a href="#cb72-85" aria-hidden="true" tabindex="-1"></a>            FlattenImage(),</span>
<span id="cb72-86"><a href="#cb72-86" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb72-87"><a href="#cb72-87" aria-hidden="true" tabindex="-1"></a>            nn.Linear(num_inputs, hidden_size),</span>
<span id="cb72-88"><a href="#cb72-88" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb72-89"><a href="#cb72-89" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p_drop),</span>
<span id="cb72-90"><a href="#cb72-90" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb72-91"><a href="#cb72-91" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb72-92"><a href="#cb72-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-93"><a href="#cb72-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q<span class="op">=</span><span class="va">None</span>, r_fn<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb72-94"><a href="#cb72-94" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb72-95"><a href="#cb72-95" aria-hidden="true" tabindex="-1"></a><span class="co">        Return r - baseline(x) and Baseline's loss.</span></span>
<span id="cb72-96"><a href="#cb72-96" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb72-97"><a href="#cb72-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape + (1,)</span></span>
<span id="cb72-98"><a href="#cb72-98" aria-hidden="true" tabindex="-1"></a>        r_hat <span class="op">=</span> <span class="va">self</span>.baseline(x)</span>
<span id="cb72-99"><a href="#cb72-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_shape</span></span>
<span id="cb72-100"><a href="#cb72-100" aria-hidden="true" tabindex="-1"></a>        r_hat <span class="op">=</span> r_hat.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb72-101"><a href="#cb72-101" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (r <span class="op">-</span> r_hat)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb72-102"><a href="#cb72-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r <span class="op">-</span> r_hat.detach(), loss</span>
<span id="cb72-103"><a href="#cb72-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-104"><a href="#cb72-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-105"><a href="#cb72-105" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CVChain(VarianceReduction):</span>
<span id="cb72-106"><a href="#cb72-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-107"><a href="#cb72-107" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb72-108"><a href="#cb72-108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()        </span>
<span id="cb72-109"><a href="#cb72-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(args) <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> <span class="bu">isinstance</span>(args[<span class="dv">0</span>], OrderedDict):</span>
<span id="cb72-110"><a href="#cb72-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> key, module <span class="kw">in</span> args[<span class="dv">0</span>].items():</span>
<span id="cb72-111"><a href="#cb72-111" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.add_module(key, module)</span>
<span id="cb72-112"><a href="#cb72-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb72-113"><a href="#cb72-113" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, module <span class="kw">in</span> <span class="bu">enumerate</span>(args):</span>
<span id="cb72-114"><a href="#cb72-114" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.add_module(<span class="bu">str</span>(idx), module)</span>
<span id="cb72-115"><a href="#cb72-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-116"><a href="#cb72-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, r, x, q, r_fn):         </span>
<span id="cb72-117"><a href="#cb72-117" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb72-118"><a href="#cb72-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> cv <span class="kw">in</span> <span class="va">self</span>._modules.values():</span>
<span id="cb72-119"><a href="#cb72-119" aria-hidden="true" tabindex="-1"></a>            r, l <span class="op">=</span> cv(r, x<span class="op">=</span>x, q<span class="op">=</span>q, r_fn<span class="op">=</span>r_fn)</span>
<span id="cb72-120"><a href="#cb72-120" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">+</span> l</span>
<span id="cb72-121"><a href="#cb72-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> r, loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="experiment-1" class="level4">
<h4 class="anchored" data-anchor-id="experiment-1">3.5.3 Experiment</h4>
<div id="cell-91" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>seed_all()</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NVIL(</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    JointDistribution(</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        BernoulliPriorNet(<span class="dv">10</span>),</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        BinarizedImageModel(</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>            num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>],</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>img_shape[<span class="dv">1</span>],</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>            height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>            latent_size<span class="op">=</span><span class="dv">10</span>,     </span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>            p_drop<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    InferenceModel(</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        latent_size<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>        num_channels<span class="op">=</span>img_shape[<span class="dv">0</span>], </span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>img_shape[<span class="dv">1</span>], </span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>img_shape[<span class="dv">2</span>],</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>        cpd_net_type<span class="op">=</span>BernoulliCPDNet</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>    VarianceReduction(), <span class="co"># no variance reduction</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In the NVIL paper, this is what they use:</span></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CVChain(</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     CentredReward(),        </span></span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     #Baseline(np.prod(img_shape), 512), # this is how you would use a trained baselined</span></span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     #ScaledReward()</span></span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># )</span></span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>).to(my_device)</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> OptCollection(</span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>    opt.RMSprop(model.gen_params(), lr<span class="op">=</span><span class="fl">5e-4</span>, weight_decay<span class="op">=</span><span class="fl">1e-6</span>),</span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a>    opt.RMSprop(model.inf_params(), lr<span class="op">=</span><span class="fl">1e-4</span>),</span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">#opt.RMSprop(model.cv_params(), lr=1e-4, weight_decay=1e-6) # you need this if your baseline has trainable parameters</span></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>NVIL(
  (gen_model): JointDistribution(
    (prior_net): BernoulliPriorNet()
    (cpd_net): BinarizedImageModel(
      (decoder): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=10, out_features=512, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): ReLU()
        (6): Dropout(p=0.1, inplace=False)
        (7): Linear(in_features=512, out_features=4096, bias=True)
        (8): ReshapeLast()
      )
    )
  )
  (inf_model): InferenceModel(
    (encoder): Sequential(
      (0): FlattenImage()
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=4096, out_features=512, bias=True)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): Linear(in_features=512, out_features=512, bias=True)
      (6): ReLU()
      (7): Dropout(p=0.0, inplace=False)
      (8): Linear(in_features=512, out_features=1024, bias=True)
    )
    (cpd_net): BernoulliCPDNet(
      (encoder): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=1024, out_features=20, bias=True)
        (2): ReLU()
      )
      (logits): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): Linear(in_features=20, out_features=10, bias=True)
        (2): ReshapeLast()
      )
    )
  )
  (cv_model): VarianceReduction()
)</code></pre>
</div>
</div>
<div id="cell-92" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>log <span class="op">=</span> train_vae(</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    opts<span class="op">=</span>opts,</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    training_data<span class="op">=</span>train_ds, </span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    dev_data<span class="op">=</span>val_ds,</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">256</span>, </span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span><span class="dv">3</span>,  <span class="co"># use more for better models</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    check_every<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    sample_size_training<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    sample_size_eval<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>    grad_clip<span class="op">=</span><span class="fl">5.</span>,</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>my_device</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"eb6dd0a740b34dd2ad00a7bce2bb217f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>log.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>dict_keys(['dev.ELBO', 'dev.D', 'dev.R', 'dev.L', 'training.loss', 'training.ELBO', 'training.D', 'training.R', 'training.cv_loss'])</code></pre>
</div>
</div>
<div id="cell-94" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span> <span class="op">+</span> <span class="bu">int</span>(<span class="st">'training.cv_loss'</span> <span class="kw">in</span> log), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.array(log[<span class="st">'training.ELBO'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.ELBO'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">"training ELBO"</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.array(log[<span class="st">'training.D'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.D'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">"training D"</span>)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.array(log[<span class="st">'training.R'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.R'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">"training R"</span>)</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'training.cv_loss'</span> <span class="kw">in</span> log:</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].plot(np.array(log[<span class="st">'training.cv_loss'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'training.cv_loss'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].set_ylabel(<span class="st">"cv loss"</span>)</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> axs[<span class="dv">3</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout(h_pad<span class="op">=</span><span class="fl">1.2</span>, w_pad<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-56-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-95" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].plot(np.array(log[<span class="st">'dev.ELBO'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.ELBO'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_ylabel(<span class="st">"dev ELBO"</span>)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">0</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].plot(np.array(log[<span class="st">'dev.D'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.D'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_ylabel(<span class="st">"dev D"</span>)</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">1</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].plot(np.array(log[<span class="st">'dev.R'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.R'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_ylabel(<span class="st">"dev R"</span>)</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">2</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].plot(np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">0</span>], np.array(log[<span class="st">'dev.L'</span>])[:,<span class="dv">1</span>])</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_ylabel(<span class="st">"dev L"</span>)</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> axs[<span class="dv">3</span>].set_xlabel(<span class="st">"steps"</span>)</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>fig.tight_layout(h_pad<span class="op">=</span><span class="fl">1.2</span>, w_pad<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-57-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-96" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>inspect_discrete_lvm(model, DataLoader(val_ds, <span class="dv">64</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>), my_device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-58-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-58-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial_2a_files/figure-html/cell-58-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="beyond" class="level2">
<h2 class="anchored" data-anchor-id="beyond">4. Beyond</h2>
<p>There are various things you can try.</p>
<ul>
<li>You can play with continuous output distributions and CNN encoder/decoders.</li>
<li>Try switching baselines on and off.</li>
<li>With VI, you should be able to use mixture models with larger <span class="math inline">\(K\)</span>, even with the CNN decoder.</li>
<li>You can change the dataset (e.g., use SVHN, whose images are coloured, thus you will be using 3 channels).</li>
<li>You can attempt to (semi-)supervise the latent code (since these datasets offer some amount of supervision). For that you can use a log-likelihood side loss based on a model component that predicts a pmf <span class="math inline">\(p_{Y|Z}(y|z, \phi)\)</span> for the image’s class given the latent code.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>