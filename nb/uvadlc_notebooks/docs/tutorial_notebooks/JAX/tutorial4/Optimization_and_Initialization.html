<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Tutorial 4 (JAX): Optimization and Initialization – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preparation" id="toc-preparation" class="nav-link active" data-scroll-target="#preparation">Preparation</a></li>
  <li><a href="#initialization" id="toc-initialization" class="nav-link" data-scroll-target="#initialization">Initialization</a>
  <ul class="collapse">
  <li><a href="#constant-initialization" id="toc-constant-initialization" class="nav-link" data-scroll-target="#constant-initialization">Constant initialization</a></li>
  <li><a href="#constant-variance" id="toc-constant-variance" class="nav-link" data-scroll-target="#constant-variance">Constant variance</a></li>
  <li><a href="#how-to-find-appropriate-initialization-values" id="toc-how-to-find-appropriate-initialization-values" class="nav-link" data-scroll-target="#how-to-find-appropriate-initialization-values">How to find appropriate initialization values</a></li>
  </ul></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a>
  <ul class="collapse">
  <li><a href="#comparing-optimizers-on-model-training" id="toc-comparing-optimizers-on-model-training" class="nav-link" data-scroll-target="#comparing-optimizers-on-model-training">Comparing optimizers on model training</a></li>
  <li><a href="#pathological-curvatures" id="toc-pathological-curvatures" class="nav-link" data-scroll-target="#pathological-curvatures">Pathological curvatures</a></li>
  <li><a href="#steep-optima" id="toc-steep-optima" class="nav-link" data-scroll-target="#steep-optima">Steep optima</a></li>
  <li><a href="#what-optimizer-to-take" id="toc-what-optimizer-to-take" class="nav-link" data-scroll-target="#what-optimizer-to-take">What optimizer to take</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tutorial 4 (JAX): Optimization and Initialization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://img.shields.io/static/v1.svg?label=Status&amp;message=Finished&amp;color=green" class="img-fluid figure-img"></p>
<figcaption>Status</figcaption>
</figure>
</div>
<p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial4/Optimization_and_Initialization.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial4/Optimization_and_Initialization.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Collab"></a><br>
<strong>Pre-trained models:</strong> <a href="https://github.com/phlippe/saved_models/tree/main/JAX/tutorial4"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" class="img-fluid" alt="View files on Github"></a><br>
<strong>PyTorch version:</strong> <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html"><img src="https://img.shields.io/static/v1.svg?logo=readthedocs&amp;label=RTD&amp;message=View%20On%20RTD&amp;color=8CA1AF" class="img-fluid" alt="View on RTD"></a><br>
<strong>Author:</strong> Phillip Lippe</p>
<div class="alert alert-info">
<p><strong>Note:</strong> This notebook is written in JAX+Flax. It is a 1-to-1 translation of the original notebook written in PyTorch+PyTorch Lightning with almost identical results. For an introduction to JAX, check out our <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">Tutorial 2 (JAX): Introduction to JAX+Flax</a>. Further, throughout the notebook, we comment on major differences to the PyTorch version and provide explanations for the major parts of the JAX code. We do not provide speed comparisons for this notebook since they tend to be uninformative for such small networks and potentially bottlenecked by other factors.</p>
</div>
<p>In this tutorial, we will review techniques for optimization and initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the following concepts: initialization and optimization.</p>
<p>In the first half of the notebook, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks. In the second half, we focus on optimization comparing the optimizers SGD, SGD with Momentum, and Adam.</p>
<p>Let’s start with importing our standard libraries:</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Sequence, Callable, NamedTuple, Optional, Tuple</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>PyTree <span class="op">=</span> Any  <span class="co"># Type definition for PyTree, for readability</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">## Progress bar</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">## JAX</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.tree_util <span class="im">import</span> tree_map</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Seeding for random operations</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>main_rng <span class="op">=</span> random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">## Flax (NN in JAX)</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> flax</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Install flax if missing</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet flax</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> flax</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax <span class="im">import</span> linen <span class="im">as</span> nn</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax.training <span class="im">import</span> train_state, checkpoints</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co">## Optax (Optimizers in JAX)</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> optax</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Install optax if missing</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet optax</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> optax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
  PyTreeDef = type(jax.tree_structure(None))
WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.</code></pre>
</div>
</div>
<p>We will use the same path variables <code>DATASET_PATH</code> and <code>CHECKPOINT_PATH</code> as in Tutorial 3. Adjust the paths if necessary.</p>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the folder where the datasets are/should be downloaded (e.g. MNIST)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>DATASET_PATH <span class="op">=</span> <span class="st">"../../data"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the folder where the pretrained models are saved</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../../saved_models/tutorial4_jax"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Verifying the device that will be used throughout this notebook</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Device:"</span>, jax.devices()[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Device: gpu:0</code></pre>
</div>
</div>
<p>In the last part of the notebook, we will train models using three different optimizers. The pretrained models for those are downloaded below.</p>
<div id="cell-8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.error <span class="im">import</span> HTTPError</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Github URL where saved models are stored for this tutorial</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>base_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial4/"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Files to download</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>pretrained_files <span class="op">=</span> [<span class="st">"FashionMNIST_SGD.config"</span>,    <span class="st">"FashionMNIST_SGD_results.json"</span>,    <span class="st">"FashionMNIST_SGD.tar"</span>, </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"FashionMNIST_SGDMom.config"</span>, <span class="st">"FashionMNIST_SGDMom_results.json"</span>, <span class="st">"FashionMNIST_SGDMom.tar"</span>, </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"FashionMNIST_Adam.config"</span>,   <span class="st">"FashionMNIST_Adam_results.json"</span>,   <span class="st">"FashionMNIST_Adam.tar"</span>   ]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create checkpoint path if it doesn't exist yet</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>os.makedirs(CHECKPOINT_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># For each file, check whether it already exists. If not, try downloading it.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> file_name <span class="kw">in</span> pretrained_files:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    file_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, file_name)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(file_path):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        file_url <span class="op">=</span> base_url <span class="op">+</span> file_name</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>file_url<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            urllib.request.urlretrieve(file_url, file_path)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> HTTPError <span class="im">as</span> e:</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:</span><span class="ch">\n</span><span class="st">"</span>, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="preparation" class="level2">
<h2 class="anchored" data-anchor-id="preparation">Preparation</h2>
<p>Throughout this notebook, we will use a deep fully connected network, similar to our previous tutorial. We will also again apply the network to FashionMNIST, so you can relate to the results of Tutorial 3. We start by loading the FashionMNIST dataset:</p>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> FashionMNIST</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformations applied on each image =&gt; bring them into a numpy array and normalize to mean 0 and std 1</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_to_numpy(img):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> np.array(img, dtype<span class="op">=</span>np.float32)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> (img <span class="op">/</span> <span class="fl">255.</span> <span class="op">-</span> <span class="fl">0.2861</span>) <span class="op">/</span> <span class="fl">0.3530</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to stack the batch elements as numpy arrays</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numpy_collate(batch):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(batch[<span class="dv">0</span>], np.ndarray):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.stack(batch)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(batch[<span class="dv">0</span>], (<span class="bu">tuple</span>,<span class="bu">list</span>)):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        transposed <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>batch)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [numpy_collate(samples) <span class="cf">for</span> samples <span class="kw">in</span> transposed]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(batch)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the training dataset. We need to split it into a training and validation part</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> FashionMNIST(root<span class="op">=</span>DATASET_PATH,</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>                             train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>                             transform<span class="op">=</span>image_to_numpy,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>                             download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>train_set, val_set <span class="op">=</span> torch.utils.data.random_split(train_dataset,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>                                                   [<span class="dv">50000</span>, <span class="dv">10000</span>],</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>                                                   generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">42</span>))</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the test set</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>test_set <span class="op">=</span> FashionMNIST(root<span class="op">=</span>DATASET_PATH,</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>                        train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>                        transform<span class="op">=</span>image_to_numpy,</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>                        download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co"># We define a set of data loaders that we can use for various purposes later.</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that for actually training a model, we will use different data loaders</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co"># with a lower batch size.</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> data.DataLoader(train_set,</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                               batch_size<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                               shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>                               drop_last<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>                               collate_fn<span class="op">=</span>numpy_collate)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>val_loader   <span class="op">=</span> data.DataLoader(val_set,</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>                               batch_size<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>                               shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>                               drop_last<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>                               collate_fn<span class="op">=</span>numpy_collate)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>test_loader  <span class="op">=</span> data.DataLoader(test_set,</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>                               batch_size<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>                               shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>                               drop_last<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>                               collate_fn<span class="op">=</span>numpy_collate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In comparison to the previous tutorial, we have changed the parameters of the normalization transformation in <code>image_to_numpy</code>. The normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels. This will be particularly relevant for the discussion about initialization we will look at below, and hence we change it here. It should be noted that in most classification tasks, both normalization techniques (between -1 and 1 or mean 0 and stddev 1) have shown to work well. We can calculate the normalization parameters by determining the mean and standard deviation on the original images:</p>
<div id="cell-13" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean"</span>, (train_dataset.data.<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span>).mean().item())</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Std"</span>, (train_dataset.data.<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span>).std().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean 0.28604060411453247
Std 0.3530242443084717</code></pre>
</div>
</div>
<p>We can verify the transformation by looking at the statistics of a single batch:</p>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>imgs, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>imgs<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>imgs<span class="sc">.</span>std()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Maximum: </span><span class="sc">{</span>imgs<span class="sc">.</span><span class="bu">max</span>()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimum: </span><span class="sc">{</span>imgs<span class="sc">.</span><span class="bu">min</span>()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean: 0.008
Standard deviation: 1.009
Maximum: 2.022
Minimum: -0.810</code></pre>
</div>
</div>
<p>Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST.</p>
<p>Next, we create a linear neural network. We use the same setup as in the previous tutorial.</p>
<div id="cell-17" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Network</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseNetwork(nn.Module):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    act_fn : Callable</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    num_classes : <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    hidden_sizes : Sequence <span class="op">=</span> (<span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    kernel_init : Callable <span class="op">=</span> nn.linear.default_kernel_init</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@nn.compact</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x, return_activations<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># Reshape images to a flat vector</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We collect all activations throughout the network for later visualizations</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remember that in jitted functions, unused tensors will anyways be removed.</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> []</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hd <span class="kw">in</span> <span class="va">self</span>.hidden_sizes:</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> nn.Dense(hd,</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                         kernel_init<span class="op">=</span><span class="va">self</span>.kernel_init)(x)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>            activations.append(x)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.act_fn(x)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>            activations.append(x)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.Dense(<span class="va">self</span>.num_classes,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>                     kernel_init<span class="op">=</span><span class="va">self</span>.kernel_init)(x)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        activations.append(x)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="cf">if</span> <span class="kw">not</span> return_activations <span class="cf">else</span> (x, activations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For the activation functions, we make use of JAX’s and Flax’s library instead of implementing ourselves. However, we also define an <code>Identity</code> activation function. Although this activation function would significantly limit the network’s modeling capabilities, we will use it in the first steps of our discussion about initialization (for simplicity).</p>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>act_fn_by_name <span class="op">=</span> {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tanh"</span>: nn.tanh,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"relu"</span>: nn.relu,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"identity"</span>: <span class="kw">lambda</span> x: x</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we define a few plotting functions that we will use for our discussions. These functions help us to (1) visualize the weight/parameter distribution inside a network, (2) visualize the gradients that the parameters at different layers receive, and (3) the activations, i.e.&nbsp;the output of the linear layers. The detailed code is not important, but feel free to take a closer look if interested.</p>
<div id="cell-21" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dists(val_dict, color<span class="op">=</span><span class="st">"C0"</span>, xlabel<span class="op">=</span><span class="va">None</span>, stat<span class="op">=</span><span class="st">"count"</span>, use_kde<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> <span class="bu">len</span>(val_dict)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, columns, figsize<span class="op">=</span>(columns<span class="op">*</span><span class="dv">3</span>, <span class="fl">2.5</span>))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    fig_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(val_dict.keys()):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        key_ax <span class="op">=</span> ax[fig_index<span class="op">%</span>columns]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        sns.histplot(val_dict[key], ax<span class="op">=</span>key_ax, color<span class="op">=</span>color, bins<span class="op">=</span><span class="dv">50</span>, stat<span class="op">=</span>stat,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                     kde<span class="op">=</span>use_kde <span class="kw">and</span> ((val_dict[key].<span class="bu">max</span>()<span class="op">-</span>val_dict[key].<span class="bu">min</span>())<span class="op">&gt;</span><span class="fl">1e-8</span>)) <span class="co"># Only plot kde if there is variance</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        key_ax.set_title(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> "</span> <span class="op">+</span> (<span class="vs">r"(</span><span class="sc">%i</span><span class="vs"> $\to$ </span><span class="sc">%i</span><span class="vs">)"</span> <span class="op">%</span> (val_dict[key].shape[<span class="dv">1</span>], val_dict[key].shape[<span class="dv">0</span>]) <span class="cf">if</span> <span class="bu">len</span>(val_dict[key].shape)<span class="op">&gt;</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">""</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> xlabel <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            key_ax.set_xlabel(xlabel)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        fig_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    fig.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_weight_distribution(params, color<span class="op">=</span><span class="st">"C0"</span>):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    params, _ <span class="op">=</span> jax.tree_util.tree_flatten(params)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> [p.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> p <span class="kw">in</span> params <span class="cf">if</span> <span class="bu">len</span>(p.shape) <span class="op">&gt;</span> <span class="dv">1</span>]  <span class="co"># Remove biases</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> jax.device_get(params)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> {<span class="ss">f'Layer </span><span class="sc">{</span>layer_idx<span class="op">*</span><span class="dv">2</span><span class="sc">}</span><span class="ss">'</span>: p <span class="cf">for</span> layer_idx, p <span class="kw">in</span> <span class="bu">enumerate</span>(params)}</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Plotting</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plot_dists(weights, color<span class="op">=</span>color, xlabel<span class="op">=</span><span class="st">"Weight vals"</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"Weight distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    plt.close() </span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>small_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>numpy_collate)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>exmp_imgs, exmp_labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(small_loader))</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_gradients(model, params, color<span class="op">=</span><span class="st">"C0"</span>, print_variance<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co">        net - Object of class BaseNetwork</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="co">        color - Color in which we want to visualize the histogram (for easier separation of activation functions)</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pass one batch through the network, and calculate the gradients for the weights</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss_func(p):</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model.<span class="bu">apply</span>(p, exmp_imgs)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> optax.softmax_cross_entropy_with_integer_labels(logits, exmp_labels).mean()</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> jax.grad(loss_func)(params)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> jax.device_get(grads)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> jax.tree_util.tree_leaves(grads)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> [g.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> g <span class="kw">in</span> grads <span class="cf">if</span> <span class="bu">len</span>(g.shape) <span class="op">&gt;</span> <span class="dv">1</span>]</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> {<span class="ss">f'Layer </span><span class="sc">{</span>layer_idx<span class="op">*</span><span class="dv">2</span><span class="sc">}</span><span class="ss">'</span>: g <span class="cf">for</span> layer_idx, g <span class="kw">in</span> <span class="bu">enumerate</span>(grads)}</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Plotting</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plot_dists(grads, color<span class="op">=</span>color, xlabel<span class="op">=</span><span class="st">"Grad magnitude"</span>)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"Gradient distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>    plt.close() </span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_variance:</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(grads.keys()):</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> - Variance: </span><span class="sc">{</span>np<span class="sc">.</span>var(grads[key])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_activations(model, params, color<span class="op">=</span><span class="st">"C0"</span>, print_variance<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pass one batch through the network, and calculate the activations</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    _, activations <span class="op">=</span> model.<span class="bu">apply</span>(params, exmp_imgs, return_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> {<span class="ss">f'Layer </span><span class="sc">{</span>layer_idx<span class="op">*</span><span class="dv">2</span><span class="sc">}</span><span class="ss">'</span>: act.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> layer_idx, act <span class="kw">in</span> <span class="bu">enumerate</span>(activations[::<span class="dv">2</span>])}</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Plotting</span></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plot_dists(activations, color<span class="op">=</span>color, stat<span class="op">=</span><span class="st">"density"</span>, xlabel<span class="op">=</span><span class="st">"Activation vals"</span>)</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"Activation distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>    plt.close() </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_variance:</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(activations.keys()):</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> - Variance: </span><span class="sc">{</span>np<span class="sc">.</span>var(activations[key])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="initialization" class="level2">
<h2 class="anchored" data-anchor-id="initialization">Initialization</h2>
<p>Before starting our discussion about initialization, it should be noted that there exist many very good blog posts about the topic of neural network initialization (for example <a href="https://www.deeplearning.ai/ai-notes/initialization/">deeplearning.ai</a>, or a more <a href="https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K">math-focused blog post</a>). In case something remains unclear after this tutorial, we recommend skimming through these blog posts as well.</p>
<p>When initializing a neural network, there are a few properties we would like to have. First, the variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons. If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value. Similarly, if the variance increases, it is likely to explode (i.e.&nbsp;head to infinity) the deeper we design our model. The second property we look out for in initialization techniques is a gradient distribution with equal variance across layers. If the first layer receives much smaller gradients than the last layer, we will have difficulties in choosing an appropriate learning rate.</p>
<p>As a starting point for finding a good method, we will analyze different initialization based on our linear neural network with no activation function (i.e.&nbsp;an identity). We do this because initializations depend on the specific activation function used in the network, and we can adjust the initialization schemes later on for our specific choice.</p>
<div id="cell-23" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_simple_model(kernel_init, act_fn<span class="op">=</span>act_fn_by_name[<span class="st">'identity'</span>]):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>act_fn,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                        kernel_init<span class="op">=</span>kernel_init)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> model.init(random.PRNGKey(<span class="dv">42</span>), exmp_imgs)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that in contrast to the <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html">PyTorch version</a> of this tutorial, we keep the bias initialization fixed. As the default, JAX uses a zero initialization. In practice, the initialization of the bias is less relevant as long as it is reasonably close to zero, since it is only a (learnable) constant added to the features.</p>
<section id="constant-initialization" class="level3">
<h3 class="anchored" data-anchor-id="constant-initialization">Constant initialization</h3>
<p>The first initialization we can consider is to initialize all weights with the same constant value. Intuitively, setting all weights to zero is not a good idea as the propagated gradient will be zero. However, what happens if we set all weights to a value slightly larger or smaller than 0? To find out, we can implement a function for setting all parameters below and visualize the gradients.</p>
<div id="cell-26" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An initialization function in JAX takes as input a PRNG key,</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the shape of the parameter to create, and the data type </span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (usually jnp.float32). We create this function based on the</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># input parameter 'c' here, indicating the constant value</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_const_init_func(c<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="kw">lambda</span> key, shape, dtype: c<span class="op">*</span>jnp.ones(shape, dtype<span class="op">=</span>dtype)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(get_const_init_func(c<span class="op">=</span><span class="fl">0.005</span>))</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>visualize_gradients(model, params)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-12-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-12-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 2.0582759380340576
Layer 2 - Variance: 13.489117622375488
Layer 4 - Variance: 22.100563049316406
Layer 6 - Variance: 36.20956039428711
Layer 8 - Variance: 14.831436157226562</code></pre>
</div>
</div>
<p>As we can see, only the first and the last layer have diverse gradient distributions while the other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it). Having the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters. This would make our layer useless and reduce our effective number of parameters to 1. Thus, we cannot use a constant initialization to train our networks.</p>
</section>
<section id="constant-variance" class="level3">
<h3 class="anchored" data-anchor-id="constant-variance">Constant variance</h3>
<p>From the experiment above, we have seen that a constant value is not working. So instead, how about we initialize the parameters by randomly sampling from a distribution like a Gaussian? The most intuitive way would be to choose one variance that is used for all layers in the network. Let’s implement it below, and visualize the activation distribution across layers.</p>
<div id="cell-29" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_var_init_func(std<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="kw">lambda</span> key, shape, dtype: std<span class="op">*</span>random.normal(key, shape, dtype<span class="op">=</span>dtype)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(get_var_init_func(std<span class="op">=</span><span class="fl">0.01</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-13-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 0.08219309151172638
Layer 2 - Variance: 0.00415463000535965
Layer 4 - Variance: 0.00010465682862559333
Layer 6 - Variance: 2.8326080609986093e-06
Layer 8 - Variance: 3.75452451351066e-08</code></pre>
</div>
</div>
<p>The variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer. Alternatively, we could use a higher standard deviation:</p>
<div id="cell-31" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(get_var_init_func(std<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-14-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 8.21930980682373
Layer 2 - Variance: 41.54630661010742
Layer 4 - Variance: 104.6568603515625
Layer 6 - Variance: 283.26092529296875
Layer 8 - Variance: 375.4525451660156</code></pre>
</div>
</div>
<p>With a higher standard deviation, the activations are likely to explode. You can play around with the specific standard deviation values, but it will be hard to find one that gives us a good activation distribution across layers and is very specific to our model. If we would change the hidden sizes or number of layers, you would have to search all over again, which is neither efficient nor recommended.</p>
</section>
<section id="how-to-find-appropriate-initialization-values" class="level3">
<h3 class="anchored" data-anchor-id="how-to-find-appropriate-initialization-values">How to find appropriate initialization values</h3>
<p>From our experiments above, we have seen that we need to sample the weights from a distribution, but are not sure which one exactly. As a next step, we will try to find the optimal initialization from the perspective of the activation distribution. For this, we state two requirements:</p>
<ol type="1">
<li>The mean of the activations should be zero</li>
<li>The variance of the activations should stay the same across every layer</li>
</ol>
<p>Suppose we want to design an initialization for the following layer: <span class="math inline">\(y=Wx+b\)</span> with <span class="math inline">\(y\in\mathbb{R}^{d_y}\)</span>, <span class="math inline">\(x\in\mathbb{R}^{d_x}\)</span>. Our goal is that the variance of each element of <span class="math inline">\(y\)</span> is the same as the input, i.e.&nbsp;<span class="math inline">\(\text{Var}(y_i)=\text{Var}(x_i)=\sigma_x^{2}\)</span>, and that the mean is zero. We assume <span class="math inline">\(x\)</span> to also have a mean of zero, because, in deep neural networks, <span class="math inline">\(y\)</span> would be the input of another layer. This requires the bias and weight to have an expectation of 0. Actually, as <span class="math inline">\(b\)</span> is a single element per output neuron and is constant across different inputs, we set it to 0 overall.</p>
<p>Next, we need to calculate the variance with which we need to initialize the weight parameters. Along the calculation, we will need the following variance rule: given two independent variables, the variance of their product is <span class="math inline">\(\text{Var}(X\cdot Y) = \mathbb{E}(Y)^2\text{Var}(X) + \mathbb{E}(X)^2\text{Var}(Y) + \text{Var}(X)\text{Var}(Y) = \mathbb{E}(Y^2)\mathbb{E}(X^2)-\mathbb{E}(Y)^2\mathbb{E}(X)^2\)</span> (<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not refering to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, but any random variable).</p>
<p>The needed variance of the weights, <span class="math inline">\(\text{Var}(w_{ij})\)</span>, is calculated as follows:</p>
<p><span class="math display">\[
\begin{split}
    y_i &amp; = \sum_{j} w_{ij}x_{j}\hspace{10mm}\text{Calculation of a single output neuron without bias}\\
    \text{Var}(y_i) = \sigma_x^{2} &amp; = \text{Var}\left(\sum_{j} w_{ij}x_{j}\right)\\
    &amp; = \sum_{j} \text{Var}(w_{ij}x_{j}) \hspace{10mm}\text{Inputs and weights are independent of each other}\\
    &amp; = \sum_{j} \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance rule (see above) with expectations being zero}\\
    &amp; = d_x \cdot \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance equal for all $d_x$ elements}\\
    &amp; = \sigma_x^{2} \cdot d_x \cdot \text{Var}(w_{ij})\\
    \Rightarrow \text{Var}(w_{ij}) = \sigma_{W}^2 &amp; = \frac{1}{d_x}\\
\end{split}
\]</span></p>
<p>Thus, we should initialize the weight distribution with a variance of the inverse of the input dimension <span class="math inline">\(d_x\)</span>. Let’s implement it below and check whether this holds:</p>
<div id="cell-34" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>equal_var_init <span class="op">=</span> <span class="kw">lambda</span> key, shape, dtype: <span class="fl">1.0</span><span class="op">/</span>np.sqrt(shape[<span class="dv">0</span>]) <span class="op">*</span> random.normal(key, shape, dtype<span class="op">=</span>dtype)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(equal_var_init)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>visualize_weight_distribution(params)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-15-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-15-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.0483813285827637
Layer 2 - Variance: 1.0350143909454346
Layer 4 - Variance: 1.018454670906067
Layer 6 - Variance: 1.0767643451690674
Layer 8 - Variance: 1.1150107383728027</code></pre>
</div>
</div>
<p>As we expected, the variance stays indeed constant across layers. Note that our initialization does not restrict us to a normal distribution, but allows any other distribution with a mean of 0 and variance of <span class="math inline">\(1/d_x\)</span>. You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.</p>
<p>Besides the variance of the activations, another variance we would like to stabilize is the one of the gradients. This ensures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from <span class="math inline">\(\Delta x=W\Delta y\)</span>, and come to the conclusion that we should initialize our layers with <span class="math inline">\(1/d_y\)</span> where <span class="math inline">\(d_y\)</span> is the number of output neurons. You can do the calculation as a practice, or check a thorough explanation in <a href="https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K">this blog post</a>. As a compromise between both constraints, <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi">Glorot and Bengio (2010)</a> proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:</p>
<p><span class="math display">\[W\sim \mathcal{N}\left(0,\frac{2}{d_x+d_y}\right)\]</span></p>
<p>If we use a uniform distribution, we would initialize the weights with:</p>
<p><span class="math display">\[W\sim U\left[-\frac{\sqrt{6}}{\sqrt{d_x+d_y}}, \frac{\sqrt{6}}{\sqrt{d_x+d_y}}\right]\]</span></p>
<p>Let’s shortly implement it and validate its effectiveness:</p>
<div id="cell-36" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> xavier_init(key, shape, dtype):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    bound <span class="op">=</span> math.sqrt(<span class="dv">6</span>)<span class="op">/</span>math.sqrt(shape[<span class="dv">0</span>]<span class="op">+</span>shape[<span class="dv">1</span>])</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.uniform(key, shape, dtype,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                          minval<span class="op">=-</span>bound, maxval<span class="op">=</span>bound)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(xavier_init)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>visualize_gradients(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-16-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 0.00029831365100108087
Layer 2 - Variance: 0.0005463268025778234
Layer 4 - Variance: 0.0007796576828695834
Layer 6 - Variance: 0.0011752902064472437
Layer 8 - Variance: 0.011736484244465828</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-16-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.2806072235107422
Layer 2 - Variance: 1.6784099340438843
Layer 4 - Variance: 1.6767947673797607
Layer 6 - Variance: 2.5439999103546143
Layer 8 - Variance: 4.41333532333374</code></pre>
</div>
</div>
<p>We see that the Xavier initialization balances the variance of gradients and activations. Note that the significantly higher variance for the output layer is due to the large difference of input and output dimension (<span class="math inline">\(128\)</span> vs <span class="math inline">\(10\)</span>). However, we currently assumed the activation function to be linear. So what happens if we add a non-linearity? In a tanh-based network, a common assumption is that for small values during the initial steps in training, the <span class="math inline">\(\tanh\)</span> works as a linear function such that we don’t have to adjust our calculation. We can check if that is the case for us as well:</p>
<div id="cell-38" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(xavier_init, act_fn<span class="op">=</span>nn.tanh)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>visualize_gradients(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-17-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.520933165011229e-05
Layer 2 - Variance: 2.4512757590855472e-05
Layer 4 - Variance: 3.6233024729881436e-05
Layer 6 - Variance: 4.8036039515864104e-05
Layer 8 - Variance: 0.0004025915695820004</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-17-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.2806072235107422
Layer 2 - Variance: 0.5610313415527344
Layer 4 - Variance: 0.29278191924095154
Layer 6 - Variance: 0.2838047742843628
Layer 8 - Variance: 0.39267846941947937</code></pre>
</div>
</div>
<p>Although the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values. Therefore, our variance will stabilize around 0.25 if we would go even deeper. Hence, we can conclude that the Xavier initialization works well for Tanh networks. But what about ReLU networks? Here, we cannot take the previous assumption of the non-linearity becoming linear for small values. The ReLU activation function sets (in expectation) half of the inputs to 0 so that also the expectation of the input is not zero. However, as long as the expectation of <span class="math inline">\(W\)</span> is zero and <span class="math inline">\(b=0\)</span>, the expectation of the output is zero. The part where the calculation of the ReLU initialization differs from the identity is when determining <span class="math inline">\(\text{Var}(w_{ij}x_{j})\)</span>:</p>
<p><span class="math display">\[\text{Var}(w_{ij}x_{j})=\underbrace{\mathbb{E}[w_{ij}^2]}_{=\text{Var}(w_{ij})}\mathbb{E}[x_{j}^2]-\underbrace{\mathbb{E}[w_{ij}]^2}_{=0}\mathbb{E}[x_{j}]^2=\text{Var}(w_{ij})\mathbb{E}[x_{j}^2]\]</span></p>
<p>If we assume now that <span class="math inline">\(x\)</span> is the output of a ReLU activation (from a previous layer, <span class="math inline">\(x=max(0,\tilde{y})\)</span>), we can calculate the expectation as follows:</p>
<p><span class="math display">\[
\begin{split}
    \mathbb{E}[x^2] &amp; =\mathbb{E}[\max(0,\tilde{y})^2]\\
                    &amp; =\frac{1}{2}\mathbb{E}[{\tilde{y}}^2]\hspace{2cm}\tilde{y}\text{ is zero-centered and symmetric}\\
                    &amp; =\frac{1}{2}\text{Var}(\tilde{y})
\end{split}\]</span></p>
<p>Thus, we see that we have an additional factor of 1/2 in the equation, so that our desired weight variance becomes <span class="math inline">\(2/d_x\)</span>. This gives us the Kaiming initialization (see <a href="https://arxiv.org/pdf/1502.01852.pdf">He, K. et al.&nbsp;(2015)</a>). Note that the Kaiming initialization does not use the harmonic mean between input and output size. In their paper (Section 2.2, Backward Propagation, last paragraph), they argue that using <span class="math inline">\(d_x\)</span> or <span class="math inline">\(d_y\)</span> both lead to stable gradients throughout the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input <span class="math inline">\(d_x\)</span>:</p>
<div id="cell-40" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>num_input_feats <span class="op">=</span> np.prod(exmp_imgs.shape[<span class="dv">1</span>:])</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kaiming_init(key, shape, dtype):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The first layer does not have ReLU applied on its input</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note that this detection only works if we do not use 784 </span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># feature size anywhere - better to explicitly handle </span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer numbers</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> shape[<span class="dv">0</span>] <span class="op">==</span> num_input_feats:</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>np.sqrt(shape[<span class="dv">0</span>])</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> np.sqrt(<span class="dv">2</span><span class="op">/</span>shape[<span class="dv">0</span>])</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> std <span class="op">*</span> random.normal(key, shape, dtype)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(kaiming_init, act_fn<span class="op">=</span>nn.relu)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>visualize_gradients(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>visualize_activations(model, params, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-18-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 3.25944201904349e-05
Layer 2 - Variance: 4.664550942834467e-05
Layer 4 - Variance: 6.605686212424189e-05
Layer 6 - Variance: 0.00015451121726073325
Layer 8 - Variance: 0.002561226487159729</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-18-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.0483813285827637
Layer 2 - Variance: 1.0137324333190918
Layer 4 - Variance: 0.9987665414810181
Layer 6 - Variance: 0.8383973836898804
Layer 8 - Variance: 1.316560983657837</code></pre>
</div>
</div>
<p>The variance stays stable across layers. We can conclude that the Kaiming initialization indeed works well for ReLU-based networks. Note that for Leaky-ReLU etc., we have to slightly adjust the factor of <span class="math inline">\(2\)</span> in the variance as half of the values are not set to zero anymore.</p>
</section>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<p>Besides initialization, selecting a suitable optimization algorithm can be an important choice for deep neural networks. Before taking a closer look at them, we should define code for training the models. Most of the following code is copied from the previous tutorial, and only slightly altered to fit our needs.</p>
<div id="cell-43" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _get_config_file(model_path, model_name):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Name of the file for storing hyperparameter details</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">".config"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _get_model_file(model_path, model_name):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Name of the file for storing network parameters</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">".tar"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _get_result_file(model_path, model_name):</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">"_results.json"</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_model(model_path, model_name, state<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Loads a saved model from disk.</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co">        model_path - Path of the checkpoint directory</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="co">        model_name - Name of the model (str)</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="co">        state - (Optional) If given, the parameters are loaded into this training state. Otherwise,</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co">                a new one is created alongside a network architecture.</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    config_file, model_file <span class="op">=</span> _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> os.path.isfile(config_file), <span class="ss">f"Could not find the config file </span><span class="ch">\"</span><span class="sc">{</span>config_file<span class="sc">}</span><span class="ch">\"</span><span class="ss">. Are you sure this is the correct path and you have your model config stored here?"</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> os.path.isfile(model_file), <span class="ss">f"Could not find the model file </span><span class="ch">\"</span><span class="sc">{</span>model_file<span class="sc">}</span><span class="ch">\"</span><span class="ss">. Are you sure this is the correct path and you have your model stored here?"</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(config_file, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        config_dict <span class="op">=</span> json.load(f)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        net <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>nn.relu, <span class="op">**</span>config_dict)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> train_state.TrainState(step<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>                                       params<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>                                       apply_fn<span class="op">=</span>net.<span class="bu">apply</span>,</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>                                       tx<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>                                       opt_state<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        net <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># You can also use flax's checkpoint package. To show an alternative,</span></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># you can instead load the parameters simply from a pickle file.</span></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(model_file, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> pickle.load(f)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> state.replace(params<span class="op">=</span>params)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> state, net</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_model(model, params, model_path, model_name):</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Given a model, we save the parameters and hyperparameters.</span></span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a><span class="co">        model - Network object without parameters</span></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a><span class="co">        params - Parameters to save of the model</span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a><span class="co">        model_path - Path of the checkpoint directory</span></span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a><span class="co">        model_name - Name of the model (str)</span></span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>    config_dict <span class="op">=</span> {<span class="st">'hidden_sizes'</span>: model.hidden_sizes,</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'num_classes'</span>: model.num_classes}</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>    os.makedirs(model_path, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>    config_file, model_file <span class="op">=</span> _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(config_file, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        json.dump(config_dict, f)</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># You can also use flax's checkpoint package. To show an alternative,</span></span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># you can instead save the parameters simply in a pickle file.</span></span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(model_file, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>        pickle.dump(params, f)</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_loss(params, apply_fn, batch):</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    imgs, labels <span class="op">=</span> batch</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> apply_fn(params, imgs)</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> (labels <span class="op">==</span> logits.argmax(axis<span class="op">=-</span><span class="dv">1</span>)).mean()</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, acc</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(state, batch):</span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>    grad_fn <span class="op">=</span> jax.value_and_grad(calculate_loss,</span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>                                 has_aux<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>    (_, acc), grads <span class="op">=</span> grad_fn(state.params, state.apply_fn, batch)</span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> state.apply_gradients(grads<span class="op">=</span>grads)</span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> state, acc</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_step(state, batch):</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>    _, acc <span class="op">=</span> calculate_loss(state.params, state.apply_fn, batch)</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(net, params, optimizer, model_name, max_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">256</span>, overwrite<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a><span class="co">    Train a model on the training set of FashionMNIST</span></span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a><span class="co">        net - Object of BaseNetwork</span></span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a><span class="co">        params - The parameters to use as initialization</span></span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer - Optimizer to use</span></span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a><span class="co">        model_name - (str) Name of the model, used for creating the checkpoint names</span></span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a><span class="co">        max_epochs - Number of epochs we want to (maximally) train for</span></span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size - Size of batches used in training</span></span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a><span class="co">        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.</span></span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-97"><a href="#cb32-97" aria-hidden="true" tabindex="-1"></a>    file_exists <span class="op">=</span> os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))</span>
<span id="cb32-98"><a href="#cb32-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> file_exists <span class="kw">and</span> <span class="kw">not</span> overwrite:</span>
<span id="cb32-99"><a href="#cb32-99" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Model file already exists. Skipping training..."</span>)</span>
<span id="cb32-100"><a href="#cb32-100" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-101"><a href="#cb32-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(_get_result_file(CHECKPOINT_PATH, model_name), <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb32-102"><a href="#cb32-102" aria-hidden="true" tabindex="-1"></a>            results <span class="op">=</span> json.load(f)</span>
<span id="cb32-103"><a href="#cb32-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-104"><a href="#cb32-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> file_exists:</span>
<span id="cb32-105"><a href="#cb32-105" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Model file exists, but will be overwritten..."</span>)</span>
<span id="cb32-106"><a href="#cb32-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-107"><a href="#cb32-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initializing training state</span></span>
<span id="cb32-108"><a href="#cb32-108" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-109"><a href="#cb32-109" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> train_state.TrainState.create(apply_fn<span class="op">=</span>net.<span class="bu">apply</span>,</span>
<span id="cb32-110"><a href="#cb32-110" aria-hidden="true" tabindex="-1"></a>                                              params<span class="op">=</span>params,</span>
<span id="cb32-111"><a href="#cb32-111" aria-hidden="true" tabindex="-1"></a>                                              tx<span class="op">=</span>optimizer)</span>
<span id="cb32-112"><a href="#cb32-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-113"><a href="#cb32-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Defining data loader</span></span>
<span id="cb32-114"><a href="#cb32-114" aria-hidden="true" tabindex="-1"></a>        train_loader_local <span class="op">=</span> data.DataLoader(train_set,</span>
<span id="cb32-115"><a href="#cb32-115" aria-hidden="true" tabindex="-1"></a>                                             batch_size<span class="op">=</span>batch_size,</span>
<span id="cb32-116"><a href="#cb32-116" aria-hidden="true" tabindex="-1"></a>                                             shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-117"><a href="#cb32-117" aria-hidden="true" tabindex="-1"></a>                                             drop_last<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-118"><a href="#cb32-118" aria-hidden="true" tabindex="-1"></a>                                             collate_fn<span class="op">=</span>numpy_collate,</span>
<span id="cb32-119"><a href="#cb32-119" aria-hidden="true" tabindex="-1"></a>                                             generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">42</span>))</span>
<span id="cb32-120"><a href="#cb32-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-121"><a href="#cb32-121" aria-hidden="true" tabindex="-1"></a>        train_scores <span class="op">=</span> []</span>
<span id="cb32-122"><a href="#cb32-122" aria-hidden="true" tabindex="-1"></a>        val_scores <span class="op">=</span> []</span>
<span id="cb32-123"><a href="#cb32-123" aria-hidden="true" tabindex="-1"></a>        best_val_epoch <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb32-124"><a href="#cb32-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(max_epochs):</span>
<span id="cb32-125"><a href="#cb32-125" aria-hidden="true" tabindex="-1"></a>            <span class="co">############</span></span>
<span id="cb32-126"><a href="#cb32-126" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Training #</span></span>
<span id="cb32-127"><a href="#cb32-127" aria-hidden="true" tabindex="-1"></a>            <span class="co">############</span></span>
<span id="cb32-128"><a href="#cb32-128" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb32-129"><a href="#cb32-129" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch <span class="kw">in</span> tqdm(train_loader_local, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>, leave<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb32-130"><a href="#cb32-130" aria-hidden="true" tabindex="-1"></a>                state, acc <span class="op">=</span> train_step(state, batch)</span>
<span id="cb32-131"><a href="#cb32-131" aria-hidden="true" tabindex="-1"></a>                train_acc <span class="op">+=</span> acc</span>
<span id="cb32-132"><a href="#cb32-132" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">/=</span> <span class="bu">len</span>(train_loader_local)</span>
<span id="cb32-133"><a href="#cb32-133" aria-hidden="true" tabindex="-1"></a>            train_scores.append(train_acc.item())</span>
<span id="cb32-134"><a href="#cb32-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-135"><a href="#cb32-135" aria-hidden="true" tabindex="-1"></a>            <span class="co">##############</span></span>
<span id="cb32-136"><a href="#cb32-136" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Validation #</span></span>
<span id="cb32-137"><a href="#cb32-137" aria-hidden="true" tabindex="-1"></a>            <span class="co">##############</span></span>
<span id="cb32-138"><a href="#cb32-138" aria-hidden="true" tabindex="-1"></a>            val_acc <span class="op">=</span> test_model(state, val_loader)</span>
<span id="cb32-139"><a href="#cb32-139" aria-hidden="true" tabindex="-1"></a>            val_scores.append(val_acc)</span>
<span id="cb32-140"><a href="#cb32-140" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:2d}</span><span class="ss">] Training accuracy: </span><span class="sc">{</span>train_acc<span class="sc">:05.2%}</span><span class="ss">, Validation accuracy: </span><span class="sc">{</span>val_acc<span class="sc">:4.2%}</span><span class="ss">"</span>)</span>
<span id="cb32-141"><a href="#cb32-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-142"><a href="#cb32-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(val_scores) <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> val_acc <span class="op">&gt;</span> val_scores[best_val_epoch]:</span>
<span id="cb32-143"><a href="#cb32-143" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"</span><span class="ch">\t</span><span class="st">   (New best performance, saving model...)"</span>)</span>
<span id="cb32-144"><a href="#cb32-144" aria-hidden="true" tabindex="-1"></a>                save_model(net, state.params, CHECKPOINT_PATH, model_name)</span>
<span id="cb32-145"><a href="#cb32-145" aria-hidden="true" tabindex="-1"></a>                best_val_epoch <span class="op">=</span> epoch</span>
<span id="cb32-146"><a href="#cb32-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-147"><a href="#cb32-147" aria-hidden="true" tabindex="-1"></a>    state, _ <span class="op">=</span> load_model(CHECKPOINT_PATH, model_name, state<span class="op">=</span>state)</span>
<span id="cb32-148"><a href="#cb32-148" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> results <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb32-149"><a href="#cb32-149" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> test_model(state, test_loader)</span>
<span id="cb32-150"><a href="#cb32-150" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> {<span class="st">"test_acc"</span>: test_acc, <span class="st">"val_scores"</span>: val_scores, </span>
<span id="cb32-151"><a href="#cb32-151" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"train_scores"</span>: train_scores}</span>
<span id="cb32-152"><a href="#cb32-152" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(_get_result_file(CHECKPOINT_PATH, model_name), <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb32-153"><a href="#cb32-153" aria-hidden="true" tabindex="-1"></a>            json.dump(results, f)</span>
<span id="cb32-154"><a href="#cb32-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-155"><a href="#cb32-155" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot a curve of the validation accuracy</span></span>
<span id="cb32-156"><a href="#cb32-156" aria-hidden="true" tabindex="-1"></a>    sns.<span class="bu">set</span>()</span>
<span id="cb32-157"><a href="#cb32-157" aria-hidden="true" tabindex="-1"></a>    plt.plot([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(results[<span class="st">"train_scores"</span>])<span class="op">+</span><span class="dv">1</span>)], results[<span class="st">"train_scores"</span>], label<span class="op">=</span><span class="st">"Train"</span>)</span>
<span id="cb32-158"><a href="#cb32-158" aria-hidden="true" tabindex="-1"></a>    plt.plot([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(results[<span class="st">"val_scores"</span>])<span class="op">+</span><span class="dv">1</span>)], results[<span class="st">"val_scores"</span>], label<span class="op">=</span><span class="st">"Val"</span>)</span>
<span id="cb32-159"><a href="#cb32-159" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb32-160"><a href="#cb32-160" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Validation accuracy"</span>)</span>
<span id="cb32-161"><a href="#cb32-161" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="bu">min</span>(results[<span class="st">"val_scores"</span>]), <span class="bu">max</span>(results[<span class="st">"train_scores"</span>])<span class="op">*</span><span class="fl">1.01</span>)</span>
<span id="cb32-162"><a href="#cb32-162" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Validation performance of </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-163"><a href="#cb32-163" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb32-164"><a href="#cb32-164" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb32-165"><a href="#cb32-165" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb32-166"><a href="#cb32-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-167"><a href="#cb32-167" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>((<span class="ss">f" Test accuracy: </span><span class="sc">{</span>results[<span class="st">'test_acc'</span>]<span class="sc">:4.2%}</span><span class="ss"> "</span>).center(<span class="dv">50</span>, <span class="st">"="</span>)<span class="op">+</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb32-168"><a href="#cb32-168" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> state</span>
<span id="cb32-169"><a href="#cb32-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-170"><a href="#cb32-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-171"><a href="#cb32-171" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_model(state, data_loader):</span>
<span id="cb32-172"><a href="#cb32-172" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-173"><a href="#cb32-173" aria-hidden="true" tabindex="-1"></a><span class="co">    Test a model on a specified dataset.</span></span>
<span id="cb32-174"><a href="#cb32-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-175"><a href="#cb32-175" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb32-176"><a href="#cb32-176" aria-hidden="true" tabindex="-1"></a><span class="co">        state - Training state including parameters and model apply function.</span></span>
<span id="cb32-177"><a href="#cb32-177" aria-hidden="true" tabindex="-1"></a><span class="co">        data_loader - DataLoader object of the dataset to test on (validation or test)</span></span>
<span id="cb32-178"><a href="#cb32-178" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-179"><a href="#cb32-179" aria-hidden="true" tabindex="-1"></a>    true_preds, count <span class="op">=</span> <span class="fl">0.</span>, <span class="dv">0</span></span>
<span id="cb32-180"><a href="#cb32-180" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> data_loader:</span>
<span id="cb32-181"><a href="#cb32-181" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">=</span> eval_step(state, batch)</span>
<span id="cb32-182"><a href="#cb32-182" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> batch[<span class="dv">0</span>].shape[<span class="dv">0</span>]</span>
<span id="cb32-183"><a href="#cb32-183" aria-hidden="true" tabindex="-1"></a>        true_preds <span class="op">+=</span> acc <span class="op">*</span> batch_size</span>
<span id="cb32-184"><a href="#cb32-184" aria-hidden="true" tabindex="-1"></a>        count <span class="op">+=</span> batch_size</span>
<span id="cb32-185"><a href="#cb32-185" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> true_preds <span class="op">/</span> count</span>
<span id="cb32-186"><a href="#cb32-186" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_acc.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network’s parameters given the gradients. Hence, we effectively implement a function <span class="math inline">\(w^{t} = f(w^{t-1}, g^{t}, ...)\)</span> with <span class="math inline">\(w\)</span> being the parameters, and <span class="math inline">\(g^{t} = \nabla_{w^{(t-1)}} \mathcal{L}^{(t)}\)</span> the gradients at time step <span class="math inline">\(t\)</span>. A common, additional parameter to this function is the learning rate, here denoted by <span class="math inline">\(\eta\)</span>. Usually, the learning rate can be seen as the “step size” of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps.</p>
<p>As most optimizers only differ in the implementation of <span class="math inline">\(f\)</span>, we can define a template for an optimizer in JAX below. Note that in comparison to PyTorch, the optimizers must be functional and have no effects outside the function. Hence, we follow the basic implementation principles of <a href="https://optax.readthedocs.io/en/latest/">optax</a> and define an optimizer to be a tuple of two main functions:</p>
<ul>
<li><code>init</code>: Given a set of parameters, initialize any state variables that the optimizer may need. While is might not be necessary for the simplest optimizers, more sophisticated like Adam need to keep track of gradient statistics, which can be initialized here.</li>
<li><code>update</code>: Given the gradients, the optimizer state, and the parameters, return the updates that should be applied to the parameters, and an updated optimizer state.</li>
</ul>
<p>The updates can be applied to the parameters using <code>optax.apply_updates(params, updates)</code> which adds each update value to its respective parameter. In simplified form, it applies <code>tree_map(lambda p, u: p + u, params, updates)</code>. As a reminder, <code>jax.tree_util.tree_map</code> is a function that runs over PyTrees and applies a function to all leafs, returning a new PyTree with the results. If we input multiple PyTrees, it runs over them synchronously and applies the functions on tuples of leafs, one of each PyTree. In the function above, this means that a new PyTree is created for which each leaf has the sum of the respective leaf in <code>params</code> and <code>updates</code>.</p>
<p>In contrast to PyTorch, there is no <code>zero_grad</code> function since the gradients are the output of a separate gradient function, not an object-based backpropagation through a dynamic computation graph. The template is setup below:</p>
<div id="cell-45" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Optimizer(NamedTuple):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Given the parameters, initialize any optimizer state as tuple</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    init : Callable[[PyTree], <span class="bu">tuple</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Given the gradients, optimizer state, and evt. parameters, return</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the parameter updates and new optimizer state</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    update : Callable[[PyTree, <span class="bu">tuple</span>, Optional[PyTree]], Tuple[PyTree, <span class="bu">tuple</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:</p>
<p><span class="math display">\[
\begin{split}
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot g^{(t)}
\end{split}
\]</span></p>
<p>As simple as the equation is also our implementation of SGD:</p>
<div id="cell-47" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd(lr):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init(params):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">tuple</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(updates, state, params<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        updates <span class="op">=</span> tree_map(<span class="kw">lambda</span> u: <span class="op">-</span>lr <span class="op">*</span> u, updates)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> updates, state</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Optimizer(init, update)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we define SGD as a function that returns a tuple of the init and update function, which depends on the input parameter <code>lr</code>, i.e., the learning rate. SGD does not have any state it needs to keep track of. Hence, the init function returns an empty tuple. The update function simply multiplies all gradients with the learning rate. The negative sign is needed since the <code>apply_updates</code> function <em>adds</em> the updates to the parameters, not subtracts.</p>
<p>In the lecture, we also have discussed the concept of momentum which replaces the gradient in the update by an exponential average of all past gradients including the current one:</p>
<p><span class="math display">\[
\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot m^{(t)}\\
\end{split}
\]</span></p>
<p>Let’s also implement it below:</p>
<div id="cell-50" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd_momentum(lr, momentum<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init(params):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>        param_momentum <span class="op">=</span> tree_map(jnp.zeros_like, params)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> param_momentum</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(updates, state, params<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> tree_map(<span class="kw">lambda</span> m, g: (<span class="dv">1</span> <span class="op">-</span> momentum) <span class="op">*</span> g <span class="op">+</span> momentum <span class="op">*</span> m,</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>                         state,</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>                         updates)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        updates <span class="op">=</span> tree_map(<span class="kw">lambda</span> m: <span class="op">-</span> lr <span class="op">*</span> m,</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>                           state)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> updates, state</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Optimizer(init, update)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We track the momentum parameters <span class="math inline">\(m^{(t)}\)</span> in a PyTree, which is initialized with all zeros, but with the same shape and structure as the parameters. At each update iteration, we update our momentum parameters with above’s equation, and use it to calculate the new updates.</p>
<p>As a third and final optimizer, we arrive at Adam. Adam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e.&nbsp;the gradients norm. Furthermore, we add a bias correction for the momentum and adaptive learning rate for the first iterations:</p>
<p><span class="math display">\[
\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    v^{(t)} &amp; = \beta_2 v^{(t-1)} + (1 - \beta_2)\cdot \left(g^{(t)}\right)^2\\
    \hat{m}^{(t)} &amp; = \frac{m^{(t)}}{1-\beta^{t}_1}, \hat{v}^{(t)} = \frac{v^{(t)}}{1-\beta^{t}_2}\\
    w^{(t)} &amp; = w^{(t-1)} - \frac{\eta}{\sqrt{\hat{v}^{(t)}} + \epsilon}\circ \hat{m}^{(t)}\\
\end{split}
\]</span></p>
<p>Epsilon is a small constant used to improve numerical stability for very small gradient norms. Remember that the adaptive learning rate does not replace the learning rate hyperparameter <span class="math inline">\(\eta\)</span>, but rather acts as an extra factor and ensures that the gradients of various parameters have a similar norm.</p>
<div id="cell-53" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(lr, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.999</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init(params):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        step <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        param_momentum <span class="op">=</span> tree_map(jnp.zeros_like, params)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        param_2nd_momentum <span class="op">=</span> tree_map(jnp.zeros_like, params)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (step, param_momentum, param_2nd_momentum)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(updates, state, params<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update momentum and adapt. lr</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        step, param_momentum, param_2nd_momentum <span class="op">=</span> state</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        param_momentum <span class="op">=</span> tree_map(<span class="kw">lambda</span> m, g: (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> g <span class="op">+</span> beta1 <span class="op">*</span> m,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>                                  param_momentum,</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>                                  updates)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        param_2nd_momentum <span class="op">=</span> tree_map(<span class="kw">lambda</span> m2, g: (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> g<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> beta2 <span class="op">*</span> m2,</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>                                      param_2nd_momentum,</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>                                      updates)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate update for single parameter</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> update_param(m, m2):</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Bias correction</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>            m <span class="op">/=</span> <span class="dv">1</span> <span class="op">-</span> beta1 <span class="op">**</span> step</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>            m2 <span class="op">/=</span> <span class="dv">1</span> <span class="op">-</span> beta2 <span class="op">**</span> step</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="op">-</span> m <span class="op">*</span> lr <span class="op">/</span> (jnp.sqrt(m2) <span class="op">+</span> eps)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update for all parameters</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        updates <span class="op">=</span> tree_map(update_param,</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>                           param_momentum,</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>                           param_2nd_momentum)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> updates, (step, param_momentum, param_2nd_momentum)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Optimizer(init, update)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our state consists of a counter <code>step</code> that keeps track of the time step <span class="math inline">\(t\)</span>, the momentums <span class="math inline">\(m^{(t)}\)</span> and the second momentum/adaptive learning rate parameters <span class="math inline">\(v^{(t)}\)</span>. At each update step, we update <span class="math inline">\(m^{(t)}\)</span> and <span class="math inline">\(v^{(t)}\)</span>, and afterwards calculate the updates for each parameters based on <span class="math inline">\(m^{(t)}\)</span> and <span class="math inline">\(v^{(t)}\)</span>.</p>
<section id="comparing-optimizers-on-model-training" class="level3">
<h3 class="anchored" data-anchor-id="comparing-optimizers-on-model-training">Comparing optimizers on model training</h3>
<p>After we have implemented three optimizers (SGD, SGD with momentum, and Adam), we can start to analyze and compare them. First, we test them on how well they can optimize a neural network on the FashionMNIST dataset. We use again our linear network, this time with a ReLU activation and the kaiming initialization, which we have found before to work well for ReLU-based networks. Note that the model is over-parameterized for this task, and we can achieve similar performance with a much smaller network (for example <code>100,100,100</code>). However, our main interest is in how well the optimizer can train <em>deep</em> neural networks, hence the over-parameterization.</p>
<div id="cell-56" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> init_simple_model(kaiming_init, act_fn<span class="op">=</span>nn.relu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For a fair comparison, we train the exact same model with the same seed with the three optimizers below. Feel free to change the hyperparameters if you want, the models are relatively fast to train.</p>
<div id="cell-58" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> train_model(model, params, sgd(lr<span class="op">=</span><span class="fl">1e-1</span>),</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                <span class="st">"FashionMNIST_SGD"</span>,  </span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-25-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 89.61% ==============
</code></pre>
</div>
</div>
<div id="cell-59" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> train_model(model, params, sgd_momentum(lr<span class="op">=</span><span class="fl">1e-1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>                <span class="st">"FashionMNIST_SGDMom"</span>,  </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-26-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 89.30% ==============
</code></pre>
</div>
</div>
<div id="cell-60" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> train_model(model, params, adam(lr<span class="op">=</span><span class="fl">1e-3</span>), </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>                <span class="st">"FashionMNIST_Adam"</span>, </span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>                max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-27-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 89.06% ==============
</code></pre>
</div>
</div>
<p>The result is that all optimizers perform similarly well with the given model. The differences are too small to find any significant conclusion. However, keep in mind that this can also be attributed to the initialization we chose. When changing the initialization to worse (e.g.&nbsp;constant variance), Adam usually shows to be more robust because of its adaptive learning rate. To show the specific benefits of the optimizers, we will continue to look at some possible loss surfaces in which momentum and adaptive learning rate are crucial.</p>
</section>
<section id="pathological-curvatures" class="level3">
<h3 class="anchored" data-anchor-id="pathological-curvatures">Pathological curvatures</h3>
<p>A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let’s first create an example surface of this and visualize it:</p>
<div id="cell-63" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pathological_curve_loss(w1, w2):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example of a pathological curvature. There are many more possible, feel free to experiment here!</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    x1_loss <span class="op">=</span> nn.tanh(w1)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> jnp.<span class="bu">abs</span>(w1)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    x2_loss <span class="op">=</span> nn.sigmoid(w2)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x1_loss <span class="op">+</span> x2_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_curve(curve_fn, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), plot_3d<span class="op">=</span><span class="va">False</span>, cmap<span class="op">=</span>cm.viridis, title<span class="op">=</span><span class="st">"Pathological curvature"</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.axes(projection<span class="op">=</span><span class="st">'3d'</span>) <span class="cf">if</span> plot_3d <span class="cf">else</span> plt.axes()</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.arange(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], (x_range[<span class="dv">1</span>]<span class="op">-</span>x_range[<span class="dv">0</span>])<span class="op">/</span><span class="fl">100.</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.arange(y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>], (y_range[<span class="dv">1</span>]<span class="op">-</span>y_range[<span class="dv">0</span>])<span class="op">/</span><span class="fl">100.</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> curve_fn(x, y)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> jax.device_get(z)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> plot_3d:</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        ax.plot_surface(x, y, z, cmap<span class="op">=</span>cmap, linewidth<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">"#000"</span>, antialiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        ax.set_zlabel(<span class="st">"loss"</span>)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>        ax.imshow(z[::<span class="op">-</span><span class="dv">1</span>], cmap<span class="op">=</span>cmap, extent<span class="op">=</span>(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>]))</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r"$w_1$"</span>)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r"$w_2$"</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>sns.reset_orig()</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plot_curve(pathological_curve_loss, plot_3d<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-29-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In terms of optimization, you can image that <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are weight parameters, and the curvature represents the loss surface over the space of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.</p>
<p>Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of <span class="math inline">\(w_2\)</span>. However, if we encounter a point along the ridges, the gradient is much greater in <span class="math inline">\(w_1\)</span> than <span class="math inline">\(w_2\)</span>, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.</p>
<p>To test our algorithms, we can implement a simple function to train two parameters on such a surface:</p>
<div id="cell-66" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_curve(optimizer, curve_func<span class="op">=</span>pathological_curve_loss, num_updates<span class="op">=</span><span class="dv">100</span>, init<span class="op">=</span>[<span class="dv">5</span>,<span class="dv">5</span>]):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer - Optimizer to use</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co">        curve_func - Loss function (e.g. pathological curvature)</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">        num_updates - Number of updates/steps to take when optimizing </span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co">        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> jnp.array(init, dtype<span class="op">=</span>jnp.float32)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    grad_fn <span class="op">=</span> jax.jit(jax.value_and_grad(<span class="kw">lambda</span> w: curve_func(w[<span class="dv">0</span>], w[<span class="dv">1</span>])))</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(weights)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    list_points <span class="op">=</span> []</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_updates):</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> grad_fn(weights)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>        list_points.append(jnp.concatenate([weights, loss[<span class="va">None</span>]], axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> optax.apply_updates(weights, updates)</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> jnp.stack(list_points, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> jax.device_get(points)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> points</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s apply the different optimizers on our curvature. Note that we set a much higher learning rate for the optimization algorithms as you would in a standard neural network. This is because we only have 2 parameters instead of tens of thousands or even millions.</p>
<div id="cell-68" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>SGD_points <span class="op">=</span> train_curve(sgd(lr<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>SGDMom_points <span class="op">=</span> train_curve(sgd_momentum(lr<span class="op">=</span><span class="dv">10</span>, momentum<span class="op">=</span><span class="fl">0.9</span>))</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>Adam_points <span class="op">=</span> train_curve(adam(lr<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To understand best how the different algorithms worked, we visualize the update step as a line plot through the loss surface. We will stick with a 2D representation for readability.</p>
<div id="cell-70" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>all_points <span class="op">=</span> np.concatenate([SGD_points, SGDMom_points, Adam_points], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_curve(pathological_curve_loss,</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>                x_range<span class="op">=</span>(<span class="op">-</span>np.absolute(all_points[:,<span class="dv">0</span>]).<span class="bu">max</span>(), np.absolute(all_points[:,<span class="dv">0</span>]).<span class="bu">max</span>()),</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>                y_range<span class="op">=</span>(all_points[:,<span class="dv">1</span>].<span class="bu">min</span>(), all_points[:,<span class="dv">1</span>].<span class="bu">max</span>()),</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>                plot_3d<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>ax.plot(SGD_points[:,<span class="dv">0</span>], SGD_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"SGD"</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>ax.plot(SGDMom_points[:,<span class="dv">0</span>], SGDMom_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"SGDMom"</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>ax.plot(Adam_points[:,<span class="dv">0</span>], Adam_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"grey"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"Adam"</span>)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-32-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can clearly see that SGD is not able to find the center of the optimization curve and has a problem converging due to the steep gradients in <span class="math inline">\(w_1\)</span>. In contrast, Adam and SGD with momentum nicely converge as the changing direction of <span class="math inline">\(w_1\)</span> is canceling itself out. On such surfaces, it is crucial to use momentum.</p>
</section>
<section id="steep-optima" class="level3">
<h3 class="anchored" data-anchor-id="steep-optima">Steep optima</h3>
<p>A second type of challenging loss surfaces are steep optima. In those, we have a larger part of the surface having very small gradients while around the optimum, we have very large gradients. For instance, take the following loss surfaces:</p>
<div id="cell-73" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bivar_gaussian(w1, w2, x_mean<span class="op">=</span><span class="fl">0.0</span>, y_mean<span class="op">=</span><span class="fl">0.0</span>, x_sig<span class="op">=</span><span class="fl">1.0</span>, y_sig<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> x_sig <span class="op">*</span> y_sig)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    x_exp <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> (w1 <span class="op">-</span> x_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> x_sig<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    y_exp <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> (w2 <span class="op">-</span> y_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> y_sig<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> norm <span class="op">*</span> jnp.exp(x_exp <span class="op">+</span> y_exp)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> comb_func(w1, w2):</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="op">-</span>bivar_gaussian(w1, w2, x_mean<span class="op">=</span><span class="fl">1.0</span>, y_mean<span class="op">=-</span><span class="fl">0.5</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    z <span class="op">-=</span> bivar_gaussian(w1, w2, x_mean<span class="op">=-</span><span class="fl">1.0</span>, y_mean<span class="op">=</span><span class="fl">0.5</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    z <span class="op">-=</span> bivar_gaussian(w1, w2, x_mean<span class="op">=-</span><span class="fl">0.5</span>, y_mean<span class="op">=-</span><span class="fl">0.8</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plot_curve(comb_func, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), plot_3d<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">"Steep optima"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-33-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Most of the loss surface has very little to no gradients. However, close to the optima, we have very steep gradients. To reach the minimum when starting in a region with lower gradients, we expect an adaptive learning rate to be crucial. To verify this hypothesis, we can run our three optimizers on the surface:</p>
<div id="cell-75" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>SGD_points <span class="op">=</span> train_curve(sgd(lr<span class="op">=</span><span class="fl">.5</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>SGDMom_points <span class="op">=</span> train_curve(sgd_momentum(lr<span class="op">=</span><span class="dv">1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>Adam_points <span class="op">=</span> train_curve(adam(lr<span class="op">=</span><span class="fl">0.2</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>all_points <span class="op">=</span> np.concatenate([SGD_points, SGDMom_points, Adam_points], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_curve(comb_func,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>                x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>                y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>                plot_3d<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>                title<span class="op">=</span><span class="st">"Steep optima"</span>)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>ax.plot(SGD_points[:,<span class="dv">0</span>], SGD_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"SGD"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>ax.plot(SGDMom_points[:,<span class="dv">0</span>], SGDMom_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"SGDMom"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>ax.plot(Adam_points[:,<span class="dv">0</span>], Adam_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"grey"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"Adam"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Optimization_and_Initialization_files/figure-html/cell-34-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>SGD first takes very small steps until it touches the border of the optimum. First reaching a point around <span class="math inline">\((-0.75,-0.5)\)</span>, the gradient direction has changed and pushes the parameters to <span class="math inline">\((0.8,0.5)\)</span> from which SGD cannot recover anymore (only with many, many steps). A similar problem has SGD with momentum, only that it continues the direction of the touch of the optimum. The gradients from this time step are so much larger than any other point that the momentum <span class="math inline">\(m_t\)</span> is overpowered by it. Finally, Adam is able to converge in the optimum showing the importance of adaptive learning rates.</p>
</section>
<section id="what-optimizer-to-take" class="level3">
<h3 class="anchored" data-anchor-id="what-optimizer-to-take">What optimizer to take</h3>
<p>After seeing the results on optimization, what is our conclusion? Should we always use Adam and never look at SGD anymore? The short answer: no. There are many papers saying that in certain situations, SGD (with momentum) generalizes better where Adam often tends to overfit [5,6]. This is related to the idea of finding wider optima. For instance, see the illustration of different optima below (credit: <a href="https://arxiv.org/pdf/1609.04836.pdf">Keskar et al., 2017</a>):</p>
<center width="100%">
<img src="../../tutorial4/flat_vs_sharp_minima.svg" width="500px">
</center>
<p>The black line represents the training loss surface, while the dotted red line is the test loss. Finding sharp, narrow minima can be helpful for finding the minimal training loss. However, this doesn’t mean that it also minimizes the test loss as especially flat minima have shown to generalize better. You can imagine that the test dataset has a slightly shifted loss surface due to the different examples than in the training set. A small change can have a significant influence for sharp minima, while flat minima are generally more robust to this change.</p>
<p>In the next tutorial, we will see that some network types can still be better optimized with SGD and learning rate scheduling than Adam. Nevertheless, Adam is the most commonly used optimizer in Deep Learning as it usually performs better than other optimizers, especially for deep networks.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this tutorial, we have looked at initialization and optimization techniques for neural networks. We have seen that a good initialization has to balance the preservation of the gradient variance as well as the activation variance. This can be achieved with the Xavier initialization for tanh-based networks, and the Kaiming initialization for ReLU-based networks. In optimization, concepts like momentum and adaptive learning rate can help with challenging loss surfaces but don’t guarantee an increase in performance for neural networks.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.” Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010. <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">link</a></p>
<p>[2] He, Kaiming, et al.&nbsp;“Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” Proceedings of the IEEE international conference on computer vision. 2015. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html">link</a></p>
<p>[3] Kingma, Diederik P. &amp; Ba, Jimmy. “Adam: A Method for Stochastic Optimization.” Proceedings of the third international conference for learning representations (ICLR). 2015. <a href="https://arxiv.org/abs/1412.6980">link</a></p>
<p>[4] Keskar, Nitish Shirish, et al.&nbsp;“On large-batch training for deep learning: Generalization gap and sharp minima.” Proceedings of the fifth international conference for learning representations (ICLR). 2017. <a href="https://arxiv.org/abs/1609.04836">link</a></p>
<p>[5] Wilson, Ashia C., et al.&nbsp;“The Marginal Value of Adaptive Gradient Methods in Machine Learning.” Advances in neural information processing systems. 2017. <a href="https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf">link</a></p>
<p>[6] Ruder, Sebastian. “An overview of gradient descent optimization algorithms.” arXiv preprint. 2017. <a href="https://arxiv.org/abs/1609.04747">link</a></p>
<hr>
<p><a href="https://github.com/phlippe/uvadlc_notebooks/"><img src="https://img.shields.io/static/v1.svg?logo=star&amp;label=⭐&amp;message=Star%20Our%20Repository&amp;color=yellow" class="img-fluid" alt="Star our repository"></a> If you found this tutorial helpful, consider ⭐-ing our repository.<br>
<a href="https://github.com/phlippe/uvadlc_notebooks/issues"><img src="https://img.shields.io/static/v1.svg?logo=star&amp;label=❔&amp;message=Ask%20Questions&amp;color=9cf" class="img-fluid" alt="Ask questions"></a> For any questions, typos, or bugs that you found, please raise an issue on GitHub.</p>
<hr>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>