<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Guide 3: Debugging in PyTorch – Deep Learning/NLP course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Deep Learning/NLP course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../dl.html"> 
<span class="menu-text">Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../nlp.html"> 
<span class="menu-text">Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#my-model-is-underperforming" id="toc-my-model-is-underperforming" class="nav-link active" data-scroll-target="#my-model-is-underperforming">My model is underperforming</a>
  <ul class="collapse">
  <li><a href="#softmax-crossentropyloss-and-nllloss" id="toc-softmax-crossentropyloss-and-nllloss" class="nav-link" data-scroll-target="#softmax-crossentropyloss-and-nllloss">Softmax, CrossEntropyLoss and NLLLoss</a></li>
  <li><a href="#softmax-over-the-right-dimension" id="toc-softmax-over-the-right-dimension" class="nav-link" data-scroll-target="#softmax-over-the-right-dimension">Softmax over the right dimension</a></li>
  <li><a href="#categorical-data-and-embedding" id="toc-categorical-data-and-embedding" class="nav-link" data-scroll-target="#categorical-data-and-embedding">Categorical data and Embedding</a></li>
  <li><a href="#time-dimension-in-nn.lstm" id="toc-time-dimension-in-nn.lstm" class="nav-link" data-scroll-target="#time-dimension-in-nn.lstm">Time dimension in nn.LSTM</a></li>
  <li><a href="#hidden-shape-mismatch" id="toc-hidden-shape-mismatch" class="nav-link" data-scroll-target="#hidden-shape-mismatch">Hidden shape mismatch</a></li>
  <li><a href="#training-and-evaluation-switch" id="toc-training-and-evaluation-switch" class="nav-link" data-scroll-target="#training-and-evaluation-switch">Training and Evaluation switch</a></li>
  <li><a href="#parameter-handling" id="toc-parameter-handling" class="nav-link" data-scroll-target="#parameter-handling">Parameter handling</a></li>
  <li><a href="#parameters-and-.todevice" id="toc-parameters-and-.todevice" class="nav-link" data-scroll-target="#parameters-and-.todevice">Parameters and “.to(device)”</a></li>
  <li><a href="#my-model-runs-fine-on-cpu-but-gets-nan-loss-on-gpu" id="toc-my-model-runs-fine-on-cpu-but-gets-nan-loss-on-gpu" class="nav-link" data-scroll-target="#my-model-runs-fine-on-cpu-but-gets-nan-loss-on-gpu">My model runs fine on CPU, but gets NaN loss on GPU</a></li>
  <li><a href="#initialization" id="toc-initialization" class="nav-link" data-scroll-target="#initialization">Initialization</a></li>
  <li><a href="#zero-grad-in-optimizers" id="toc-zero-grad-in-optimizers" class="nav-link" data-scroll-target="#zero-grad-in-optimizers">Zero-grad in optimizers</a></li>
  <li><a href="#weight-decay-and-adam" id="toc-weight-decay-and-adam" class="nav-link" data-scroll-target="#weight-decay-and-adam">Weight decay and Adam</a></li>
  <li><a href="#check-your-metric-calculation" id="toc-check-your-metric-calculation" class="nav-link" data-scroll-target="#check-your-metric-calculation">Check your metric calculation</a></li>
  <li><a href="#my-bits-per-dimension-score-is-very-low" id="toc-my-bits-per-dimension-score-is-very-low" class="nav-link" data-scroll-target="#my-bits-per-dimension-score-is-very-low">My bits per dimension score is very low</a></li>
  </ul></li>
  <li><a href="#pytorch-throws-an-error" id="toc-pytorch-throws-an-error" class="nav-link" data-scroll-target="#pytorch-throws-an-error">PyTorch throws an error</a>
  <ul class="collapse">
  <li><a href="#trying-to-backward-through-the-graph-a-second-time-specify-retain_graphtrue" id="toc-trying-to-backward-through-the-graph-a-second-time-specify-retain_graphtrue" class="nav-link" data-scroll-target="#trying-to-backward-through-the-graph-a-second-time-specify-retain_graphtrue">Trying to backward through the graph a second time, specify retain_graph=True</a></li>
  <li><a href="#size-mismatch" id="toc-size-mismatch" class="nav-link" data-scroll-target="#size-mismatch">Size mismatch</a></li>
  <li><a href="#device-mismatch" id="toc-device-mismatch" class="nav-link" data-scroll-target="#device-mismatch">Device mismatch</a></li>
  </ul></li>
  <li><a href="#good-practices" id="toc-good-practices" class="nav-link" data-scroll-target="#good-practices">Good practices</a>
  <ul class="collapse">
  <li><a href="#use-nn.sequential-and-nn.modulelist" id="toc-use-nn.sequential-and-nn.modulelist" class="nav-link" data-scroll-target="#use-nn.sequential-and-nn.modulelist">Use nn.Sequential and nn.ModuleList</a></li>
  <li><a href="#in-place-activation-functions" id="toc-in-place-activation-functions" class="nav-link" data-scroll-target="#in-place-activation-functions">In-place activation functions</a></li>
  <li><a href="#create-modules-for-repeating-blocks" id="toc-create-modules-for-repeating-blocks" class="nav-link" data-scroll-target="#create-modules-for-repeating-blocks">Create modules for repeating blocks</a></li>
  <li><a href="#stack-layersweights-with-same-input" id="toc-stack-layersweights-with-same-input" class="nav-link" data-scroll-target="#stack-layersweights-with-same-input">Stack layers/weights with same input</a></li>
  <li><a href="#use-loss-functions-on-logits" id="toc-use-loss-functions-on-logits" class="nav-link" data-scroll-target="#use-loss-functions-on-logits">Use loss functions on logits</a></li>
  <li><a href="#make-use-of-torch.nn.functional" id="toc-make-use-of-torch.nn.functional" class="nav-link" data-scroll-target="#make-use-of-torch.nn.functional">Make use of torch.nn.functional</a></li>
  <li><a href="#clip-gradient-norms" id="toc-clip-gradient-norms" class="nav-link" data-scroll-target="#clip-gradient-norms">Clip gradient norms</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Guide 3: Debugging in PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>When you start learning PyTorch, it is expected that you hit bugs and errors. To help you debug your code, we will summarize the most common mistakes in this guide, explain why they happen, and how you can solve them.</p>
<section id="my-model-is-underperforming" class="level2">
<h2 class="anchored" data-anchor-id="my-model-is-underperforming">My model is underperforming</h2>
<p>Situation: Your model is not reaching the performance that it should, but PyTorch is not telling you why that happens. These errors are the most annoying bugs since those can be hard to debug. Nonetheless, there are couple of things you can check. If none of these solve the problem for you, one of us TAs will help you debug your code more in detail.</p>
<section id="softmax-crossentropyloss-and-nllloss" class="level3">
<h3 class="anchored" data-anchor-id="softmax-crossentropyloss-and-nllloss">Softmax, CrossEntropyLoss and NLLLoss</h3>
<p>The most common mistake is the mismatch between loss function and output activation function. The loss module <code>nn.CrossEntropyLoss</code> in PyTorch performs two operations: <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code>. Hence, the input to this loss module should be the output of your last linear layer. <strong>Do not apply a softmax before the Cross-Entropy loss</strong>. Otherwise, PyTorch will apply a log-softmax on your softmax outputs, which will significantly worsen the performance, and give you headaches.</p>
<p>If you use the loss module <code>nn.NLLLoss</code>, you need to apply the log-softmax yourself. <strong>NLLLoss requires log-probabilities, not plain probabilities.</strong> Hence, make sure to apply <code>nn.LogSoftmax</code> or <code>nn.functional.log_softmax</code>, and <strong>not</strong> <code>nn.Softmax</code>.</p>
</section>
<section id="softmax-over-the-right-dimension" class="level3">
<h3 class="anchored" data-anchor-id="softmax-over-the-right-dimension">Softmax over the right dimension</h3>
<p>Pay attention to the dimension you apply your softmax over. Usually, this is the last dimension of your output tensor, which you can identify with e.g.&nbsp;<code>nn.Softmax(dim=-1)</code>. If you mix up the dimension, your model ends up with random predictions.</p>
</section>
<section id="categorical-data-and-embedding" class="level3">
<h3 class="anchored" data-anchor-id="categorical-data-and-embedding">Categorical data and Embedding</h3>
<p>Categorical data, as for example language characters or the datasets you are given in assignment 2, require special care. Data like language characters ‘a’, ‘b’, ‘c’ etc. are usually represented as integers 0, 1, 2, etc. <strong>Do not use integers as input for categorical data.</strong> If you would enter those integers as inputs to the model, two problems arise.</p>
<ol type="1">
<li>You bias the model to see relations where there are none. In the language example above, the model would think that ‘a’ is closer to ‘b’ than to ‘o’, although ‘a’ and ‘o’ are both vocals, and the closeness of ‘a’ and ‘b’ does not necessarily say anything about their usage.</li>
<li>If you have many categories, you will have input values between 0 and &gt;50. The model will have a hard time separating all those &gt;50 categories without blending over some. Hence, the model loses a lot of information although this is not necessary.</li>
</ol>
<p>The much better option in the case of categorical data is to use one-hot vectors, or embeddings. A one-hot vector represents each category by a vector of 0s, with one index being 1. This makes the model’s life much easier as it can distinguish between the categories in a very simple manner (if feature !=0, it is a specific category). Alternatively, you can learn an embedding with the help of <code>nn.Embedding</code>. The inputs to this module are:</p>
<ul>
<li><code>num_embeddings</code> which is the number of different categories you have in your input data (in case of language characters, something like 26 as you have ‘a’ to ‘z’)</li>
<li><code>embedding_dim</code> which is the number of features you want to represent each category with. If you use the embedding directly as input to an LSTM or RNN, a good rule of thumb is to use 1/4 - 1/2 of your hidden size inside the LSTM.</li>
<li><code>padding_idx</code> which would allow you to assign a specific index for the padding symbol. Can be skipped if you do not use “-1” as padding index.</li>
</ul>
<p>The embedding feature vectors are randomly initialized from <span class="math inline">\(\mathcal{N}(0,1)\)</span>. <strong>Do not overwrite this init by Kaiming, Xavier or similar.</strong> The used standard deviation is 1 because the initialization, activation functions etc. have been designed to have a input standard deviation of 1. Example usage of the embedding module:</p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create 5 embedding vectors each with 32 features</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                         embedding_dim<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Example integer input</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.LongTensor([[<span class="dv">0</span>, <span class="dv">4</span>], [<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get embeddings</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>embed_vectors <span class="op">=</span> embedding(input_tensor)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input shape:"</span>, input_tensor.shape)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, embed_vectors.shape)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Example features:</span><span class="ch">\n</span><span class="st">"</span>, embed_vectors[:,:,:<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([3, 2])
Output shape: torch.Size([3, 2, 32])
Example features:
 tensor([[[ 0.0504, -0.2422],
         [ 0.0342,  0.2217]],

        [[ 2.7813, -0.3641],
         [-0.0981,  0.4069]],

        [[ 0.0504, -0.2422],
         [ 1.2092,  0.1760]]], grad_fn=&lt;SliceBackward&gt;)</code></pre>
</div>
</div>
<p>The <code>nn.Embedding</code> object is a module like a linear layer or convolution. Thus, it needs to be defined in the <code>__init__</code> function of your higher-level module. <strong>Do not create the Embedding module in the forward pass.</strong> Otherwise, you will have different embeddings every time you run the model, and hence, your model is not able to learn.</p>
</section>
<section id="time-dimension-in-nn.lstm" class="level3">
<h3 class="anchored" data-anchor-id="time-dimension-in-nn.lstm">Time dimension in nn.LSTM</h3>
<p>By default, PyTorch’s <code>nn.LSTM</code> module assumes the input to be sorted as <code>[seq_len, batch_size, input_size]</code>. Make sure that you do not confuse the sequence length and batch dimension. The LSTM would still run without an error, but will give you wrong results. If you want to change this behavior to accepting an input shape of <code>[batch_size, seq_len, input_size]</code>, you can specify the argument <code>batch_first=True</code> when creating the LSTM object. Have a closer look at the <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">documentation</a> for details.</p>
</section>
<section id="hidden-shape-mismatch" class="level3">
<h3 class="anchored" data-anchor-id="hidden-shape-mismatch">Hidden shape mismatch</h3>
<p>If you perform matrix multiplications and have a shape mismatch between two matrices, PyTorch will complain and throw an error. However, there are also situations where PyTorch does not throw an error because the misaligned dimensions have (unluckily) the same size. For instance, imagine you have a weight matrix of size <span class="math inline">\(d_{in}\times d_{out}\)</span>. If you take the input <span class="math inline">\(x\)</span> of size <span class="math inline">\(B \times d_{in}\)</span> (<span class="math inline">\(B\)</span> being the batch dimension), and in your hyperparameter setting, <span class="math inline">\(B=d_{in}\)</span>, you can end up performing the matrix multiplication over the wrong dimension while PyTorch is not detecting it. <strong>Test your code with multiple, different batch sizes to prevent shape misalignments with the batch dimension.</strong></p>
</section>
<section id="training-and-evaluation-switch" class="level3">
<h3 class="anchored" data-anchor-id="training-and-evaluation-switch">Training and Evaluation switch</h3>
<p>In PyTorch, a module and/or neural network has two modes: training and evaluation. You switch between them using <code>model.eval()</code> and <code>model.train()</code>. The modes decide for instance whether to apply dropout or not, and how to handle the forward of Batch Normalization. However, a common mistake is to forget to set your model back into training mode after evaluation. <strong>Make sure to set your model back to train mode after validation.</strong> In case your model does not contain dropout, BatchNorm or similar modules, this might not affect your performance.</p>
</section>
<section id="parameter-handling" class="level3">
<h3 class="anchored" data-anchor-id="parameter-handling">Parameter handling</h3>
<p>As you might know from the PyTorch Tutorial, PyTorch supports hierarchical usage of <code>nn.Modules</code>. One module can contain another module, which can again contain a module, and so on. When you call <code>.parameters()</code> on a module, PyTorch looks for all modules inside the module to also add their parameters to the highest-level module’s parameter. However, <strong>PyTorch does not detect parameters of modules in lists, dicts or similar structures.</strong> If you have a list of modules, make sure to put them into a <code>nn.ModuleList</code> or <code>nn.Sequential</code> object. Parameters of modules inside those containers are detected. Similarly, for dictionaries, you can use <code>nn.ModuleDict</code>.</p>
</section>
<section id="parameters-and-.todevice" class="level3">
<h3 class="anchored" data-anchor-id="parameters-and-.todevice">Parameters and “.to(device)”</h3>
<p>To push your model and/or data to GPU, you can use <code>.to(device)</code> where <code>device</code> is an device object or string (“cpu” for CPU-only machines, and “cuda”/“cuda:0” for GPUs). However, <strong>do not call <code>.to(device)</code> during parameter init</strong>. If you define a parameter like <code>self.W = nn.Parameter(torch.Tensor(64, 128)).to(device)</code>, your model will not register the parameter on GPU because the “.to” operator creates a new Tensor. Parameters, nonetheless, have to be leaf Tensors, hence your parameters will not be recognized (corresponding <a href="https://github.com/pytorch/pytorch/issues/17484">GitHub issue</a>). It is much better practice to only call <code>.to(device)</code> once <em>after</em> finishing the init of the model, and not inside the model.</p>
</section>
<section id="my-model-runs-fine-on-cpu-but-gets-nan-loss-on-gpu" class="level3">
<h3 class="anchored" data-anchor-id="my-model-runs-fine-on-cpu-but-gets-nan-loss-on-gpu">My model runs fine on CPU, but gets NaN loss on GPU</h3>
<p>If this is the case, you likely have the bug of parameters and <code>.to(device)</code> as explained above.</p>
</section>
<section id="initialization" class="level3">
<h3 class="anchored" data-anchor-id="initialization">Initialization</h3>
<p>Initializing the parameters of your model correctly is very important (see <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html">Tutorial 4</a> for details on this). Initializing parameters with a standard normal distribution is not a good practice and often fails. It can occasionally work for very shallow networks, but don’t risk it! <strong>Think about your initialization, and use proper methods like Kaiming or Xavier.</strong></p>
</section>
<section id="zero-grad-in-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="zero-grad-in-optimizers">Zero-grad in optimizers</h3>
<p><strong>Remember to call optimizer.zero_grad() before doing loss.backward().</strong> If you do not reset the gradients for all parameters before performing backpropagation, your gradients will be added to those from the previous batch. Hence, your gradients end up to be not the ones you intended them to be.</p>
</section>
<section id="weight-decay-and-adam" class="level3">
<h3 class="anchored" data-anchor-id="weight-decay-and-adam">Weight decay and Adam</h3>
<p>Adam is known to have a different implementation of weight decay in many frameworks than you would expect. Specifically, the weight decay is usually added as gradients <strong>before</strong> determining the adaptive learning rate, and hence scaling up the weight decay for parameters with low gradient norms. Details on this problem, which is actually shared across most common DL frameworks, can be found <a href="https://arxiv.org/pdf/1711.05101.pdf">here</a>. In PyTorch, you can use the desired version of weight decay in Adam using <code>torch.optim.AdamW</code> (identical to <code>torch.optim.Adam</code> besides the weight decay implementation).</p>
</section>
<section id="check-your-metric-calculation" class="level3">
<h3 class="anchored" data-anchor-id="check-your-metric-calculation">Check your metric calculation</h3>
<p>This might sound a bit stupid but check your metric calculation twice or more often before doubting yourself or your model. Metrics like accuracy are easy to calculate, but it is as easy to add a bug into the code. For instance, check that you are averaging over the batch dimension and not accidentally over the class dimension or any other.</p>
</section>
<section id="my-bits-per-dimension-score-is-very-low" class="level3">
<h3 class="anchored" data-anchor-id="my-bits-per-dimension-score-is-very-low">My bits per dimension score is very low</h3>
<p>If you obtain a very low bits per dimension score for likelihood-based generative models after already the first iteration, the calculation might not be fully correct. Specifically, the negative log likelihood input to the bpd-metric function is expected to be the <strong>sum</strong> of the individual pixel’s log likelihood of an image, not the mean. The mean is taken inside the bpd function.</p>
<hr>
</section>
</section>
<section id="pytorch-throws-an-error" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-throws-an-error">PyTorch throws an error</h2>
<p>These errors are the easier bugs to correct because PyTorch actually talks to you about what is wrong. Until these are not solved, you probably cannot train your model.</p>
<section id="trying-to-backward-through-the-graph-a-second-time-specify-retain_graphtrue" class="level3">
<h3 class="anchored" data-anchor-id="trying-to-backward-through-the-graph-a-second-time-specify-retain_graphtrue">Trying to backward through the graph a second time, specify retain_graph=True</h3>
<p>This error occurs if you re-use a tensor from the computation graph of the previous batch. This should usually not happen. Make sure to not keep tensors across batches if not strictly necessary. Example where this issue can occur: when implementing your own LSTM, make sure that the initial hidden state is a constant zero tensor, and not the last hidden state of the previous batch.</p>
</section>
<section id="size-mismatch" class="level3">
<h3 class="anchored" data-anchor-id="size-mismatch">Size mismatch</h3>
<p>This usually occurs if your dimension of the input to a module does not match the specified input dimension of the weight tensor, like in a linear layer. Make sure to have specified the correct dimensions. Usually, a good way to debug this is to print the shape of the input tensor before every layer you call.</p>
<p>If this happens for a matrix multiplication you have implemented, print the shapes of both matrices, and try to figure out over which dimension the matrix multiplication should actually have been performed, and over which PyTorch currently does it.</p>
</section>
<section id="device-mismatch" class="level3">
<h3 class="anchored" data-anchor-id="device-mismatch">Device mismatch</h3>
<p>You might sometimes see a mistake such as: <code>Runtime Error: Input type (torch.FloatTensor) dand weigh type (torch.cuda.FloatTensor) should be on the same device</code>. This error indicates that the input data is on CPU, while your weights are on the GPU. Make sure that all data is on the same device. This is usually the GPU as it support acceleration for both training and testing.</p>
<hr>
</section>
</section>
<section id="good-practices" class="level2">
<h2 class="anchored" data-anchor-id="good-practices">Good practices</h2>
<p>There are many good practices in PyTorch. We try to add a few below that might make your life easier. Another list of good practices can be found <a href="https://github.com/vahidk/EffectivePyTorch">here</a>.</p>
<section id="use-nn.sequential-and-nn.modulelist" class="level3">
<h3 class="anchored" data-anchor-id="use-nn.sequential-and-nn.modulelist">Use nn.Sequential and nn.ModuleList</h3>
<p>If you have a model with a lot of layers, you might want to summarize them into a <code>nn.Sequential</code> or <code>nn.ModuleList</code> object. In the forward pass, you only need to call the sequential, or iterate through the module list. A MLP can be implemented as follows:</p>
<div id="cell-7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dims<span class="op">=</span><span class="dv">64</span>, hidden_dims<span class="op">=</span>[<span class="dv">128</span>,<span class="dv">256</span>], output_dims<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        hidden_dims <span class="op">=</span> [input_dims] <span class="op">+</span> hidden_dims</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_dims)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            layers <span class="op">+=</span> [</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                nn.Linear(hidden_dims[i], hidden_dims[i<span class="op">+</span><span class="dv">1</span>]),</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>                nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="in-place-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="in-place-activation-functions">In-place activation functions</h3>
<p>Some activation functions like <code>nn.ReLU</code> or <code>nn.LeakyReLU</code> have the argument <code>inplace</code>. By default it is <code>False</code>, but it is recommended to set it to <code>True</code> in neural networks. What it does is that the forward pass overwrites the original values of the input with the new output. This option is only available for activation functions where we do not need to know the original input for backpropagation. For instance, in ReLU, the values that are set to zero, have a gradient of zero independent of its specific input value. In-place operation can save a bit of memory, especially if you have large feature maps.</p>
</section>
<section id="create-modules-for-repeating-blocks" class="level3">
<h3 class="anchored" data-anchor-id="create-modules-for-repeating-blocks">Create modules for repeating blocks</h3>
<p>In deep neural networks, you usually have blocks that are repeatedly added to the model. If those blocks require a more complex forward function than just <code>x = layer(x)</code>, it is recommended to implement them in a separate module. For example, a ResNet consists of multiple ResNet blocks with a residual connection. The ResNet blocks apply a small neural network, and add the output back to the input. It is better to implement this dynamic in a separate <code>nn.Module</code> class to keep the main model class small and clear.</p>
</section>
<section id="stack-layersweights-with-same-input" class="level3">
<h3 class="anchored" data-anchor-id="stack-layersweights-with-same-input">Stack layers/weights with same input</h3>
<p>If you have multiple linear layers or convolutions that have the same input, you can stack them together to increase efficiency. Suppose we have two layers on <span class="math inline">\(x\)</span>: <span class="math inline">\(y_1 = W_1x+b_1\)</span>, <span class="math inline">\(y_2=W_2x+b_2\)</span>. While you could implement it by two linear layers, you can get the exact same neural network by stacking the two layers into one. The single layer is more efficient as this represents a single matrix operation instead of two for the GPU, and hence we can parallelize the computation. An example is shown below:</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Implementation of separate layers:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>y1_layer <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">20</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y2_layer <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">30</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> y1_layer(x)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> y2_layer(x)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Implementation of a stacked layer:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>y_layer <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">50</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y_layer(x)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>y1, y2 <span class="op">=</span> y[:,:<span class="dv">20</span>], y[:,<span class="dv">20</span>:<span class="dv">50</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you implement the linear layer manually, you need to stack the weight and bias tensor accordingly. Note that you should change your initialization for the stacked case if necessary. If your initialization depends on the output size of a layer (as for example in Xavier), you would get a different standard deviation for initialization in the two implementations. Still, if your initialization solely depends on the input dimension (e.g.&nbsp;Kaiming), no change is necessary. An example case where this stacking can be beneficial is LSTMs as all four gates use the exact same input.</p>
</section>
<section id="use-loss-functions-on-logits" class="level3">
<h3 class="anchored" data-anchor-id="use-loss-functions-on-logits">Use loss functions on logits</h3>
<p>Classification loss functions such as Binary Cross Entropy have two versions in PyTorch: with and without logits. It is recommended and good practice to use the loss functions on logits. This is because it is numerically more stable and prevents any instabilities when your model is very wrong in its prediction. If you do not use the logit loss functions, you might run into problems when the model predicts very high or low values that are not correct. In BCE, you will then encounter a log over a value very close to 0. If you are lucky, you just get a very high number (and your model might still diverge because of this), or actually end up with NaN values.</p>
</section>
<section id="make-use-of-torch.nn.functional" class="level3">
<h3 class="anchored" data-anchor-id="make-use-of-torch.nn.functional">Make use of torch.nn.functional</h3>
<p>You do not always need modules. Many methods that do not have parameters are implemented as both modules and functions (e.g.&nbsp;log-softmax/softmax, binary cross entropy, etc.). If you need a softmax but do not have a <code>nn.Sequential</code> where you could add it to, the function option <code>F.softmax(...,dim=...)</code> is cleaner than defining a separate module first.</p>
</section>
<section id="clip-gradient-norms" class="level3">
<h3 class="anchored" data-anchor-id="clip-gradient-norms">Clip gradient norms</h3>
<p>Another good training practice is to clip gradient norms. Even if you set a high threshold, it can stop your model from diverging, even when it gets very high losses. While in MLPs not strictly necessary, RNNs, Transformers, and likelihood models can often benefit from gradient norm clipping. In PyTorch, you can use it via <code>torch.nn.utils.clip_grad_norm_(...)</code> (remember to call it after <code>loss.backward()</code> but before <code>optimizer.step()</code>). In PyTorch Lightning, you can set the clipping norm via <code>gradient_clip_val=...</code> in the Trainer.</p>
<hr>
<p><a href="https://github.com/phlippe/uvadlc_notebooks/"><img src="https://img.shields.io/static/v1.svg?logo=star&amp;label=⭐&amp;message=Star%20Our%20Repository&amp;color=yellow" class="img-fluid" alt="Star our repository"></a> If you found this tutorial helpful, consider ⭐-ing our repository.<br>
<a href="https://github.com/phlippe/uvadlc_notebooks/issues"><img src="https://img.shields.io/static/v1.svg?logo=star&amp;label=❔&amp;message=Ask%20Questions&amp;color=9cf" class="img-fluid" alt="Ask questions"></a> For any questions, typos, or bugs that you found, please raise an issue on GitHub.</p>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>LNU</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>