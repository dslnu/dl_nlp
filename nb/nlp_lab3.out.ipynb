{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan:\n",
        "\n",
        "1.  **Get tokens** for positive and negative tweets (by `token` in this\n",
        "    context we mean `word`).\n",
        "2.  **Lemmatize** them (convert to base word forms). For that we will\n",
        "    use a Part-of-Speech tagger.\n",
        "3.  **Clean’em up** (remove mentions, URLs, stop words).\n",
        "4.  **Prepare models** for the classifier, based on cleaned-up tokens.\n",
        "5.  **Run the Naive Bayes classifier**.\n",
        "\n",
        "First, download necessary prepared samples."
      ],
      "id": "264c48cf-3e28-431b-a38b-a5117929ec41"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk"
      ],
      "id": "42819310-88ea-463b-b6a8-cd88a312c4d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "id": "b2ee03cc-4142-4362-85e6-508597ad4f91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get some sample positive/negative tweets."
      ],
      "id": "36f11e03-d5c6-46d1-97e9-f26b4a7bae00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import twitter_samples\n"
      ],
      "id": "727ac170-6412-41e6-b6c6-1038906ffbd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can either get the actual string content of those tweets:"
      ],
      "id": "c8db614b-40f5-4262-b8f5-467895751af4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "id": "25b8a199-3b39-4635-b658-375493490988"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_tweets[50]"
      ],
      "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we can get a list of tokens using [tokenized\n",
        "method](https://www.nltk.org/howto/twitter.html) on `twitter_samples`."
      ],
      "id": "a30ac2db-8cda-4063-9357-3b77c2d9c274"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(tweet_tokens[50])"
      ],
      "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s setup a Part-of-Speech tagger. Download a perceptron tagger\n",
        "that will be used by the PoS tagger."
      ],
      "id": "528c0a1b-fef3-4efa-9b37-050a97e7461d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "id": "313b4107-4729-43c4-bb72-31be6498e4ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Part-of-Speech tagger that will be used for lemmatization"
      ],
      "id": "bbe15947-627f-4096-9f43-8151b8143ba0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "id": "8c64174c-0358-474c-a206-8f0038f7fb35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check how it works. Note that it returns tuples, where second element is\n",
        "a Part-of-Speech identifier."
      ],
      "id": "c3ede0df-4cb5-418a-b508-db9de08a6dd4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_tag(tweet_tokens[50])"
      ],
      "id": "4a962113-b87c-40f9-896c-a2799447d4f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s write a function that will lemmatize twitter tokens.\n",
        "\n",
        "For that, let’s first fetch a WordNet resource. WordNet is a\n",
        "semantically-oriented dictionary of English - check chapter 2.5 of the\n",
        "NLTK book. In online version, this is part 5\n",
        "[here](https://www.nltk.org/book/ch02.html)."
      ],
      "id": "95201c7c-313c-49c6-9a74-84349dc4a8b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ],
      "id": "560e3e7b-a172-471c-8052-6dabbae85813"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now fetch PoS tokens so that they can be passed to `WordNetLemmatizer`."
      ],
      "id": "6f80cdb1-9599-405e-a59d-defd22e7c3b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "tokens = tweet_tokens[50]\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_sentence = []\n",
        "# Convert PoS tags into a format used by the lemmatizer\n",
        "# and run lemmatize\n",
        "for word, tag in pos_tag(tokens):\n",
        "    if tag.startswith('NN'):\n",
        "        pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "        pos = 'v'\n",
        "    else:\n",
        "        pos = 'a'\n",
        "    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "print(lemmatized_sentence)"
      ],
      "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it converts words to their base forms (‘are’ -\\> ‘be’, ‘comes’\n",
        "-\\> ‘come’).\n",
        "\n",
        "Now we can proceed to processing. During processing, we will perform\n",
        "cleanup: - remove URLs and mentions using regexes - after lemmatization,\n",
        "remove *stopwords*"
      ],
      "id": "6efedf49-cc43-4b3f-bd90-c8c286a1ed0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ],
      "id": "478174c9-8375-4722-88f7-8b551ed5ea59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are these stopwords? Let’s see some."
      ],
      "id": "1b6e536e-f4e0-48e9-9029-df2766cb020a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(len(stop_words))\n",
        "for i in range(10):\n",
        "    print(stop_words[i])\n"
      ],
      "id": "ac4a45d9-327b-4d06-beb1-259809529c11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here comes the `process_tokens` function:"
      ],
      "id": "f5d18f19-7689-4b3f-9fa2-43cbbd8e50aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, string\n",
        "\n",
        "def process_tokens(tweet_tokens):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    stop_words = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        # Now note the sheer size of regex for URLs :)\n",
        "        # Mentions regex is comparatively short and sweet\n",
        "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
        "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
        "            continue\n",
        "\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "   \n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "id": "2862fbeb-2a75-4b09-997d-1767c07e0541"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s test `process_tokens`:"
      ],
      "id": "04ed0731-293f-4887-9007-29ba9277bf01"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Before:\", tweet_tokens[50])\n",
        "print(\"After:\", process_tokens(tweet_tokens[50]))"
      ],
      "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run `process_tokens` on all positive/negative tokens."
      ],
      "id": "3d07f978-0911-4012-aba2-7e936be5124c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
        "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
      ],
      "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s see how did the processing go."
      ],
      "id": "0de45ff4-c600-4174-b6cb-bc3c1f290cce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(positive_tweet_tokens[500])\n",
        "print(positive_cleaned_tokens_list[500])"
      ],
      "id": "6d90aa3b-6606-4492-901c-6f26519e926d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s see what is most common there. Add a helper function\n",
        "`get_all_words`:"
      ],
      "id": "b4c904cc-21df-46ee-937b-a3148efcf536"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "id": "400ab380-4422-48bb-a513-27ca12d3ac87"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform frequency analysis using `FreqDist`:"
      ],
      "id": "5add71c4-96c5-4932-94e2-1cf405b17109"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "id": "05fa8805-6013-496e-8673-51308d607e1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine. Now we’ll convert these to a data structure usable for NLTK’s\n",
        "naive Bayes classifier ([docs\n",
        "here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
      ],
      "id": "4bbb82b4-ebba-4eb6-9bdc-20ff7db77e32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
      ],
      "id": "790f1f82-d35d-45d9-90b3-52682501eb7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_token_dict(tokens):\n",
        "    return dict([token, True] for token in tokens)\n",
        "    \n",
        "def get_tweets_for_model(cleaned_tokens_list):   \n",
        "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create two datasets for positive and negative tweets. Use 7000/3000\n",
        "split for train and test data."
      ],
      "id": "70d626a9-7e14-4943-811a-061ef2d2b0c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "id": "18964303-5c20-4a88-bcd1-df5f7e703aad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we use the nltk’s NaiveBayesClassifier on the training data\n",
        "we’ve just created:"
      ],
      "id": "991d986a-c791-43fc-a076-81edc9e7f6cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "id": "a928308c-c221-4047-a3ff-dd56b2686c0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the Positive:Negative ratios.\n",
        "\n",
        "Let’s check some test phrase. First, download punkt sentence tokenizer\n",
        "([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
      ],
      "id": "1a07ac8e-9d50-43e1-b895-ff7a3d0c5c78"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ],
      "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we won’t rely on `twitter_samples.tokenized`, but rather will use a\n",
        "generic tokenization routine - `word_tokenize`."
      ],
      "id": "8ab70eaf-d911-48d1-bd46-29f008929951"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"the service was so bad\"\n",
        "\n",
        "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(get_token_dict(custom_tokens)))"
      ],
      "id": "01c24fbd-1dcd-4068-9831-c56c9482a636"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s package it as a function:"
      ],
      "id": "a1257deb-f53a-41cd-a5a4-d214817b1219"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentiment(text):\n",
        "    custom_tokens = process_tokens(word_tokenize(text))\n",
        "    return classifier.classify(get_token_dict(custom_tokens))\n",
        "\n",
        "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
        "for t in texts:\n",
        "    print(t, \": \", get_sentiment(t))\n"
      ],
      "id": "eea00802-a825-438b-b8ef-5c36519e6fba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seems ok!"
      ],
      "id": "2bce35fe-7cce-4b9a-8b87-8a1e70c6f3d2"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  }
}