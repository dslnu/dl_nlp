<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.29">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Transformers 1</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Transformers 1</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="attention" class="slide level2">
<h2>Attention</h2>
<div class="hidden">

</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Early methods</strong></p>
</div>
<div class="callout-content">
<ul>
<li>MLP</li>
<li>Convolutions</li>
<li>Recurrent NNs</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li>convolutions dominating <strong>image processing</strong></li>
<li>LSTM RNNs dominating <strong>NLP</strong></li>
</ul>
</div>
</div>
</div>
</section>
<section id="applications" class="slide level2">
<h2>Applications</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Applications of transformers</strong></p>
</div>
<div class="callout-content">
<ul>
<li>NLP</li>
<li>image recognition</li>
<li>speech recognition</li>
<li>reinforcement learning</li>
</ul>
</div>
</div>
</div>
</section>
<section id="attention-1" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>History</strong></p>
</div>
<div class="callout-content">
<p>Attention - originally proposed for encoded-decoder architectures.</p>
<p>Vaswani paper - “Attention is all you need”</p>
<p>Large-scale pretrained models, now sometimes called <strong>foundation models</strong>.</p>
</div>
</div>
</div>

<img data-src="img/attention_is_all_you_need.png" class="r-stretch"></section>
<section>
<section id="seq2seq-models" class="title-slide slide level1 center">
<h1>seq2seq models</h1>

</section>
<section id="seq2seq" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.</p>
</div>
</div>
</div>
<p><img data-src="img/seq2seq_1.png"> <img data-src="img/seq2seq_2.png"></p>
</section>
<section id="seq2seq-1" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Encoder/Decoder</strong></p>
</div>
<div class="callout-content">
<p>Under the hood, the model is composed of an encoder and a decoder.</p>
<p>The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the ). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</p>
<p>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks</p>
</div>
</div>
</div>

<img data-src="img/seq2seq_context.png" class="r-stretch"></section>
<section id="seq2seq-2" class="slide level2">
<h2>seq2seq</h2>
<p>RNN takes two inputs: an input (one word from the input sentence) and a hidden state.</p>
<p>Context: You can set the size of the context vector when you set up your model. It is basically the number of hidden units in the encoder RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024.</p>

<img data-src="img/seq2seq_embedding.png" class="r-stretch"></section>
<section id="seq2seq-3" class="slide level2">
<h2>seq2seq</h2>
<p><img data-src="img/seq2seq_dec1.png" height="150"> <img data-src="img/seq2seq_dec2.png" height="150"> <img data-src="img/seq2seq_dec3.png" height="150"></p>
</section>
<section id="seq2seq-4" class="slide level2">
<h2>seq2seq</h2>

<img data-src="img/seq2seq_rnn.png" class="r-stretch"></section>
<section id="seq2seq-5" class="slide level2">
<h2>seq2seq</h2>
<p>In the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.</p>
<p>Let’s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder.</p>
</section>
<section id="seq2seq-6" class="slide level2">
<h2>seq2seq</h2>
<p><img data-src="img/seq2seq_ts1.png" height="150"> <img data-src="img/seq2seq_ts3.png" height="150"> <img data-src="img/seq2seq_ts7.png" height="150"></p>
</section>
<section id="seq2seq-7" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Unrolled view</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/seq2seq_unrolled1.png" height="250"> <img data-src="img/seq2seq_unrolled2.png" height="250"></p>
</div>
</div>
</div>
</section>
<section id="seq2seq-8" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>The context vector turned out to be a bottleneck for these types of models: it was challenging for the models to deal with long sentences.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>A solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015.</p>
<ul>
<li>These papers introduced and refined a technique called <strong>attention</strong>, which highly improved the quality of machine translation systems.</li>
<li>Attention allows the model to focus on the relevant parts of the input sequence as needed.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="seq2seq-9" class="slide level2">
<h2>seq2seq</h2>

<!-- This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention. -->
<img data-src="img/seq2seq_att1.png" class="r-stretch quarto-figure-center"><p class="caption">At time step 7, the attention mechanism enables the decoder to focus on the word “étudiant” (“student” in french) before it generates the English translation.</p></section>
<section id="seq2seq-10" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Attention model differences</strong></p>
</div>
<div class="callout-content">
<p><strong>Encoder</strong> passes all hidden states to the decoder, not just the last state.</p>
<p><strong>Decoder</strong> does some extra step before producing its output:</p>
<ul>
<li>Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence</li>
<li>Give each hidden state a score</li>
<li>Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores</li>
<li>The scoring is done at each time step.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="seq2seq-11" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Illustrated</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/seq2seq_att2.png"></p>
</div>
</div>
</div>
</section>
<section id="seq2seq-12" class="slide level2">
<h2>seq2seq</h2>

<img data-src="img/seq2seq_att3.png" class="r-stretch"></section>
<section id="seq2seq-13" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Process</strong></p>
</div>
<div class="callout-content">
<ul>
<li>The attention decoder RNN takes in the embedding of the <end> token, and an initial decoder hidden state.</end></li>
<li>The RNN processes its inputs, producing an output and a new hidden state vector <span class="math inline">\(h_4\)</span>. The output is discarded.</li>
<li>Attention Step: We use the encoder hidden states and the <span class="math inline">\(h_4\)</span> vector to calculate a context vector <span class="math inline">\(c_4\)</span> for this time step.</li>
<li>We concatenate <span class="math inline">\(h_4\)</span> and <span class="math inline">\(c_4\)</span> into one vector.</li>
<li>We pass this vector through a feedforward neural network (one trained jointly with the model).</li>
<li>The output of the feedforward neural networks indicates the output word of this time step.</li>
<li>Repeat for the next time steps</li>
</ul>
</div>
</div>
</div>
</section>
<section id="seq2seq-14" class="slide level2">
<h2>seq2seq</h2>

<img data-src="img/seq2seq_att4.png" class="r-stretch"></section>
<section id="seq2seq-15" class="slide level2">
<h2>seq2seq</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Another way</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/seq2seq_att5.png"></p>
</div>
</div>
</div>
</section>
<section id="seq2seq-16" class="slide level2">
<h2>seq2seq</h2>

<img data-src="img/seq2seq_att6.png" class="r-stretch quarto-figure-center"><p class="caption">Model learns how to align words (example from paper).</p></section></section>
<section>
<section id="transformers" class="title-slide slide level1 center">
<h1>Transformers</h1>

</section>
<section id="transformers-1" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_arch_orig.png" class="r-stretch quarto-figure-center"><p class="caption">Attention is all you need (2017)</p></section>
<section id="transformers-2" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_arch1.png" class="r-stretch quarto-figure-center"><p class="caption">Compared to seq2seq, performance is improved!</p></section>
<section id="transformers-3" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_arch2.png" class="r-stretch quarto-figure-center"><p class="caption">Encoder and decoder structure.</p></section>
<section id="transformers-4" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_enc_dec_flow.png" class="r-stretch quarto-figure-center"><p class="caption">A key property of the Transformer: the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p></section>
<section id="transformers-5" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_enc_dec_flow2.png" class="r-stretch quarto-figure-center"><p class="caption">The word at each position passes through a <strong>self-attention process</strong>. Then, they each pass through a FFNN.</p></section>
<section id="transformers-6" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>High-level view</strong></p>
</div>
<div class="callout-content">
<p>What does the word “it” refer to?</p>
<blockquote>
<p>The animal didn’t cross the street because it was too tired</p>
</blockquote>
</div>
</div>
</div>

<img data-src="img/tfs_self_attention.png" class="r-stretch"></section>
<section id="transformers-7" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>High-level flow</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>when the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p></li>
<li><p>as the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p></li>
<li><p>for RNNs, maintaining a hidden state allows them to incorporate its representation of previous words/vectors it has processed with the current one it’s processing.</p></li>
<li><p>Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-8" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How is self-attention calculated?</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li>Create 3 vectors (Query, Key, Value) from each input embedding.</li>
</ol>
<p><img data-src="img/tfs_self_attention2.png" height="450"></p>
</div>
</div>
</div>
</section>
<section id="transformers-9" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How is self-attention calculated?</strong></p>
</div>
<div class="callout-content">
<ol start="2" type="1">
<li>Calculate <strong>score</strong>. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring.</li>
</ol>
<p><img data-src="img/tfs_self_attention3.png" height="450"></p>
</div>
</div>
</div>
</section>
<section id="transformers-10" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How is self-attention calculated?</strong></p>
</div>
<div class="callout-content">
<ol start="3" type="1">
<li><p>Divide by <span class="math inline">\(\sqrt{d_k}\)</span>.</p></li>
<li><p>Normalize via softmax.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/tfs_self_attention4.png" height="400"></p>
<figcaption>width=9cm</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="transformers-11" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How is self-attention calculated?</strong></p>
</div>
<div class="callout-content">
<ol start="5" type="1">
<li><p>Multiply each value vector by softmax score.</p>
<p><strong>Intuition</strong>: keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p></li>
<li><p>Sum up weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="transformers-12" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_self_attention5.png" class="r-stretch"></section>
<section id="transformers-13" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Matrix calculation</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/tfs_self_attention6.png" height="450"></p>
</div>
</div>
</div>
</section>
<section id="transformers-14" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Matrix calculation</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/tfs_self_attention7.png" height="450"></p>
</div>
</div>
</div>
</section>
<section id="transformers-15" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Multiheaded attention</strong></p>
</div>
<div class="callout-content">
<p>Introduced in the original paper.</p>
<p>This improves the performance of the attention layer in two ways:</p>
<ul>
<li>It expands the model’s ability to focus on different positions.</li>
<li>It gives the attention layer multiple “representation subspaces”.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-16" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_multiheaded_attention.png" class="r-stretch"></section>
<section id="transformers-17" class="slide level2">
<h2>Transformers</h2>
<p>Calculating e.g.&nbsp;8 times: <img data-src="img/tfs_multiheaded_attention2.png"></p>
</section>
<section id="transformers-18" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>Feed-forward layer is not expecting 8 matrices, but 1.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>Concat the matrices then multiply them by an additional weights matrix <span class="math inline">\(W_O\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="transformers-19" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_multiheaded_attention3.png" class="r-stretch"></section>
<section id="transformers-20" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_multiheaded_attention4.png" class="r-stretch quarto-figure-center"><p class="caption">Everything together.</p></section>
<section id="transformers-21" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_multiheaded_attention5.png" class="r-stretch quarto-figure-center"><p class="caption">The model’s representation of the word “it” bakes in some of the representation of both “animal” and “tired”.</p></section>
<section id="transformers-22" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_multiheaded_attention6.png" class="r-stretch quarto-figure-center"><p class="caption">With all attention heads.</p></section>
<section id="transformers-23" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>How do we account for the order of the words in the input sequence?</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>The transformer adds a vector to each input embedding.</p>
<p>These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence.</p>
<p>The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>
</div>
</div>
</div>
</section>
<section id="transformers-24" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_pe.png" class="r-stretch quarto-figure-center"><p class="caption">Values of positional encoding vectors follow a specific pattern.</p></section>
<section id="transformers-25" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_pe2.png" class="r-stretch quarto-figure-center"><p class="caption">A real example of positional encoding with a toy embedding size of 4.</p></section>
<section id="transformers-26" class="slide level2">
<h2>Transformers</h2>
<p><!-- %You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors. --> <img data-src="img/tfs_pe3.png" height="600" alt="PE for 20 words (rows) with an embedding size of 512 (columns)."></p>
</section>
<section id="transformers-27" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>A formule for PE from the paper</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
  PE_{(pos,2i)} =sin(pos/10000^{2i/d_{model}}),\\
  PE_{(pos,2i+1)} =cos(pos/10000^{2i/d_{model}})
\]</span> where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension.</p>
<p>That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from <span class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000 \cdot 2\pi\)</span>. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math inline">\(PE_{pos}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="transformers-28" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_layer_norm.png" class="r-stretch quarto-figure-center"><p class="caption">Each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.</p></section>
<section id="transformers-29" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_layer_norm2.png" class="r-stretch quarto-figure-center"><p class="caption">With vectors and operations visualized.</p></section>
<section id="transformers-30" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_layer_norm3.png" class="r-stretch quarto-figure-center"><p class="caption">Transformer of 2 stacked encoders and decoders</p></section>
<section id="transformers-31" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder</strong></p>
</div>
<div class="callout-content">
<ul>
<li>The encoders start by processing the input sequence</li>
<li>The output of the top encoder is then transformed into a set of attention vectors <span class="math inline">\(\boldsymbol{K}\)</span> and <span class="math inline">\(\boldsymbol{V}\)</span></li>
<li>These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.</li>
<li>Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-32" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_decoder.png" class="r-stretch"></section>
<section id="transformers-33" class="slide level2">
<h2>Transformers</h2>
<!-- The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. -->

<img data-src="img/tfs_decoder2.png" class="r-stretch"></section>
<section id="transformers-34" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder</strong></p>
</div>
<div class="callout-content">
<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>
<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>
</div>
</div>
</div>
</section>
<section id="transformers-35" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder</strong></p>
</div>
<div class="callout-content">
<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>
<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>
<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>
<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>
</div>
</div>
</div>
</section>
<section id="transformers-36" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_decoder_output.png" class="r-stretch quarto-figure-center"><p class="caption">This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.</p></section>
<section id="transformers-37" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training a model</strong></p>
</div>
<div class="callout-content">
<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>
<!-- To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)). -->
<p><img data-src="img/tfs_training.png"></p>
</div>
</div>
</div>
</section>
<section id="transformers-38" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_training2.png" class="r-stretch quarto-figure-center"><p class="caption">Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector.</p></section>
<section id="transformers-39" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The loss function</strong></p>
</div>
<div class="callout-content">
<p>We want the output to be a probability distribution indicating the word “thanks”.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/tfs_training3.png" height="400"></p>
<figcaption>The (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model’s weights.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="transformers-40" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Comparison</strong></p>
</div>
<div class="callout-content">
<p>How do you compare two probability distributions? <strong>Kullback–Leibler divergence</strong>.</p>
<p>We want our model to successively output probability distributions where:</p>
<ul>
<li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
<li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
<li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
<li>And so on, until the fifth output distribution indicates <end_of_sentence symbol="">, which also has a cell associated with it from the 10,000 element vocabulary.</end_of_sentence></li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-41" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_training4.png" class="r-stretch quarto-figure-center"><p class="caption">The targeted probability distributions.</p></section>
<section id="transformers-42" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/tfs_training5.png" class="r-stretch quarto-figure-center"><p class="caption">Produced probability distributions.</p></section>
<section id="attention-2" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Database analogy</strong></p>
</div>
<div class="callout-content">
<p>Denote by <span class="math display">\[
D \equiv \left\{(k_1, v_1, \dots, (k_m, v_m)\right\}
\]</span> a database of <span class="math inline">\(m\)</span> tuples of keys and values. Denote by <span class="math inline">\(\boldsymbol{q}\)</span> a query.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Attention definition</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
Attention(\boldsymbol{q}, D) \equiv \sum\limits_{i=1}^m \alpha(\boldsymbol{q}, k_i)v_i,
\]</span> where <span class="math inline">\(\alpha(\boldsymbol{q},k_i) \in \mathbb{R}\)</span> are scalar attention weights.</p>
<p>The operation itself is typically referred to as attention pooling. The name attention derives from the fact that the operation pays particular attention to the terms for which the weight <span class="math inline">\(\alpha\)</span> is significant (i.e., large).</p>
<!-- As such, the attention over generates a linear combination of values contained in the database. In fact, this contains the above example as a special case where all but one weight is zero. -->
</div>
</div>
</div>
</section>
<section id="attention-3" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Special cases</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(\alpha(\boldsymbol{q}, k_i) \geq 0\)</span> – output of the attention mechanism is contained in the convex cone spanned by the values <span class="math inline">\(\boldsymbol{v}_i\)</span></li>
<li><span class="math inline">\(\sum_i \alpha(\boldsymbol{q}, k_i) = 1, \alpha(q,k_i) \geq 0 \; \forall i\)</span> – most common</li>
<li><span class="math inline">\(\exists j: \alpha(\boldsymbol{q}, k_j) = 1, \; \alpha(\boldsymbol{q}, k_i) = 0, i \neq j\)</span> – traditional database query</li>
<li><span class="math inline">\(\alpha(\boldsymbol{q}, k_i) = \frac{1}{m} \; \forall i\)</span> – <strong>average pooling</strong></li>
</ul>
</div>
</div>
</div>
</section>
<section id="attention-4" class="slide level2">
<h2>Attention</h2>
<p>Strategies:</p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Normalization</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
       \alpha(\boldsymbol{q}, k_i) = \frac{\alpha(\boldsymbol{q}, k_i)}{\sum_j \alpha(\boldsymbol{q}, k_j)}.
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Exponentiation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\alpha(\boldsymbol{q}, k_i) = \frac{exp\left(\alpha(\boldsymbol{q}, k_i)\right)}{\sum_j exp\left(\alpha(\boldsymbol{q}, k_j)\right)}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="attention-5" class="slide level2">
<h2>Attention</h2>



<img data-src="img/attention_mechanism.png" class="r-stretch"></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":"5a0d421e46d0c2c78e7b182b8b29b957","id":"6014523e1f7f8eebaf61465d275fa1cada70b712be9ad2192f79bc31536bc3a1"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>