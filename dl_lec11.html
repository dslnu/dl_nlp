<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.29">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Transformers 1</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Transformers 1</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="attention" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualization</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">def</span> show_heatmaps(matrices, xlabel, ylabel, titles<span class="op">=</span><span class="va">None</span>, figsize<span class="op">=</span>(<span class="fl">2.5</span>, <span class="fl">2.5</span>),</span>
<span id="cb1-2"><a></a>                cmap<span class="op">=</span><span class="st">'Reds'</span>):</span>
<span id="cb1-3"><a></a>  <span class="co">"""Show heatmaps of matrices."""</span></span>
<span id="cb1-4"><a></a>  d2l.use_svg_display()</span>
<span id="cb1-5"><a></a>  num_rows, num_cols, _, _ <span class="op">=</span> matrices.shape</span>
<span id="cb1-6"><a></a>  fig, axes <span class="op">=</span> d2l.plt.subplots(num_rows, num_cols, figsize<span class="op">=</span>figsize,</span>
<span id="cb1-7"><a></a>                               sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>, squeeze<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-8"><a></a>  <span class="cf">for</span> i, (row_axes, row_matrices) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes, matrices)):</span>
<span id="cb1-9"><a></a>      <span class="cf">for</span> j, (ax, matrix) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(row_axes, row_matrices)):</span>
<span id="cb1-10"><a></a>          pcm <span class="op">=</span> ax.imshow(matrix.detach().numpy(), cmap<span class="op">=</span>cmap)</span>
<span id="cb1-11"><a></a>          <span class="cf">if</span> i <span class="op">==</span> num_rows <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb1-12"><a></a>              ax.set_xlabel(xlabel)</span>
<span id="cb1-13"><a></a>          <span class="cf">if</span> j <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-14"><a></a>              ax.set_ylabel(ylabel)</span>
<span id="cb1-15"><a></a>          <span class="cf">if</span> titles:</span>
<span id="cb1-16"><a></a>              ax.set_title(titles[j])</span>
<span id="cb1-17"><a></a>  fig.colorbar(pcm, ax<span class="op">=</span>axes, shrink<span class="op">=</span><span class="fl">0.6</span>)<span class="op">;</span> </span>
<span id="cb1-18"><a></a></span>
<span id="cb1-19"><a></a>attention_weights <span class="op">=</span> torch.eye(<span class="dv">10</span>).reshape((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb1-20"><a></a>show_heatmaps(attention_weights, xlabel<span class="op">=</span><span class="st">'Keys'</span>, ylabel<span class="op">=</span><span class="st">'Queries'</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="attention-1" class="slide level2">
<h2>Attention</h2>

<img data-src="img/attention_identity_vis.png" class="r-stretch"></section>
<section id="attention-2" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>Regression and classification via kernel density estimation.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Nadaraya-Watson estimators</strong></p>
</div>
<div class="callout-content">
<p>Rely on a similarity kernel <span class="math inline">\(\alpha(q,k)\)</span> relating queries <span class="math inline">\(q\)</span> to keys <span class="math inline">\(k\)</span>.</p>
<ul>
<li><span class="math inline">\(\alpha(\bb{q}, k) = \exp\left(-\frac{1}{2} \|q-k\|^2\right)\)</span> – Gaussian</li>
<li><span class="math inline">\(\alpha(\bb{q}, k) = 1 \text{ if } \|q-k\| \leq 1\)</span> – Boxcar</li>
<li><span class="math inline">\(\alpha(\bb{q}, k) = \max\left(0, 1-\|q-k\|\right)\)</span> – Epanechikov</li>
</ul>
</div>
</div>
</div>
</section>
<section id="attention-3" class="slide level2">
<h2>Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>An equation for regression and classification</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
f(\bb{q}) = \sum_i v_i \frac{\alpha(\bb{q}, k_i)}{\sum_j \alpha(\bb{q}, k_j)}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="attention-4" class="slide level2">
<h2>Attention</h2>

<img data-src="img/kernel_vis.png" class="r-stretch"></section>
<section id="attention-5" class="slide level2">
<h2>Attention</h2>

<img data-src="img/kernel_estimates.png" class="r-stretch"><div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>These estimators demonstrate the limits of hand-crafted attention mechanisms.</p>
</div>
</div>
</div>
</section>
<section>
<section id="attention-scoring-functions" class="title-slide slide level1 center">
<h1>Attention scoring functions</h1>

</section>
<section id="attention-scoring-functions-1" class="slide level2">
<h2>Attention scoring functions</h2>

<img data-src="img/attention_scoring_function.png" class="r-stretch quarto-figure-center"><p class="caption">Computing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function and the softmax operation.</p></section></section>
<section>
<section id="historical-perspective" class="title-slide slide level1 center">
<h1>Historical perspective</h1>
<!-- From "Machine learning with PyTorch and Scikit-Learn" book -->
</section>
<section id="history" class="slide level2">
<h2>History</h2>

<img data-src="img/enc_dec_traditional.png" class="r-stretch quarto-figure-center"><p class="caption">RNN model for seq2seq. Parses entire input sequence.</p></section>
<section id="history-1" class="slide level2">
<h2>History</h2>

<img data-src="img/att_word_by_word.png" class="r-stretch quarto-figure-center"><p class="caption">Why parse the whole sentence? Translating a sentence word by word can lead to grammatical errors.</p></section>
<section id="history-2" class="slide level2">
<h2>History</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problems</strong></p>
</div>
<div class="callout-content">
<ul>
<li>RNN is trying to remember the entire input before translation</li>
<li>compression (encoding) might cause loss of information</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>Use attention mechanism to assign different attention weights to each input element.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="bahdanau-cho-bengio-paper" class="title-slide slide level1 center">
<h1>Bahdanau, Cho, Bengio paper</h1>

</section>
<section id="bahdanau" class="slide level2">
<h2>Bahdanau</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rnn_bahdanau.png"></p>
<figcaption>RNN with attention mechanism.</figcaption>
</figure>
</div>

<aside><div>
<p>Title: “Neural Machine Translation by Jointly Learning to Align and Translate”, 2014.</p>
</div></aside></section>
<section id="bahdanau-1" class="slide level2">
<h2>Bahdanau</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>RNN #1</strong></p>
</div>
<div class="callout-content">
<ul>
<li>generate hidden states from forward (<span class="math inline">\(h_F^{(i)}\)</span>) and backward (<span class="math inline">\(h_B^{(i)}\)</span>) passes</li>
<li>concatenate the above into <span class="math inline">\(h^{(i)}\)</span>.</li>
<li>generate context vectors <span class="math inline">\(c_i\)</span> from <span class="math inline">\(h^{(i)}\)</span> via attention mechanism</li>
</ul>
<p><span class="math display">\[
c_i = \sum\limits_{j=1}^T a_{ij}h^{(j)}.
\]</span></p>
</div>
</div>
</div>
<p>\begin{frame}{Bahdanau} :::{.callout-tip icon=false} ## RNN #2</p>
<p>Hidden states <span class="math inline">\(s^{(i)}\)</span> depend on:</p>
<ul>
<li>previous hidden state <span class="math inline">\(s^{(i-1)}\)</span></li>
<li>previous target word <span class="math inline">\(y^{(i-1)}\)</span></li>
<li>context vector <span class="math inline">\(c^{(i)}\)</span>. :::</li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Attention weights computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp (e_{ij})}{\sum\limits_{k=1}^T \exp (e_{ik})},
\]</span> where <span class="math inline">\(e_{ij}\)</span> is an alignment score evaluating how well the input around position <span class="math inline">\(j\)</span> matches the output around position <span class="math inline">\(i\)</span>.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="self-attention" class="title-slide slide level1 center">
<h1>Self-attention</h1>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Self-attention</strong></p>
</div>
<div class="callout-content">
<p>Transformer can be thought of as a NN architecture entirely based on attention, without the RNN parts.</p>
<p>This is also called <strong>self-attention</strong>.</p>
</div>
</div>
</div>
</section>
<section id="self-attention-2" class="slide level2">
<h2>Self-attention</h2>
<p>Suppose we have an input sequence <span class="math inline">\(x^{(1)},\dots,x^{(T)}\)</span>, and output sequence <span class="math inline">\(z^{(1)},\dots,z^{(T)}\)</span>. Here <span class="math inline">\(x^{(i)}, z^{(i)} \in \mathbb{R}^d\)</span>.</p>
<p>For a seq2seq task, the goal of self-attention is to model the dependencies of the current input element to all other input elements.</p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Self-attention stages</strong></p>
</div>
<div class="callout-content">
<ul>
<li>derive importance weights based on the similarity between the current element and all other elements in the sequence</li>
<li>normalize the weights, which usually involves softmax</li>
<li>use these weights in combination with the corresponding sequence elements to compute attention value</li>
</ul>
</div>
</div>
</div>
</section>
<section id="self-attention-3" class="slide level2">
<h2>Self-attention</h2>
<p><span class="math display">\[
z^{(i)} = \sum\limits_{j=1}^T \alpha_{ij} x^{(j)},\\
\omega_{ij} = x^{(i)T}x^{(j)},\\
\alpha_{ij} = \frac{\exp \omega_{ij}}{\sum\limits_{j=1}^T \exp \omega_{ij}} = softmax \left([w_{ij}]_{j=1\dots T}\right), \\
\sum\limits_{j=1}^T \alpha_{ij} = 1.
\]</span></p>
</section>
<section id="self-attention-4" class="slide level2">
<h2>Self-attention</h2>

<img data-src="img/basic_self_attention.png" class="r-stretch"></section>
<section id="self-attention-5" class="slide level2">
<h2>Self-attention</h2>
<p>Scaled dot-product attention: introduce learnable parameters.</p>
<p>Introduce three weight matrices: <span class="math inline">\(U_q, U_k, U_v\)</span>: <span class="math display">\[
q^{(i)} = U_q x^{(i)},\\
k^{(i)} = U_k x^{(i)},\\
v^{(i)} = U_v x^{(i)},\\
\]</span></p>
</section>
<section id="self-attention-6" class="slide level2">
<h2>Self-attention</h2>

<img data-src="img/att_scaled_self_attention.png" class="r-stretch"></section>
<section id="self-attention-7" class="slide level2">
<h2>Self-attention</h2>
<p><span class="math display">\[
\omega_{ij} = q^{(i)T} k^{(j)},\\
\alpha_{ij} = softmax \left(\frac{\omega_{ij}}{\sqrt{m}}\right),\\
z^{(i)} = \sum\limits_{j=1}^T \alpha_{ij} v^{(j)}.
\]</span></p>
</section>
<section id="self-attention-8" class="slide level2">
<h2>Self-attention</h2>

<img data-src="img/qk_matrix_eval.png" class="r-stretch quarto-figure-center"><p class="caption">How is <span class="math inline">\(\bb{q}k\)</span> evaluated.</p></section>
<section id="self-attention-9" class="slide level2">
<h2>Self-attention</h2>

<img data-src="img/att_information_flow.png" class="r-stretch quarto-figure-center"><p class="caption">Information flow in a scaled dot-product self-attention layer. How is <span class="math inline">\(\bb{q}k\)</span> evaluated.</p></section></section>
<section>
<section id="transformers" class="title-slide slide level1 center">
<h1>Transformers</h1>

<img data-src="img/transformers.png" class="r-stretch quarto-figure-center"><p class="caption">Original transformer architecture.</p></section>
<section id="transformers-1" class="slide level2">
<h2>Transformers</h2>
<p><strong>Multi-head self attention:</strong> a modification of scaled dot-product attention.</p>
<p>Multiple heads (sets of query, key, value matrices), similar to multiple kernels in CNNs.</p>
<p>Read sequential input <span class="math inline">\(\bb{X} = \left(x^{(1)},\dots,x^{(T)}\right)\)</span>. Suppose each element is embedded by a vector of length <span class="math inline">\(d\)</span>. Therefore, input can be embedded into a <span class="math inline">\(T\times d\)</span> matrix. Then, create <span class="math inline">\(h\)</span> sets of query, key, value matrices:</p>
<p><span class="math display">\[
U_{q_1},  U_{k_1}, U_{v_1},\\
\cdots \\
U_{q_h},  U_{k_h}, U_{v_h}.
\]</span></p>
<p><span class="math inline">\(U_{q_j},U_{k_j}\)</span> have shape <span class="math inline">\(d_k \times d\)</span>.</p>
<p><span class="math inline">\(U_{v_j}\)</span> has shape <span class="math inline">\(d_v \times d\)</span>.</p>
<p>Resulting value sequence has length <span class="math inline">\(d_v\)</span>.</p>
</section>
<section id="transformers-2" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Practical considerations</strong></p>
</div>
<div class="callout-content">
<ul>
<li>In practice, <span class="math inline">\(d_k = d_v = m\)</span> is often chosen for simplicity.</li>
<li>In practice, rather than having a separate matrix for each attention head, transformer implementations use a single matrix for all attention heads. The attention heads are then organized into logically separate regions in this matrix, which can be accessed via Boolean masks. This makes it possible to implement multi-head attention more efficiently because multiple matrix multiplications can be implemented as a single matrix multiplication instead. However, for simplicity, we are omitting this implementation detail in this section.</li>
<li>computation can all be done in parallel because there are no dependencies between the multiple heads.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-3" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Vectorized computations</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
q_j^{(i)} = U_{q_j}x^{(i)}.
\]</span> Then we concatenate vectors.</p>
</div>
</div>
</div>
</section>
<section id="transformers-4" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/transformers_concat.png" class="r-stretch quarto-figure-center"><p class="caption">Concatenating the scaled dot-product attention vectors into one vector and passing it through a linear projection.</p></section>
<section id="transformers-5" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder layer</strong></p>
</div>
<div class="callout-content">
<p>Similar to the encoder, the decoder also contains several repeated layers. Besides the two sublayers that we have already introduced in the previous encoder section (the multi-head self-attention layer and fully connected layer), each repeated layer also contains a masked multi-head attention sublayer.</p>
<p>Masked attention is a variation of the original attention mechanism, where masked attention only passes a limited input sequence into the model by “masking” out a certain number of words. For example, if we are building a language translation model with a labeled dataset, at sequence position <span class="math inline">\(i\)</span> during the training procedure, we only feed in the correct output words from positions <span class="math inline">\(1,\dots,i-1\)</span>. All other words (for instance, those that come after the current position) are hidden from the model to prevent the model from “cheating.” This is also consistent with the nature of text generation: although the true translated words are known during training, we know nothing about the ground truth in practice. Thus, we can only feed the model the solutions to what it has already generated, at position <span class="math inline">\(i\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="transformers-6" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/transformers_decoder.png" class="r-stretch quarto-figure-center"><p class="caption">Concatenating the scaled dot-product attention vectors into one vector and passing it through a linear projection.</p></section>
<section id="transformers-7" class="slide level2">
<h2>Transformers</h2>
<ul>
<li>the previous output words (output embeddings) are passed into the masked multi-head attention layer.</li>
<li>the second layer receives both the encoded inputs from the encoder block and the output of the masked multi-head attention layer into a multi-head attention layer.</li>
<li>Finally, we pass the multi-head attention outputs into a fully connected layer that generates the overall model output: a probability vector corresponding to the output words.</li>
</ul>
</section>
<section id="transformers-8" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/transformers_layer_normalization.png" class="r-stretch quarto-figure-center"><p class="caption">Layer normalization mechanism, which was first introduced by J. Ba, J.R. Kiros, and G.E. Hinton in 2016 in the same-named paper Layer Normalization (URL: https://arxiv.org/ abs/1607.06450).</p></section>
<section id="transformers-9" class="slide level2">
<h2>Transformers</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Recursion vs recurrence</strong></p>
</div>
<div class="callout-content">
<p>RNNs are <strong>recurrent</strong>.</p>
<p>Transformers are <strong>recursive</strong> – therefore no need to unroll.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Attention benefits</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Much simpler than CNNs or RNNs.</li>
<li>No vanishing/exploding gradients problems.</li>
<li>Attention can be thought of as a single convolution kernel spanning the whole sequence of tokens.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers-10" class="slide level2">
<h2>Transformers</h2>

<img data-src="img/transformers_recursion.png" class="r-stretch"></section>
<section id="transformers-11" class="slide level2">
<h2>Transformers</h2>



<img data-src="img/att_learned_weights.png" class="r-stretch quarto-figure-center"><p class="caption">Example of learned attention weights (Vaswani 2017 paper)</p></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"422433a0ba05fc61fae86fba5472bf329e67b28592de698028ca670214f8a8b2"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>