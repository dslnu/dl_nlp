<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="MSDE">
  <title>Deep Learning/NLP course – Current frontiers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Current frontiers</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
MSDE 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="llm-tree" class="slide level2">
<h2>LLM tree</h2>

<img data-src="img/llm_dev_paths.png" class="r-stretch"><p><!-- % note: https://arxiv.org/pdf/2310.03003 --> <!-- % Transformer-based: non-grey --> <!-- % Encoder-only: pink --> <!-- % Decoder-only: blue --> <!-- % Encoder-Decoder: green --></p>
<!-- % note: https://arxiv.org/pdf/2310.11453 -->
</section>
<section id="bit-llms" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="hidden">

</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>LLM challenges</strong></p>
</div>
<div class="callout-content">
<ul>
<li>complex deployment</li>
<li>memory bandwidth issues</li>
<li>environmental impact</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BitNet paper</strong></p>
</div>
<div class="callout-content">
<p>BitNet: Scaling Transformers for Large Language Models (2023 paper)</p>
</div>
</div>
</div>
</section>
<section id="bit-llms-1" class="slide level2">
<h2>1-bit LLMs</h2>
<p><!-- % note: https://arxiv.org/pdf/2106.08295 --></p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>One solution - Quantization</strong></p>
</div>
<div class="callout-content">
<p>In neural network quantization, the weights and activation tensors are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in.</p>
<ul>
<li>post-training approach - easier to apply</li>
<li>quantization-aware training - better accuracy</li>
</ul>
<p>A floating-point vector <span class="math inline">\(\boldsymbol{x}\)</span> can be expressed approximately as a scalar multiplied by a vector of integer values: <span class="math display">\[
\hat{\boldsymbol{x}} = s_{\boldsymbol{x}} \cdot \boldsymbol{x}_{int} \approx \boldsymbol{x}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-2" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/quantization1.png" class="r-stretch"></section>
<section id="bit-llms-3" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/quantization2.png" class="r-stretch"></section>
<section id="bit-llms-4" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hw_accelerators.png" height="550"></p>
<figcaption>Another solution: Hardware accelerators</figcaption>
</figure>
</div>
</section>
<section id="bit-llms-5" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hardware accelerator types</strong></p>
</div>
<div class="callout-content">
<ul>
<li>FPGA-based accelerators</li>
<li>CPU and GPU-based</li>
<li>ASICs</li>
<li>In-memory</li>
</ul>
</div>
</div>
</div>
</section>
<section id="bit-llms-6" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/groq.jpg" class="r-stretch quarto-figure-center"><p class="caption">Groq LPU die</p></section>
<section id="bit-llms-7" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Binarization</strong></p>
</div>
<div class="callout-content">
<p>An extreme case of quantization, applied to large language models.</p>
<p>Previuosly applied to CNNs and Transformers.</p>
</div>
</div>
</div>
</section>
<section id="bitnet" class="slide level2">
<h2>BitNet</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>BitNet architecture is a Transformer that replaces <strong>nn.Linear</strong> with <strong>BitLinear</strong>.</p>
<ul>
<li>1-bit Transformer architecture</li>
<li>low-precision binary weights</li>
<li>quantized activations</li>
</ul>
</div>
</div>
</div>
</section>
<section id="bit-llms-8" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet1.png" class="r-stretch quarto-figure-center"><p class="caption">BitNet trains 1-bit Transformers from scratch, obtaining competitive results in an energy-efficient way.</p></section>
<section id="bit-llms-9" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet2.png" class="r-stretch quarto-figure-center"><p class="caption">Left: The computation flow of BitLinear. Right: The architecture of BitNet, consisting of the stacks of attentions and FFNs, where matrix multiplication is implemented as BitLinear.</p></section>
<section id="bit-llms-10" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BitLinear: Binarization</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\tilde{W} = sign\left(W-\alpha\right),\\
sign(W_{ij}) = \begin{cases} +1, \; W_{ij} &gt; 0,\\
   -1, \; W_{ij} \leq 0\end{cases},\\
\alpha=\frac{1}{nm} \sum\limits{ij}W_{ij},\\
  W \in \mathbb{R}^{n \times m}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-11" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BitLinear: Quantization</strong></p>
</div>
<div class="callout-content">
<p>Scale activations into the <span class="math inline">\([-Q_b, Q_b]\)</span> range: <span class="math display">\[
\tilde{x} = Quant(x) = Clip(x \times \frac{Q_b}{\gamma}, -Q_b+\epsilon, Q_b - \epsilon),\\
Clip(x,a,b) = \max(a, \min(b,x)), \; \gamma = \|x\|_{\infty}, \; Q_b = 2^{b-1}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-12" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BitLinear: Formulation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
y = \tilde{W} \tilde{x} = \tilde{W} \times Quant(LN(x)) \times \frac{\beta \gamma}{Q_b},\\
LN(x) = \frac{x-E(x)}{\sqrt{Var(x) + \epsilon}}, \; \beta = \frac{1}{nm}\|W\|_1.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-parallelism" class="slide level2">
<h2>1-bit LLMs: Parallelism</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Model parallelism with Group Quantization and Normalization</strong></p>
</div>
<div class="callout-content">
<p>Partitioning matrix multiplication on multiple devices.</p>
</div>
</div>
</div>

<!-- % this frame is from https://huggingface.co/docs/transformers/v4.17.0/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism -->
<img data-src="img/model_parallelism1.png" class="r-stretch"></section>
<section id="bit-llms-parallelism-1" class="slide level2">
<h2>1-bit LLMs: Parallelism</h2>

<img data-src="img/model_parallelism2.png" class="r-stretch quarto-figure-center"><p class="caption">Tensor parallelism</p></section>
<section id="bit-llms-parallelism-2" class="slide level2">
<h2>1-bit LLMs: Parallelism</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>A prerequisite for the existing model parallelism approaches is that the tensors are independent along the partition dimension. However, all of the parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\eta\)</span> are calculated from the whole tensors, breaking the independent prerequisite.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>Divide the weights and activations into groups and then independently estimate each group’s parameters. This way, the parameters can be calculated locally without requiring additional communication. This approachis called <strong>Group Quantization</strong>.</p>
</div>
</div>
</div>
</section>
<section id="bit-llms-parallelism-3" class="slide level2">
<h2>1-bit LLMs: Parallelism</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Group Quantization</strong></p>
</div>
<div class="callout-content">
<p>Divide the weight matrix <span class="math inline">\(W\)</span> into <span class="math inline">\(G\)</span> groups, and estimate parameters independently: <span class="math display">\[
\alpha_g = \frac{G}{nm} \sum\limits_{ij} W_{ij}^{(g)}, \\
\beta_g = \frac{G}{nm} \|W^{(g)}\|_1,\\
\gamma_g = \|x^{(g)}\|_{\infty},\\
\eta_g = \min\limits_{ij} x_{ij}^{(g)}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-13" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet_energy.png" class="r-stretch"></section>
<section id="bit-llms-14" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/1bit_energy_consumption.png" class="r-stretch"></section>
<section id="bit-llms-15" class="slide level2">
<h2>1-bit LLMs</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Vanilla transfomers</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
E_{add} = m \times (n-1) \times p \times \hat{E}_{add},\\
E_{mul} = m \times n \times p \times \hat{E}_{mul}.
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BitNet</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
E_{add} = m \times (n-1) \times p \times \hat{E}_{add},\\
E_{mul} = (m \times p + m \times n) \hat{E}_{mul}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bit-llms-16" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet3.png" class="r-stretch quarto-figure-center"><p class="caption">Scaling curves of BitNet and FP16 Transformers.</p></section>
<section id="bit-llms-17" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet_performance.png" class="r-stretch quarto-figure-center"><p class="caption">Zero-shot (Left) and few-shot (Right) performance of BitNet and FP16 Transformer against the inference cost.</p></section>
<section id="bit-llms-18" class="slide level2">
<h2>1-bit LLMs</h2>

<img data-src="img/bitnet_stability.png" class="r-stretch quarto-figure-center"><p class="caption">BitNet is more stable than FP16 Transformer with a same learning rate (Left). The training stability enables BitNet a larger learning rate, resulting in better convergence (Right).</p></section>
<section id="bit-llms-19" class="slide level2">
<h2>1-bit LLMs</h2>

<!-- % note: https://arxiv.org/pdf/2402.17764 -->
<img data-src="img/bitnet4.png" class="r-stretch quarto-figure-center"><p class="caption">Zero-shot (Left) and few-shot (Right) results for BitNet and the post-training quantization baselines on downstream tasks.</p></section>
<section id="bitnet-b1.58" class="slide level2">
<h2>BitNet b1.58</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>b1.58</strong></p>
</div>
<div class="callout-content">
<p><strong>BitNet b1.58</strong>: every single parameter (or weight) of the LLM is ternary <span class="math inline">\(\{-1, 0, 1\}\)</span>.</p>
<p>Matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.</p>
</div>
</div>
</div>
</section>
<section id="bitnet-b1.58-1" class="slide level2">
<h2>BitNet b1.58</h2>

<img data-src="img/b158.png" class="r-stretch"></section>
<section id="bitnet-b1.58-2" class="slide level2">
<h2>BitNet b1.58</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Advantages vs vanilla BitNet</strong></p>
</div>
<div class="callout-content">
<ul>
<li>explicit support for <strong>feature filtering</strong>, made possible by the inclusion of <span class="math inline">\(0\)</span> in the model weights<br>
</li>
<li>experiments show that BitNet b1.58 can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a 3B size, when using the same configuration</li>
</ul>
</div>
</div>
</div>
</section>
<section id="bitnet-b1.58-3" class="slide level2">
<h2>BitNet b1.58</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>b1.58: Quantization</strong></p>
</div>
<div class="callout-content">
<p><strong>absmean</strong> quantization function: first scales the weight matrix by its average absolute value, then rounds each value to the nearest integer among <span class="math inline">\(\{-1, 0, +1\}\)</span>: <span class="math display">\[
\tilde{W} = RoundClip\left(\frac{W}{\gamma + \epsilon}, -1, 1\right),\\
RoundClip(x, a, b) = \max(a, \min(b, round(x))), \\
\gamma = \frac{1}{nm} \sum\limits_{ij} |W_{ij}|.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bitnet-b1.58-4" class="slide level2">
<h2>BitNet b1.58</h2>

<img data-src="img/b158_perplexity.png" class="r-stretch quarto-figure-center"><p class="caption">Perplexity as well as the cost of BitNet b1.58 and LLaMA LLM.</p></section>
<section id="bitnet-b1.58-5" class="slide level2">
<h2>BitNet b1.58</h2>

<img data-src="img/b158_latency.png" class="r-stretch quarto-figure-center"><p class="caption">Decoding latency (Left) and memory consumption (Right) of BitNet b1.58 varying the model size.</p></section>
<section id="bitnet-b1.58-6" class="slide level2">
<h2>BitNet b1.58</h2>

<img data-src="img/b158_energy.png" class="r-stretch quarto-figure-center"><p class="caption">Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On the left is the components of arithmetic operations energy. On the right is the end-to-end energy cost across different model sizes.</p></section>
<section id="bitnet-b1.58-7" class="slide level2">
<h2>BitNet b1.58</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Conclusion</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Mobile device applicability</li>
<li>CPU-friendliness</li>
<li>1-bit Mixture-of-Experts (MoE) LLMs</li>
<li>1-bit LLM hardware</li>
</ul>
</div>
</div>
</div>
<!-- % note: https://arxiv.org/pdf/2307.08663v1 -->
</section>
<section id="quaternion-cnns" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Complex CNNs</strong></p>
</div>
<div class="callout-content">
<ul>
<li>avoid local minima caused by hierarchical structure</li>
<li>better generalization</li>
<li>faster learning</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Quaternion CNNs</strong></p>
</div>
<div class="callout-content">
<ul>
<li>reduction in number of parameters</li>
<li>improved classification accuracy</li>
<li>deal with 4D signals as a single entity</li>
<li>model 3D transformations</li>
</ul>
</div>
</div>
</div>
</section>
<section id="quaternion-cnns-2" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Quaternion algebra <span class="math inline">\(\mathbb{H}\)</span> is the 4-dimensional vector space over <span class="math inline">\(\mathbb{R}\)</span>, generated by the basis <span class="math inline">\(\{1, \hat{i}, \hat{j}, \hat{k}\}\)</span>, and endowed with the following multiplication rules (Hamilton product): <span class="math display">\[
(1)(1) = 1,\\
(1)(\hat{i}) = \hat{j} \hat{k} = -\hat{k} \hat{j} = \hat{i},\\
(1)(\hat{j}) = \hat{k} \hat{i} = -\hat{i} \hat{k} = \hat{j},\\
(1)(\hat{k}) = \hat{i} \hat{j} = -\hat{j} \hat{i} = \hat{k},\\
(\hat{i})^2 = (\hat{j})^2 = (\hat{k})^2 = -1.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="quaternion-cnns-3" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multiplication</strong></p>
</div>
<div class="callout-content">
<p>For two arbitrary quaternions <span class="math inline">\(\boldsymbol{p} = p_R + p_I \hat{i} + p_J \hat{j} + p_K \hat{k}\)</span> and <span class="math inline">\(\boldsymbol{q} = q_R + q_I \hat{i} + q_J \hat{j} + q_K \hat{k}\)</span> the multiplication is calculated as follows: <span class="math display">\[
\boldsymbol{p} \boldsymbol{q} = p_R q_R - p_I q_I - p_J q_J-  p_K q_K + \\
+ (p_R q_I + p_I q_R + p_J q_K - p_K q_J) \hat{i} \\
+ (p_R q_J - p_I q_K + p_J q_R + p_K q_I) \hat{j} \\
+ (p_R q_K + p_I q_J - p_J q_I + p_K q_R) \hat{k}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="quaternion-cnns-4" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Convolutions</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
(\boldsymbol{w} \ast \boldsymbol{q})(x,y) = \sum\limits_{r=-\frac{L}{2}}^{\frac{L}{2}}  \sum\limits_{s=-\frac{L}{2}}^{\frac{L}{2}}\left[\boldsymbol{w}(r,s)\boldsymbol{q}(x-r,y-s)\right], \text{ (left-side)}\\
(\boldsymbol{q} \ast \boldsymbol{w})(x,y) = \sum\limits_{r=-\frac{L}{2}}^{\frac{L}{2}}  \sum\limits_{s=-\frac{L}{2}}^{\frac{L}{2}}\left[\boldsymbol{q}(x-r,y-s)\boldsymbol{w}(r,s)\right], \text{ (right-side)}\\
(\boldsymbol{w_{left}} \ast \boldsymbol{q} \ast \boldsymbol{w_{right}})(x,y) = \\
= \sum\limits_{r=-\frac{L}{2}}^{\frac{L}{2}}  \sum\limits_{s=-\frac{L}{2}}^{\frac{L}{2}}\left[\boldsymbol{w_{left}}(r,s)\boldsymbol{q}(x-r,y-s)\boldsymbol{w_{right}}(r,s)\right], \text{ (two-sided)}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="quaternion-cnns-5" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Chain rule</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">\(L\)</span> is a real-valued loss function, and <span class="math inline">\(\boldsymbol{q} = q_R + q_I \hat{i} + q_J \hat{j} + q_K \hat{k}\)</span>, then <span class="math display">\[
\frac{\partial L}{\partial \boldsymbol{q}} = \frac{\partial L}{\partial q_R} + \frac{\partial L}{\partial q_I} + \frac{\partial L}{\partial q_J} + \frac{\partial L}{\partial q_K}.
\]</span> For <span class="math inline">\(\boldsymbol{g} = g_R + g_I \hat{i} + g_J \hat{j} + g_K \hat{k}\)</span> we have: <span class="math display">\[
\frac{\partial L}{\partial \boldsymbol{g}} =  \frac{\partial L}{\partial q_R}\left(\frac{\partial q_R}{\partial g_R} + \frac{\partial q_R}{\partial g_I} \hat{i} + \frac{\partial q_R}{\partial g_J}\hat{j} + \frac{\partial q_R}{\partial g_K}\hat{k} \right) + \\
+ \frac{\partial L}{\partial q_I}\left(\frac{\partial q_I}{\partial g_R} + \frac{\partial q_I}{\partial g_I} \hat{i} + \frac{\partial q_I}{\partial g_J}\hat{j} + \frac{\partial q_I}{\partial g_K}\hat{k} \right) + \\
+ \frac{\partial L}{\partial q_J}\left(\frac{\partial q_J}{\partial g_R} + \frac{\partial q_J}{\partial g_I} \hat{i} + \frac{\partial q_J}{\partial g_J}\hat{j} + \frac{\partial q_J}{\partial g_K}\hat{k} \right) + \\
+ \frac{\partial L}{\partial q_K}\left(\frac{\partial q_K}{\partial g_R} + \frac{\partial q_K}{\partial g_I} \hat{i} + \frac{\partial q_K}{\partial g_J}\hat{j} + \frac{\partial q_K}{\partial g_K}\hat{k} \right)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="quaternion-cnns-6" class="slide level2">
<h2>Quaternion CNNs</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Reduction in number of parameters</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(y = Wx, \; y \in \mathbb{R}^{4K},\; x \in \mathbb{R}^{4L}, \; W \in \mathbb{R}^{4K \times 4L}\)</span>,]</p>
<p>where <span class="math inline">\(W\)</span> contains <span class="math inline">\(4 \times 4 \times K \times L\)</span> parameters,</p>
<p><strong>is mapped to</strong></p>
<p><span class="math inline">\(\boldsymbol{y} = \boldsymbol{W} \boldsymbol{x}, \; y \in \mathbb{H}^{K},\; x \in \mathbb{H}^{L}, \; W \in \mathbb{H}^{K \times L}\)</span>,</p>
<p>where <span class="math inline">\(\boldsymbol{W}\)</span> contains <span class="math inline">\(4 \times K \times L\)</span> parameters.</p>
</div>
</div>
</div>
</section>
<section id="dynamic-neural-networks" class="slide level2">
<h2>Dynamic Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>Dynamic networks can adjust their computational path based on the input for better efficiency, making it possible to train models with trillions of parameters and accelerate models in a low-resource setting.</p>
</div>
</div>
</div>

<img data-src="img/dnn_types.png" class="r-stretch quarto-figure-center"><p class="caption">The three types of DNNs dynamically adjust computation timewise, widthwise and depthwise, respectively.</p></section>
<section id="dynamic-neural-networks-1" class="slide level2">
<h2>Dynamic Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Skimming</strong></p>
</div>
<div class="callout-content">
<p><strong>Skimming</strong> was well-researched in the era of recurrent neural networks (RNN). Skimming models save computation <strong>timewise</strong> by dynamically allocating computation to different time steps, based on the input tokens.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Mixture-of-Experts</strong></p>
</div>
<div class="callout-content">
<p><strong>MoE</strong> horizontally extends a feedforward neural network (FFNN) with multiple sub-networks. During inference, only one or a few of these sub-networks will be activated for computation, thus can save <strong>widthwise</strong> computation.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Early exit</strong></p>
</div>
<div class="callout-content">
<p><strong>Early exit</strong> terminates inference at an early layer, without exhausting full computational capacity, thus saves <strong>depthwise</strong> computation.</p>
</div>
</div>
</div>
</section>
<section id="neuroevolution" class="slide level2">
<h2>Neuroevolution</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p><strong>Neuroevolution</strong>: processes of automated configuration and training of neural networks using evolutionary algorithms.</p>
<p><strong>Evolutionary Algorithms</strong>, also known as Evolutionary Computation systems, are nature-inspired stochastic techniques that mimic basic principles of life.</p>
</div>
</div>
</div>
</section>
<section id="neuroevolution-1" class="slide level2">
<h2>Neuroevolution</h2>

<img data-src="img/ea_nasnet.png" class="r-stretch"></section>
<section>
<section id="graph-neural-networks" class="title-slide slide level1 center">
<h1>Graph Neural Networks</h1>
<!-- % note: https://arxiv.org/pdf/1901.00596 -->
<!-- % note: "ai trends in 2024" article -->
<!-- % note: "A review of graph neural networks" -->
</section>
<section id="graph-neural-networks-1" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/graph_nn_pubcount.png" class="r-stretch"></section>
<section id="graph-neural-networks-2" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/graph_nn_evolution.png" class="r-stretch"></section>
<section id="graph-neural-networks-3" class="slide level2">
<h2>Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Vanilla DL algorithms</strong></p>
</div>
<div class="callout-content">
<ul>
<li>extract high-level features from data by passing it through non-linear layers</li>
<li>Euclidean-structured data, such as tabular data, images, text, and audio</li>
<li>graph data has been largely ignored.</li>
<li>“rigid” structures: images are typically encoded as fixed-size 2-dimensional grids of pixels, and text as a 1-dimensional sequence of words (or tokens).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="graph-neural-networks-4" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/region_adjacency_graph.png" class="r-stretch quarto-figure-center"><p class="caption">Region Adjacency Graph.</p></section>
<section id="graph-neural-networks-5" class="slide level2">
<h2>Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Use-cases</strong></p>
</div>
<div class="callout-content">
<ul>
<li>recommender systems</li>
<li>traffic prediction</li>
<li>Google GraphCast weather forecasting</li>
<li>relational deep learning</li>
<li>materials science</li>
<li>explainable AI</li>
<li>protein design</li>
</ul>
</div>
</div>
</div>
</section>
<section id="graph-neural-networks-6" class="slide level2">
<h2>Graph Neural Networks</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Types</strong></p>
</div>
<div class="callout-content">
<ul>
<li>recurrent GNNs</li>
<li>convolutional GNNs</li>
<li>graph autoencoders</li>
<li>spatial-temporal GNNs</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Data examples</strong></p>
</div>
<div class="callout-content">
<ul>
<li>molecules in chemistry</li>
<li>citation network</li>
<li>interactions between users and products in e-commerce</li>
</ul>
</div>
</div>
</div>
<!-- % note: https://arxiv.org/pdf/2010.05234 -->
</section>
<section id="graph-neural-networks-7" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/gnn_sentence.png" class="r-stretch quarto-figure-center"><p class="caption">Reed-Kellogg sentence diagram.</p></section>
<section id="graph-neural-networks-8" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/gnn_molecule.png" class="r-stretch quarto-figure-center"><p class="caption">Diagram of an alcohol molecule (left), graph representation with indices (middle), and adjacency matrix (right).</p></section>
<section id="graph-neural-networks-9" class="slide level2">
<h2>Graph Neural Networks</h2>

<img data-src="img/gnn_learning_tasks.png" class="r-stretch quarto-figure-center"><p class="caption">Graph Learning Tasks.</p></section>
<section id="graph-neural-networks-10" class="slide level2">
<h2>Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Challenges</strong></p>
</div>
<div class="callout-content">
<ul>
<li>graph irregularity</li>
<li>interdependence</li>
<li>convolutions difficult to compute</li>
</ul>
</div>
</div>
</div>
<!-- % note: https://arxiv.org/pdf/2010.05234 -->
</section>
<section id="recurrent-graph-neural-networks" class="slide level2">
<h2>Recurrent Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Transition function</strong></p>
</div>
<div class="callout-content">
<p>Graph is defined as a set of vertices and a set of edges <span class="math inline">\(G = G(V,E)\)</span>.</p>
<p><strong>Transition function</strong> <span class="math inline">\(f\)</span> computes <span class="math inline">\(k\)</span>-th embeddings <span class="math inline">\(h_i^k\)</span> at each vertex <span class="math inline">\(v_i\)</span>: in other words, calculates the next representation of a neighborhood from the current representation. <span class="math display">\[
h_i^k = \sum\limits_{j \in  N_{v_i}} f\left(v_i^F, e_{ij}^F, v_j^F, h_j^{k-1}\right)
\]</span> where <span class="math inline">\(v_i^F\)</span> is the central vertex,</p>
<p><span class="math inline">\(N_{v_i}\)</span> is the set of vertex indices for the direct neighbors of <span class="math inline">\(v_i\)</span>,</p>
<p><span class="math inline">\(v_i^F\)</span> and <span class="math inline">\(e_{ij}^F\)</span> are feature vectors for vertices <span class="math inline">\(v_i\)</span> and edges <span class="math inline">\(e_{ij}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="recurrent-graph-neural-networks-1" class="slide level2">
<h2>Recurrent Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Convergence</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(h_i^0\)</span> can be defined arbitrarily on initialisation.</p>
<p><strong>Banach’s fixed point theorem</strong> will guarantee that the subsequently calculated embeddings will converge to some optimal value exponentially (if <span class="math inline">\(f\)</span> is implemented as a contraction map).</p>
</div>
</div>
</div>
</section>
<section id="recurrent-graph-neural-networks-2" class="slide level2">
<h2>Recurrent Graph Neural Networks</h2>

<img data-src="img/rgnn_forward_pass1.png" class="r-stretch quarto-figure-center"><p class="caption">An RGNN forward pass for graph <span class="math inline">\(G(V,E)\)</span> with <span class="math inline">\(|V|=4, |E|=4\)</span>.</p></section>
<section id="recurrent-graph-neural-networks-3" class="slide level2">
<h2>Recurrent Graph Neural Networks</h2>

<img data-src="img/rgnn_forward_pass2.png" class="r-stretch quarto-figure-center"><p class="caption">Graph <span class="math inline">\(G\)</span> goes through <span class="math inline">\(k\)</span> layers of processing.</p></section>
<section id="recurrent-graph-neural-networks-4" class="slide level2">
<h2>Recurrent Graph Neural Networks</h2>

<img data-src="img/gnn_message_passing.png" class="r-stretch quarto-figure-center"><p class="caption">Single node aggregates messages from adjacent nodes.</p></section>
<section id="convolutional-graph-neural-networks" class="slide level2">
<h2>Convolutional Graph Neural Networks</h2>

<img data-src="img/2d_conv_vs_graph_conv.png" class="r-stretch quarto-figure-center"><p class="caption">2D Convolution vs.&nbsp;Graph Convolution.</p></section>
<section id="convolutional-graph-neural-networks-1" class="slide level2">
<h2>Convolutional Graph Neural Networks</h2>
<p><img data-src="img/cgnn_cnn.png" height="300"> <img data-src="img/cgnn_convolution.png" height="300"></p>
</section>
<section id="convolutional-graph-neural-networks-2" class="slide level2">
<h2>Convolutional Graph Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Graph Convolutional Network</strong></p>
</div>
<div class="callout-content">
<p>GCNs produce embeddings by summing features extracted from each neighboring vertex and then applying non-linearity.</p>
<p>Graph convolutional operation: <span class="math display">\[
h_i^{k} = \sigma\left(\sum\limits_{j \in N_{v_i}} \frac{\boldsymbol{W} h_j^{k-1}}{\sqrt{|N_{v_i}||N_{v_j}|}}\right).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="graph-convolutional-network-1" class="slide level2">
<h2>Graph Convolutional Network</h2>

<img data-src="img/gnn_gcn.png" class="r-stretch"></section>
<section id="graph-convolutional-network-2" class="slide level2">
<h2>Graph Convolutional Network</h2>

<img data-src="img/graph_nn1.png" class="r-stretch quarto-figure-center"><p class="caption">A ConvGNN with multiple graph convolutional layers. A graph convolutional layer encapsulates each node’s hidden representation by aggregating feature information from its neighbors. After feature aggregation, a non-linear transformation is applied to the resulted outputs. By stacking multiple layers, the final hidden representation of each node receives messages from a further neighborhood.</p></section>
<section id="graph-convolutional-network-3" class="slide level2">
<h2>Graph Convolutional Network</h2>

<img data-src="img/graph_nn2.png" class="r-stretch quarto-figure-center"><p class="caption">A ConvGNN with pooling and readout layers for graph classification. A graph convolutional layer is followed by a pooling layer to coarsen a graph into sub-graphs so that node representations on coarsened graphs represent higher graph-level representations. A readout layer summarizes the final graph representation by taking the sum/mean of hidden representations of sub-graphs.</p></section>
<section id="graph-attention-network" class="slide level2">
<h2>Graph Attention Network</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Graph Attention Network</strong></p>
</div>
<div class="callout-content">
<p>Graph Attention Networks (GATs) extend GCNs: instead of using the size of the neighborhoods to weight the importance of <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span>, they implicitly calculate the weighting based on the normalised product of an attention mechanism. <!-- %In this case, the attention mechanism is dependent on the embeddings of two vertices and the edge between them. Vertices are constrained to only be able to attend to neighboring vertices, thus localising the filters. GATs are stabilised during training using multi-head attention and regularisation, and are considered less general than MPNNs [88]. Although GATs limit the attention mechanism to the direct neighborhood, the scalability to large graphs is not guaranteed, as attention mechanisms have compute complexities that grow quadratically with the number of vertices being considered. --> <span class="math display">\[
h_i^{k} = \sigma\left(\sum\limits_{j \in N_{v_i}} \alpha_{ij} \boldsymbol{W} h_j^{k-1}\right),\\
\alpha_{ij} = \frac{\exp\left(att\left(h_i^{k-1}, h_j^{k-1}, e_{ij}\right)\right)}{\sum\limits_{l \in N_{v_i}} \exp\left(att\left(h_i^{k-1}, h_l^{k-1}, e_{il}\right)\right)}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="graph-attention-network-2" class="slide level2">
<h2>Graph Attention Network</h2>

<img data-src="img/gnn_gat.png" class="r-stretch"></section>
<section id="graph-autoencoders" class="slide level2">
<h2>Graph Autoencoders</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>GAEs represent the application of GNNs (often CGNNs) to autoencoding. The goal of an AE can be summarised as follows: to project the inputs features into a new space (known as the <strong>latent space</strong>) where the projection has more desirable properties than the input representation.</p>
<p>These properties may include:</p>
<ul>
<li>The data being more separable (i.e.&nbsp;classifiable) in the latent space.</li>
<li>The dimensionality of the dataset being smaller in the latent space than in the input space</li>
<li>The data being obfuscated for security or privacy concerns in the latent space.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="graph-autoencoders-1" class="slide level2">
<h2>Graph Autoencoders</h2>

<img data-src="img/gnn_ae.png" class="r-stretch quarto-figure-center"><p class="caption">The architecture for a simple traditional standard AE.</p></section>
<section id="graph-autoencoders-2" class="slide level2">
<h2>Graph Autoencoders</h2>

<img data-src="img/gnn_gae.png" class="r-stretch quarto-figure-center"><p class="caption">The architecture for a GAE. The input graph is described by the adjacency matrix <span class="math inline">\(A\)</span> and the vertex feature matrix <span class="math inline">\(X\)</span>.</p></section>
<section id="variational-graph-autoencoders" class="slide level2">
<h2>Variational Graph Autoencoders</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>Rather than representing inputs with single points in latent space, variational autoencoders (VAEs) learn to encode inputs as probability distributions in latent space.</p>
</div>
</div>
</div>

<img data-src="img/gnn_vgae.png" class="r-stretch quarto-figure-center"><p class="caption">An example of a VGAE. Graph inputs are encoded via a GNN into multivariate Guassian parameters (i.e.&nbsp;mean and variance).</p></section>
<section id="graph-adversarial-techniques" class="slide level2">
<h2>Graph Adversarial Techniques</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>Graph Adversarial Techniques (GAdvTs) use adversarial learning methods whereby an AI model acts as an adversary to another during training to mutually improve the performance of both models in tandem.</p>
</div>
</div>
</div>

<img data-src="img/gnn_gadvt.png" class="r-stretch quarto-figure-center"><p class="caption">A typical approach to adversarial training with VGAEs.</p></section>
<section id="graph-attention-network-3" class="slide level2">
<h2>Graph Attention Network</h2>

<img data-src="img/gnn_model_eqs.png" class="r-stretch quarto-figure-center"><p class="caption">Equations depending on type.</p></section></section>
<section>
<section id="neurosymbolic-ai" class="title-slide slide level1 center">
<h1>Neurosymbolic AI</h1>

</section>
<section id="ai-timeline-recap" class="slide level2">
<h2>AI timeline: recap</h2>

<img data-src="img/ai_timeline.png" class="r-stretch"></section>
<section id="neurosymbolic-ai-1" class="slide level2">
<h2>Neurosymbolic AI</h2>

<img data-src="img/ansi_common_lisp.png" class="r-stretch"></section>
<section id="neurosymbolic-ai-2" class="slide level2">
<h2>Neurosymbolic AI</h2>

<img data-src="img/macro_examples.png" class="r-stretch"></section>
<section id="neurosymbolic-ai-3" class="slide level2">
<h2>Neurosymbolic AI</h2>
<p><!-- % note: Norvig's book --></p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Neats vs Scruffies (Norvig)</strong></p>
</div>
<div class="callout-content">
<p><strong>Neats</strong>: AI theories should be grounded in mathematical rigor.</p>
<p><strong>Scruffies</strong>: try out lots of ideas, write some programs, and then assess what seems to be working.</p>
</div>
</div>
</div>
<!-- % note: https://arxiv.org/pdf/2305.00813 -->
</section>
<section id="neurosymbolic-ai-4" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Human-environment interaction</strong></p>
</div>
<div class="callout-content">
<p><strong>Perception</strong> – transforming sensory inputs from their environment into symbols. Roughly maps to <strong>neural networks</strong>.</p>
<p><strong>Cognition</strong> – mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Roughly maps to <strong>symbolic AI</strong>.</p>
<p>These are Daniel Kahneman’s System 1 and System 2.</p>
</div>
</div>
</div>
</section>
<section id="neurosymbolic-ai-5" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Why add symbolic AI?</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Symbolic structures represent background knowledge.</li>
<li>which neural networks don’t have explicit representations for</li>
<li>thus cannot be evaluated reliably</li>
<li>or “explain” whether their outcomes adhere to some safety standards</li>
</ul>
</div>
</div>
</div>
</section>
<section id="neurosymbolic-ai-6" class="slide level2">
<h2>Neurosymbolic AI</h2>

<img data-src="img/neuro_symbolic_ai.png" class="r-stretch"></section>
<section id="neurosymbolic-ai-7" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Deep Learning Criticisms</strong></p>
</div>
<div class="callout-content">
<ul>
<li>brittleness (being susceptible to adversarial attacks),</li>
<li>lack of explainability (not having a formally defined computational semantics or even intuitive explanation, leading to questions around the trustworthiness of AI systems),</li>
<li>lack of parsimony (requiring far too much data, computational power at training time or unacceptable levels of energy consumption)</li>
<li>accountability and impact on humanity.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="neurosymbolic-ai-8" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Two types:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>methods that compress structured symbolic knowledge to integrate with neural patterns and reason using the integrated neural patterns (<strong>lowering</strong>)</li>
<li>methods that extract information from neural patterns to allow for mapping to structured symbolicknowledge (<strong>lifting</strong>) and perform symbolic reasoning</li>
</ul>
</div>
</div>
</div>
</section>
<section id="neurosymbolic-ai-9" class="slide level2">
<h2>Neurosymbolic AI</h2>

<img data-src="img/ns_types.png" class="r-stretch"></section>
<section id="neurosymbolic-ai-10" class="slide level2">
<h2>Neurosymbolic AI</h2>

<img data-src="img/ns_graph_compression.png" class="r-stretch quarto-figure-center"><p class="caption">Two methods for compressing knowledge graphs.</p></section>
<section id="neurosymbolic-ai-11" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Challenges</strong></p>
</div>
<div class="callout-content">
<ul>
<li>What is the best way to integrate neural and symbolic architectures?</li>
<li>How should symbolic structures be represented within neural networks and extracted from them?</li>
<li>How should common-sense knowledge be learned and reasoned about?</li>
<li>How can abstract knowledge that is hard to encode logically be handled?</li>
</ul>
</div>
</div>
</div>
</section>
<section id="neurosymbolic-ai-12" class="slide level2">
<h2>Neurosymbolic AI</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Implementations</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Scallop</strong>: a language based on Datalog that supports differentiable logical and relational reasoning. Scallop can be integrated in Python and with a PyTorch learning module.</li>
<li><strong>Logic Tensor Networks</strong>: encode logical formulas as neural networks and simultaneously learn term encodings, term weights, and formula weights.</li>
<li><strong>SymbolicAI</strong>: a compositional differentiable programming library.</li>
<li><strong>Explainable Neural Networks (XNNs)</strong>: combine neural networks with symbolic hypergraphs and train using a mixture of backpropagation and symbolic learning called induction.</li>
</ul>
</div>
</div>
</div>
<!-- % \begin{frame}{Neurosymbolic AI} -->
<!-- %   Taxonomy -->
<!-- %   \begin{itemize} -->
<!-- %     \item [symbolic Neuro symbolic] refers to an approach where input and output are presented in symbolic form, however all the actual processing is neural. This, in his words, is the "standard operating procedure" whenever inputs and outputs are symbolic. -->
<!-- %     \item [Symbolic[Neuro]] refers to a neural pattern recognition subroutine within a symbolic problem solver, with exam- ples such as AlphaGo, AlphaZero, and current approaches to self-driving cars. -->
<!-- %     \item [Neuro $\cap$ compile(Symbolic)] refers to an approach where symbolic rules are "compiled" away during training, e.g. like the 2020 work on Deep Learning For Symbolic Mathematics [7]. -->
<!-- %     \item [Neuro $\rightarrow$ Symbolic] refers to a cascading from a neural system into a symbolic reasoner, such as in the Neuro- Symbolic Concept-Learner [8]. -->
<!-- %     \item [Neuro[Symbolic]] refers to the embedding of symbolic reasoning inside a neural engine, where symbolic rea- soning is understood as "deliberative, type 2 reasoning" as common, e.g., in business AI, and including an internal model of the system’s state of attention: Concepts are decoded to symbolic entities in an attention schema when attention to these concepts is high. A goal in the attention schema then signals that deliberative reasoning be executed. -->
<!-- %   \end{itemize} -->
<!-- % \end{frame} -->
<!-- % \begin{frame}{Neurosymbolic AI} -->
<!-- %   A key question however is that of identifying -->
<!-- % the necessary and sufficient building blocks of AI, and how systems that evolve automatically based on machine learning can be developed and analysed in effective ways that make AI trustworthy. -->
<!-- % \end{frame} -->


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"5ab825911099445d8fd201a3fc394cf428a7d217aa2ec5d05192de6aafe32206"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>