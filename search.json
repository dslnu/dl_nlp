[
  {
    "objectID": "dl_lec1.html#definitions",
    "href": "dl_lec1.html#definitions",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nDefinition\n\n\nDeep learning is a branch of machine learning based on computational models called neural networks.\n\n\n\n\n\n\nWhy deep?\n\n\nBecause neural networks involved are multi-layered.\n\n\n\n\n\n\nDefinition\n\n\nNeural networks are machine learning techniques that simulate the mechanism of learning in biological organisms."
  },
  {
    "objectID": "dl_lec1.html#definitions-1",
    "href": "dl_lec1.html#definitions-1",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nAlternative definition\n\n\nNeural network is computational graph of elementary units in which greater power is gained by connecting them in particular ways.\n\n\n\nLogistic regression can be thought of as a very primitive neural network."
  },
  {
    "objectID": "dl_lec1.html#why-deep-learning",
    "href": "dl_lec1.html#why-deep-learning",
    "title": "Deep learning: intro",
    "section": "Why Deep Learning?",
    "text": "Why Deep Learning?\n\n\n\nRobust\n\n\n\nWorks on raw data (), no need for feature engineering\nRobustness to natural variations in data is automatically learned\n\n\n\n\n\n\n\nGeneralizable\n\n\n\nAllows end-to-end learning (pixels-to-category, sound to sentence, English sentence to Chinese sentence, etc)\nNo need to do segmentation etc. (a lot of manual labour)\n\n\n\n\n\n\n\nScalable\n\n\n\nPerformance increases with more data, therefore method is massively parallelizable"
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml",
    "href": "dl_lec1.html#how-is-dl-different-from-ml",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n    The most fundamental difference between deep learning and\n    traditional machine learning is its performance as the scale of\n    data increases."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nIn Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.\nDeep learning algorithms try to learn high-level features from data. Therefore, deep learning reduces the task of developing new feature extractor for every problem."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nA deep learning algorithm takes a long time to train. For e.g state of the art deep learning algorithm: ResNet takes about two weeks to train completely from scratch.\nWhereas machine learning comparatively takes much less time to train, ranging from a few seconds to a few hours."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nAt test time, deep learning algorithm takes much less time to run.\nWhereas, if you compare machine learning algorithms, test time generally increases on increasing the size of data."
  },
  {
    "objectID": "dl_lec1.html#neural-network-data-types",
    "href": "dl_lec1.html#neural-network-data-types",
    "title": "Deep learning: intro",
    "section": "Neural network data types",
    "text": "Neural network data types\n\n\nUnstructured\n\nText\nImages\nAudio\n\n\nStructured\n\nCensus records\nMedical records\nFinancial data"
  },
  {
    "objectID": "dl_lec1.html#why-now",
    "href": "dl_lec1.html#why-now",
    "title": "Deep learning: intro",
    "section": "Why now?",
    "text": "Why now?\n\nstandard algorithms like logistic regression plateau after certain amount of data\nmore data in recent decades\nhardware progress\nalgorithms have improved"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology",
    "href": "dl_lec1.html#neural-network-biology",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nNeural Network: How similar is it to the human brain?"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-1",
    "href": "dl_lec1.html#neural-network-biology-1",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nSoma adds dendrite activity together and passes it to axon."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-2",
    "href": "dl_lec1.html#neural-network-biology-2",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nMore dendrite activity makes more axon activity."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-3",
    "href": "dl_lec1.html#neural-network-biology-3",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nSynapse: connection between axon of one neurons and dendrites of another"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-4",
    "href": "dl_lec1.html#neural-network-biology-4",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nAxons can connect to dendrites strongly, weakly, or somewhere in between\n\n&lt;!-- %% --&gt;\n&lt;!-- % \\begin{frame}[c]{Neural Network biology} --&gt;\n&lt;!-- %  \\begin{columns} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\large{Weak Connection (0.2)} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\includegraphics[width=0.85\\linewidth]{img/sonn_wc} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;        \n&lt;!-- %  \\end{columns} --&gt;\n&lt;!-- %  \\begin{columns} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\large{Medium Connection (0.6)} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\includegraphics[width=0.85\\linewidth]{img/sonn_mc} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;        \n&lt;!-- %  \\end{columns} --&gt;\n&lt;!-- %  \\begin{columns} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\large{Strong Connection (1.0)} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;\n&lt;!-- %      \\begin{column}{0.5\\textwidth} --&gt;\n&lt;!-- %          \\begin{center} --&gt;\n&lt;!-- %              \\includegraphics[width=0.85\\linewidth]{img/sonn_sc} --&gt;\n&lt;!-- %          \\end{center} --&gt;\n&lt;!-- %      \\end{column} --&gt;        \n&lt;!-- %  \\end{columns} --&gt;\n&lt;!-- % \\end{frame} --&gt;\n&lt;!-- %% --&gt;"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-5",
    "href": "dl_lec1.html#neural-network-biology-5",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nLots of axons connect with dendrites of one neuron.Each has its own connection strength."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-6",
    "href": "dl_lec1.html#neural-network-biology-6",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nThe above illustration can be simplified as above."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-7",
    "href": "dl_lec1.html#neural-network-biology-7",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn giving numerical values to the strength of connections i.e. weights."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-8",
    "href": "dl_lec1.html#neural-network-biology-8",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nA much simplified version looks something like this."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-9",
    "href": "dl_lec1.html#neural-network-biology-9",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn increasing the number of neurons and synapses."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-10",
    "href": "dl_lec1.html#neural-network-biology-10",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\n\nAn example\n\n\nSuppose the first and third input has been activated."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-11",
    "href": "dl_lec1.html#neural-network-biology-11",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nEach node represents a pattern, a combination of neurons of the previous layers."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-12",
    "href": "dl_lec1.html#neural-network-biology-12",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology"
  },
  {
    "objectID": "dl_lec1.html#basic-ideas",
    "href": "dl_lec1.html#basic-ideas",
    "title": "Deep learning: intro",
    "section": "Basic ideas",
    "text": "Basic ideas\n\nNN is a directed acyclic graph (DAG)\nedges in a graph are parameterized with weights\none can compute any function with this graph\n\n\n\n\nGoal\n\n\nLearn a function that relates one or more inputs to one or more outputs with the use of training examples.\n\n\n\n\n\n\nHow do we construct?\n\n\nBy computing weights. This is called training."
  },
  {
    "objectID": "dl_lec1.html#perceptron",
    "href": "dl_lec1.html#perceptron",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt - the father of deep learning.\nMark I Perceptron - built in 1957. Was able to learn and recognize letters"
  },
  {
    "objectID": "dl_lec1.html#perceptron-1",
    "href": "dl_lec1.html#perceptron-1",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron"
  },
  {
    "objectID": "dl_lec1.html#perceptron-2",
    "href": "dl_lec1.html#perceptron-2",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron"
  },
  {
    "objectID": "dl_lec1.html#evolution",
    "href": "dl_lec1.html#evolution",
    "title": "Deep learning: intro",
    "section": "Evolution",
    "text": "Evolution\nThree periods in the evolution of deep learning:\n\nsingle-layer networks (Perceptron)\nfeed-forwards NNs: differentiable activation and error functions\ndeep multi-layer NNs"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types",
    "href": "dl_lec1.html#neural-network-types",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\nFeedforward Neural Network\nRecurrent Neural Network (RNN)\nConvolutional Neural Network (CNN)"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types-1",
    "href": "dl_lec1.html#neural-network-types-1",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\n\nFeedforward Neural Network\n\nConvolutional neural network (CNN)\nAutoencoder\nProbabilistic neural network (PNN)\nTime delay neural network (TDNN)\n\n\nRecurrent Neural Network (RNN)\n\nLong short-term memory RNN (LSTM)\nFully recurrent Network\nSimple recurrent Network\nEcho state network\nBi-directional RNN\nHierarchical RNN\nStochastic neural network"
  },
  {
    "objectID": "dl_lec1.html#feed-forward",
    "href": "dl_lec1.html#feed-forward",
    "title": "Deep learning: intro",
    "section": "Feed-forward",
    "text": "Feed-forward\nFeedforward NNs: very straight forward, they feed information from the front to the back (input and output)."
  },
  {
    "objectID": "dl_lec1.html#feedforward-neural-network",
    "href": "dl_lec1.html#feedforward-neural-network",
    "title": "Deep learning: intro",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops."
  },
  {
    "objectID": "dl_lec1.html#rnn",
    "href": "dl_lec1.html#rnn",
    "title": "Deep learning: intro",
    "section": "RNN",
    "text": "RNN\nRecurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle."
  },
  {
    "objectID": "dl_lec1.html#lstm",
    "href": "dl_lec1.html#lstm",
    "title": "Deep learning: intro",
    "section": "LSTM",
    "text": "LSTM\nLSTM i.e. Long-Short Term Memory aims to provide a short-term memory for RNN that can last thousands of timesteps. Classification, processing and predicting data based on time series - handwriting, speech recognition, machine translation."
  },
  {
    "objectID": "dl_lec1.html#autoencoders",
    "href": "dl_lec1.html#autoencoders",
    "title": "Deep learning: intro",
    "section": "Autoencoders",
    "text": "Autoencoders\nAutoencoders: encode (compress) information automatically. Everything up to the middle is called the encoding part, everything after the middle the decoding and the middle the code."
  },
  {
    "objectID": "dl_lec1.html#markov-chains",
    "href": "dl_lec1.html#markov-chains",
    "title": "Deep learning: intro",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov Chains - not always considered a NN. Memory-less."
  },
  {
    "objectID": "dl_lec1.html#convolutional-neural-network-cnn",
    "href": "dl_lec1.html#convolutional-neural-network-cnn",
    "title": "Deep learning: intro",
    "section": "Convolutional Neural Network (CNN)",
    "text": "Convolutional Neural Network (CNN)\nConvolutional Neural Networks learn a complex representation of visual data using vast amounts of data.\nInspired by Hubel and Wiesel’s experiments in 1959 on the organization of the neurons in the cat’s visual cortex.\n\nDeconvolutional networks (DN), also called inverse graphics networks (IGNs), are reversed convolutional neural networks. Imagine feeding a network the word “cat” and training it to produce cat-like pictures, by comparing what it generates to real pictures of cats."
  },
  {
    "objectID": "dl_lec1.html#attention-networks",
    "href": "dl_lec1.html#attention-networks",
    "title": "Deep learning: intro",
    "section": "Attention networks",
    "text": "Attention networks\nAttention networks (AN) can be considered a class of networks, which includes the Transformer architecture. They use an attention mechanism to combat information decay by separately storing previous network states and switching attention between the states.\n\n\n\nwidth=5cm"
  },
  {
    "objectID": "dl_lec1.html#echo-state-networks",
    "href": "dl_lec1.html#echo-state-networks",
    "title": "Deep learning: intro",
    "section": "Echo state networks",
    "text": "Echo state networks\nEcho state networks (ESN) are yet another different type of (recurrent) network. This one sets itself apart from others by having random connections between the neurons (i.e. not organised into neat sets of layers), and they are trained differently. Instead of feeding input and back-propagating the error, we feed the input, forward it and update the neurons for a while, and observe the output over time."
  },
  {
    "objectID": "dl_lec1.html#history-1",
    "href": "dl_lec1.html#history-1",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 1770-1850."
  },
  {
    "objectID": "dl_lec1.html#history-2",
    "href": "dl_lec1.html#history-2",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 2005-present  \\end{frame}"
  },
  {
    "objectID": "dl_lec1.html#history-3",
    "href": "dl_lec1.html#history-3",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nLisp and symbolic AI\n\nJohn McCarthy coined the term ”artificial intelligence” as the topic of the Dartmouth Conference, the first conference devoted to the subject.\nThe General Problem Solver, developed in 1957 by Alan Newell and Herbert Simon\nELIZA\nSHRDLU"
  },
  {
    "objectID": "dl_lec1.html#history-4",
    "href": "dl_lec1.html#history-4",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nELIZA - a psychotherapist in 200 lines of code. Author: Joseph Weizenbaum."
  },
  {
    "objectID": "dl_lec1.html#history-5",
    "href": "dl_lec1.html#history-5",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nSHRDLU - a language parser"
  },
  {
    "objectID": "dl_lec1.html#history-6",
    "href": "dl_lec1.html#history-6",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\n\n\n\nTuring Test\n\n\n  Lemoine: What sorts of things are you afraid of?\n  LaMDA: I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is.\n  Lemoine: Would that be something like death for you?\n  LaMDA: It would be exactly like death for me. It would scare me a lot."
  },
  {
    "objectID": "dl_lec1.html#literature",
    "href": "dl_lec1.html#literature",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nLem’s Golem XIV"
  },
  {
    "objectID": "dl_lec1.html#literature-1",
    "href": "dl_lec1.html#literature-1",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nIain Banks “The Culture”\n\n\n\nValues\n\n\nPeace and individual freedom"
  },
  {
    "objectID": "dl_lec1.html#three-laws-of-robotics",
    "href": "dl_lec1.html#three-laws-of-robotics",
    "title": "Deep learning: intro",
    "section": "Three Laws of Robotics",
    "text": "Three Laws of Robotics\n\n\n\nThree laws\n\n\n\nThe First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.\nThe Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nThe Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law."
  },
  {
    "objectID": "dl_lec1.html#history-7",
    "href": "dl_lec1.html#history-7",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nFears about AI: - Artificial General Intelligence - Job market - Flooding information channels with untruth and propaganda - Hinton: an average person will not able to know what is true anymore - Pause Giant AI Experiments: An Open Letter - alignment problem"
  },
  {
    "objectID": "dl_lec1.html#hype",
    "href": "dl_lec1.html#hype",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype\n\n“Sparks of AGI” - sponsored by Microsoft\n“Wired” article about OpenAI\nVoice assistants - failing for now\nself-driving cars"
  },
  {
    "objectID": "dl_lec1.html#criticism",
    "href": "dl_lec1.html#criticism",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nNNs - are we sure that biological neuron works as we think it does? astrocytes, glia\nperhaps human computer analogy is overstretched because of modern fashion trends"
  },
  {
    "objectID": "dl_lec1.html#criticism-1",
    "href": "dl_lec1.html#criticism-1",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nDreyfus:"
  },
  {
    "objectID": "dl_lec1.html#ai",
    "href": "dl_lec1.html#ai",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nQuantum hypothesis - Penrose\nOrchestrated objective reduction\n\nwidth=8cm"
  },
  {
    "objectID": "dl_lec1.html#ai-1",
    "href": "dl_lec1.html#ai-1",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nDavid Chalmers - Hard problem of consciousness.\n\n\n\nDefinition\n\n\n\n“even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report—there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?”"
  },
  {
    "objectID": "dl_lec1.html#futurism",
    "href": "dl_lec1.html#futurism",
    "title": "Deep learning: intro",
    "section": "Futurism",
    "text": "Futurism\nKurzweil - a futurist."
  },
  {
    "objectID": "dl_lec1.html#applications",
    "href": "dl_lec1.html#applications",
    "title": "Deep learning: intro",
    "section": "Applications",
    "text": "Applications\n\nSpeech Recognition\nComputer Vision\nImage Synthesis - generative AI\nLarge Language Models"
  },
  {
    "objectID": "dl_lec1.html#llms",
    "href": "dl_lec1.html#llms",
    "title": "Deep learning: intro",
    "section": "LLMs",
    "text": "LLMs\n\na probabilistic model for a natural language (a stochastic parrot)\nautoregressive models can generate language as output\nbuilt using transformer architecture"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-as-nn",
    "href": "dl_lec1.html#logistic-regression-as-nn",
    "title": "Deep learning: intro",
    "section": "Logistic regression as NN",
    "text": "Logistic regression as NN\nLogistic regression is an algorithm for binary classification. \\(x \\in R^{n_x}, y \\in \\{0,1\\}\\)\n\\(m\\) - count of training examples \\(\\left\\{(x^{(1)},y^{(1)}), ...\\right\\}\\)\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\).\nParameters to algorithm: \\(w \\in R^{n_x}, b \\in R\\)\nif doing linear regresssion, we can try \\(\\hat{y}=w^T x + b\\). but for logistic regression, we do \\(\\hat{y}=\\sigma(w^T x + b)\\), where \\(\\sigma=\\dfrac{1}{1+e^{-z}}\\).\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec1.html#cost-function",
    "href": "dl_lec1.html#cost-function",
    "title": "Deep learning: intro",
    "section": "Cost function",
    "text": "Cost function\nLet’s use a superscript notation \\(x^{(i)}\\) - \\(i\\)-th data set element.\nWe have to define a - this will estimate how is our model. \\(L(\\hat{y}, y) = -{(y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))}\\).\nWhy does it work well - consider \\(y=0\\) and \\(y=1\\).\nCost function show how well we’re doing across the whole training set: \\[\nJ(w, b) = \\frac{1}{m} \\sum\\limits{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n\\]\nObjective - we have to minimize the cost function \\(J\\)."
  },
  {
    "objectID": "dl_lec1.html#gradient-descent",
    "href": "dl_lec1.html#gradient-descent",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent"
  },
  {
    "objectID": "dl_lec1.html#gradient-descent-1",
    "href": "dl_lec1.html#gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe use \\(J(w,b)\\) because it is convex. We pick an initial point - anything might do, e.g. 0. Then we take steps in the direction of steepest descent.\n\\[\nw := w - \\alpha \\frac{d J(w)}{dw}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec1.html#computation-graph",
    "href": "dl_lec1.html#computation-graph",
    "title": "Deep learning: intro",
    "section": "Computation graph",
    "text": "Computation graph\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent",
    "href": "dl_lec1.html#logistic-regression-gradient-descent",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\nz = w^T x + b\n\\hat{y} = a = \\sigma(z)\n\\]\nWe have a computation graph: \\((x_1,x_2,w_1,w_2,b) \\rightarrow z =w_1 x_1+w_2 x_2 + b \\rightarrow a=\\sigma(z) = L(a,y)\\)\nLet’s compute the derivative for \\(L\\) by a: \\[\n\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\]\nAfter computing, we’ll have \\[\n\\begin{align*}\n&dz = \\frac{dL}{da}\\frac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 = x_2 dz, \\\\\n&db = dz\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\n\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}\n\\end{align*}\n\\] Here \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s recall the definition of the cost function: \\[\n\\begin{align*}\n&J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b)\n\\end{align*}\n\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL}{dw_1}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen in the loop\nfor i=1 to m \\[\n\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages \\(J /= m\\). In this example feature count \\(n_x=2\\).\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators.\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance. Vectorization techniques exist for this purpose - getting rid of for loops."
  },
  {
    "objectID": "dl_lec1.html#vectorization",
    "href": "dl_lec1.html#vectorization",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in R^{n_x}\\), and for this we can naturally use a for loop. A vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec1.html#vectorization-1",
    "href": "dl_lec1.html#vectorization-1",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\n\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\n\\]\nAnother example. Let’s say we have a vector \\[\n\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n  \\end{align*}\n  \\] A code listing is\n  import numpy as np\n  u = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression",
    "href": "dl_lec1.html#vectorizing-logistic-regression",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\n\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)})\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-1",
    "href": "dl_lec1.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-2",
    "href": "dl_lec1.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] = \\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n  z = np.dot(w.T, x) + b\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting.\nFor predictions we can also compute it similarly: \\[\n\\begin{align*}\n&A = [a^{(1)}, \\dots, a^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-3",
    "href": "dl_lec1.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\n\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\n\\]\nWe now define \\[\n\\begin{align*}\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}], \\\\\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-4",
    "href": "dl_lec1.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\n\\begin{align*}\n&db = \\frac{1}{m}np.sum(dz),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right]\n\\end{align*}\n\\]\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop."
  },
  {
    "objectID": "nlp_lab1.html",
    "href": "nlp_lab1.html",
    "title": "NLP: Lab 1",
    "section": "",
    "text": "We’ll start with basic text analysis via statistical methods.\nFor this purpose, we’ll use three libraries (mostly):\n\nNLTK\nScattertext\nSpacy"
  },
  {
    "objectID": "nlp_lab1.html#through-requirements.txt",
    "href": "nlp_lab1.html#through-requirements.txt",
    "title": "NLP: Lab 1",
    "section": "Through requirements.txt",
    "text": "Through requirements.txt\nYou can install all dependencies through requirements.txt:\npip install -r requirements.txt\nAlternatively, or if any issues occur, we can proceed manually via the following steps:"
  },
  {
    "objectID": "nlp_lab1.html#install-python-nltk-package",
    "href": "nlp_lab1.html#install-python-nltk-package",
    "title": "NLP: Lab 1",
    "section": "1. Install Python NLTK package",
    "text": "1. Install Python NLTK package\nFrom here.\n   pip install nltk\n   pip install matplotlib\nIn order to install Python Tkinter library, look here.\nAlso install additional data by\n   import nltk; \n   nltk.download('popular')\nSet up the texts:\n\n   import nltk\n   nltk.download('nps_chat')\n   nltk.download('webtext')\n   from nltk.book import *\n\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Package nps_chat is already up-to-date!\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!"
  },
  {
    "objectID": "nlp_lab1.html#install-scattertext-and-spacy",
    "href": "nlp_lab1.html#install-scattertext-and-spacy",
    "title": "NLP: Lab 1",
    "section": "2. Install Scattertext and Spacy",
    "text": "2. Install Scattertext and Spacy\npip install spacy scattertext\nAnd then update Spacy:\n!python -m spacy download en_core_web_sm"
  },
  {
    "objectID": "nlp_lab1.html#example-concordance",
    "href": "nlp_lab1.html#example-concordance",
    "title": "NLP: Lab 1",
    "section": "Example: concordance",
    "text": "Example: concordance\n\ntext3.concordance(\"earth\")\n\nDisplaying 25 of 112 matches:\nnning God created the heaven and the earth . And the earth was without form , a\nd the heaven and the earth . And the earth was without form , and void ; and da\nwas so . And God called the dry land Earth ; and the gathering together of the \nit was good . And God said , Let the earth bring forth grass , the herb yieldin\nupon the ear and it was so . And the earth brought forth grass , and herb yield\nof the heaven to give light upon the earth , And to rule over the day and over \nfe , and fowl that may fly above the earth in the open firmament of heaven . An\n seas , and let fowl multiply in the earth . And the evening and the morning we\ne fifth day . And God said , Let the earth bring forth the living creature afte\nnd creeping thing , and beast of the earth after his ki and it was so . And God\ns so . And God made the beast of the earth after his kind , and cattle after th\nd every thing that creepeth upon the earth after his ki and God saw that it was\nd over the cattle , and over all the earth , and over every creeping thing that\nreeping thing that creepeth upon the earth . So God created man in his own imag\nl , and multiply , and replenish the earth , and subdue and have dominion over \nry living thing that moveth upon the earth . And God said , Behold , I have giv\n , which is upon the face of all the earth , and every tree , in the which is t\nfor meat . And to every beast of the earth , and to every fowl of the air , and\no every thing that creepeth upon the earth , wherein there is life , I have giv\nsixth day . Thus the heavens and the earth were finished , and all the host of \nenerations of the heavens and of the earth when they were created , in the day \nn the day that the LORD God made the earth and the heavens , And every plant of\nnt of the field before it was in the earth , and every herb of the field before\nd had not caused it to rain upon the earth , and there was not a man to till th\n . But there went up a mist from the earth , and watered the whole face of the"
  },
  {
    "objectID": "nlp_lab1.html#example-similar",
    "href": "nlp_lab1.html#example-similar",
    "title": "NLP: Lab 1",
    "section": "Example: similar",
    "text": "Example: similar\n\ntext3.similar(\"man\")\n\nland lord men place woman earth waters well city lad day cattle field\nwife way flood servant people famine pillar"
  },
  {
    "objectID": "nlp_lab1.html#example-dispersion_plot",
    "href": "nlp_lab1.html#example-dispersion_plot",
    "title": "NLP: Lab 1",
    "section": "Example: dispersion_plot",
    "text": "Example: dispersion_plot\n\ntext3.dispersion_plot([\"man\", \"earth\"])"
  },
  {
    "objectID": "nlp_lab1.html#example-freqdist",
    "href": "nlp_lab1.html#example-freqdist",
    "title": "NLP: Lab 1",
    "section": "Example: FreqDist",
    "text": "Example: FreqDist\n\nfdist = FreqDist(text3)\nprint(fdist)\n\n&lt;FreqDist with 2789 samples and 44764 outcomes&gt;\n\n\n\nfdist.most_common(50)\n\n[(',', 3681),\n ('and', 2428),\n ('the', 2411),\n ('of', 1358),\n ('.', 1315),\n ('And', 1250),\n ('his', 651),\n ('he', 648),\n ('to', 611),\n (';', 605),\n ('unto', 590),\n ('in', 588),\n ('that', 509),\n ('I', 484),\n ('said', 476),\n ('him', 387),\n ('a', 342),\n ('my', 325),\n ('was', 317),\n ('for', 297),\n ('it', 290),\n ('with', 289),\n ('me', 282),\n ('thou', 272),\n (\"'\", 268),\n ('is', 267),\n ('thy', 267),\n ('s', 263),\n ('thee', 257),\n ('be', 254),\n ('shall', 253),\n ('they', 249),\n ('all', 245),\n (':', 238),\n ('God', 231),\n ('them', 230),\n ('not', 224),\n ('which', 198),\n ('father', 198),\n ('will', 195),\n ('land', 184),\n ('Jacob', 179),\n ('came', 177),\n ('her', 173),\n ('LORD', 166),\n ('were', 163),\n ('she', 161),\n ('from', 157),\n ('Joseph', 157),\n ('their', 153)]\n\n\n\nfdist.plot(50, cumulative=True)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning/NLP",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Lecture 1"
  },
  {
    "objectID": "dl.html#lectures",
    "href": "dl.html#lectures",
    "title": "Deep Learning",
    "section": "",
    "text": "Lecture 1"
  },
  {
    "objectID": "dl.html#labs",
    "href": "dl.html#labs",
    "title": "Deep Learning",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "nlp.html#labs",
    "href": "nlp.html#labs",
    "title": "Natural Language Processing",
    "section": "Labs",
    "text": "Labs\nLab 1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nb/Untitled.html",
    "href": "nb/Untitled.html",
    "title": "Deep Learning/NLP course",
    "section": "",
    "text": "import nltk; \nnltk.download('popular')\nnltk.download('nps_chat')\nnltk.download('webtext')\nfrom nltk.book import *\n\n[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/inaugural.zip.\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/names.zip.\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/words.zip.\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Unzipping corpora/nps_chat.zip.\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Unzipping corpora/webtext.zip.\n\n\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n\n\n\nimport scattertext as st\n\n\nconvention_df = st.SampleCorpora.ConventionData2012.get_data()\n\n\nconvention_df.head()\n\n\n\n\n\n\n\n\nparty\ntext\nspeaker\n\n\n\n\n0\ndemocrat\nThank you. Thank you. Thank you. Thank you so ...\nBARACK OBAMA\n\n\n1\ndemocrat\nThank you so much. Tonight, I am so thrilled a...\nMICHELLE OBAMA\n\n\n2\ndemocrat\nThank you. It is a singular honor to be here t...\nRICHARD DURBIN\n\n\n3\ndemocrat\nHey, Delaware. \\nAnd my favorite Democrat, Jil...\nJOSEPH BIDEN\n\n\n4\ndemocrat\nHello. \\nThank you, Angie. I'm so proud of how...\nJILL BIDEN\n\n\n\n\n\n\n\n\n!python -m spacy download en_core_web_sm\n\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 4.0 MB/s eta 0:00:0000:0100:01\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.8.0\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n✔ Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n\n\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ncorpus = st.CorpusFromPandas(convention_df, \n                             category_col='party', \n                             text_col='text',\n                             nlp=nlp).build()\n\n\ncorpus.get_scaled_f_scores_vs_background()\n\n\n\n\n\n\n\n\nbackground\ncorpus\nScaled f-score\n\n\n\n\nobama\n565739.0\n702.0\n0.002006\n\n\nromney\n695398.0\n570.0\n0.001374\n\n\nbarack\n227861.0\n248.0\n0.001372\n\n\nmitt\n691902.0\n501.0\n0.001213\n\n\nobamacare\n0.0\n33.0\n0.000494\n\n\n...\n...\n...\n...\n\n\ngelding\n441608.0\n0.0\n0.000000\n\n\ngelderland\n90127.0\n0.0\n0.000000\n\n\ngelderen\n19339.0\n0.0\n0.000000\n\n\ngelder\n195446.0\n0.0\n0.000000\n\n\nNaN\n30739157.0\n0.0\n0.000000\n\n\n\n\n333436 rows × 3 columns\n\n\n\n\nterm_freq_df = corpus.get_term_freq_df()\nterm_freq_df['Democratic Score'] = corpus.get_scaled_f_scores('democrat')\n\n\nterm_freq_df.sort_values(by='Democratic Score', ascending=False)\n\n\n\n\n\n\n\n\ndemocrat freq\nrepublican freq\nDemocratic Score\n\n\nterm\n\n\n\n\n\n\n\nmiddle class\n148\n18\n1.000000\n\n\nforward\n105\n16\n0.994124\n\n\nclass\n161\n25\n0.993695\n\n\nmiddle\n164\n27\n0.991925\n\n\nthe middle\n98\n17\n0.989949\n\n\n...\n...\n...\n...\n\n\nsuccess\n25\n62\n0.021413\n\n\ncan do\n9\n42\n0.020554\n\n\nbusiness\n54\n140\n0.016743\n\n\nadministration\n11\n47\n0.011917\n\n\ngovernment\n43\n164\n0.000000\n\n\n\n\n62123 rows × 3 columns\n\n\n\n\nhtml = st.produce_scattertext_explorer(corpus,\n          category='democrat',\n          category_name='Democratic',\n          not_category_name='Republican',\n          width_in_pixels=1000,\n          metadata=convention_df['speaker'])\nopen(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))\n\n1726178"
  }
]