[
  {
    "objectID": "dl_lec5.html#history",
    "href": "dl_lec5.html#history",
    "title": "Intro to PyTorch",
    "section": "History",
    "text": "History\n\n\n\nTorch (2002)\n\n\nA Lua-based framework\n\n\n\n\n\n\n\nPyTorch (2016)\n\n\nPython impl based on Torch"
  },
  {
    "objectID": "dl_lec5.html#lua",
    "href": "dl_lec5.html#lua",
    "title": "Intro to PyTorch",
    "section": "Lua",
    "text": "Lua\n-- defines a factorial function\nfunction fact (n)\n  if n == 0 then\n    return 1\n  else\n    return n * fact(n-1)\n  end\nend\n\nprint(\"enter a number:\")\na = io.read(\"*number\")        -- read a number\nprint(fact(a))"
  },
  {
    "objectID": "dl_lec5.html#tensors",
    "href": "dl_lec5.html#tensors",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n\n\n\n\n\nWhat is it?\n\n\nTensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration. A vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we use tensors of various shapes and number of dimensions."
  },
  {
    "objectID": "dl_lec5.html#imports",
    "href": "dl_lec5.html#imports",
    "title": "Intro to PyTorch",
    "section": "Imports",
    "text": "Imports\n\n\n\nGeneric\n\n\n\n## Standard libraries\nimport os\nimport math\nimport numpy as np\nimport time\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nfrom matplotlib.colors import to_rgba\nimport seaborn as sns\nsns.set()\n\n## Progress bar\nfrom tqdm.notebook import tqdm\n\n\n\n\n\n\n\nBasic PyTorch import\n\n\n\nimport torch\nprint(\"Using torch\", torch.__version__)\n\nUsing torch 2.6.0"
  },
  {
    "objectID": "dl_lec5.html#set-the-seed",
    "href": "dl_lec5.html#set-the-seed",
    "title": "Intro to PyTorch",
    "section": "Set the seed",
    "text": "Set the seed\n\ntorch.manual_seed(42) # Setting the seed\n\n&lt;torch._C.Generator at 0x330026d10&gt;"
  },
  {
    "objectID": "dl_lec5.html#tensors-1",
    "href": "dl_lec5.html#tensors-1",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\nx = torch.Tensor(2, 3, 4)\nprint(x)\n\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "dl_lec5.html#tensors-2",
    "href": "dl_lec5.html#tensors-2",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n\n\nCreation\n\n\nThe function torch.Tensor allocates memory for the desired tensor, but reuses any values that have already been in the memory. To directly assign values to the tensor during initialization, there are many alternatives including:\n\ntorch.zeros: Creates a tensor filled with zeros\ntorch.ones: Creates a tensor filled with ones\ntorch.rand: Creates a tensor with random values uniformly sampled between 0 and 1\ntorch.randn: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\ntorch.arange: Creates a tensor containing the values \\(N,N+1,N+2,...,M\\)\ntorch.Tensor (input list): Creates a tensor from the list elements you provide"
  },
  {
    "objectID": "dl_lec5.html#tensors-3",
    "href": "dl_lec5.html#tensors-3",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n# Create a tensor from a (nested) list\nx = torch.Tensor([[1, 2], [3, 4]])\nprint(x)\n\ntensor([[1., 2.],\n        [3., 4.]])"
  },
  {
    "objectID": "dl_lec5.html#tensors-4",
    "href": "dl_lec5.html#tensors-4",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n# Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]\nx = torch.rand(2, 3, 4)\nprint(x)\n\ntensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n         [0.3904, 0.6009, 0.2566, 0.7936],\n         [0.9408, 0.1332, 0.9346, 0.5936]],\n\n        [[0.8694, 0.5677, 0.7411, 0.4294],\n         [0.8854, 0.5739, 0.2666, 0.6274],\n         [0.2696, 0.4414, 0.2969, 0.8317]]])"
  },
  {
    "objectID": "dl_lec5.html#tensors-5",
    "href": "dl_lec5.html#tensors-5",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n\n\nTensor shape\n\n\n\nshape = x.shape\nprint(\"Shape:\", x.shape)\n\nsize = x.size()\nprint(\"Size:\", size)\n\ndim1, dim2, dim3 = x.size()\nprint(\"Size:\", dim1, dim2, dim3)\n\nShape: torch.Size([2, 3, 4])\nSize: torch.Size([2, 3, 4])\nSize: 2 3 4"
  },
  {
    "objectID": "dl_lec5.html#tensors-6",
    "href": "dl_lec5.html#tensors-6",
    "title": "Intro to PyTorch",
    "section": "Tensors",
    "text": "Tensors\n\n\n\nTo Numpy and back again\n\n\n\nnp_arr = np.array([[1, 2], [3, 4]])\ntensor = torch.from_numpy(np_arr)\n\nprint(\"Numpy array:\", np_arr)\nprint(\"PyTorch tensor:\", tensor)\n\nNumpy array: [[1 2]\n [3 4]]\nPyTorch tensor: tensor([[1, 2],\n        [3, 4]])\n\n\n\n\ntensor = torch.arange(4)\nnp_arr = tensor.numpy()\n\nprint(\"PyTorch tensor:\", tensor)\nprint(\"Numpy array:\", np_arr)\n\nPyTorch tensor: tensor([0, 1, 2, 3])\nNumpy array: [0 1 2 3]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU. In case you have a tensor on GPU, then: np_arr = tensor.cpu().numpy()."
  },
  {
    "objectID": "dl_lec5.html#operations",
    "href": "dl_lec5.html#operations",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nAddition\n\n\n\nx1 = torch.rand(2, 3)\nx2 = torch.rand(2, 3)\ny = x1 + x2\n\nprint(\"X1\", x1)\nprint(\"X2\", x2)\nprint(\"Y\", y)\n\nX1 tensor([[0.1053, 0.2695, 0.3588],\n        [0.1994, 0.5472, 0.0062]])\nX2 tensor([[0.9516, 0.0753, 0.8860],\n        [0.5832, 0.3376, 0.8090]])\nY tensor([[1.0569, 0.3448, 1.2448],\n        [0.7826, 0.8848, 0.8151]])\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nCalling x1 + x2 creates a new tensor containing the sum of the two inputs."
  },
  {
    "objectID": "dl_lec5.html#operations-1",
    "href": "dl_lec5.html#operations-1",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nIn-place\n\n\n\nx1 = torch.rand(2, 3)\nx2 = torch.rand(2, 3)\nprint(\"X1 (before)\", x1)\nprint(\"X2 (before)\", x2)\n\nx2.add_(x1)\nprint(\"X1 (after)\", x1)\nprint(\"X2 (after)\", x2)\n\nX1 (before) tensor([[0.5779, 0.9040, 0.5547],\n        [0.3423, 0.6343, 0.3644]])\nX2 (before) tensor([[0.7104, 0.9464, 0.7890],\n        [0.2814, 0.7886, 0.5895]])\nX1 (after) tensor([[0.5779, 0.9040, 0.5547],\n        [0.3423, 0.6343, 0.3644]])\nX2 (after) tensor([[1.2884, 1.8504, 1.3437],\n        [0.6237, 1.4230, 0.9539]])\n\n\n\n\n\n\n\nIn-place operations are usually marked with a underscore postfix (e.g. “add_” instead of “add”)."
  },
  {
    "objectID": "dl_lec5.html#operations-2",
    "href": "dl_lec5.html#operations-2",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nShape change\n\n\nAnother common operation aims at changing the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g. a tensor of size (6), or (3,2), …). In PyTorch, this operation is called view:\n\nx = torch.arange(6)\nprint(\"X\", x)\n\nX tensor([0, 1, 2, 3, 4, 5])\n\n\n\nx = x.view(2, 3)\nprint(\"X\", x)\n\nX tensor([[0, 1, 2],\n        [3, 4, 5]])\n\n\nYou can also swap dimensions:\n\nx = x.permute(1, 0) # Swapping dimension 0 and 1\nprint(\"X\", x)\n\nX tensor([[0, 3],\n        [1, 4],\n        [2, 5]])"
  },
  {
    "objectID": "dl_lec5.html#operations-3",
    "href": "dl_lec5.html#operations-3",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nMultiplication\n\n\nQuite often, we have an input vector \\(\\mathbf{x}\\), which is transformed using a learned weight matrix \\(\\mathbf{W}\\). There are multiple ways and functions to perform matrix multiplication:\n\ntorch.matmul: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the documentation). Can also be written as a @ b, similar to numpy.\ntorch.mm: Performs the matrix product over two matrices, but doesn’t support broadcasting (see documentation)"
  },
  {
    "objectID": "dl_lec5.html#operations-4",
    "href": "dl_lec5.html#operations-4",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nMultiplication\n\n\n\ntorch.bmm: Performs the matrix product with a support batch dimension. If the first tensor \\(T\\) is of shape (\\(b\\times n\\times m\\)), and the second tensor \\(R\\) (\\(b\\times m\\times p\\)), the output \\(O\\) is of shape (\\(b\\times n\\times p\\)), and has been calculated by performing \\(b\\) matrix multiplications of the submatrices of \\(T\\) and \\(R\\): \\(O_i = T_i @ R_i\\)\ntorch.einsum: Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention. Explanation of the Einstein sum can be found in assignment 1.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nUsually, we use torch.matmul or torch.bmm."
  },
  {
    "objectID": "dl_lec5.html#operations-5",
    "href": "dl_lec5.html#operations-5",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nMultiplication\n\n\nWe can try a matrix multiplication with torch.matmul:\n\nx = torch.arange(6)\nx = x.view(2, 3)\nprint(\"X\", x)\n\nX tensor([[0, 1, 2],\n        [3, 4, 5]])\n\n\n\nW = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line\nprint(\"W\", W)\n\nW tensor([[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\n\n\n\nh = torch.matmul(x, W) # Verify the result by calculating it by hand too!\nprint(\"h\", h)\n\nh tensor([[15, 18, 21],\n        [42, 54, 66]])"
  },
  {
    "objectID": "dl_lec5.html#operations-6",
    "href": "dl_lec5.html#operations-6",
    "title": "Intro to PyTorch",
    "section": "Operations",
    "text": "Operations\n\n\n\nIndexing\n\n\nWe often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let’s try it:\n\nx = torch.arange(12).view(3, 4)\nprint(\"X\", x)\n\nX tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\n\n\nprint(x[:, 1])   # Second column\n\ntensor([1, 5, 9])\n\n\n\nprint(x[0])      # First row\n\ntensor([0, 1, 2, 3])\n\n\n\nprint(x[:2, -1]) # First two rows, last column\n\ntensor([3, 7])\n\n\n\nprint(x[1:3, :]) # Middle two rows\n\ntensor([[ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
  },
  {
    "objectID": "dl_lec5.html#computation",
    "href": "dl_lec5.html#computation",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nDynamic computation graph: recap\n\n\n\nPyTorch automatically gives us gradients/derivatives of functions that we define.\ngiven an input \\(\\mathbf{x}\\), we define our function by manipulating that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a computational graph.\nPyTorch is a define-by-run framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us.\nthe only thing we have to do is to compute the output, and then we can ask PyTorch to automatically get the gradients."
  },
  {
    "objectID": "dl_lec5.html#computation-1",
    "href": "dl_lec5.html#computation-1",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nGradients\n\n\nThe first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients.\n\nx = torch.ones((3,))\nprint(x.requires_grad)\n\nFalse\n\n\n\n\n\n\n\n\nGradients: enabling\n\n\nWe can change this for an existing tensor using the function requires_grad_() (underscore indicating that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument requires_grad=True to most initializers we have seen above.\n\nx.requires_grad_(True)\nprint(x.requires_grad)\n\nTrue"
  },
  {
    "objectID": "dl_lec5.html#computation-2",
    "href": "dl_lec5.html#computation-2",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nComputation graph: example\n\n\nIn order to get familiar with the concept of a computation graph, we will create one for the following function:\n\\[y = \\frac{1}{\\ell(x)}\\sum_i \\left[(x_i + 2)^2 + 3\\right],\\]\nwhere we use \\(\\ell(x)\\) to denote the number of elements in \\(x\\).\nWe want to optimize (either maximize or minimize) the output \\(y\\). For this, we want to obtain the gradients \\(\\partial y / \\partial \\mathbf{x}\\). For our example, we’ll use \\(\\mathbf{x}=[0,1,2]\\) as our input.\n\nx = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\nprint(\"X\", x)\n\nX tensor([0., 1., 2.], requires_grad=True)"
  },
  {
    "objectID": "dl_lec5.html#computation-3",
    "href": "dl_lec5.html#computation-3",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nComputation graph: example\n\n\n\n\na = x + 2\nb = a ** 2\nc = b + 3\ny = c.mean()\nprint(\"Y\", y)\n\nY tensor(12.6667, grad_fn=&lt;MeanBackward0&gt;)"
  },
  {
    "objectID": "dl_lec5.html#computation-4",
    "href": "dl_lec5.html#computation-4",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nComputation graph: example\n\n\n\nEach node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, grad_fn.\nThis is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs)."
  },
  {
    "objectID": "dl_lec5.html#computation-5",
    "href": "dl_lec5.html#computation-5",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nComputation graph: example\n\n\nWe can perform backpropagation on the computation graph by calling the function backward() on the last output, which effectively calculates the gradients for each tensor that has the property requires_grad=True:\n\ny.backward()\n\nx.grad will now contain the gradient \\(\\partial y/ \\partial \\mathcal{x}\\), and this gradient indicates how a change in \\(\\mathbf{x}\\) will affect output \\(y\\) given the current input \\(\\mathbf{x}=[0,1,2]\\):\n\nprint(x.grad)\n\ntensor([1.3333, 2.0000, 2.6667])"
  },
  {
    "objectID": "dl_lec5.html#computation-6",
    "href": "dl_lec5.html#computation-6",
    "title": "Intro to PyTorch",
    "section": "Computation",
    "text": "Computation\n\n\n\nComputation graph: example\n\n\nWe can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n\\[\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}\\]\nNote that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n\\[\n\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n\\]\nHence, with the input being \\(\\mathbf{x}=[0,1,2]\\), our gradients are \\(\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]\\). The previous code cell should have printed the same result."
  },
  {
    "objectID": "dl_lec5.html#gpu-support",
    "href": "dl_lec5.html#gpu-support",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nGPU overview\n\n\nA crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit.\n\nA GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks.\nGPUs can accelerate the training of your network up to a factor of \\(100\\) which is essential for large neural networks.\nPyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries CUDA and cuDNN)."
  },
  {
    "objectID": "dl_lec5.html#gpu-support-1",
    "href": "dl_lec5.html#gpu-support-1",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support"
  },
  {
    "objectID": "dl_lec5.html#gpu-support-2",
    "href": "dl_lec5.html#gpu-support-2",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nGPU\n\n\nFirst, let’s check whether you have a GPU available:\n\ngpu_avail = torch.cuda.is_available()\nprint(f\"Is the Nvidia GPU available? {gpu_avail}\")\n\ngpu_avail = torch.mps.is_available()\nprint(f\"Is the Apple GPU available? {gpu_avail}\")\n\nIs the Nvidia GPU available? False\nIs the Apple GPU available? True"
  },
  {
    "objectID": "dl_lec5.html#gpu-support-3",
    "href": "dl_lec5.html#gpu-support-3",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nPushing to GPU\n\n\nBy default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function .to(...), or .cuda().\n\n\n\n\n\n\n\n\n\nTip\n\n\nIt is often a good practice to define a device object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU.\nWe can specify the device as follows:\n\ndevice = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\nprint(\"Device\", device)\n\nDevice mps"
  },
  {
    "objectID": "dl_lec5.html#gpu-support-4",
    "href": "dl_lec5.html#gpu-support-4",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nPushing to GPU\n\n\nLet’s create a tensor and push it now:\n\nx = torch.zeros(2, 3)\nx = x.to(device)\nprint(\"X\", x)\n\nX tensor([[0., 0., 0.],\n        [0., 0., 0.]], device='mps:0')\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe zero next to mps indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train."
  },
  {
    "objectID": "dl_lec5.html#gpu-support-5",
    "href": "dl_lec5.html#gpu-support-5",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nGPU-to-CPU comparison\n\n\n\nx = torch.randn(5000, 5000)\n\n## CPU version\nstart_time = time.time()\n_ = torch.matmul(x, x)\nend_time = time.time()\nprint(f\"CPU time: {(end_time - start_time):6.5f}s\")\n\n## GPU version\nx = x.to(device)\n_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n# CUDA is asynchronous, so we need to use different timing functions\nstart = torch.mps.Event(enable_timing=True)\nend = torch.mps.Event(enable_timing=True)\nstart.record()\n_ = torch.matmul(x, x)\nend.record()\ntorch.mps.synchronize()  # Waits for everything to finish running on the GPU\nprint(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds\n\nCPU time: 0.19913s\nGPU time: 0.05936s"
  },
  {
    "objectID": "dl_lec5.html#gpu-support-6",
    "href": "dl_lec5.html#gpu-support-6",
    "title": "Intro to PyTorch",
    "section": "GPU support",
    "text": "GPU support\n\n\n\nGPU: seed sync\n\n\nWhen generating random numbers, the seed between CPU and GPU is not synchronized.\nHence, we need to set the seed on the GPU separately to ensure a reproducible code.\n\n# GPU operations have a separate seed we also want to set\nif torch.mps.is_available():\n    torch.mps.manual_seed(42)\n    #torch.mps.manual_seed_all(42)\n\n# Additionally, some operations on a GPU are implemented stochastic for efficiency\n# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.mps.deterministic = True\ntorch.backends.mps.benchmark = False"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example",
    "href": "dl_lec5.html#continuous-xor-example",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nManual way\n\n\nIf we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using Tensors (with requires_grad=True), ask PyTorch to calculate the gradients and then adjust the parameters.\nBut things can quickly get cumbersome if we have a lot of parameters.\n\n\n\n\n\n\nUsing torch.nn\n\n\nIn PyTorch, there is a package called torch.nn that makes building neural networks more convenient."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-1",
    "href": "dl_lec5.html#continuous-xor-example-1",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nExample description\n\n\nGiven two binary inputs \\(x_1\\) and \\(x_2\\), the label to predict is \\(1\\) if either \\(x_1\\) or \\(x_2\\) is \\(1\\) while the other is \\(0\\), or the label is \\(0\\) in all other cases.\nThe example became famous by the fact that a single neuron, i.e. a linear classifier, cannot learn this simple function."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-2",
    "href": "dl_lec5.html#continuous-xor-example-2",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nModel\n\n\nThe package torch.nn defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found here.\n\nimport torch.nn as nn\n\n\n\n\n\n\n\nFunctional\n\n\nAdditionally to torch.nn, there is also torch.nn.functional. It contains functions that are used in network layers. This is in contrast to torch.nn which defines them as nn.Modules, and torch.nn actually uses a lot of functionalities from torch.nn.functional.\n\nimport torch.nn.functional as F"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-3",
    "href": "dl_lec5.html#continuous-xor-example-3",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nnn.Module\n\n\nIn PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:\n\nclass MyModule(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # Some init for my module\n\n    def forward(self, x):\n        # Function for performing the calculation of the module.\n        pass\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe forward function is where the computation of the module is taken place, and is executed when you call the module (nn = MyModule(); nn(x)). The backward calculation is done automatically, but could be overwritten as well if wanted."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-4",
    "href": "dl_lec5.html#continuous-xor-example-4",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier\n\n\nWe can now make use of the pre-defined modules in the torch.nn package, and define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-5",
    "href": "dl_lec5.html#continuous-xor-example-5",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier: Module def\n\n\n\nclass SimpleClassifier(nn.Module):\n\n    def __init__(self, num_inputs, num_hidden, num_outputs):\n        super().__init__()\n        # Initialize the modules we need to build the network\n        self.linear1 = nn.Linear(num_inputs, num_hidden)\n        self.act_fn = nn.Tanh()\n        self.linear2 = nn.Linear(num_hidden, num_outputs)\n\n    def forward(self, x):\n        # Perform the calculation of the model to determine the prediction\n        x = self.linear1(x)\n        x = self.act_fn(x)\n        x = self.linear2(x)\n        return x\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNote that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-6",
    "href": "dl_lec5.html#continuous-xor-example-6",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier: Module creation\n\n\n\nmodel = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n# Printing a module shows all its submodules\nprint(model)\n\nSimpleClassifier(\n  (linear1): Linear(in_features=2, out_features=4, bias=True)\n  (act_fn): Tanh()\n  (linear2): Linear(in_features=4, out_features=1, bias=True)\n)"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-7",
    "href": "dl_lec5.html#continuous-xor-example-7",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier: Module parameters\n\n\nThe parameters of a module can be obtained by using its parameters() functions, or named_parameters() to get a name to each parameter object. For our small neural network, we have the following parameters:\n\nfor name, param in model.named_parameters():\n    print(f\"Parameter {name}, shape {param.shape}\")\n\nParameter linear1.weight, shape torch.Size([4, 2])\nParameter linear1.bias, shape torch.Size([4])\nParameter linear2.weight, shape torch.Size([1, 4])\nParameter linear2.bias, shape torch.Size([1])"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-8",
    "href": "dl_lec5.html#continuous-xor-example-8",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier: Module parameters\n\n\n\nEach linear layer has a weight matrix of the shape [output, input], and a bias of the shape [output].\nThe tanh activation function does not have any parameters.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nParameters are only registered for nn.Module objects that are direct object attributes, i.e. self.a = .... There are alternatives, like nn.ModuleList, nn.ModuleDict and nn.Sequential, that allow you to have different data structures of modules."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-9",
    "href": "dl_lec5.html#continuous-xor-example-9",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nSimple classifier: data\n\n\nPyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package torch.utils.data.\n\nimport torch.utils.data as data\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe data package defines two classes which are the standard interface for handling data in PyTorch:\n\ndata.Dataset: provides an uniform interface to access the training/test data\ndata.DataLoader: makes sure to efficiently load and stack the data points from the dataset into batches during training."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-10",
    "href": "dl_lec5.html#continuous-xor-example-10",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nDataset class\n\n\nTo define a dataset in PyTorch, we simply specify two functions:\n\n__getitem__: has to return the \\(i\\)-th data point in the dataset\n__len__: returns the size of the dataset."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-11",
    "href": "dl_lec5.html#continuous-xor-example-11",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nDataset class\n\n\n\nclass XORDataset(data.Dataset):\n\n    def __init__(self, size, std=0.1):\n        \"\"\"\n        Inputs:\n            size - Number of data points we want to generate\n            std - Standard deviation of the noise (see generate_continuous_xor function)\n        \"\"\"\n        super().__init__()\n        self.size = size\n        self.std = std\n        self.generate_continuous_xor()\n\n    def generate_continuous_xor(self):\n        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n        # If x=y, the label is 0.\n        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n        label = (data.sum(dim=1) == 1).to(torch.long)\n        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n        data += self.std * torch.randn(data.shape)\n\n        self.data = data\n        self.label = label\n\n    def __len__(self):\n        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n        return self.size\n\n    def __getitem__(self, idx):\n        # Return the idx-th data point of the dataset\n        # If we have multiple things to return (data point and label), we can return them as tuple\n        data_point = self.data[idx]\n        data_label = self.label[idx]\n        return data_point, data_label"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-12",
    "href": "dl_lec5.html#continuous-xor-example-12",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nDataset class\n\n\nLet’s create and inspect:\n\ndataset = XORDataset(size=200)\nprint(\"Size of dataset:\", len(dataset))\nprint(\"Data point 0:\", dataset[0])\n\nSize of dataset: 200\nData point 0: (tensor([0.9632, 0.1117]), tensor(1))"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-13",
    "href": "dl_lec5.html#continuous-xor-example-13",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nDataset visualization\n\n\n\nFunc definitionResult\n\n\n\ndef visualize_samples(data, label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n    data_0 = data[label == 0]\n    data_1 = data[label == 1]\n\n    plt.figure(figsize=(4,4))\n    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(r\"$x_2$\")\n    plt.xlabel(r\"$x_1$\")\n    plt.legend()\n\n\n\n\nvisualize_samples(dataset.data, dataset.label)\nplt.show()"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-14",
    "href": "dl_lec5.html#continuous-xor-example-14",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nThe data loader class\n\n\nThe class torch.utils.data.DataLoader represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function __getitem__, and stacks its outputs as tensors over the first dimension to form a batch. We can configure our data loader with the following input arguments:\n\nbatch_size: Number of samples to stack per batch\nshuffle: If True, the data is returned in a random order. This is important during training for introducing stochasticity."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-15",
    "href": "dl_lec5.html#continuous-xor-example-15",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nThe data loader class\n\n\n\nnum_workers: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\npin_memory: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.\ndrop_last: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-16",
    "href": "dl_lec5.html#continuous-xor-example-16",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nThe data loader class\n\n\nCreate:\n\ndata_loader = data.DataLoader(dataset, batch_size=8, shuffle=True)\n\nFetch some data:\n\n# next(iter(...)) catches the first batch of the data loader\n# If shuffle is True, this will return a different batch every time we run this cell\n# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\ndata_inputs, data_labels = next(iter(data_loader))\n\n# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n# dimensions of the data point returned from the dataset class\nprint(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\nprint(\"Data labels\", data_labels.shape, \"\\n\", data_labels)\n\nData inputs torch.Size([8, 2]) \n tensor([[-0.0890,  0.8608],\n        [ 1.0905, -0.0128],\n        [ 0.7967,  0.2268],\n        [-0.0688,  0.0371],\n        [ 0.8732, -0.2240],\n        [-0.0559, -0.0282],\n        [ 0.9277,  0.0978],\n        [ 1.0150,  0.9689]])\nData labels torch.Size([8]) \n tensor([1, 1, 1, 0, 1, 0, 1, 0])"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-17",
    "href": "dl_lec5.html#continuous-xor-example-17",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nOptimization\n\n\nAfter defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n\nGet a batch from the data loader\nObtain the predictions from the model for the batch\nCalculate the loss based on the difference between predictions and labels\nBackpropagation: calculate the gradients for every parameter with respect to the loss\nUpdate the parameters of the model in the direction of the gradients\n\nWe have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-18",
    "href": "dl_lec5.html#continuous-xor-example-18",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nLoss\n\n\nWe can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:\n\\[\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]\\]\nwhere \\(y\\) are our labels, and \\(x\\) our predictions, both in the range of \\([0,1]\\)."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-19",
    "href": "dl_lec5.html#continuous-xor-example-19",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nPredefined funcs\n\n\nPyTorch already provides a list of predefined loss functions which we can use (see here for a full list).\nFor BCE, PyTorch has two modules:\n\nnn.BCELoss(): expects the inputs \\(x\\) to be in the range \\([0,1]\\), i.e. the output of a sigmoid,\nnn.BCEWithLogitsLoss() combines a sigmoid layer and the BCE loss in a single class. This version is numerically more stable than using a plain Sigmoid followed by a BCE loss because of the logarithms applied in the loss function.\n\n\n\n\n\n\nIt is adviced to use loss functions applied on “logits” where possible (remember to not apply a sigmoid on the output of the model in this case!)."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-20",
    "href": "dl_lec5.html#continuous-xor-example-20",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nLoss\n\n\nFor our model defined above, we therefore use the module nn.BCEWithLogitsLoss.\n\nloss_module = nn.BCEWithLogitsLoss()"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-21",
    "href": "dl_lec5.html#continuous-xor-example-21",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nStochastic Gradient Descent\n\n\nFor updating the parameters, PyTorch provides the package torch.optim that has most popular optimizers implemented.\ntorch.optim.SGD (Stochastic Gradient Descent). Updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss).\nA good default value of the learning rate for a small network as ours is 0.1.\n\n# Input to the optimizer are the parameters of the model: model.parameters()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-22",
    "href": "dl_lec5.html#continuous-xor-example-22",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nOptimizer\n\n\nThe optimizer provides two useful functions:\n\noptimizer.step(): updates the parameters based on the gradients as explained above.\noptimizer.zero_grad(): sets the gradients of all parameters to zero.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nzero_grad() is a crucial pre-step before performing backpropagation. If we call the backward function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-23",
    "href": "dl_lec5.html#continuous-xor-example-23",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nTraining\n\n\nFinally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size.\n\ntrain_dataset = XORDataset(size=2500)\ntrain_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n\nThen, push to GPU:\n\n# Push model to device. Has to be only done once\nmodel.to(device)\n\nSimpleClassifier(\n  (linear1): Linear(in_features=2, out_features=4, bias=True)\n  (act_fn): Tanh()\n  (linear2): Linear(in_features=4, out_features=1, bias=True)\n)"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-24",
    "href": "dl_lec5.html#continuous-xor-example-24",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nTraining\n\n\nSet model to training mode via model.train() (there is also model.eval()):\n\ndef train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n    # Set model to train mode\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        for data_inputs, data_labels in data_loader:\n\n            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n\n            ## Step 2: Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n\n            ## Step 3: Calculate the loss\n            loss = loss_module(preds, data_labels.float())\n\n            ## Step 4: Perform backpropagation\n            # Before calculating the gradients, we need to ensure that they are all zero.\n            # The gradients would not be overwritten, but actually added to the existing ones.\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            ## Step 5: Update the parameters\n            optimizer.step()"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-25",
    "href": "dl_lec5.html#continuous-xor-example-25",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nTraining\n\n\nTrain:\n\ntrain_model(model, optimizer, train_data_loader, loss_module)"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-26",
    "href": "dl_lec5.html#continuous-xor-example-26",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nTraining: save a model\n\n\nAfter finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called state_dict from the model which contains all learnable parameters.\n\nstate_dict = model.state_dict()\nprint(state_dict)\n\nOrderedDict({'linear1.weight': tensor([[-2.6034, -3.3292],\n        [ 1.9774, -2.4076],\n        [-2.5968, -1.5908],\n        [-0.5717, -0.8101]], device='mps:0'), 'linear1.bias': tensor([ 1.4459, -1.3992,  2.9882, -0.1375], device='mps:0'), 'linear2.weight': tensor([[-4.4623,  3.0885,  4.4030, -0.1377]], device='mps:0'), 'linear2.bias': tensor([-1.6853], device='mps:0')})\n\n\nTo save the state dictionary, we can use torch.save:\n\n# torch.save(object, filename). For the filename, any extension can be used\ntorch.save(state_dict, \"our_model.tar\")"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-27",
    "href": "dl_lec5.html#continuous-xor-example-27",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nTraining: load a model\n\n\nTo load a model from a state dict, we use the function torch.load to load the state dict from the disk, and the module function load_state_dict to overwrite our parameters with the new values:\n\n# Load state dict from the disk (make sure it is the same name as above)\nstate_dict = torch.load(\"our_model.tar\")\n\n# Create a new model and load the state\nnew_model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\nnew_model.load_state_dict(state_dict)\n\n# Verify that the parameters are the same\nprint(\"Original model\\n\", model.state_dict())\nprint(\"\\nLoaded model\\n\", new_model.state_dict())\n\nOriginal model\n OrderedDict({'linear1.weight': tensor([[-2.6034, -3.3292],\n        [ 1.9774, -2.4076],\n        [-2.5968, -1.5908],\n        [-0.5717, -0.8101]], device='mps:0'), 'linear1.bias': tensor([ 1.4459, -1.3992,  2.9882, -0.1375], device='mps:0'), 'linear2.weight': tensor([[-4.4623,  3.0885,  4.4030, -0.1377]], device='mps:0'), 'linear2.bias': tensor([-1.6853], device='mps:0')})\n\nLoaded model\n OrderedDict({'linear1.weight': tensor([[-2.6034, -3.3292],\n        [ 1.9774, -2.4076],\n        [-2.5968, -1.5908],\n        [-0.5717, -0.8101]]), 'linear1.bias': tensor([ 1.4459, -1.3992,  2.9882, -0.1375]), 'linear2.weight': tensor([[-4.4623,  3.0885,  4.4030, -0.1377]]), 'linear2.bias': tensor([-1.6853])})"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-28",
    "href": "dl_lec5.html#continuous-xor-example-28",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nEvaluation\n\n\nOnce we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader.\n\ntest_dataset = XORDataset(size=500)\n# drop_last -&gt; Don't drop the last batch although it is smaller than 128\ntest_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)\n\nAs metric, we will use accuracy which is calculated as follows:\n\\[acc = \\frac{\\#\\text{correct predictions}}{\\#\\text{all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}\\]\nwhere TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives."
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-29",
    "href": "dl_lec5.html#continuous-xor-example-29",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nEvaluation\n\n\n\nwe don’t need to keep track of the computation graph as we don’t intend to calculate the gradients. In PyTorch this is done via with torch.no_grad(): ....\nremember to additionally set the model to eval mode.\n\n\ndef eval_model(model, data_loader):\n    model.eval() # Set model to eval mode\n    true_preds, num_preds = 0., 0.\n\n    with torch.no_grad(): # Deactivate gradients for the following code\n        for data_inputs, data_labels in data_loader:\n\n            # Determine prediction of model on dev set\n            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1)\n            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n            pred_labels = (preds &gt;= 0.5).long() # Binarize predictions to 0 and 1\n\n            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n            true_preds += (pred_labels == data_labels).sum()\n            num_preds += data_labels.shape[0]\n\n    acc = true_preds / num_preds\n    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-30",
    "href": "dl_lec5.html#continuous-xor-example-30",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nEvaluation\n\n\n\neval_model(model, test_data_loader)\n\nAccuracy of the model: 100.00%"
  },
  {
    "objectID": "dl_lec5.html#continuous-xor-example-31",
    "href": "dl_lec5.html#continuous-xor-example-31",
    "title": "Intro to PyTorch",
    "section": "Continuous XOR example",
    "text": "Continuous XOR example\n\n\n\nVisualizing classification boundaries\n\n\nTo visualize what our model has learned, we can perform a prediction for every data point in a range of \\([-0.5, 1.5]\\), and visualize the predicted class (class 0 is blue and class 1 is orange).\n\n@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\ndef visualize_classification(model, data, label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n    data_0 = data[label == 0]\n    data_1 = data[label == 1]\n\n    fig = plt.figure(figsize=(4,4), dpi=500)\n    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(r\"$x_2$\")\n    plt.xlabel(r\"$x_1$\")\n    plt.legend()\n\n    # Let's make use of a lot of operations we have learned above\n    model.to(device)\n    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n    model_inputs = torch.stack([xx1, xx2], dim=-1)\n    preds = model(model_inputs)\n    preds = torch.sigmoid(preds)\n    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n    plt.grid(False)\n    return fig\n\n_ = visualize_classification(model, dataset.data, dataset.label)\nplt.show()"
  },
  {
    "objectID": "dl_lec5.html#additional-features",
    "href": "dl_lec5.html#additional-features",
    "title": "Intro to PyTorch",
    "section": "Additional features",
    "text": "Additional features\n\n\n\nTensorboard\n\n\nTensorBoard is a logging and visualization tool that is a popular choice for training deep learning models.\n\n# Import tensorboard logger from PyTorch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n%load_ext tensorboard\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard"
  },
  {
    "objectID": "dl_lec5.html#additional-features-1",
    "href": "dl_lec5.html#additional-features-1",
    "title": "Intro to PyTorch",
    "section": "Additional features",
    "text": "Additional features\n\n\n\nTensorboard API\n\n\n\nWe start the logging process by creating a new object, writer = SummaryWriter(...), where we specify the directory in which the logging file should be saved\nWith this object, we can log different aspects of our model by calling functions of the style writer.add_....\nFor example, we can visualize the computation graph with the function writer.add_graph, or add a scalar value like the loss with writer.add_scalar."
  },
  {
    "objectID": "dl_lec5.html#additional-features-2",
    "href": "dl_lec5.html#additional-features-2",
    "title": "Intro to PyTorch",
    "section": "Additional features",
    "text": "Additional features\n\n\n\nTensorboard\n\n\n\ndef train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs=100, logging_dir='runs/our_experiment'):\n    # Create TensorBoard logger\n    writer = SummaryWriter(logging_dir)\n    model_plotted = False\n\n    # Set model to train mode\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        epoch_loss = 0.0\n        for data_inputs, data_labels in data_loader:\n\n            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n\n            # For the very first batch, we visualize the computation graph in TensorBoard\n            if not model_plotted:\n                writer.add_graph(model, data_inputs)\n                model_plotted = True\n\n            ## Step 2: Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n\n            ## Step 3: Calculate the loss\n            loss = loss_module(preds, data_labels.float())\n\n            ## Step 4: Perform backpropagation\n            # Before calculating the gradients, we need to ensure that they are all zero.\n            # The gradients would not be overwritten, but actually added to the existing ones.\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            ## Step 5: Update the parameters\n            optimizer.step()\n\n            ## Step 6: Take the running average of the loss\n            epoch_loss += loss.item()\n\n        # Add average loss to TensorBoard\n        epoch_loss /= len(data_loader)\n        writer.add_scalar('training_loss',\n                          epoch_loss,\n                          global_step = epoch + 1)\n\n        # Visualize prediction and add figure to TensorBoard\n        # Since matplotlib figures can be slow in rendering, we only do it every 10th epoch\n        if (epoch + 1) % 10 == 0:\n            fig = visualize_classification(model, val_dataset.data, val_dataset.label)\n            writer.add_figure('predictions',\n                              fig,\n                              global_step = epoch + 1)\n\n    writer.close()"
  },
  {
    "objectID": "dl_lec5.html#additional-features-3",
    "href": "dl_lec5.html#additional-features-3",
    "title": "Intro to PyTorch",
    "section": "Additional features",
    "text": "Additional features\n\n\n\nTensorboard\n\n\nTrain:\n\nmodel = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\ntrain_model_with_logger(model, optimizer, train_data_loader, loss_module, val_dataset=dataset)"
  },
  {
    "objectID": "dl_lec5.html#additional-features-4",
    "href": "dl_lec5.html#additional-features-4",
    "title": "Intro to PyTorch",
    "section": "Additional features",
    "text": "Additional features\n\n\n%tensorboard --logdir runs/our_experiment"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-1",
    "href": "dl_lec5.html#activation-functions-1",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\nSetupData fetch\n\n\n\n# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\nDATASET_PATH = \"./data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"./saved_models/activation_funcs\"\n\n# Function for setting the seed\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.mps.is_available(): # GPU operation have separate seed\n        torch.mps.manual_seed(seed)\n        #torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\n\n\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial3/\"\n# Files to download\npretrained_files = [\"FashionMNIST_elu.config\", \"FashionMNIST_elu.tar\", \n                    \"FashionMNIST_leakyrelu.config\", \"FashionMNIST_leakyrelu.tar\",\n                    \"FashionMNIST_relu.config\", \"FashionMNIST_relu.tar\",\n                    \"FashionMNIST_sigmoid.config\", \"FashionMNIST_sigmoid.tar\",\n                    \"FashionMNIST_swish.config\", \"FashionMNIST_swish.tar\",\n                    \"FashionMNIST_tanh.config\", \"FashionMNIST_tanh.tar\"]\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-2",
    "href": "dl_lec5.html#activation-functions-2",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions\n\n\nLet’s implement some ourselves. Start with the base class\n\nclass ActivationFunction(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.name = self.__class__.__name__\n        self.config = {\"name\": self.name}"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-4",
    "href": "dl_lec5.html#activation-functions-4",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nBasic funcs:\n\n\n\n##############################\n\nclass Sigmoid(ActivationFunction):\n    \n    def forward(self, x):\n        return 1 / (1 + torch.exp(-x))\n\n##############################   \n    \nclass Tanh(ActivationFunction):\n    \n    def forward(self, x):\n        x_exp, neg_x_exp = torch.exp(x), torch.exp(-x)\n        return (x_exp - neg_x_exp) / (x_exp + neg_x_exp)\n    \n##############################"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-5",
    "href": "dl_lec5.html#activation-functions-5",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: RELU and friend\n\n\n\n##############################\n\nclass ReLU(ActivationFunction):\n    \n    def forward(self, x):\n        return x * (x &gt; 0).float()\n\n##############################\n\nclass LeakyReLU(ActivationFunction):\n    \n    def __init__(self, alpha=0.1):\n        super().__init__()\n        self.config[\"alpha\"] = alpha\n        \n    def forward(self, x):\n        return torch.where(x &gt; 0, x, self.config[\"alpha\"] * x)\n\n##############################\n    \nclass ELU(ActivationFunction):\n    \n    def forward(self, x):\n        return torch.where(x &gt; 0, x, torch.exp(x)-1)\n\n##############################\n    \nclass Swish(ActivationFunction):\n    \n    def forward(self, x):\n        return x * torch.sigmoid(x)\n    \n##############################\n\n\n\n\n\n\nSwish is both smooth and non-monotonic (i.e. contains a change of sign in the gradient). This has been shown to prevent dead neurons as in standard ReLU activation, especially for deep networks."
  },
  {
    "objectID": "dl_lec5.html#activation-functions-6",
    "href": "dl_lec5.html#activation-functions-6",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: convenience dict\n\n\n\nact_fn_by_name = {\n    \"sigmoid\": Sigmoid,\n    \"tanh\": Tanh,\n    \"relu\": ReLU,\n    \"leakyrelu\": LeakyReLU,\n    \"elu\": ELU,\n    \"swish\": Swish\n}"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-7",
    "href": "dl_lec5.html#activation-functions-7",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: visualizing\n\n\nFirst, a helper function:\n\ndef get_grads(act_fn, x):\n    \"\"\"\n    Computes the gradients of an activation function at specified positions.\n    \n    Inputs:\n        act_fn - An object of the class \"ActivationFunction\" with an implemented forward pass.\n        x - 1D input tensor. \n    Output:\n        A tensor with the same size of x containing the gradients of act_fn at x.\n    \"\"\"\n    x = x.clone().requires_grad_() # Mark the input as tensor for which we want to store gradients\n    out = act_fn(x)\n    out.sum().backward() # Summing results in an equal gradient flow to each element in x\n    return x.grad # Accessing the gradients of x by \"x.grad\""
  },
  {
    "objectID": "dl_lec5.html#activation-functions-8",
    "href": "dl_lec5.html#activation-functions-8",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: visualizing\n\n\n\ndef vis_act_fn(act_fn, ax, x):\n    # Run activation function\n    y = act_fn(x)\n    y_grads = get_grads(act_fn, x)\n    # Push x, y and gradients back to cpu for plotting\n    x, y, y_grads = x.cpu().numpy(), y.cpu().numpy(), y_grads.cpu().numpy()\n    ## Plotting\n    ax.plot(x, y, linewidth=2, label=\"ActFn\")\n    ax.plot(x, y_grads, linewidth=2, label=\"Gradient\")\n    ax.set_title(act_fn.name)\n    ax.legend()\n    ax.set_ylim(-1.5, x.max())\n\n# Add activation functions if wanted\nact_fns = [act_fn() for act_fn in act_fn_by_name.values()]\nx = torch.linspace(-5, 5, 1000) # Range on which we want to visualize the activation functions\n## Plotting\nrows = math.ceil(len(act_fns)/2.0)\nfig, ax = plt.subplots(rows, 2, figsize=(8, rows*4))\nfor i, act_fn in enumerate(act_fns):\n    vis_act_fn(act_fn, ax[divmod(i,2)], x)\nfig.subplots_adjust(hspace=0.3)\nplt.show()"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-9",
    "href": "dl_lec5.html#activation-functions-9",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: analyzing effect\n\n\nModel:\n\nclass BaseNetwork(nn.Module):\n    \n    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n        \"\"\"\n        Inputs:\n            act_fn - Object of the activation function that should be used as non-linearity in the network.\n            input_size - Size of the input images in pixels\n            num_classes - Number of classes we want to predict\n            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n        \"\"\"\n        super().__init__()\n        \n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [input_size] + hidden_sizes\n        for layer_index in range(1, len(layer_sizes)):\n            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n                       act_fn]\n        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n        self.layers = nn.Sequential(*layers) # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        \n        # We store all hyperparameters in a dictionary for saving and loading of the model\n        self.config = {\"act_fn\": act_fn.config, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes} \n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1) # Reshape images to a flat vector\n        out = self.layers(x)\n        return out"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-10",
    "href": "dl_lec5.html#activation-functions-10",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: analyzing effect\n\n\nSave/load:\n\nimport json\n\ndef _get_config_file(model_path, model_name):\n    # Name of the file for storing hyperparameter details\n    return os.path.join(model_path, model_name + \".config\")\n\ndef _get_model_file(model_path, model_name):\n    # Name of the file for storing network parameters\n    return os.path.join(model_path, model_name + \".tar\")\n\ndef load_model(model_path, model_name, net=None):\n    \"\"\"\n    Loads a saved model from disk.\n    \n    Inputs:\n        model_path - Path of the checkpoint directory\n        model_name - Name of the model (str)\n        net - (Optional) If given, the state dict is loaded into this model. Otherwise, a new model is created.\n    \"\"\"\n    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n    with open(config_file, \"r\") as f:\n        config_dict = json.load(f)\n    if net is None:\n        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n        act_fn = act_fn_by_name[act_fn_name](**config_dict.pop(\"act_fn\"))\n        net = BaseNetwork(act_fn=act_fn, **config_dict)\n    net.load_state_dict(torch.load(model_file, map_location=device))\n    return net\n    \ndef save_model(model, model_path, model_name):\n    \"\"\"\n    Given a model, we save the state_dict and hyperparameters.\n    \n    Inputs:\n        model - Network object to save parameters from\n        model_path - Path of the checkpoint directory\n        model_name - Name of the model (str)\n    \"\"\"\n    config_dict = model.config\n    os.makedirs(model_path, exist_ok=True)\n    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n    with open(config_file, \"w\") as f:\n        json.dump(config_dict, f)\n    torch.save(model.state_dict(), model_file)"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-11",
    "href": "dl_lec5.html#activation-functions-11",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: analyzing effect\n\n\nDataset:\n\nimport torchvision\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\n\n# Transformations applied on each image =&gt; first make them a tensor, then normalize them in the range -1 to 1\ntransform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Normalize((0.5,), (0.5,))])\n\n# Loading the training dataset. We need to split it into a training and validation part\ntrain_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\ntrain_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n\n# Loading the test set\ntest_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\n# Note that for actually training a model, we will use different data loaders\n# with a lower batch size.\ntrain_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\nval_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\ntest_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-12",
    "href": "dl_lec5.html#activation-functions-12",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nActivation functions: analyzing effect\n\n\nDataset samples:\n\nexmp_imgs = [train_set[i][0] for i in range(16)]\n# Organize the images into a grid for nicer visualization\nimg_grid = torchvision.utils.make_grid(torch.stack(exmp_imgs, dim=0), nrow=4, normalize=True, pad_value=0.5)\nimg_grid = img_grid.permute(1, 2, 0)\n\nplt.figure(figsize=(8,8))\nplt.title(\"FashionMNIST examples\")\nplt.imshow(img_grid)\nplt.axis('off')\nplt.show()\nplt.close()"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-13",
    "href": "dl_lec5.html#activation-functions-13",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nVisualizing the gradient flow after initialization\n\n\nTo get a feeling of how every activation function influences the gradients, we can look at a freshly initialized network and measure the gradients for each parameter for a batch of 256 images:\n\ndef visualize_gradients(net, color=\"C0\"):\n    \"\"\"\n    Inputs:\n        net - Object of class BaseNetwork\n        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n    \"\"\"\n    net.eval()\n    small_loader = data.DataLoader(train_set, batch_size=256, shuffle=False)\n    imgs, labels = next(iter(small_loader))\n    imgs, labels = imgs.to(device), labels.to(device)\n    \n    # Pass one batch through the network, and calculate the gradients for the weights\n    net.zero_grad()\n    preds = net(imgs)\n    loss = F.cross_entropy(preds, labels)\n    loss.backward()\n    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n    grads = {name: params.grad.data.view(-1).cpu().clone().numpy() for name, params in net.named_parameters() if \"weight\" in name}\n    net.zero_grad()\n    \n    ## Plotting\n    columns = len(grads)\n    fig, ax = plt.subplots(1, columns, figsize=(columns*3.5, 2.5))\n    fig_index = 0\n    for key in grads:\n        key_ax = ax[fig_index%columns]\n        sns.histplot(data=grads[key], bins=30, ax=key_ax, color=color, kde=True)\n        key_ax.set_title(str(key))\n        key_ax.set_xlabel(\"Grad magnitude\")\n        fig_index += 1\n    fig.suptitle(f\"Gradient magnitude distribution for activation function {net.config['act_fn']['name']}\", fontsize=14, y=1.05)\n    fig.subplots_adjust(wspace=0.45)\n    plt.show()\n    plt.close()"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-14",
    "href": "dl_lec5.html#activation-functions-14",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nVisualizing the gradient flow after initialization\n\n\nResult:\n\n# Seaborn prints warnings if histogram has small values. We can ignore them for now\nimport warnings\nwarnings.filterwarnings('ignore')\n## Create a plot for every activation function\nfor i, act_fn_name in enumerate(act_fn_by_name):\n    set_seed(42) # Setting the seed ensures that we have the same weight initialization for each activation function\n    act_fn = act_fn_by_name[act_fn_name]()\n    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n    visualize_gradients(net_actfn, color=f\"C{i}\")"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-15",
    "href": "dl_lec5.html#activation-functions-15",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nTraining\n\n\n\nDefinitionExecution\n\n\n\ndef train_model(net, model_name, max_epochs=50, patience=7, batch_size=256, overwrite=False):\n    \"\"\"\n    Train a model on the training set of FashionMNIST\n    \n    Inputs:\n        net - Object of BaseNetwork\n        model_name - (str) Name of the model, used for creating the checkpoint names\n        max_epochs - Number of epochs we want to (maximally) train for\n        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n        batch_size - Size of batches used in training\n        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n    \"\"\"\n    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n    if file_exists and not overwrite:\n        print(\"Model file already exists. Skipping training...\")\n    else:\n        if file_exists:\n            print(\"Model file exists, but will be overwritten...\")\n            \n        # Defining optimizer, loss and data loader\n        optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # Default parameters, feel free to change\n        loss_module = nn.CrossEntropyLoss() \n        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n\n        val_scores = []\n        best_val_epoch = -1\n        for epoch in range(max_epochs):\n            ############\n            # Training #\n            ############\n            net.train()\n            true_preds, count = 0., 0\n            for imgs, labels in tqdm(train_loader_local, desc=f\"Epoch {epoch+1}\", leave=False):\n                imgs, labels = imgs.to(device), labels.to(device) # To GPU\n                optimizer.zero_grad() # Zero-grad can be placed anywhere before \"loss.backward()\"\n                preds = net(imgs)\n                loss = loss_module(preds, labels)\n                loss.backward()\n                optimizer.step()\n                # Record statistics during training\n                true_preds += (preds.argmax(dim=-1) == labels).sum()\n                count += labels.shape[0]\n            train_acc = true_preds / count\n\n            ##############\n            # Validation #\n            ##############\n            val_acc = test_model(net, val_loader)\n            val_scores.append(val_acc)\n            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n\n            if len(val_scores) == 1 or val_acc &gt; val_scores[best_val_epoch]:\n                print(\"\\t   (New best performance, saving model...)\")\n                save_model(net, CHECKPOINT_PATH, model_name)\n                best_val_epoch = epoch\n            elif best_val_epoch &lt;= epoch - patience:\n                print(f\"Early stopping due to no improvement over the last {patience} epochs\")\n                break\n\n        # Plot a curve of the validation accuracy\n        plt.plot([i for i in range(1,len(val_scores)+1)], val_scores)\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Validation accuracy\")\n        plt.title(f\"Validation performance of {model_name}\")\n        plt.show()\n        plt.close()\n    \n    load_model(CHECKPOINT_PATH, model_name, net=net)\n    test_acc = test_model(net, test_loader)\n    print((f\" Test accuracy: {test_acc*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n    return test_acc\n    \n\ndef test_model(net, data_loader):\n    \"\"\"\n    Test a model on a specified dataset.\n    \n    Inputs:\n        net - Trained model of type BaseNetwork\n        data_loader - DataLoader object of the dataset to test on (validation or test)\n    \"\"\"\n    net.eval()\n    true_preds, count = 0., 0\n    for imgs, labels in data_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.no_grad():\n            preds = net(imgs).argmax(dim=-1)\n            true_preds += (preds == labels).sum().item()\n            count += labels.shape[0]\n    test_acc = true_preds / count\n    return test_acc\n\n\n\n\nfor act_fn_name in act_fn_by_name:\n    print(f\"Training BaseNetwork with {act_fn_name} activation...\")\n    set_seed(42)\n    act_fn = act_fn_by_name[act_fn_name]()\n    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n    train_model(net_actfn, f\"FashionMNIST_{act_fn_name}\", overwrite=False)\n\nTraining BaseNetwork with sigmoid activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 10.00% ==============\n\nTraining BaseNetwork with tanh activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 87.59% ==============\n\nTraining BaseNetwork with relu activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 88.62% ==============\n\nTraining BaseNetwork with leakyrelu activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 88.92% ==============\n\nTraining BaseNetwork with elu activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 87.27% ==============\n\nTraining BaseNetwork with swish activation...\nModel file already exists. Skipping training...\n============= Test accuracy: 88.73% =============="
  },
  {
    "objectID": "dl_lec5.html#activation-functions-16",
    "href": "dl_lec5.html#activation-functions-16",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nVisualizing distribution\n\n\n\nDefinitionFigures\n\n\n\ndef visualize_activations(net, color=\"C0\"):\n    activations = {}\n    \n    net.eval()\n    small_loader = data.DataLoader(train_set, batch_size=1024)\n    imgs, labels = next(iter(small_loader))\n    with torch.no_grad():\n        layer_index = 0\n        imgs = imgs.to(device)\n        imgs = imgs.view(imgs.size(0), -1)\n        # We need to manually loop through the layers to save all activations\n        for layer_index, layer in enumerate(net.layers[:-1]):\n            imgs = layer(imgs)\n            activations[layer_index] = imgs.view(-1).cpu().numpy()\n    \n    ## Plotting\n    columns = 4\n    rows = math.ceil(len(activations)/columns)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns*2.7, rows*2.5))\n    fig_index = 0\n    for key in activations:\n        key_ax = ax[fig_index//columns][fig_index%columns]\n        sns.histplot(data=activations[key], bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n        key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(f\"Activation distribution for activation function {net.config['act_fn']['name']}\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    plt.show()\n    plt.close()\n\n\n\n\nfor i, act_fn_name in enumerate(act_fn_by_name):\n    net_actfn = load_model(model_path=CHECKPOINT_PATH, model_name=f\"FashionMNIST_{act_fn_name}\").to(device)\n    visualize_activations(net_actfn, color=f\"C{i}\")"
  },
  {
    "objectID": "dl_lec5.html#activation-functions-17",
    "href": "dl_lec5.html#activation-functions-17",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\n\n\n\nDead neurons\n\n\nOne known drawback of the ReLU activation is the occurrence of “dead neurons”, i.e. neurons with no gradient for any training input.\nFor dead neurons to happen, the output value of a specific neuron of the linear layer before the ReLU has to be negative for all input images."
  },
  {
    "objectID": "dl_lec5.html#activation-functions-18",
    "href": "dl_lec5.html#activation-functions-18",
    "title": "Intro to PyTorch",
    "section": "Activation functions",
    "text": "Activation functions\n\nDefinitionUntrainedTrainedInc. layer depth\n\n\n\ndef measure_number_dead_neurons(net):\n\n    # For each neuron, we create a boolean variable initially set to 1. If it has an activation unequals 0 at any time,\n    # we set this variable to 0. After running through the whole training set, only dead neurons will have a 1.\n    neurons_dead = [\n        torch.ones(layer.weight.shape[0], device=device, dtype=torch.bool) for layer in net.layers[:-1] if isinstance(layer, nn.Linear)\n    ] # Same shapes as hidden size in BaseNetwork\n\n    net.eval()\n    with torch.no_grad():\n        for imgs, labels in tqdm(train_loader, leave=False): # Run through whole training set\n            layer_index = 0\n            imgs = imgs.to(device)\n            imgs = imgs.view(imgs.size(0), -1)\n            for layer in net.layers[:-1]:\n                imgs = layer(imgs)\n                if isinstance(layer, ActivationFunction):\n                    # Are all activations == 0 in the batch, and we did not record the opposite in the last batches?\n                    neurons_dead[layer_index] = torch.logical_and(neurons_dead[layer_index], (imgs == 0).all(dim=0))\n                    layer_index += 1\n    number_neurons_dead = [t.sum().item() for t in neurons_dead]\n    print(\"Number of dead neurons:\", number_neurons_dead)\n    print(\"In percentage:\", \", \".join([f\"{(100.0 * num_dead / tens.shape[0]):4.2f}%\" for tens, num_dead in zip(neurons_dead, number_neurons_dead)]))\n\n\n\n\nset_seed(42)\nnet_relu = BaseNetwork(act_fn=ReLU()).to(device)\nmeasure_number_dead_neurons(net_relu)\n\n\n\n\nNumber of dead neurons: [0, 0, 3, 10]\nIn percentage: 0.00%, 0.00%, 1.17%, 7.81%\n\n\n\n\n\nnet_relu = load_model(model_path=CHECKPOINT_PATH, model_name=\"FashionMNIST_relu\").to(device)\nmeasure_number_dead_neurons(net_relu)\n\n\n\n\nNumber of dead neurons: [0, 0, 0, 3]\nIn percentage: 0.00%, 0.00%, 0.00%, 2.34%\n\n\n\n\n\nset_seed(42)\nnet_relu = BaseNetwork(act_fn=ReLU(), hidden_sizes=[256, 256, 256, 256, 256, 128, 128, 128, 128, 128]).to(device)\nmeasure_number_dead_neurons(net_relu)\n\n\n\n\nNumber of dead neurons: [0, 0, 7, 27, 89, 60, 58, 61, 72, 56]\nIn percentage: 0.00%, 0.00%, 2.73%, 10.55%, 34.77%, 46.88%, 45.31%, 47.66%, 56.25%, 43.75%"
  },
  {
    "objectID": "dl_lab5.html",
    "href": "dl_lab5.html",
    "title": "DL: Lab 5",
    "section": "",
    "text": "Lab overview\nImplement various optimization approaches for a deep neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "nlp_lab12.html",
    "href": "nlp_lab12.html",
    "title": "NLP: Lab 12 (LSH)",
    "section": "",
    "text": "Locality-sensitive hashing\nUse attached notebook (lsh.ipynb)."
  },
  {
    "objectID": "dl_lec6.html#overview",
    "href": "dl_lec6.html#overview",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Overview",
    "text": "Overview\n\n\n\nChallenges\n\n\n\nensuring stable gradient flow\navoiding exploding/vanishing gradients\n\n\n\n\n\n\n\nPlan\n\n\n\nInitialization: review different techniques, go from simple to complex ones\n\nconstant/Gaussian\nXavier/Kaiming\n\nOptimization:\n\nStochastic Gradient Descent (SGD)\nSGD with Momentum\nAdam"
  },
  {
    "objectID": "dl_lec6.html#imports",
    "href": "dl_lec6.html#imports",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Imports",
    "text": "Imports\n\n## Standard libraries\nimport os\nimport json\nimport math\nimport numpy as np \nimport copy\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline \nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nimport seaborn as sns\nsns.set()\n\n## Progress bar\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim"
  },
  {
    "objectID": "dl_lec6.html#preparation",
    "href": "dl_lec6.html#preparation",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Preparation",
    "text": "Preparation\n\n# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\nDATASET_PATH = \"../data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial4\"\n\n# Function for setting the seed\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.mps.is_available():\n        torch.mps.manual_seed(seed)\n        #torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.mps.deterministic = True\ntorch.backends.mps.benchmark = False\n\n# Fetching the device that will be used throughout this notebook\ndevice = torch.device(\"cpu\") if not torch.mps.is_available() else torch.device(\"mps:0\")\nprint(\"Using device\", device)\n\nUsing device mps:0"
  },
  {
    "objectID": "dl_lec6.html#download",
    "href": "dl_lec6.html#download",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Download",
    "text": "Download\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/\"\n# Files to download\npretrained_files = [\"FashionMNIST_SGD.config\",    \"FashionMNIST_SGD_results.json\",    \"FashionMNIST_SGD.tar\", \n                    \"FashionMNIST_SGDMom.config\", \"FashionMNIST_SGDMom_results.json\", \"FashionMNIST_SGDMom.tar\", \n                    \"FashionMNIST_Adam.config\",   \"FashionMNIST_Adam_results.json\",   \"FashionMNIST_Adam.tar\"   ]\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
  },
  {
    "objectID": "dl_lec6.html#preparation-1",
    "href": "dl_lec6.html#preparation-1",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Preparation",
    "text": "Preparation\n\n\n\nPreload FashionMNIST\n\n\n\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\n\n# Transformations applied on each image =&gt; first make them a tensor, then normalize them with mean 0 and std 1\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.2861,), (0.3530,))\n                               ])\n\n# Loading the training dataset. We need to split it into a training and validation part\ntrain_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\ntrain_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n\n# Loading the test set\ntest_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\n# Note that for actually training a model, we will use different data loaders\n# with a lower batch size.\ntrain_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\nval_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\ntest_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
  },
  {
    "objectID": "dl_lec6.html#transformation",
    "href": "dl_lec6.html#transformation",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Transformation",
    "text": "Transformation\n\n\n\n\n\n\nNote\n\n\nWe have changed the parameters of the normalization transformation transforms.Normalize:\n\nthe normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels\n\nWe can calculate the normalization parameters by determining the mean and standard deviation on the original images:\n\nprint(\"Mean\", (train_dataset.data.float() / 255.0).mean().item())\nprint(\"Std\", (train_dataset.data.float() / 255.0).std().item())\n\nMean 0.2860405743122101\nStd 0.3530242443084717"
  },
  {
    "objectID": "dl_lec6.html#transformation-1",
    "href": "dl_lec6.html#transformation-1",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Transformation",
    "text": "Transformation\n\n\n\nVerification\n\n\n\nimgs, _ = next(iter(train_loader))\nprint(f\"Mean: {imgs.mean().item():5.3f}\")\nprint(f\"Standard deviation: {imgs.std().item():5.3f}\")\nprint(f\"Maximum: {imgs.max().item():5.3f}\")\nprint(f\"Minimum: {imgs.min().item():5.3f}\")\n\nMean: 0.020\nStandard deviation: 1.011\nMaximum: 2.022\nMinimum: -0.810\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNote that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST."
  },
  {
    "objectID": "dl_lec6.html#linear-network",
    "href": "dl_lec6.html#linear-network",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Linear network",
    "text": "Linear network\n\nclass BaseNetwork(nn.Module):\n    \n    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n        \"\"\"\n        Inputs:\n            act_fn - Object of the activation function that should be used as non-linearity in the network.\n            input_size - Size of the input images in pixels\n            num_classes - Number of classes we want to predict\n            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n        \"\"\"\n        super().__init__()\n        \n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [input_size] + hidden_sizes\n        for layer_index in range(1, len(layer_sizes)):\n            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n                       act_fn]\n        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n        self.layers = nn.ModuleList(layers) # A module list registers a list of modules as submodules (e.g. for parameters)\n        \n        self.config = {\"act_fn\": act_fn.__class__.__name__, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes} \n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        for l in self.layers:\n            x = l(x)\n        return x"
  },
  {
    "objectID": "dl_lec6.html#activation",
    "href": "dl_lec6.html#activation",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Activation",
    "text": "Activation\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n    \nact_fn_by_name = {\n    \"tanh\": nn.Tanh,\n    \"relu\": nn.ReLU,\n    \"identity\": Identity\n}\n\n\n\nWe use the Identity function for simplicity when discussing initialization."
  },
  {
    "objectID": "dl_lec6.html#plotting",
    "href": "dl_lec6.html#plotting",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Plotting",
    "text": "Plotting\n\nFunctionsPlotsWeightsGradientsActivations\n\n\nThese functions help us to:\n\nvisualize the weight/parameter distribution inside a network\nvisualize the gradients that the parameters at different layers receive\nvisualize the activations, i.e. the output of the linear layers.\n\n\n\n\ndef plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n    columns = len(val_dict)\n    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n    fig_index = 0\n    for key in sorted(val_dict.keys()):\n        key_ax = ax[fig_index%columns]\n        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())&gt;1e-8)) # Only plot kde if there is variance\n        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)&gt;1 else \"\"))\n        if xlabel is not None:\n            key_ax.set_xlabel(xlabel)\n        fig_index += 1\n    fig.subplots_adjust(wspace=0.4)\n    return fig\n\n\n\n\ndef visualize_weight_distribution(model, color=\"C0\"):\n    weights = {}\n    for name, param in model.named_parameters():\n        if name.endswith(\".bias\"):\n            continue\n        key_name = f\"Layer {name.split('.')[1]}\"\n        weights[key_name] = param.detach().view(-1).cpu().numpy()\n    \n    ## Plotting\n    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n    plt.show()\n    plt.close() \n\n\n\n\ndef visualize_gradients(model, color=\"C0\", print_variance=False):\n    \"\"\"\n    Inputs:\n        net - Object of class BaseNetwork\n        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n    \"\"\"\n    model.eval()\n    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n    imgs, labels = next(iter(small_loader))\n    imgs, labels = imgs.to(device), labels.to(device)\n    \n    # Pass one batch through the network, and calculate the gradients for the weights\n    model.zero_grad()\n    preds = model(imgs)\n    loss = F.cross_entropy(preds, labels) # Same as nn.CrossEntropyLoss, but as a function instead of module\n    loss.backward()\n    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n    grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n    model.zero_grad()\n    \n    ## Plotting\n    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n    plt.show()\n    plt.close() \n    \n    if print_variance:\n        for key in sorted(grads.keys()):\n            print(f\"{key} - Variance: {np.var(grads[key])}\")\n\n\n\n\ndef visualize_activations(model, color=\"C0\", print_variance=False):\n    model.eval()\n    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n    imgs, labels = next(iter(small_loader))\n    imgs, labels = imgs.to(device), labels.to(device)\n    \n    # Pass one batch through the network, and calculate the gradients for the weights\n    feats = imgs.view(imgs.shape[0], -1)\n    activations = {}\n    with torch.no_grad():\n        for layer_index, layer in enumerate(model.layers):\n            feats = layer(feats)\n            if isinstance(layer, nn.Linear):\n                activations[f\"Layer {layer_index}\"] = feats.view(-1).detach().cpu().numpy()\n    \n    ## Plotting\n    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n    plt.show()\n    plt.close() \n    \n    if print_variance:\n        for key in sorted(activations.keys()):\n            print(f\"{key} - Variance: {np.var(activations[key])}\")"
  },
  {
    "objectID": "dl_lec6.html#initialization",
    "href": "dl_lec6.html#initialization",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nProperties\n\n\n\nThe variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons.\n\nIf the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value.\nSimilarly, if the variance increases, it is likely to explode (i.e. head to infinity) the deeper we design our model.\n\nGradient distribution should have equal variance across layers. If the first layer receives much smaller gradients than the last layer, choosing an appropriate learning rate will be difficult."
  },
  {
    "objectID": "dl_lec6.html#initialization-1",
    "href": "dl_lec6.html#initialization-1",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\n\n\n\nImportant\n\n\nInitializations depend on the specific activation function used in the network.\n\n\n\n\nmodel = BaseNetwork(act_fn=Identity()).to(device)"
  },
  {
    "objectID": "dl_lec6.html#initialization-2",
    "href": "dl_lec6.html#initialization-2",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nConstant initialization\n\n\n\ndef const_init(model, c=0.0):\n    for name, param in model.named_parameters():\n        param.data.fill_(c)\n\nconst_init(model, c=0.005)\nvisualize_gradients(model)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 2.0582754611968994\nLayer 2 - Variance: 13.489115715026855\nLayer 4 - Variance: 22.100555419921875\nLayer 6 - Variance: 36.209537506103516\nLayer 8 - Variance: 14.831426620483398"
  },
  {
    "objectID": "dl_lec6.html#initialization-3",
    "href": "dl_lec6.html#initialization-3",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nConstant initialization: analysis\n\n\n\nonly the first and the last layer have diverse gradient distributions\nthe other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it).\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nhaving the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters.\nthis would make our layer useless and effectively reduce number of parameters to 1."
  },
  {
    "objectID": "dl_lec6.html#initialization-4",
    "href": "dl_lec6.html#initialization-4",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nConstant variance\n\n\n\ndef var_init(model, std=0.01):\n    for name, param in model.named_parameters():\n        param.data.normal_(std=std)\n        \nvar_init(model, std=0.01)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 0.07771595567464828\nLayer 2 - Variance: 0.00406030984595418\nLayer 4 - Variance: 0.00020879440126009285\nLayer 6 - Variance: 9.545485954731703e-05\nLayer 8 - Variance: 3.9654176362091675e-05\n\n\n\n\n\n\n\nThe variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer."
  },
  {
    "objectID": "dl_lec6.html#initialization-5",
    "href": "dl_lec6.html#initialization-5",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nConstant variance: higher std\n\n\n\nvar_init(model, std=0.1)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 7.925612926483154\nLayer 2 - Variance: 43.7206916809082\nLayer 4 - Variance: 113.83338928222656\nLayer 6 - Variance: 312.2013244628906\nLayer 8 - Variance: 347.9194641113281\n\n\n\n\n\n\n\nWith a higher standard deviation, the activations are likely to explode."
  },
  {
    "objectID": "dl_lec6.html#initialization-6",
    "href": "dl_lec6.html#initialization-6",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nHow to find appropriate initialization values\n\n\nTwo requirements:\n\nThe mean of the activations should be zero\nThe variance of the activations should stay the same across every layer\n\n\n\n\n\n\n\nProcedure\n\n\nSuppose we want to design an initialization for the following layer: \\[\ny=Wx+b, \\; y\\in\\mathbb{R}^{d_y}, \\; x\\in\\mathbb{R}^{d_x}\n\\] Goal: \\[\n\\text{Var}(y_i)=\\text{Var}(x_i)=\\sigma_x^{2},\\\\\nmean(y_i) = 0\n\\]"
  },
  {
    "objectID": "dl_lec6.html#initialization-7",
    "href": "dl_lec6.html#initialization-7",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nProcedure\n\n\nWe assume \\(x\\) to also have a mean of zero, because, in deep neural networks, \\(y\\) would be the input of another layer. This requires the bias and weight to have an expectation of 0. Actually, as \\(b\\) is a single element per output neuron and is constant across different inputs, we set it to 0 overall.\nNext, we need to calculate the variance with which we need to initialize the weight parameters. Along the calculation, we will need the following variance rule: given two independent variables, the variance of their product is \\[\n\\text{Var}(X\\cdot Y) = \\mathbb{E}(Y)^2\\text{Var}(X) + \\mathbb{E}(X)^2\\text{Var}(Y) + \\text{Var}(X)\\text{Var}(Y) = \\\\\n= \\mathbb{E}(Y^2)\\mathbb{E}(X^2)-\\mathbb{E}(Y)^2\\mathbb{E}(X)^2\n\\]\n(\\(X\\) and \\(Y\\) are not refering to \\(x\\) and \\(y\\), but any random variable)."
  },
  {
    "objectID": "dl_lec6.html#initialization-8",
    "href": "dl_lec6.html#initialization-8",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nComputation\n\n\n\n\\[\n\\begin{split}\n    &y_i  = \\sum_{j} w_{ij}x_{j}\\hspace{10mm}\\text{Calculation of a single output neuron without bias}\\\\\n    &\\text{Var}(y_i) = \\sigma_x^{2}   \\\\\n    &= \\text{Var}\\left(\\sum_{j} w_{ij}x_{j}\\right)\\\\\n    & = \\sum_{j} \\text{Var}(w_{ij}x_{j}) \\hspace{10mm}\\text{Inputs and weights are independent of each other}\\\\\n    & = \\sum_{j} \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance rule (see above) with expectations being zero}\\\\\n    & = d_x \\cdot \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance equal for all $d_x$ elements}\\\\\n    & = \\sigma_x^{2} \\cdot d_x \\cdot \\text{Var}(w_{ij})\\\\\n    &\\Rightarrow \\text{Var}(w_{ij}) = \\sigma_{W}^2  = \\frac{1}{d_x}\n\\end{split}\n\\]"
  },
  {
    "objectID": "dl_lec6.html#initialization-9",
    "href": "dl_lec6.html#initialization-9",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\nThus, we should initialize the weight distribution with a variance of the inverse of the input dimension \\(d_x\\).\n\ndef equal_var_init(model):\n    for name, param in model.named_parameters():\n        if name.endswith(\".bias\"):\n            param.data.fill_(0)\n        else:\n            param.data.normal_(std=1.0/math.sqrt(param.shape[1]))\n        \nequal_var_init(model)\nvisualize_weight_distribution(model)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 0.9882476329803467\nLayer 2 - Variance: 0.9890449047088623\nLayer 4 - Variance: 1.0196830034255981\nLayer 6 - Variance: 1.0159821510314941\nLayer 8 - Variance: 0.7536574602127075"
  },
  {
    "objectID": "dl_lec6.html#initialization-10",
    "href": "dl_lec6.html#initialization-10",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\n\n\n\nNote\n\n\nOur initialization does not restrict us to a normal distribution, but allows any other distribution with:\n\na mean of 0\nand variance of \\(1/d_x\\).\n\nYou often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights."
  },
  {
    "objectID": "dl_lec6.html#initialization-11",
    "href": "dl_lec6.html#initialization-11",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nStabilization of gradient variance: Xavier initialization\n\n\nEnsures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from \\(\\Delta x=W\\Delta y\\), and come to the conclusion that we should initialize our layers with \\(1/d_y\\) where \\(d_y\\) is the number of output neurons. As a compromise between both constraints, Glorot and Bengio (2010) proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:\n\\[W\\sim \\mathcal{N}\\left(0,\\frac{2}{d_x+d_y}\\right)\\]\nIf we use a uniform distribution, we would initialize the weights with:\n\\[W\\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}, \\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}\\right]\\]"
  },
  {
    "objectID": "dl_lec6.html#initialization-12",
    "href": "dl_lec6.html#initialization-12",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\ndef xavier_init(model):\n    for name, param in model.named_parameters():\n        if name.endswith(\".bias\"):\n            param.data.fill_(0)\n        else:\n            bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1])\n            param.data.uniform_(-bound, bound)\n        \nxavier_init(model)\nvisualize_gradients(model, print_variance=True)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\nlayers.0.weight - Variance: 0.0005975761450827122\nlayers.2.weight - Variance: 0.0010738210985437036\nlayers.4.weight - Variance: 0.0013981545343995094\nlayers.6.weight - Variance: 0.0021510443184524775\nlayers.8.weight - Variance: 0.020080894231796265\n\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 1.2443994283676147\nLayer 2 - Variance: 1.6894475221633911\nLayer 4 - Variance: 1.7133890390396118\nLayer 6 - Variance: 2.432518482208252\nLayer 8 - Variance: 4.493688106536865"
  },
  {
    "objectID": "dl_lec6.html#initialization-13",
    "href": "dl_lec6.html#initialization-13",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nXavier - analysis\n\n\n\nXavier initialization balances the variance of gradients and activations\nthat the significantly higher variance for the output layer is due to the large difference of input and output dimension (\\(128\\) vs \\(10\\))"
  },
  {
    "objectID": "dl_lec6.html#initialization-14",
    "href": "dl_lec6.html#initialization-14",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nNon-linearity\n\n\nIn a \\(\\tanh\\)-based network, a common assumption is that for small values during the initial steps in training, the \\(\\tanh\\) works as a linear function such that we don’t have to adjust our calculation. We can check if that is the case for us as well:\n\n\n\n\nmodel = BaseNetwork(act_fn=nn.Tanh()).to(device)\nxavier_init(model)\nvisualize_gradients(model, print_variance=True)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\nlayers.0.weight - Variance: 1.6712088836356997e-05\nlayers.2.weight - Variance: 2.9089944291627035e-05\nlayers.4.weight - Variance: 3.600309355533682e-05\nlayers.6.weight - Variance: 4.9582638894207776e-05\nlayers.8.weight - Variance: 0.00044954405166208744\n\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 1.2616938352584839\nLayer 2 - Variance: 0.544843316078186\nLayer 4 - Variance: 0.2753238379955292\nLayer 6 - Variance: 0.26287850737571716\nLayer 8 - Variance: 0.2619311511516571"
  },
  {
    "objectID": "dl_lec6.html#initialization-15",
    "href": "dl_lec6.html#initialization-15",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nAnalysis for tanh\n\n\n\nAlthough the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values.\nTherefore, our variance will stabilize around 0.25 if we would go even deeper.\nHence, we can conclude that the Xavier initialization works well for Tanh networks."
  },
  {
    "objectID": "dl_lec6.html#initialization-16",
    "href": "dl_lec6.html#initialization-16",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nReLU\n\n\nThe ReLU activation function sets (in expectation) half of the inputs to 0 so that also the expectation of the input is not zero. However, as long as the expectation of \\(W\\) is zero and \\(b=0\\), the expectation of the output is zero.\nThe part where the calculation of the ReLU initialization differs from the identity is when determining \\(\\text{Var}(w_{ij}x_{j})\\):\n\\[\n\\text{Var}(w_{ij}x_{j})=\\underbrace{\\mathbb{E}[w_{ij}^2]}_{=\\text{Var}(w_{ij})}\\mathbb{E}[x_{j}^2]-\\underbrace{\\mathbb{E}[w_{ij}]^2}_{=0}\\mathbb{E}[x_{j}]^2=\\text{Var}(w_{ij})\\mathbb{E}[x_{j}^2]\n\\]"
  },
  {
    "objectID": "dl_lec6.html#initialization-17",
    "href": "dl_lec6.html#initialization-17",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nReLU: expectation calculation\n\n\nIf we assume now that \\(x\\) is the output of a ReLU activation (from a previous layer, \\(x=max(0,\\tilde{y})\\)), we can calculate the expectation as follows:\n\\[\n\\begin{split}\n    \\mathbb{E}[x^2] & =\\mathbb{E}[\\max(0,\\tilde{y})^2]\\\\\n                    & =\\frac{1}{2}\\mathbb{E}[{\\tilde{y}}^2]\\hspace{2cm}\\tilde{y}\\text{ is zero-centered and symmetric}\\\\\n                    & =\\frac{1}{2}\\text{Var}(\\tilde{y})\n\\end{split}\n\\]\nThus, we see that we have an additional factor of 1/2 in the equation, so that our desired weight variance becomes \\(2/d_x\\)."
  },
  {
    "objectID": "dl_lec6.html#initialization-18",
    "href": "dl_lec6.html#initialization-18",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\n\n\n\nNote\n\n\nThe Kaiming initialization does not use the harmonic mean between input and output size. In the original paper authors argue that using \\(d_x\\) or \\(d_y\\) both lead to stable gradients throughout the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input \\(d_x\\):\n\n\n\n\ndef kaiming_init(model):\n    for name, param in model.named_parameters():\n        if name.endswith(\".bias\"):\n            param.data.fill_(0)\n        elif name.startswith(\"layers.0\"): # The first layer does not have ReLU applied on its input\n            param.data.normal_(0, 1/math.sqrt(param.shape[1]))\n        else:\n            param.data.normal_(0, math.sqrt(2)/math.sqrt(param.shape[1]))\n\nmodel = BaseNetwork(act_fn=nn.ReLU()).to(device)\nkaiming_init(model)\nvisualize_gradients(model, print_variance=True)\nvisualize_activations(model, print_variance=True)\n\n\n\n\n\n\n\n\nlayers.0.weight - Variance: 5.593948662863113e-05\nlayers.2.weight - Variance: 8.165080362232402e-05\nlayers.4.weight - Variance: 9.53530689002946e-05\nlayers.6.weight - Variance: 0.0002848389558494091\nlayers.8.weight - Variance: 0.003961425274610519\n\n\n\n\n\n\n\n\n\nLayer 0 - Variance: 1.0332646369934082\nLayer 2 - Variance: 1.0350595712661743\nLayer 4 - Variance: 1.0465645790100098\nLayer 6 - Variance: 1.0822820663452148\nLayer 8 - Variance: 0.925536036491394"
  },
  {
    "objectID": "dl_lec6.html#initialization-19",
    "href": "dl_lec6.html#initialization-19",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization\n\n\n\nKaiming - analysis\n\n\n\nThe variance stays stable across layers.\nNote that for Leaky-ReLU etc., we have to slightly adjust the factor of \\(2\\) in the variance as half of the values are not set to zero anymore.\nPyTorch provides a function to calculate this factor for many activation function, see torch.nn.init.calculate_gain (link)."
  },
  {
    "objectID": "dl_lec6.html#initialization-20",
    "href": "dl_lec6.html#initialization-20",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Initialization",
    "text": "Initialization"
  },
  {
    "objectID": "dl_lec6.html#optimization",
    "href": "dl_lec6.html#optimization",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nPreparation\n\n\n\ndef _get_config_file(model_path, model_name):\n    return os.path.join(model_path, model_name + \".config\")\n\ndef _get_model_file(model_path, model_name):\n    return os.path.join(model_path, model_name + \".tar\")\n\ndef _get_result_file(model_path, model_name):\n    return os.path.join(model_path, model_name + \"_results.json\")\n\ndef load_model(model_path, model_name, net=None):\n    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n    with open(config_file, \"r\") as f:\n        config_dict = json.load(f)\n    if net is None:\n        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n        assert act_fn_name in act_fn_by_name, f\"Unknown activation function \\\"{act_fn_name}\\\". Please add it to the \\\"act_fn_by_name\\\" dict.\"\n        act_fn = act_fn_by_name[act_fn_name]()\n        net = BaseNetwork(act_fn=act_fn, **config_dict)\n    net.load_state_dict(torch.load(model_file))\n    return net\n    \ndef save_model(model, model_path, model_name):\n    config_dict = model.config\n    os.makedirs(model_path, exist_ok=True)\n    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n    with open(config_file, \"w\") as f:\n        json.dump(config_dict, f)\n    torch.save(model.state_dict(), model_file)\n\ndef train_model(net, model_name, optim_func, max_epochs=50, batch_size=256, overwrite=False):\n    \"\"\"\n    Train a model on the training set of FashionMNIST\n    \n    Inputs:\n        net - Object of BaseNetwork\n        model_name - (str) Name of the model, used for creating the checkpoint names\n        max_epochs - Number of epochs we want to (maximally) train for\n        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n        batch_size - Size of batches used in training\n        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n    \"\"\"\n    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n    if file_exists and not overwrite:\n        print(f\"Model file of \\\"{model_name}\\\" already exists. Skipping training...\")\n        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"r\") as f:\n            results = json.load(f)\n    else:\n        if file_exists:\n            print(\"Model file exists, but will be overwritten...\")\n            \n        # Defining optimizer, loss and data loader\n        optimizer =  optim_func(net.parameters())\n        loss_module = nn.CrossEntropyLoss()\n        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n    \n        results = None\n        val_scores = []\n        train_losses, train_scores = [], []\n        best_val_epoch = -1\n        for epoch in range(max_epochs):\n            ############\n            # Training #\n            ############\n            net.train()\n            true_preds, count = 0., 0\n            t = tqdm(train_loader_local, leave=False)\n            for imgs, labels in t:\n                imgs, labels = imgs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                preds = net(imgs)\n                loss = loss_module(preds, labels)\n                loss.backward()\n                optimizer.step()\n                # Record statistics during training\n                true_preds += (preds.argmax(dim=-1) == labels).sum().item()\n                count += labels.shape[0]\n                t.set_description(f\"Epoch {epoch+1}: loss={loss.item():4.2f}\")\n                train_losses.append(loss.item())\n            train_acc = true_preds / count\n            train_scores.append(train_acc)\n\n            ##############\n            # Validation #\n            ##############\n            val_acc = test_model(net, val_loader)\n            val_scores.append(val_acc)\n            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n\n            if len(val_scores) == 1 or val_acc &gt; val_scores[best_val_epoch]:\n                print(\"\\t   (New best performance, saving model...)\")\n                save_model(net, CHECKPOINT_PATH, model_name)\n                best_val_epoch = epoch\n    \n    if results is None:\n        load_model(CHECKPOINT_PATH, model_name, net=net)\n        test_acc = test_model(net, test_loader)\n        results = {\"test_acc\": test_acc, \"val_scores\": val_scores, \"train_losses\": train_losses, \"train_scores\": train_scores}\n        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"w\") as f:\n            json.dump(results, f)\n            \n    # Plot a curve of the validation accuracy\n    sns.set()\n    plt.plot([i for i in range(1,len(results[\"train_scores\"])+1)], results[\"train_scores\"], label=\"Train\")\n    plt.plot([i for i in range(1,len(results[\"val_scores\"])+1)], results[\"val_scores\"], label=\"Val\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Validation accuracy\")\n    plt.ylim(min(results[\"val_scores\"]), max(results[\"train_scores\"])*1.01)\n    plt.title(f\"Validation performance of {model_name}\")\n    plt.legend()\n    plt.show()\n    plt.close()\n    \n    print((f\" Test accuracy: {results['test_acc']*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n    return results\n    \n\ndef test_model(net, data_loader):\n    \"\"\"\n    Test a model on a specified dataset.\n    \n    Inputs:\n        net - Trained model of type BaseNetwork\n        data_loader - DataLoader object of the dataset to test on (validation or test)\n    \"\"\"\n    net.eval()\n    true_preds, count = 0., 0\n    for imgs, labels in data_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.no_grad():\n            preds = net(imgs).argmax(dim=-1)\n            true_preds += (preds == labels).sum().item()\n            count += labels.shape[0]\n    test_acc = true_preds / count\n    return test_acc"
  },
  {
    "objectID": "dl_lec6.html#optimization-1",
    "href": "dl_lec6.html#optimization-1",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nWhat does optimizer do?\n\n\nIt updates the network’s parameters given the gradients: \\[\n\\begin{align*}\n&w^{t} = f(w^{t-1}, g^{t}, \\eta, ...), \\\\\n&w \\text{ - parameters}, \\\\\n&g^{t} = \\nabla_{w^{(t-1)}} \\mathcal{L}^{(t)} \\text{ - the gradients at time step } t,\\\\\n&\\eta \\text{ - learning rate}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec6.html#optimization-2",
    "href": "dl_lec6.html#optimization-2",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nOptimizer template\n\n\n\nclass OptimizerTemplate:\n    \n    def __init__(self, params, lr):\n        self.params = list(params)\n        self.lr = lr\n        \n    def zero_grad(self):\n        ## Set gradients of all parameters to zero\n        for p in self.params:\n            if p.grad is not None:\n                p.grad.detach_() # For second-order optimizers important\n                p.grad.zero_()\n    \n    @torch.no_grad()\n    def step(self):\n        ## Apply update step to all parameters\n        for p in self.params:\n            if p.grad is None: # We skip parameters without any gradients\n                continue\n            self.update_param(p)\n            \n    def update_param(self, p):\n        # To be implemented in optimizer-specific classes\n        raise NotImplementedError"
  },
  {
    "objectID": "dl_lec6.html#optimization-3",
    "href": "dl_lec6.html#optimization-3",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nStochastic Gradient Descent\n\n\nThe first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:\n\\[\n\\begin{split}\n    w^{(t)} & = w^{(t-1)} - \\eta \\cdot g^{(t)}\n\\end{split}\n\\]\n\n\n\n\nclass SGD(OptimizerTemplate):\n    \n    def __init__(self, params, lr):\n        super().__init__(params, lr)\n        \n    def update_param(self, p):\n        p_update = -self.lr * p.grad\n        p.add_(p_update) # In-place update =&gt; saves memory and does not create computation graph"
  },
  {
    "objectID": "dl_lec6.html#optimization-4",
    "href": "dl_lec6.html#optimization-4",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nMomentum\n\n\nMomentum replaces the gradient in the update by an exponential average of all past gradients including the current one:\n\\[\n\\begin{split}\n    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n    w^{(t)} & = w^{(t-1)} - \\eta \\cdot m^{(t)}\\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "dl_lec6.html#optimization-5",
    "href": "dl_lec6.html#optimization-5",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\nclass SGDMomentum(OptimizerTemplate):\n    \n    def __init__(self, params, lr, momentum=0.0):\n        super().__init__(params, lr)\n        self.momentum = momentum # Corresponds to beta_1 in the equation above\n        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # Dict to store m_t\n        \n    def update_param(self, p):\n        self.param_momentum[p] = (1 - self.momentum) * p.grad + self.momentum * self.param_momentum[p]\n        p_update = -self.lr * self.param_momentum[p]\n        p.add_(p_update)"
  },
  {
    "objectID": "dl_lec6.html#optimization-6",
    "href": "dl_lec6.html#optimization-6",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nAdam\n\n\nAdam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e. the gradients norm. Furthermore, we add a bias correction for the momentum and adaptive learning rate for the first iterations:\n\\[\n\\begin{split}\n    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n    v^{(t)} & = \\beta_2 v^{(t-1)} + (1 - \\beta_2)\\cdot \\left(g^{(t)}\\right)^2\\\\\n    \\hat{m}^{(t)} & = \\frac{m^{(t)}}{1-\\beta^{t}_1}, \\hat{v}^{(t)} = \\frac{v^{(t)}}{1-\\beta^{t}_2}\\\\\n    w^{(t)} & = w^{(t-1)} - \\frac{\\eta}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}\\circ \\hat{m}^{(t)}\\\\\n\\end{split}\n\\]\nEpsilon is a small constant used to improve numerical stability for very small gradient norms."
  },
  {
    "objectID": "dl_lec6.html#optimization-7",
    "href": "dl_lec6.html#optimization-7",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\nclass Adam(OptimizerTemplate):\n    \n    def __init__(self, params, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n        super().__init__(params, lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.param_step = {p: 0 for p in self.params} # Remembers \"t\" for each parameter for bias correction\n        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params}\n        self.param_2nd_momentum = {p: torch.zeros_like(p.data) for p in self.params}\n        \n    def update_param(self, p):\n        self.param_step[p] += 1\n        \n        self.param_momentum[p] = (1 - self.beta1) * p.grad + self.beta1 * self.param_momentum[p]\n        self.param_2nd_momentum[p] = (1 - self.beta2) * (p.grad)**2 + self.beta2 * self.param_2nd_momentum[p]\n        \n        bias_correction_1 = 1 - self.beta1 ** self.param_step[p]\n        bias_correction_2 = 1 - self.beta2 ** self.param_step[p]\n        \n        p_2nd_mom = self.param_2nd_momentum[p] / bias_correction_2\n        p_mom = self.param_momentum[p] / bias_correction_1\n        p_lr = self.lr / (torch.sqrt(p_2nd_mom) + self.eps)\n        p_update = -p_lr * p_mom\n        \n        p.add_(p_update)"
  },
  {
    "objectID": "dl_lec6.html#optimization-8",
    "href": "dl_lec6.html#optimization-8",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nComparison\n\n\n\nModel creationSGDMomentumAdam\n\n\n\nbase_model = BaseNetwork(act_fn=nn.ReLU(), hidden_sizes=[512,256,256,128])\nkaiming_init(base_model)\n\n\n\n\nSGD_model = copy.deepcopy(base_model).to(device)\nSGD_results = train_model(SGD_model, \"FashionMNIST_SGD\", \n                          lambda params: SGD(params, lr=1e-1), \n                          max_epochs=40, batch_size=256)\n\nModel file of \"FashionMNIST_SGD\" already exists. Skipping training...\n\n\n\n\n\n\n\n\n\n============= Test accuracy: 89.09% ==============\n\n\n\n\n\n\nSGDMom_model = copy.deepcopy(base_model).to(device)\nSGDMom_results = train_model(SGDMom_model, \"FashionMNIST_SGDMom\", \n                             lambda params: SGDMomentum(params, lr=1e-1, momentum=0.9), \n                             max_epochs=40, batch_size=256)\n\nModel file of \"FashionMNIST_SGDMom\" already exists. Skipping training...\n\n\n\n\n\n\n\n\n\n============= Test accuracy: 88.83% ==============\n\n\n\n\n\n\nAdam_model = copy.deepcopy(base_model).to(device)\nAdam_results = train_model(Adam_model, \"FashionMNIST_Adam\", \n                           lambda params: Adam(params, lr=1e-3), \n                           max_epochs=40, batch_size=256)\n\nModel file of \"FashionMNIST_Adam\" already exists. Skipping training...\n\n\n\n\n\n\n\n\n\n============= Test accuracy: 89.46% =============="
  },
  {
    "objectID": "dl_lec6.html#optimization-9",
    "href": "dl_lec6.html#optimization-9",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nChallenge 1: Pathological curvatures\n\n\nA pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization.\nIn other words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum.\n\n\n\n\ndef pathological_curve_loss(w1, w2):\n    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n    x1_loss = torch.tanh(w1)**2 + 0.01 * torch.abs(w1)\n    x2_loss = torch.sigmoid(w2)\n    return x1_loss + x2_loss\n\n\ndef plot_curve(curve_fn, x_range=(-5,5), y_range=(-5,5), plot_3d=False, cmap=cm.viridis, title=\"Pathological curvature\"):\n    fig = plt.figure()\n    ax = plt.axes(projection='3d') if plot_3d else plt.axes()\n    \n    x = torch.arange(x_range[0], x_range[1], (x_range[1]-x_range[0])/100.)\n    y = torch.arange(y_range[0], y_range[1], (y_range[1]-y_range[0])/100.)\n    x, y = torch.meshgrid(x, y, indexing='xy')\n    z = curve_fn(x, y)\n    x, y, z = x.numpy(), y.numpy(), z.numpy()\n    \n    if plot_3d:\n        ax.plot_surface(x, y, z, cmap=cmap, linewidth=1, color=\"#000\", antialiased=False)\n        ax.set_zlabel(\"loss\")\n    else:\n        ax.imshow(z[::-1], cmap=cmap, extent=(x_range[0], x_range[1], y_range[0], y_range[1]))\n    plt.title(title)\n    ax.set_xlabel(r\"$w_1$\")\n    ax.set_ylabel(r\"$w_2$\")\n    plt.tight_layout()\n    return ax\n\nsns.reset_orig()\n_ = plot_curve(pathological_curve_loss, plot_3d=True)\nplt.show()"
  },
  {
    "objectID": "dl_lec6.html#optimization-10",
    "href": "dl_lec6.html#optimization-10",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization"
  },
  {
    "objectID": "dl_lec6.html#optimization-11",
    "href": "dl_lec6.html#optimization-11",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nDiscussion\n\n\n\nIdeally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of \\(w_2\\).\nHowever, if we encounter a point along the ridges, the gradient is much greater in \\(w_1\\) than \\(w_2\\), and we might end up jumping from one side to the other.\nDue to the large gradients, we would have to reduce our learning rate slowing down learning significantly."
  },
  {
    "objectID": "dl_lec6.html#optimization-12",
    "href": "dl_lec6.html#optimization-12",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\ndef train_curve(optimizer_func, curve_func=pathological_curve_loss, num_updates=100, init=[5,5]):\n    \"\"\"\n    Inputs:\n        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list\n        curve_func - Loss function (e.g. pathological curvature)\n        num_updates - Number of updates/steps to take when optimizing \n        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2\n    Outputs:\n        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.\n    \"\"\"\n    weights = nn.Parameter(torch.FloatTensor(init), requires_grad=True)\n    optimizer = optimizer_func([weights])\n    \n    list_points = []\n    for _ in range(num_updates):\n        loss = curve_func(weights[0], weights[1])\n        list_points.append(torch.cat([weights.data.detach(), loss.unsqueeze(dim=0).detach()], dim=0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    points = torch.stack(list_points, dim=0).numpy()\n    return points"
  },
  {
    "objectID": "dl_lec6.html#optimization-13",
    "href": "dl_lec6.html#optimization-13",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nTrain\n\n\n\nSGD_points = train_curve(lambda params: SGD(params, lr=10))\nSGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=10, momentum=0.9))\nAdam_points = train_curve(lambda params: Adam(params, lr=1))"
  },
  {
    "objectID": "dl_lec6.html#optimization-14",
    "href": "dl_lec6.html#optimization-14",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nVisualize\n\n\n\nall_points = np.concatenate([SGD_points, SGDMom_points, Adam_points], axis=0)\nax = plot_curve(pathological_curve_loss,\n                x_range=(-np.absolute(all_points[:,0]).max(), np.absolute(all_points[:,0]).max()),\n                y_range=(all_points[:,1].min(), all_points[:,1].max()),\n                plot_3d=False)\nax.plot(SGD_points[:,0], SGD_points[:,1], color=\"red\", marker=\"o\", zorder=1, label=\"SGD\")\nax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"blue\", marker=\"o\", zorder=2, label=\"SGDMom\")\nax.plot(Adam_points[:,0], Adam_points[:,1], color=\"grey\", marker=\"o\", zorder=3, label=\"Adam\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "dl_lec6.html#optimization-15",
    "href": "dl_lec6.html#optimization-15",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nChallenge 2: Steep optima\n\n\nA second type of challenging loss surfaces are steep optima. In those, we have a larger part of the surface having very small gradients while around the optimum, we have very large gradients.\nAn adaptive learning rate becomes crucial.\n\n\n\n\ndef bivar_gaussian(w1, w2, x_mean=0.0, y_mean=0.0, x_sig=1.0, y_sig=1.0):\n    norm = 1 / (2 * np.pi * x_sig * y_sig)\n    x_exp = (-1 * (w1 - x_mean)**2) / (2 * x_sig**2)\n    y_exp = (-1 * (w2 - y_mean)**2) / (2 * y_sig**2)\n    return norm * torch.exp(x_exp + y_exp)\n\ndef comb_func(w1, w2):\n    z = -bivar_gaussian(w1, w2, x_mean=1.0, y_mean=-0.5, x_sig=0.2, y_sig=0.2)\n    z -= bivar_gaussian(w1, w2, x_mean=-1.0, y_mean=0.5, x_sig=0.2, y_sig=0.2)\n    z -= bivar_gaussian(w1, w2, x_mean=-0.5, y_mean=-0.8, x_sig=0.2, y_sig=0.2)\n    return z\n\n_ = plot_curve(comb_func, x_range=(-2,2), y_range=(-2,2), plot_3d=True, title=\"Steep optima\")"
  },
  {
    "objectID": "dl_lec6.html#optimization-16",
    "href": "dl_lec6.html#optimization-16",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization"
  },
  {
    "objectID": "dl_lec6.html#optimization-17",
    "href": "dl_lec6.html#optimization-17",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nTrain\n\n\n\nSGD_points = train_curve(lambda params: SGD(params, lr=.5), comb_func, init=[0,0])\nSGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=1, momentum=0.9), comb_func, init=[0,0])\nAdam_points = train_curve(lambda params: Adam(params, lr=0.2), comb_func, init=[0,0])\n\nall_points = np.concatenate([SGD_points, SGDMom_points, Adam_points], axis=0)\nax = plot_curve(comb_func,\n                x_range=(-2, 2),\n                y_range=(-2, 2),\n                plot_3d=False,\n                title=\"Steep optima\")\nax.plot(SGD_points[:,0], SGD_points[:,1], color=\"red\", marker=\"o\", zorder=3, label=\"SGD\", alpha=0.7)\nax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"blue\", marker=\"o\", zorder=2, label=\"SGDMom\", alpha=0.7)\nax.plot(Adam_points[:,0], Adam_points[:,1], color=\"grey\", marker=\"o\", zorder=1, label=\"Adam\", alpha=0.7)\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "dl_lec6.html#optimization-18",
    "href": "dl_lec6.html#optimization-18",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization"
  },
  {
    "objectID": "dl_lec6.html#optimization-19",
    "href": "dl_lec6.html#optimization-19",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nSGD vs Adam"
  },
  {
    "objectID": "dl_lec6.html#optimization-20",
    "href": "dl_lec6.html#optimization-20",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\\[\nf(x,y) = x^2 + y^2 -ae^{-\\frac{(x-1)^2+y^2}{c}} -be^{-\\frac{(x+1)^2+y^2}{c}}\n\\]"
  },
  {
    "objectID": "dl_lec6.html#optimization-21",
    "href": "dl_lec6.html#optimization-21",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nRastrigin function\n\n\n\\[\nf(x,y) = An + x^2- Acos(2\\pi x) + y^2 - Acos(2\\pi y), \\, A=10, \\; x,y \\in [-5.12,5.12]\n\\]"
  },
  {
    "objectID": "dl_lec6.html#optimization-22",
    "href": "dl_lec6.html#optimization-22",
    "title": "PyTorch: Optimization and Initialization",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nRosenbrock function\n\n\n\\[\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\]"
  },
  {
    "objectID": "dl_lab6.html",
    "href": "dl_lab6.html",
    "title": "DL: Lab 6",
    "section": "",
    "text": "Lab overview\nVarious PyTorch exercises.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "nlp_lab11.html",
    "href": "nlp_lab11.html",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "This implementation is based on Kim’s paper https://arxiv.org/pdf/1408.5882.pdf.\n\n\nImport libraries:\nimport os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport nltk\n#nltk.download(\"all\")\nimport matplotlib.pyplot as plt\nimport torch\n\n%matplotlib inline\n\n\nFetch spam detection dataset from Huggingface: https://huggingface.co/datasets/Deysi/spam-detection-dataset.\nIt will download 2 parquet files - this is a special data format. In order to be able to read it, run:\npip install fastparquet\nInvoke pandas.read_parquet(&lt;path&gt;) in order to read parquet file contents into a Pandas DataFrame.\n# CODE_START\n# parquet_data = ...\n# CODE_END\n\n\n\nNow, create 2 Numpy arrays of same size: texts and labels, where texts is a Numpy array of all texts from parquet file above (parquet_data), and labels is an array containing 0 when text is not spam and 1 otherwise.\n# CODE_START\n# texts = ... # some Numpy array\n# labels = ... # some Numpy array\n\n# assert(len(texts) == len(labels))\n# CODE_END\n\n\n\n(Needed for static and non-static models) Download FastText vectors from https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip and place them in notebook’s directory.\n\n\n\n\nTo prepare our text data for training, first we need to tokenize our sentences and build a vocabulary dictionary word2idx, which will later be used to convert our tokens into indexes and build an embedding layer.\nAn embedding layer serves as a look-up table which take word indexes in the vocabulary as input and output word vectors. Hence, the embedding layer has shape \\((N, d)\\) where \\(N\\) is the size of the vocabulary and \\(d\\) is the embedding dimension. In order to fine-tune pretrained word vectors, we need to create an embedding layer in our nn.Modules class. Our input to the model will then be input_ids, which is the tokens’ index in the vocabulary.\nThe function tokenize will tokenize our sentences, build a vocabulary and fine the maximum sentence length. The function encode will take in the outputs of tokenize, perform sentence padding and return input_ids as a numpy array.\nUse the below two functions to tokenize and encode texts:\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\n\ndef tokenize(texts):\n    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n\n    Args:\n        texts (List[str]): List of text data\n\n    Returns:\n        tokenized_texts (List[List[str]]): List of list of tokens\n        word2idx (Dict): Vocabulary built from the corpus\n        max_len (int): Maximum sentence length\n    \"\"\"\n\n    max_len = 0\n    tokenized_texts = []\n    word2idx = {}\n\n    # Add &lt;pad&gt; and &lt;unk&gt; tokens to the vocabulary\n    word2idx['&lt;pad&gt;'] = 0\n    word2idx['&lt;unk&gt;'] = 1\n\n    # Building our vocab from the corpus starting from index 2\n    idx = 2\n    for sent in texts:\n        # Tokenize a sentence\n        # CODE_START\n        # tokenized_sent = ...\n        # CODE_END\n\n        # Add `tokenized_sent` to `tokenized_texts`\n        tokenized_texts.append(tokenized_sent)\n\n        # Add new token to `word2idx`\n        for token in tokenized_sent:\n            if token not in word2idx:\n                word2idx[token] = idx\n                idx += 1\n\n        # Update `max_len`\n        max_len = max(max_len, len(tokenized_sent))\n\n    return tokenized_texts, word2idx, max_len\n\ndef encode(tokenized_texts, word2idx, max_len):\n    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n    their index in the vocabulary.\n\n    Returns:\n        input_ids (np.array): Array of token indexes in the vocabulary with\n            shape (N, max_len). It will be the input to our CNN model.\n    \"\"\"\n\n    input_ids = []\n    for tokenized_sent in tokenized_texts:\n        # Pad sentences to max_len\n        tokenized_sent += ['&lt;pad&gt;'] * (max_len - len(tokenized_sent))\n\n        # Encode tokens to input_ids\n        input_id = [word2idx.get(token) for token in tokenized_sent]\n        input_ids.append(input_id)\n\n    return np.array(input_ids)\n\n\n\nfrom tqdm import tqdm_notebook\n\ndef load_pretrained_vectors(word2idx, fname):\n    \"\"\"Load pretrained vectors and create embedding layers.\n\n    Args:\n        word2idx (Dict): Vocabulary built from the corpus\n        fname (str): Path to pretrained vector file\n\n    Returns:\n        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n            the size of word2idx and d is embedding dimension\n    \"\"\"\n\n    print(\"Loading pretrained vectors...\")\n    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    n, d = map(int, fin.readline().split())\n\n    # Initialize random embeddings\n    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n    embeddings[word2idx['&lt;pad&gt;']] = np.zeros((d,))\n\n    # Load pretrained vectors\n    count = 0\n    for line in tqdm_notebook(fin):\n        tokens = line.rstrip().split(' ')\n        word = tokens[0]\n        if word in word2idx:\n            count += 1\n            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n\n    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n\n    return embeddings\n\n# Tokenize, build vocabulary, encode tokens\nprint(\"Tokenizing...\\n\")\ntokenized_texts, word2idx, max_len = tokenize(texts)\ninput_ids = encode(tokenized_texts, word2idx, max_len)\n\n# Load pretrained vectors\nembeddings = load_pretrained_vectors(word2idx, \"../imdb/crawl-300d-2M.vec\")\nembeddings = torch.tensor(embeddings)\n\n\n\nfrom torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n                              SequentialSampler)\n\ndef data_loader(train_inputs, val_inputs, train_labels, val_labels,\n                batch_size=10):\n    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n    DataLoader.\n    \"\"\"\n\n    # Convert data type to torch.Tensor\n    # CODE_START\n    # train_inputs, val_inputs, train_labels, val_labels = tuple(...)\n    # CODE_END\n\n    # Create DataLoader for training data\n    train_data = TensorDataset(train_inputs, train_labels)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n    # Create DataLoader for validation data\n    # CODE_START\n    # val_data = ...\n    # val_sampler = SequentialSampler(...)\n    # val_dataloader = ...\n    # CODE_END\n\n    return train_dataloader, val_dataloader\nUse train_test_split function from scikit-learn library:\nfrom sklearn.model_selection import train_test_split\n\n# Train Test Split with test set size = 5%\n# CODE_START\n# train_inputs, val_inputs, train_labels, val_labels = train_test_split(...)\n# CODE_END\n\n# Load data to PyTorch DataLoader\n# Use batch_size = 50\n# CODE_START\n# train_dataloader, val_dataloader = ...\n# CODE_END\n\n\n\nThe picture below is the illustration of the CNN architecture that we are going to build with three filter sizes: 2, 3, and 4, each of which has 2 filters.\n\nCNN Architecture (Source: Zhang, 2015)\n# Sample configuration:\nfilter_sizes = [2, 3, 4]\nnum_filters = [2, 2, 2]\nSuppose that we are classifying the sentence “I like this movie very much!” (\\(N = 7\\) tokens) and the dimensionality of word vectors is \\(d=5\\). After applying the embedding layer on the input token ids, the sample sentence is presented as a 2D tensor with shape (7, 5) like an image.\n\\[\\mathrm{x_{emb}} \\quad \\in \\mathbb{R}^{7 \\times 5}\\]\nWe then use 1-dimesional convolution to extract features from the sentence. In this example, we have 6 filters in total, and each filter has shape \\((f_i, d)\\) where \\(f_i\\) is the filter size for \\(i \\in \\{1,...,6\\}\\). Each filter will then scan over \\(\\mathrm{x_{emb}}\\) and returns a feature map:\n\\[\\mathrm{x_{conv_ i} = Conv1D(x_{emb})} \\quad \\in \\mathbb{R}^{N-f_i+1}\\]\nNext, we apply the ReLU activation to \\(\\mathrm{x_{conv_{i}}}\\) and use max-over-time-pooling to reduce each feature map to a single scalar. Then we concatenate these scalars into the final feature vector which will be fed to a fully connected layer to compute the final scores for our classes (logits).\n\\[\\mathrm{x_{pool_i} = MaxPool(ReLU(x_{conv_i}))} \\quad \\in \\mathbb{R}\\]\n\\[\\mathrm{x_{fc} = \\texttt{concat}(x_{pool_i})} \\quad \\in \\mathbb{R}^6\\]\nThe idea here is that each filter will capture different semantic signals in the sentence (ie. happiness, humor, politic, anger…) and max-pooling will record only the strongest signal over the sentence. This logic makes sense because humans also perceive the sentiment of a sentence based on its strongest word/signal.\nFinally, we use a fully connected layer with the weight matrix $ ^{2 } $ and dropout to compute \\(\\mathrm{logits}\\), which is a vector of length 2 that keeps the scores for 2 classes.\n\\[\\mathrm{logits = Dropout(\\mathbf{W_{fc}}x_{fc})}  \\in \\mathbb{R}^2\\]\nAn in-depth explanation of CNN can be found in this article and this video.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN_NLP(nn.Module):\n    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n    def __init__(self,\n                 pretrained_embedding=None,\n                 freeze_embedding=False,\n                 vocab_size=None,\n                 embed_dim=300,\n                 filter_sizes=[3, 4, 5],\n                 num_filters=[100, 100, 100],\n                 num_classes=2,\n                 dropout=0.5):\n        \"\"\"\n        The constructor for CNN_NLP class.\n\n        Args:\n            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n                shape (vocab_size, embed_dim)\n            freeze_embedding (bool): Set to False to fine-tune pretraiend\n                vectors. Default: False\n            vocab_size (int): Need to be specified when not pretrained word\n                embeddings are not used.\n            embed_dim (int): Dimension of word vectors. Need to be specified\n                when pretrained word embeddings are not used. Default: 300\n            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n            num_filters (List[int]): List of number of filters, has the same\n                length as `filter_sizes`. Default: [100, 100, 100]\n            n_classes (int): Number of classes. Default: 2\n            dropout (float): Dropout rate. Default: 0.5\n        \"\"\"\n\n        super(CNN_NLP, self).__init__()\n        # Embedding layer\n        if pretrained_embedding is not None:\n            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n                                                          freeze=freeze_embedding)\n        else:\n            self.embed_dim = embed_dim\n            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n                                          embedding_dim=self.embed_dim,\n                                          padding_idx=0,\n                                          max_norm=5.0)\n        # Conv Network\n        # Build a convolutional (nn.Conv1d) layer here, containing len(filter_sizes) filters\n        # CODE_START\n        # self.conv1d_list = nn.ModuleList([\n             # ...\n        #])\n        # CODE_END\n        \n        # Fully-connected layer and Dropout\n        # CODE_START\n        # self.fc = nn.Linear(...)\n        # CODE_END\n\n        # Dropout \n        # CODE_START\n        # self.dropout = ...\n        # CODE_END \n\n    def forward(self, input_ids):\n        \"\"\"Perform a forward pass through the network.\n\n        Args:\n            input_ids (torch.Tensor): A tensor of token ids with shape\n                (batch_size, max_sent_length)\n\n        Returns:\n            logits (torch.Tensor): Output logits with shape (batch_size,\n                n_classes)\n        \"\"\"\n\n        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n        x_embed = self.embedding(input_ids).float()\n\n        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n        # Output shape: (b, embed_dim, max_len)\n        x_reshaped = x_embed.permute(0, 2, 1)\n\n        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n        # CODE_START\n        # x_conv_list = [... for conv1d in self.conv1d_list]\n        # CODE_END\n\n        # Max pooling. Output shape: (b, num_filters[i], 1)\n        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n            for x_conv in x_conv_list]\n\n        # Concatenate x_pool_list to feed the fully connected layer.\n        # Output shape: (b, sum(num_filters))\n        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n                         dim=1)\n\n        # Compute logits. Output shape: (b, n_classes)\n        logits = self.fc(self.dropout(x_fc))\n\n        return logits\nSet device to either cpu, cuda, or mps:\ndevice = 'cpu`\n\n\n\nA helper function here, plus optimizer creation.\nimport torch.optim as optim\n\ndef initialize_model(pretrained_embedding=None,\n                    freeze_embedding=False,\n                    vocab_size=None,\n                    embed_dim=300,\n                    filter_sizes=[3, 4, 5],\n                    num_filters=[100, 100, 100],\n                    num_classes=2,\n                    dropout=0.5,\n                    learning_rate=0.01):\n    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n\n    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n    num_filters need to be of the same length.\"\n\n    # Instantiate CNN model\n    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n                        freeze_embedding=freeze_embedding,\n                        vocab_size=vocab_size,\n                        embed_dim=embed_dim,\n                        filter_sizes=filter_sizes,\n                        num_filters=num_filters,\n                        num_classes=2,\n                        dropout=0.5)\n\n    # Send model to `device` (GPU/CPU)\n    cnn_model.to(device)\n\n    # Instantiate RMSprop optimizer\n    # CODE_START\n    # optimizer = ...\n    # CODE_END\n\n    return cnn_model, optimizer\n\n\n\nFor each epoch, the code below will perform a forward step to compute the Cross Entropy loss, a backward step to compute gradients and use the optimizer to update weights/parameters. At the end of each epoch, the loss on training data and the accuracy over the validation data will be printed to help us keep track of the model’s performance. The code is heavily annotated with detailed explanations.\nimport random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n    \"\"\"Train the CNN model.\"\"\"\n\n    # Tracking best validation accuracy\n    best_accuracy = 0\n\n    # Start training loop\n    print(\"Start training...\\n\")\n    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n    'Val Acc':^9} | {'Elapsed':^9}\")\n    print(\"-\"*60)\n\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n\n        # Tracking time and loss\n        t0_epoch = time.time()\n        total_loss = 0\n\n        # Put the model into the training mode\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            # Load batch to GPU\n            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            # CODE_START\n            # ...\n            # CODE_END\n\n            # Perform a forward pass. This will return logits.\n            # CODE_START\n            # logits = ...\n            # CODE_END\n\n            # Compute loss and accumulate the loss values\n            # CODE_START\n            # loss = ...\n            # total_loss += loss.item()\n            # CODE_END \n\n            # Perform a backward pass to calculate gradients and update parameters\n            # CODE_START\n            # ...\n            # CODE_END\n\n        # Calculate the average loss over the entire training data\n        # CODE_START\n        # avg_train_loss = ...\n        # CODE_END\n\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if val_dataloader is not None:\n            # After the completion of each training epoch, measure the model's\n            # performance on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Track the best accuracy\n            if val_accuracy &gt; best_accuracy:\n                best_accuracy = val_accuracy\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n\n    print(\"\\n\")\n    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's\n    performance on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled\n    # during the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy\n\n\n\n# CNN-rand: Word vectors are randomly initialized.\nset_seed(42)\ncnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n                                      embed_dim=300,\n                                      learning_rate=0.25,\n                                      dropout=0.5)\ntrain(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=3)\n# CNN-static: fastText pretrained word vectors are used and freezed during training.\nset_seed(42)\ncnn_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n                                        freeze_embedding=True,\n                                        learning_rate=0.25,\n                                        dropout=0.5)\ntrain(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=3)\n# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\nset_seed(42)\ncnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n                                            freeze_embedding=False,\n                                            learning_rate=0.25,\n                                            dropout=0.5)\ntrain(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=3)\n\n\n\ndef predict(text, model=cnn_rand.to(\"cpu\"), max_len=62):\n    \"\"\"Predict probability that a review is positive.\"\"\"\n\n    # Tokenize, pad and encode text\n    tokens = word_tokenize(text.lower())\n    padded_tokens = tokens + ['&lt;pad&gt;'] * (max_len - len(tokens))\n    input_id = [word2idx.get(token, word2idx['&lt;unk&gt;']) for token in padded_tokens]\n\n    # Convert to PyTorch tensors\n    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n\n    # Compute logits\n    logits = model.forward(input_id)\n\n    #  Compute probability\n    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n\n    print(f\"This entry is {probs[1] * 100:.2f}% not spam.\")\nInvoke predict() function on some text to see how this model works."
  },
  {
    "objectID": "nlp_lab11.html#preliminaries",
    "href": "nlp_lab11.html#preliminaries",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "Import libraries:\nimport os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport nltk\n#nltk.download(\"all\")\nimport matplotlib.pyplot as plt\nimport torch\n\n%matplotlib inline\n\n\nFetch spam detection dataset from Huggingface: https://huggingface.co/datasets/Deysi/spam-detection-dataset.\nIt will download 2 parquet files - this is a special data format. In order to be able to read it, run:\npip install fastparquet\nInvoke pandas.read_parquet(&lt;path&gt;) in order to read parquet file contents into a Pandas DataFrame.\n# CODE_START\n# parquet_data = ...\n# CODE_END\n\n\n\nNow, create 2 Numpy arrays of same size: texts and labels, where texts is a Numpy array of all texts from parquet file above (parquet_data), and labels is an array containing 0 when text is not spam and 1 otherwise.\n# CODE_START\n# texts = ... # some Numpy array\n# labels = ... # some Numpy array\n\n# assert(len(texts) == len(labels))\n# CODE_END\n\n\n\n(Needed for static and non-static models) Download FastText vectors from https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip and place them in notebook’s directory."
  },
  {
    "objectID": "nlp_lab11.html#sentence-pre-processing",
    "href": "nlp_lab11.html#sentence-pre-processing",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "To prepare our text data for training, first we need to tokenize our sentences and build a vocabulary dictionary word2idx, which will later be used to convert our tokens into indexes and build an embedding layer.\nAn embedding layer serves as a look-up table which take word indexes in the vocabulary as input and output word vectors. Hence, the embedding layer has shape \\((N, d)\\) where \\(N\\) is the size of the vocabulary and \\(d\\) is the embedding dimension. In order to fine-tune pretrained word vectors, we need to create an embedding layer in our nn.Modules class. Our input to the model will then be input_ids, which is the tokens’ index in the vocabulary.\nThe function tokenize will tokenize our sentences, build a vocabulary and fine the maximum sentence length. The function encode will take in the outputs of tokenize, perform sentence padding and return input_ids as a numpy array.\nUse the below two functions to tokenize and encode texts:\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\n\ndef tokenize(texts):\n    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n\n    Args:\n        texts (List[str]): List of text data\n\n    Returns:\n        tokenized_texts (List[List[str]]): List of list of tokens\n        word2idx (Dict): Vocabulary built from the corpus\n        max_len (int): Maximum sentence length\n    \"\"\"\n\n    max_len = 0\n    tokenized_texts = []\n    word2idx = {}\n\n    # Add &lt;pad&gt; and &lt;unk&gt; tokens to the vocabulary\n    word2idx['&lt;pad&gt;'] = 0\n    word2idx['&lt;unk&gt;'] = 1\n\n    # Building our vocab from the corpus starting from index 2\n    idx = 2\n    for sent in texts:\n        # Tokenize a sentence\n        # CODE_START\n        # tokenized_sent = ...\n        # CODE_END\n\n        # Add `tokenized_sent` to `tokenized_texts`\n        tokenized_texts.append(tokenized_sent)\n\n        # Add new token to `word2idx`\n        for token in tokenized_sent:\n            if token not in word2idx:\n                word2idx[token] = idx\n                idx += 1\n\n        # Update `max_len`\n        max_len = max(max_len, len(tokenized_sent))\n\n    return tokenized_texts, word2idx, max_len\n\ndef encode(tokenized_texts, word2idx, max_len):\n    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n    their index in the vocabulary.\n\n    Returns:\n        input_ids (np.array): Array of token indexes in the vocabulary with\n            shape (N, max_len). It will be the input to our CNN model.\n    \"\"\"\n\n    input_ids = []\n    for tokenized_sent in tokenized_texts:\n        # Pad sentences to max_len\n        tokenized_sent += ['&lt;pad&gt;'] * (max_len - len(tokenized_sent))\n\n        # Encode tokens to input_ids\n        input_id = [word2idx.get(token) for token in tokenized_sent]\n        input_ids.append(input_id)\n\n    return np.array(input_ids)"
  },
  {
    "objectID": "nlp_lab11.html#pre-trained-vectors",
    "href": "nlp_lab11.html#pre-trained-vectors",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "from tqdm import tqdm_notebook\n\ndef load_pretrained_vectors(word2idx, fname):\n    \"\"\"Load pretrained vectors and create embedding layers.\n\n    Args:\n        word2idx (Dict): Vocabulary built from the corpus\n        fname (str): Path to pretrained vector file\n\n    Returns:\n        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n            the size of word2idx and d is embedding dimension\n    \"\"\"\n\n    print(\"Loading pretrained vectors...\")\n    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    n, d = map(int, fin.readline().split())\n\n    # Initialize random embeddings\n    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n    embeddings[word2idx['&lt;pad&gt;']] = np.zeros((d,))\n\n    # Load pretrained vectors\n    count = 0\n    for line in tqdm_notebook(fin):\n        tokens = line.rstrip().split(' ')\n        word = tokens[0]\n        if word in word2idx:\n            count += 1\n            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n\n    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n\n    return embeddings\n\n# Tokenize, build vocabulary, encode tokens\nprint(\"Tokenizing...\\n\")\ntokenized_texts, word2idx, max_len = tokenize(texts)\ninput_ids = encode(tokenized_texts, word2idx, max_len)\n\n# Load pretrained vectors\nembeddings = load_pretrained_vectors(word2idx, \"../imdb/crawl-300d-2M.vec\")\nembeddings = torch.tensor(embeddings)"
  },
  {
    "objectID": "nlp_lab11.html#pytorch-data-loaders",
    "href": "nlp_lab11.html#pytorch-data-loaders",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n                              SequentialSampler)\n\ndef data_loader(train_inputs, val_inputs, train_labels, val_labels,\n                batch_size=10):\n    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n    DataLoader.\n    \"\"\"\n\n    # Convert data type to torch.Tensor\n    # CODE_START\n    # train_inputs, val_inputs, train_labels, val_labels = tuple(...)\n    # CODE_END\n\n    # Create DataLoader for training data\n    train_data = TensorDataset(train_inputs, train_labels)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n    # Create DataLoader for validation data\n    # CODE_START\n    # val_data = ...\n    # val_sampler = SequentialSampler(...)\n    # val_dataloader = ...\n    # CODE_END\n\n    return train_dataloader, val_dataloader\nUse train_test_split function from scikit-learn library:\nfrom sklearn.model_selection import train_test_split\n\n# Train Test Split with test set size = 5%\n# CODE_START\n# train_inputs, val_inputs, train_labels, val_labels = train_test_split(...)\n# CODE_END\n\n# Load data to PyTorch DataLoader\n# Use batch_size = 50\n# CODE_START\n# train_dataloader, val_dataloader = ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lab11.html#cnn-model",
    "href": "nlp_lab11.html#cnn-model",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "The picture below is the illustration of the CNN architecture that we are going to build with three filter sizes: 2, 3, and 4, each of which has 2 filters.\n\nCNN Architecture (Source: Zhang, 2015)\n# Sample configuration:\nfilter_sizes = [2, 3, 4]\nnum_filters = [2, 2, 2]\nSuppose that we are classifying the sentence “I like this movie very much!” (\\(N = 7\\) tokens) and the dimensionality of word vectors is \\(d=5\\). After applying the embedding layer on the input token ids, the sample sentence is presented as a 2D tensor with shape (7, 5) like an image.\n\\[\\mathrm{x_{emb}} \\quad \\in \\mathbb{R}^{7 \\times 5}\\]\nWe then use 1-dimesional convolution to extract features from the sentence. In this example, we have 6 filters in total, and each filter has shape \\((f_i, d)\\) where \\(f_i\\) is the filter size for \\(i \\in \\{1,...,6\\}\\). Each filter will then scan over \\(\\mathrm{x_{emb}}\\) and returns a feature map:\n\\[\\mathrm{x_{conv_ i} = Conv1D(x_{emb})} \\quad \\in \\mathbb{R}^{N-f_i+1}\\]\nNext, we apply the ReLU activation to \\(\\mathrm{x_{conv_{i}}}\\) and use max-over-time-pooling to reduce each feature map to a single scalar. Then we concatenate these scalars into the final feature vector which will be fed to a fully connected layer to compute the final scores for our classes (logits).\n\\[\\mathrm{x_{pool_i} = MaxPool(ReLU(x_{conv_i}))} \\quad \\in \\mathbb{R}\\]\n\\[\\mathrm{x_{fc} = \\texttt{concat}(x_{pool_i})} \\quad \\in \\mathbb{R}^6\\]\nThe idea here is that each filter will capture different semantic signals in the sentence (ie. happiness, humor, politic, anger…) and max-pooling will record only the strongest signal over the sentence. This logic makes sense because humans also perceive the sentiment of a sentence based on its strongest word/signal.\nFinally, we use a fully connected layer with the weight matrix $ ^{2 } $ and dropout to compute \\(\\mathrm{logits}\\), which is a vector of length 2 that keeps the scores for 2 classes.\n\\[\\mathrm{logits = Dropout(\\mathbf{W_{fc}}x_{fc})}  \\in \\mathbb{R}^2\\]\nAn in-depth explanation of CNN can be found in this article and this video.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN_NLP(nn.Module):\n    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n    def __init__(self,\n                 pretrained_embedding=None,\n                 freeze_embedding=False,\n                 vocab_size=None,\n                 embed_dim=300,\n                 filter_sizes=[3, 4, 5],\n                 num_filters=[100, 100, 100],\n                 num_classes=2,\n                 dropout=0.5):\n        \"\"\"\n        The constructor for CNN_NLP class.\n\n        Args:\n            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n                shape (vocab_size, embed_dim)\n            freeze_embedding (bool): Set to False to fine-tune pretraiend\n                vectors. Default: False\n            vocab_size (int): Need to be specified when not pretrained word\n                embeddings are not used.\n            embed_dim (int): Dimension of word vectors. Need to be specified\n                when pretrained word embeddings are not used. Default: 300\n            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n            num_filters (List[int]): List of number of filters, has the same\n                length as `filter_sizes`. Default: [100, 100, 100]\n            n_classes (int): Number of classes. Default: 2\n            dropout (float): Dropout rate. Default: 0.5\n        \"\"\"\n\n        super(CNN_NLP, self).__init__()\n        # Embedding layer\n        if pretrained_embedding is not None:\n            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n                                                          freeze=freeze_embedding)\n        else:\n            self.embed_dim = embed_dim\n            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n                                          embedding_dim=self.embed_dim,\n                                          padding_idx=0,\n                                          max_norm=5.0)\n        # Conv Network\n        # Build a convolutional (nn.Conv1d) layer here, containing len(filter_sizes) filters\n        # CODE_START\n        # self.conv1d_list = nn.ModuleList([\n             # ...\n        #])\n        # CODE_END\n        \n        # Fully-connected layer and Dropout\n        # CODE_START\n        # self.fc = nn.Linear(...)\n        # CODE_END\n\n        # Dropout \n        # CODE_START\n        # self.dropout = ...\n        # CODE_END \n\n    def forward(self, input_ids):\n        \"\"\"Perform a forward pass through the network.\n\n        Args:\n            input_ids (torch.Tensor): A tensor of token ids with shape\n                (batch_size, max_sent_length)\n\n        Returns:\n            logits (torch.Tensor): Output logits with shape (batch_size,\n                n_classes)\n        \"\"\"\n\n        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n        x_embed = self.embedding(input_ids).float()\n\n        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n        # Output shape: (b, embed_dim, max_len)\n        x_reshaped = x_embed.permute(0, 2, 1)\n\n        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n        # CODE_START\n        # x_conv_list = [... for conv1d in self.conv1d_list]\n        # CODE_END\n\n        # Max pooling. Output shape: (b, num_filters[i], 1)\n        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n            for x_conv in x_conv_list]\n\n        # Concatenate x_pool_list to feed the fully connected layer.\n        # Output shape: (b, sum(num_filters))\n        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n                         dim=1)\n\n        # Compute logits. Output shape: (b, n_classes)\n        logits = self.fc(self.dropout(x_fc))\n\n        return logits\nSet device to either cpu, cuda, or mps:\ndevice = 'cpu`"
  },
  {
    "objectID": "nlp_lab11.html#optimizer",
    "href": "nlp_lab11.html#optimizer",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "A helper function here, plus optimizer creation.\nimport torch.optim as optim\n\ndef initialize_model(pretrained_embedding=None,\n                    freeze_embedding=False,\n                    vocab_size=None,\n                    embed_dim=300,\n                    filter_sizes=[3, 4, 5],\n                    num_filters=[100, 100, 100],\n                    num_classes=2,\n                    dropout=0.5,\n                    learning_rate=0.01):\n    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n\n    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n    num_filters need to be of the same length.\"\n\n    # Instantiate CNN model\n    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n                        freeze_embedding=freeze_embedding,\n                        vocab_size=vocab_size,\n                        embed_dim=embed_dim,\n                        filter_sizes=filter_sizes,\n                        num_filters=num_filters,\n                        num_classes=2,\n                        dropout=0.5)\n\n    # Send model to `device` (GPU/CPU)\n    cnn_model.to(device)\n\n    # Instantiate RMSprop optimizer\n    # CODE_START\n    # optimizer = ...\n    # CODE_END\n\n    return cnn_model, optimizer"
  },
  {
    "objectID": "nlp_lab11.html#training-loop",
    "href": "nlp_lab11.html#training-loop",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "For each epoch, the code below will perform a forward step to compute the Cross Entropy loss, a backward step to compute gradients and use the optimizer to update weights/parameters. At the end of each epoch, the loss on training data and the accuracy over the validation data will be printed to help us keep track of the model’s performance. The code is heavily annotated with detailed explanations.\nimport random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n    \"\"\"Train the CNN model.\"\"\"\n\n    # Tracking best validation accuracy\n    best_accuracy = 0\n\n    # Start training loop\n    print(\"Start training...\\n\")\n    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n    'Val Acc':^9} | {'Elapsed':^9}\")\n    print(\"-\"*60)\n\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n\n        # Tracking time and loss\n        t0_epoch = time.time()\n        total_loss = 0\n\n        # Put the model into the training mode\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            # Load batch to GPU\n            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            # CODE_START\n            # ...\n            # CODE_END\n\n            # Perform a forward pass. This will return logits.\n            # CODE_START\n            # logits = ...\n            # CODE_END\n\n            # Compute loss and accumulate the loss values\n            # CODE_START\n            # loss = ...\n            # total_loss += loss.item()\n            # CODE_END \n\n            # Perform a backward pass to calculate gradients and update parameters\n            # CODE_START\n            # ...\n            # CODE_END\n\n        # Calculate the average loss over the entire training data\n        # CODE_START\n        # avg_train_loss = ...\n        # CODE_END\n\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if val_dataloader is not None:\n            # After the completion of each training epoch, measure the model's\n            # performance on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Track the best accuracy\n            if val_accuracy &gt; best_accuracy:\n                best_accuracy = val_accuracy\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n\n    print(\"\\n\")\n    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's\n    performance on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled\n    # during the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy"
  },
  {
    "objectID": "nlp_lab11.html#model-evaluation",
    "href": "nlp_lab11.html#model-evaluation",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "# CNN-rand: Word vectors are randomly initialized.\nset_seed(42)\ncnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n                                      embed_dim=300,\n                                      learning_rate=0.25,\n                                      dropout=0.5)\ntrain(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=3)\n# CNN-static: fastText pretrained word vectors are used and freezed during training.\nset_seed(42)\ncnn_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n                                        freeze_embedding=True,\n                                        learning_rate=0.25,\n                                        dropout=0.5)\ntrain(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=3)\n# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\nset_seed(42)\ncnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n                                            freeze_embedding=False,\n                                            learning_rate=0.25,\n                                            dropout=0.5)\ntrain(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=3)"
  },
  {
    "objectID": "nlp_lab11.html#test-predictions",
    "href": "nlp_lab11.html#test-predictions",
    "title": "NLP: Lab 11 (CNNs for NLP)",
    "section": "",
    "text": "def predict(text, model=cnn_rand.to(\"cpu\"), max_len=62):\n    \"\"\"Predict probability that a review is positive.\"\"\"\n\n    # Tokenize, pad and encode text\n    tokens = word_tokenize(text.lower())\n    padded_tokens = tokens + ['&lt;pad&gt;'] * (max_len - len(tokens))\n    input_id = [word2idx.get(token, word2idx['&lt;unk&gt;']) for token in padded_tokens]\n\n    # Convert to PyTorch tensors\n    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n\n    # Compute logits\n    logits = model.forward(input_id)\n\n    #  Compute probability\n    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n\n    print(f\"This entry is {probs[1] * 100:.2f}% not spam.\")\nInvoke predict() function on some text to see how this model works."
  },
  {
    "objectID": "dl_lec3.html#deep-networks",
    "href": "dl_lec3.html#deep-networks",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep networks",
    "text": "Deep networks\nSuppose we have this network:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-1",
    "href": "dl_lec3.html#deep-networks-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nWhat happens in Hidden layer 1 and Output layer?\nWe will use this notation: \\[\\begin{align*}\n  & z^{[1]} = W^{[1]}x+b^{[1]} \\\\\n  & a^{[1]} = \\sigma(z^{[1]})\\\\\n  & z^{[2]} = W^{[2]}a^{[1]}+b^{[2]} \\\\\n  & a^{[2]} = \\sigma(z^{[2]})\n\\end{align*}\\] And then we compute \\(L(a^{[2]}, y)\\).\nAnd then, similarly, for backpropagation, we will compute \\(da^{[2]}, dz^{[2]}, dW^{[2]}\\).\n\n\nDo not confuse round and square brackets."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-2",
    "href": "dl_lec3.html#deep-networks-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nAlternative notation: \\(a^{[0]} = x\\). So in the picture \\[\\begin{align*}\n  a = \\begin{bmatrix}\n    a_1^{[1]} \\\\\n    a_2^{[1]}\\\\\n    a_3^{[1]}\\\\\n    a_4^{[1]}\n  \\end{bmatrix}\n\\end{align*}\\] 2-layer network (input layer not counted). Input layer is layer 0."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-3",
    "href": "dl_lec3.html#deep-networks-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\n\n\n\n\nFirst hidden layer params\n\n\n\n\\(W^{[1]}\\) ((4,3) matrix)\n\\(b^{[1]}\\) ((4,1) matrix).\n\n\n\n\n\n\n\nSecond hidden layer params\n\n\n\n\\(W^{[2]}\\) is a (1,4) matrix\n\\(b^{[2]}\\) is a (1,1) matrix."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-4",
    "href": "dl_lec3.html#deep-networks-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor all nodes: \\[\\begin{align*}\n  & z_1^{[1]} = W_1^{[1]T}x + b_1^{[1]},\\; & a_1^{[1]} = \\sigma(z_1^{[1]}), \\\\\n  & z_2^{[1]} = W_2^{[1]T}x + b_2^{[1]},\\; & a_2^{[1]} = \\sigma(z_2^{[1]}), \\\\\n  & z_3^{[1]} = W_3^{[1]T}x + b_3^{[1]},\\; & a_3^{[1]} = \\sigma(z_3^{[1]}), \\\\\n  & z_4^{[1]} = W_4^{[1]T}x + b_4^{[1]},\\; & a_4^{[1]} = \\sigma(z_4^{[1]})\n\\end{align*}\\] So, if we have \\(a_i^{[l]}\\), then \\(l\\) means layer, and \\(i\\) means node number in a layer."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-5",
    "href": "dl_lec3.html#deep-networks-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-6",
    "href": "dl_lec3.html#deep-networks-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor \\(m\\) training examples, we compute \\(x^{(m)} \\rightarrow a^{[2](m)} = \\hat{y}^{(m)}\\).\nThe block of code:\nfor i = 1 to m:\n\\[\\begin{align*}\n  &z^{[1](i)} = W^{[1]}x^{(i)}+b^{[1]},\\\\\n  &a^{[1](i)} = \\sigma(z^{[1](i)}),\\\\\n  &z^{[2](i)} = W^{[2]}a^{[1](i)}+b^{[2]},\\\\\n  &a^{[2](i)} = \\sigma(z^{[2](i)})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-7",
    "href": "dl_lec3.html#deep-networks-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we vectorize it across multiple training examples? We compute by going to matrices: \\[\\begin{align*}\n  &Z^{[1]} = W^{[1]}X+b^{[1]},\\; &A^{[1]} = \\sigma(Z^{[1]}),\\\\\n  &Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]},\\; &A^{[2]} = \\sigma(Z^{[2]})\n\\end{align*}\\]\n\\[\\begin{align*}\n&Z^{[1]} =\\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  z^{[1](1)} & z^{[1](2)} & \\dots & z^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix},\\\\\n&A^{[1]} = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  a^{[1](1)} & a^{[1](2)} & \\dots & a^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#activation-functions",
    "href": "dl_lec3.html#activation-functions",
    "title": "Deep learning: multi-layer NNs",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\n\n\n\n\n\n\n\nSigmoid derivative\n\n\n\\[\n\\dfrac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))\n\\]\n\n\n\n\n\n\nTanh derivative\n\n\n\\[\n\\dfrac{d\\tanh}{dz} = 1-(\\tanh(z))^2\n\\]\n\n\n\n\n\n\nReLU derivative\n\n\n\\[\\begin{align*}\n  &g'(z) = \\begin{cases}\n    0 , & \\text{if } z &lt; 0,\\\\\n    1, & \\text{if } z &gt; 0, \\\\\n    \\text{undefined}, & \\text{if } z = 0\n  \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-8",
    "href": "dl_lec3.html#deep-networks-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nParameters\n\n\nReview:\n\\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\).\n\\(n_x = n^{[0]}\\), \\(n^{[2]} = 1\\).\n\n\n\n\n\n\nCost function\n\n\nIn our case will be \\[\\begin{align*}\n  &J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})= \\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y},y).\n\\end{align*}\\]\n\n\n\nDimensions for \\(W^{[1]}\\) are \\((n^{[1]}, n^{[0]})\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-9",
    "href": "dl_lec3.html#deep-networks-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor gradient descent we compute\n\n\\(\\hat{y}^{(i)}\\)\n\\(dW^{[1]} \\equiv \\dfrac{dJ}{dW^{[1]}}, dB^{[1]} \\equiv \\dfrac{dJ}{dB^{[1]}}\\)\n\\(W^{[1]} = W^{[1]} - \\alpha dW^{[1]}\\)\n\\(B^{[1]} = B^{[1]} - \\alpha dB^{[1]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-10",
    "href": "dl_lec3.html#deep-networks-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we compute the derivatives? \\[\\begin{align*}\n  &dZ^{[2]} = A^{[2]} - Y,\\\\\n  &dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T},\\\\\n  &dB^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-11",
    "href": "dl_lec3.html#deep-networks-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nNext step: \\[\\begin{align*}\n  &dZ^{[1]} = W^{[2]T}dZ^{[2]} \\odot g^{[1]'}(Z^{[1]}),\\\\\n  &dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{T},\\\\\n  &dB^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-12",
    "href": "dl_lec3.html#deep-networks-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nForward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-13",
    "href": "dl_lec3.html#deep-networks-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nBackward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-14",
    "href": "dl_lec3.html#deep-networks-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nVectorized versions: \\[\\begin{align*}\n  &d\\vec{Z}^{[2]} = A^{[2]} - Y,\\\\\n  &d\\vec{W}^{[2]} = \\frac{1}{m}d\\vec{Z}^{[2]}\\vec{A}^{[1]T},\\\\\n  &d\\vec{b}^{[2]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[2]}, axis=1,keepdims=True),\\\\\n  &d\\vec{Z}^{[1]} = W^{[2]T}d\\vec{Z}^{[2]} \\odot g^{[1]'}(\\vec{Z}^{[1]}),\\\\\n  &d\\vec{W}^{[1]} = \\frac{1}{m}d\\vec{Z}^{[1]}\\vec{X}^{T},\\\\\n  &d\\vec{b}^{[1]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[1]}, axis=1,keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-random-initialization",
    "href": "dl_lec3.html#deep-networks-random-initialization",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks: Random Initialization",
    "text": "Deep Networks: Random Initialization\n\n\n\nSymmetry-breaking problem\n\n\nIf we initialize weights to zero, the hidden units will be symmetric. \\[\nW^{[1]} = np.random.randn((2,2))*0.01,\\\\\nb^{[1]} = np.zero((2,1))\n\\]\n\n\n\n\n\nThe 0.01 multiplier is because we don’t want to end up at flat parts of the activation function."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-15",
    "href": "dl_lec3.html#deep-networks-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\\(a^{[l]}\\) - activations in layer \\(l\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-16",
    "href": "dl_lec3.html#deep-networks-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nGeneral rule\n\n\n\\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\\), \\(a^{[l]} = g^{[l]}(z^{[l]})\\).\n\n\n\n\n\n\nVectorized versions\n\n\n\\[\\begin{align*}\n&Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}, \\\\\n&A^{[l]} = g^{[l]}(Z^{[l]})\n\\end{align*}\\]\n\n\n\n\n\n\nImportant\n\n\nFor loop is necessary for multiple layers."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-17",
    "href": "dl_lec3.html#deep-networks-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-18",
    "href": "dl_lec3.html#deep-networks-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-19",
    "href": "dl_lec3.html#deep-networks-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\nGetting your Matrix Dimensions Right\n\n\n\nDimensions of \\(W^{[l]}\\) are \\((n^{[l]}, n^{[l-1]})\\).\nDimensions of \\(b^{[l]}\\) should be \\((n^{[l]}, 1)\\).\nDimensions of dW and db should be identical to the ones for W and b.\nDimension of \\(Z^{[1]}\\) is \\((n^{[1]}, m)\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-20",
    "href": "dl_lec3.html#deep-networks-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\nIntuition from circuit theory\n\n\nSmall L-layer network requires exponentially less hidden units than shallower networks.\n\n\n\n\n\n\nExample\n\n\nTo compute XOR, we’ll need \\(O(\\log \\, n)\\) layers.\nWith a single hidden layer, we’ll need \\(2^{n-1}\\) hidden units."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-21",
    "href": "dl_lec3.html#deep-networks-21",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nForward propagation:\n\n\n\nInputs: \\(a^{[l-1]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(a^{[l]}\\)\nCache: \\(z^{[l]}\\)\n\n\n\n\n\n\n\nBackward propagation:\n\n\n\nInputs: \\(da^{[l]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(da^{[l-1]}\\)\nCache: \\(dz^{[l]}\\), \\(dW^{[l]}\\), \\(db^{[l]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-22",
    "href": "dl_lec3.html#deep-networks-22",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nBackward propagation steps:\n\n\n\\[\\begin{align*}\n  &dz^{[l]} = da^{[l]}\\odot g^{[l]'}(z^{[l]})\\\\\n  &dW^{[l]} = dz^{[l]} \\cdot (a^{[l-1]T})\\\\\n  &db^{[l]} = dz^{[l]} \\\\\n  &da^{[l-1]} = W^{[l]T} \\cdot dz^{[l]}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-23",
    "href": "dl_lec3.html#deep-networks-23",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nVectorized versions:\n\n\n\\[\\begin{align*}\n  &dZ^{[l]} = dA^{[l]}\\odot g^{[l]'}(Z^{[l]})\\\\\n  &dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot (A^{[l-1]T})\\\\\n  &db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\\\\n  &dA^{[l-1]} = W^{[l]T} \\cdot dZ^{[l]}\n\\end{align*}\\]\n\n\n\n\n\n\nFinal layer\n\n\n\\[\\begin{align*}\n  &da^{[l]} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#parameters-vs-hyperparameters",
    "href": "dl_lec3.html#parameters-vs-hyperparameters",
    "title": "Deep learning: multi-layer NNs",
    "section": "Parameters vs Hyperparameters",
    "text": "Parameters vs Hyperparameters\n\n\n\nParameters\n\n\n\n\\(W^{[i]}\\)\n\\(b^{[i]}\\)\n\n\n\n\n\n\n\nHyperparameters\n\n\n\nlearning rate\nn of iterations\nn of hidden layers (\\(L\\))\nn of hidden units (\\(n^{[i]}\\))\nchoice of activation functions"
  },
  {
    "objectID": "dl_lec3.html#backpropagation",
    "href": "dl_lec3.html#backpropagation",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align}\nz_{j, i}^{[l]} &= \\sum_k w_{j, k}^{[l]} a_{k, i}^{[l - 1]} + b_j^{[l]}, \\label{eq:z_scalar} \\\\\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}). \\label{eq:a_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-1",
    "href": "dl_lec3.html#backpropagation-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\n\nEntity\nDescription\n\n\n\n\n\\(l\\)\nThe current layer \\(l = 1, \\dots, L\\)\n\n\n\\(n^{[l]}\\)\nThe number of nodes in the current layer\n\n\n\\(n^{[l - 1]}\\)\nThe number of nodes in the previous layer\n\n\n\\(j\\)\nThe \\(j\\)-th node of the current layer, \\(j = 1, \\dots, n^{[l]}\\)\n\n\n\\(k\\)\nThe \\(k\\)-th node of the previous layer,\\(k = 1, \\dots, n^{[l - 1]}\\)\n\n\n\\(i\\)\nThe current training example \\(i = 1, \\dots, m\\)\n\n\n\\(z_{j, i}^{[l]}\\)\nA weighted sum of the activations of the previous layer"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-2",
    "href": "dl_lec3.html#backpropagation-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\n\nEntity\nDescription\n\n\n\n\n\\(w_{j, k}^{[l]}\\)\nA weight that scales the \\(k\\)-th activation of the previous layer\n\n\n\\(b_j^{[l]}\\)\nA bias in the current layer\n\n\n\\(a_{j, i}^{[l]}\\)\nAn activation in the current layer\n\n\n\\(a_{k, i}^{[l - 1]}\\)\nAn activation in the previous layer\n\n\n\\(g_j^{[l]}\\)\nAn activation function \\(g_j^{[l]} \\colon \\mathbb{R}^{n^{[l]}} \\to \\mathbb{R}\\)"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-3",
    "href": "dl_lec3.html#backpropagation-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe vectorize the nodes:\n\\[\\begin{align*}\n\\begin{bmatrix}\nz_{1, i}^{[l]} \\\\\n\\vdots \\\\\nz_{j, i}^{[l]} \\\\\n\\vdots \\\\\nz_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{1, k}^{[l]} & \\dots & w_{1, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{j, 1}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{n^{[l]}, 1}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix}\n\n\\begin{bmatrix}\na_{1, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{k, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{n^{[l - 1]}, i}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_1^{[l]} \\\\\n\\vdots \\\\\nb_j^{[l]} \\\\\n\\vdots \\\\\nb_{n^{[l]}}^{[l]}\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-4",
    "href": "dl_lec3.html#backpropagation-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align*}\n\\begin{bmatrix}\na_{1, i}^{[l]} \\\\\n\\vdots \\\\\na_{j, i}^{[l]} \\\\\n\\vdots \\\\\na_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\ng_1^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_{n^{[l]}}^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-5",
    "href": "dl_lec3.html#backpropagation-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nwhich we can write as\n\\[\\begin{align}\n\\vec{z}_{:, i}^{[l]} &= \\vec{W}^{[l]} \\vec{a}_{:, i}^{[l - 1]} + \\vec{b}^{[l]}, \\label{eq:z} \\\\\n\\vec{a}_{:, i}^{[l]} &= \\vec{g}^{[l]}(\\vec{z}_{:, i}^{[l]}), \\label{eq:a}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{z}_{:, i}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\,\n  &\\vec{W}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l - 1]}}, \\,\n  &\\vec{b}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\\\\n  &\\vec{a}_{:, i}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\;\n  &\\vec{a}_{:, i}^{[l - 1]} \\in \\mathbb{R}^{n^{[l - 1]}}, \\;\n  &\\vec{g}^{[l]} \\colon \\mathbb{R}^{n^{[l]}} \\to \\mathbb{R}^{n^{[l]}}.\n\\end{align*}\\]\n\n\nWe have used a colon to clarify that \\(\\vec{z}_{:, i}^{[l]}\\) is the \\(i\\)-th column of \\(\\vec{Z}^{[l]}\\), and so on."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-6",
    "href": "dl_lec3.html#backpropagation-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nNext, we vectorize the training examples:\n\\[\\begin{align}\n\\vec{Z}^{[l]} &=\n\\begin{bmatrix}\n\\vec{z}_{:, 1}^{[l]} & \\dots & \\vec{z}_{:, i}^{[l]} & \\dots & \\vec{z}_{:, m}^{[l]}\n\\end{bmatrix} \\label{eq:Z} \\\\\n&= \\vec{W}^{[l]}\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l - 1]} & \\dots & \\vec{a}_{:, i}^{[l - 1]} & \\dots & \\vec{a}_{:, m}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]}\n\\end{bmatrix} \\notag \\\\\n&= \\vec{W}^{[l]} \\vec{A}^{[l - 1]} + \\text{broadcast}(\\vec{b}^{[l]}), \\notag \\\\\n\\vec{A}^{[l]} &=\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l]} & \\dots & \\vec{a}_{:, i}^{[l]} & \\dots & \\vec{a}_{:, m}^{[l]}\n\\end{bmatrix}, \\label{eq:A}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{Z}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l - 1]} \\in \\mathbb{R}^{n^{[l - 1]} \\times m}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-7",
    "href": "dl_lec3.html#backpropagation-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nNumpy broadcasting\n\n\nSmaller array is “broadcast” across the larger array so that they have compatible shapes.\nBroadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-8",
    "href": "dl_lec3.html#backpropagation-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe would also like to establish two additional notations:\n\\[\\begin{align}\n\\vec{A}^{[0]} &= \\vec{X}, \\label{eq:A_zero} \\\\\n\\vec{A}^{[L]} &= \\vec{\\hat{Y}}, \\label{eq:A_L}\n\\end{align}\\]\nwhere \\(\\vec{X} \\in \\mathbb{R}^{n^{[0]} \\times m}\\) denotes the inputs and \\(\\vec{\\hat{Y}} \\in \\mathbb{R}^{n^{[L]} \\times m}\\) denotes the predictions/outputs.\nFinally, we are ready to define the cost function:\n\\[\\begin{equation}\nJ = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}), \\label{eq:J}\n\\end{equation}\\]\nwhere \\(\\vec{Y} \\in \\mathbb{R}^{n^{[L]} \\times m}\\) denotes the targets and \\(f \\colon \\mathbb{R}^{2 n^{[L]}} \\to \\mathbb{R}\\) can be tailored to our needs."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-9",
    "href": "dl_lec3.html#backpropagation-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\n\\def\\pdv#1#2{\\frac{\\partial #1}{\\partial #2}}\n\\def\\dpdv#1#2{\\frac{\\partial #1}{\\partial #2}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\peq}{\\phantom{=}}\n\\]\n\n\n\nChain rule\n\n\n\\[\\begin{align}\nu_i &= g_i(x_1, \\dots, x_j, \\dots, x_n), \\label{eq:example_u_scalar} \\\\\ny_k &= f_k(u_1, \\dots, u_i, \\dots, u_m). \\label{eq:example_y_scalar}\n\\end{align}\\]\n\\[\\begin{equation}\n\\pdv{y_k}{x_j} = \\sum_i \\pdv{y_k}{u_i} \\pdv{u_i}{x_j}. \\label{eq:chain_rule}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-10",
    "href": "dl_lec3.html#backpropagation-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nLet’s write out first derivatives of \\(J\\) with respect to parameters \\(w\\) and \\(b\\):\n\\[\\begin{align}\n\\pdv{J}{w_{j, k}^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{w_{j, k}^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}} a_{k, i}^{[l - 1]}, \\label{eq:dw_scalar} \\\\\n\\pdv{J}{b_j^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{b_j^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}}. \\label{eq:db_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-11",
    "href": "dl_lec3.html#backpropagation-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nVectorization results in\n\\[\\begin{align*}\n&\n\\begin{bmatrix}\n\\dpdv{J}{w_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{1, k}^{[l]}} & \\dots & \\dpdv{J}{w_{1, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{j, k}^{[l]}} & \\dots & \\dpdv{J}{w_{j, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, k}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, n^{[l - 1]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix} \\notag \\\\\n&\\peq{} \\cdot\n\\begin{bmatrix}\na_{1, 1}^{[l - 1]} & \\dots & a_{k, 1}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, 1}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, i}^{[l - 1]} & \\dots & a_{k, i}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, i}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, m}^{[l - 1]} & \\dots & a_{k, m}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, m}^{[l - 1]}\n\\end{bmatrix}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-12",
    "href": "dl_lec3.html#backpropagation-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\dpdv{J}{b_1^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_j^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_{n^{[l]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-13",
    "href": "dl_lec3.html#backpropagation-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align}\n\\pdv{J}{\\vec{W}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} \\vec{a}_{:, i}^{[l - 1]T} = \\pdv{J}{\\vec{Z}^{[l]}} \\vec{A}^{[l - 1]^T}, \\label{eq:dW} \\\\\n\\pdv{J}{\\vec{b}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\underbrace{\\sum_{\\text{axis} = 1} \\pdv{J}{\\vec{Z}^{[l]}}}_\\text{column vector}, \\label{eq:db}\n\\end{align}\\] where \\(\\pdv{J}{\\vec{z}_{:, i}^{[l]}} \\in \\R^{n^{[l]}}\\), \\(\\pdv{J}{\\vec{Z}^{[l]}} \\in \\R^{n^{[l]} \\times m}\\), \\(\\pdv{J}{\\vec{W}^{[l]}} \\in \\R^{n^{[l]} \\times n^{[l - 1]}}\\), and \\(\\pdv{J}{\\vec{b}^{[l]}} \\in \\R^{n^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-14",
    "href": "dl_lec3.html#backpropagation-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nLooking back at \\(\\eqref{eq:dw_scalar}\\) and \\(\\eqref{eq:db_scalar}\\), we see that the only unknown entity is \\(\\pdv{J}{z_{j, i}^{[l]}}\\). By applying the chain rule once again, we get\n\\[\\begin{equation}\n\\pdv{J}{z_{j, i}^{[l]}} = \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}}, \\label{eq:dz_scalar}\n\\end{equation}\\]\nwhere \\(p = 1, \\dots, n^{[l]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-15",
    "href": "dl_lec3.html#backpropagation-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nNext, we present the vectorized version of \\(\\eqref{eq:dz_scalar}\\):\n\\[\\begin{equation*}\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{a_{1, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{1, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix},\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-16",
    "href": "dl_lec3.html#backpropagation-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWhich can be converted into\n\\[\\begin{equation}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\pdv{\\vec{a}_{:, i}^{[l]}}{\\vec{z}_{:, i}^{[l]}} \\pdv{J}{\\vec{a}_{:, i}^{[l]}}, \\label{eq:dz}\n\\end{equation}\\]\nwhere \\(\\pdv{J}{\\vec{a}_{:, i}^{[l]}} \\in \\R^{n^{[l]}}\\) and \\(\\pdv{\\vec{a}_{:, i}^{[l]}}{\\vec{z}_{:, i}^{[l]}} \\in \\R^{n^{[l]} \\times n^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-17",
    "href": "dl_lec3.html#backpropagation-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe have already encountered\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{z}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dZ}\n\\end{equation}\\]\nand for the sake of completeness, we also clarify that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{a}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dA}\n\\end{equation}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l]}} \\in \\R^{n^{[l]} \\times m}\\).\nOn purpose, we have omitted the details of \\(g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]})\\); consequently, we cannot derive an analytic expression for \\(\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\), which we depend on in \\(\\eqref{eq:dz_scalar}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-18",
    "href": "dl_lec3.html#backpropagation-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nFurthermore, according to \\(\\eqref{eq:dz_scalar}\\), we see that \\(\\pdv{J}{z_{j, i}^{[l]}}\\) also depends on \\(\\pdv{J}{a_{j, i}^{[l]}}\\).\n\\(\\pdv{J}{a_{j, i}^{[l]}}\\) has already been computed when we reach the \\(l\\)-th layer during backward propagation.\nHow? Each layer paves the way for the previous layer by also computing \\(\\pdv{J}{a_{k, i}^{[l - 1]}}\\), which we shall do now:\n\\[\\begin{equation}\n\\pdv{J}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} w_{j, k}^{[l]}. \\label{eq:da_prev_scalar}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-19",
    "href": "dl_lec3.html#backpropagation-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\\[\\begin{equation*}\n\\begin{split}\n&\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{k, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{n^{[l - 1]}, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, m}^{[l - 1]}}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{j, 1}^{[l]} & \\dots & w_{n^{[l]}, 1}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, k}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, n^{[l - 1]}}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-20",
    "href": "dl_lec3.html#backpropagation-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nwhich we can write as \\[\\begin{align}\n&\\pdv{J}{\\vec{A}^{[l - 1]}} =\\vec{W}^{[l]T} \\pdv{J}{\\vec{Z}^{[l]}}, \\label{eq:dA_prev}\n\\end{align}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l - 1]}} \\in \\R^{n^{[l - 1]} \\times m}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-21",
    "href": "dl_lec3.html#backpropagation-21",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nInitial values\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}\\)\nBackward: \\(\\pdv{J}{\\vec{A}^{[L]}} = \\pdv{J}{\\vec{\\hat{Y}}}\\)\n\n\n\n\n\n\n\nComputations\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}, \\vec{A}^{[L]} = \\vec{\\hat{Y}}, J = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y})\\)\nBackward: \\(\\dfrac{\\partial J}{\\partial \\vec{A}^{[L]}} = \\dfrac{\\partial J}{\\partial \\vec{\\hat{Y}}}, \\dfrac{\\partial J}{\\partial \\vec{W}^{[l]}}, \\dfrac{\\partial J}{\\partial \\vec{b}^{[l]}}\\)"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-22",
    "href": "dl_lec3.html#backpropagation-22",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\nBackpropagation seed\n\n\nWe have yet to derive an analytic expression for the backpropagation seed \\(\\dpdv{J}{\\vec{A}^{[L]}} = \\dpdv{J}{\\vec{\\hat{Y}}}\\).\n\n\n\nLet’s derive an analytic expression for \\(\\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\) or, by extension, \\(\\dpdv{J}{z_{j, i}^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu",
    "href": "dl_lec3.html#backpropagation-relu",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nThe rectified linear unit, or ReLU for short, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\max(0, z_{j, i}^{[l]}) = \\\\\n&=\n\\begin{cases}\nz_{j, i}^{[l]} &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\max(0, \\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu-1",
    "href": "dl_lec3.html#backpropagation-relu-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nCompute the partial derivatives of the activations in the current layer:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &:=\n\\begin{cases}\n1 &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise,}\n\\end{cases} \\\\\n&= I(z_{j, i}^{[l]} &gt; 0), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= 0, \\quad \\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu-2",
    "href": "dl_lec3.html#backpropagation-relu-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} I(z_{j, i}^{[l]} &gt; 0),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot I(\\vec{Z}^{[l]} &gt; 0),\n\\end{equation}\\]\nwhere \\(\\odot\\) denotes element-wise multiplication (Hadamard product)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid",
    "href": "dl_lec3.html#backpropagation-sigmoid",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nThe sigmoid activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\sigma(z_{j, i}^{[l]}) = \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization yields\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{1 + \\exp(-\\vec{Z}^{[l]})}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-1",
    "href": "dl_lec3.html#backpropagation-sigmoid-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nTo practice backward propagation, first, we construct a computation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-2",
    "href": "dl_lec3.html#backpropagation-sigmoid-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nCompute, starting from outside:\n\\[\\begin{align*}\n&\\pdv{a_{j, i}^{[l]}}{u_4} = 1, \\; \\pdv{a_{j, i}^{[l]}}{u_3} = \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_2} = \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_2} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_1} = \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_1} = -\\frac{1}{u_3^2} \\exp(u_1) = -\\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_0} = \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} = \\frac{1}{u_3^2} \\exp(u_1) = \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-3",
    "href": "dl_lec3.html#backpropagation-sigmoid-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nLet us simplify:\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{1 + \\exp(-z_{j, i}^{[l]}) - 1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})} - \\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-4",
    "href": "dl_lec3.html#backpropagation-sigmoid-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nWe also note that\n\\[\\begin{equation*}\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\n\\end{equation*}\\]\nConsequently,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-5",
    "href": "dl_lec3.html#backpropagation-sigmoid-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nLastly, no summations mean trivial vectorization:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]} \\odot (1 - \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh",
    "href": "dl_lec3.html#backpropagation-tanh",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nThe hyperbolic tangent function, i.e., the tanh activation function, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\tanh(z_{j, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nBy utilizing element-wise multiplication, we get\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\exp(\\vec{Z}^{[l]}) + \\exp(-\\vec{Z}^{[l]})} \\odot (\\exp(\\vec{Z}^{[l]}) - \\exp(-\\vec{Z}^{[l]})).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-1",
    "href": "dl_lec3.html#backpropagation-tanh-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nComputation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-2",
    "href": "dl_lec3.html#backpropagation-tanh-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nWe compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_7} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_6} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_6} = u_4 = \\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_5} &= \\pdv{a_{j, i}^{[l]}}{u_6} \\pdv{u_6}{u_5} = -u_4 \\frac{1}{u_5^2} = -\\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_4} = u_6 = \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-3",
    "href": "dl_lec3.html#backpropagation-tanh-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_3} \\\\\n&= -u_6 - u_4 \\frac{1}{u_5^2} \\notag \\\\\n&= -\\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_2} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_2} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_2} = u_6 - u_4 \\frac{1}{u_5^2} =\\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-4",
    "href": "dl_lec3.html#backpropagation-tanh-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n&= \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{2 \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} \\\\\n&= \\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-5",
    "href": "dl_lec3.html#backpropagation-tanh-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_0} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} + \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_0} \\\\\n&= -\\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) + \\Bigl(u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_0) \\notag \\\\\n&= \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} + \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-6",
    "href": "dl_lec3.html#backpropagation-tanh-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})^2 + 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&\\peq\\negmedspace{} - \\frac{\\exp(z_{j, i}^{[l]})^2 - 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= 1 - \\frac{(\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}))^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} = 1 - a_{j, i}^{[l]} a_{j, i}^{[l]}.\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-7",
    "href": "dl_lec3.html#backpropagation-tanh-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nSimilarly to the sigmoid activation function: \\(\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\\)\nThus,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\left(1 - a_{j, i}^{[l]} a_{j, i}^{[l]}\\right),\n\\end{split}\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot (1 - \\vec{A}^{[l]} \\odot \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax",
    "href": "dl_lec3.html#backpropagation-softmax",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nThe softmax activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization results in\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\text{broadcast}(\\underbrace{\\sum_{\\text{axis} = 0} \\exp(\\vec{Z}^{[l]})}_\\text{row vector})} \\odot \\exp(\\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-1",
    "href": "dl_lec3.html#backpropagation-softmax-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nTo begin with, we construct a computation graph for the \\(j\\)-th activation of the current layer:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-2",
    "href": "dl_lec3.html#backpropagation-softmax-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nBy applying the chain rule, we get\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_1 = \\exp(z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_1 \\frac{1}{u_3^2} = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_1} \\\\\n&= -u_1 \\frac{1}{u_3^2} + u_4 = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{1}{\\sum_p \\exp(z_{p, i}^{[l]})}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-3",
    "href": "dl_lec3.html#backpropagation-softmax-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_{-1}} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = \\Bigl(-u_1 \\frac{1}{u_3^2} + u_4\\Bigr) \\exp(u_{-1}) \\notag \\\\\n&= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-4",
    "href": "dl_lec3.html#backpropagation-softmax-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nNext, we need to take into account that (z_{j, i}^{[l]}) also affects other activations in the same layer:\n\\[\\begin{align*}\nu_{-1} &= z_{j, i}^{[l]}, \\\\\nu_{0, p} &= z_{p, i}^{[l]}, &&\\forall p \\ne j, \\\\\nu_1 &= \\exp(u_{-1}), \\\\\nu_{2, p} &= \\exp(u_{0, p}), &&\\forall p \\ne j, \\\\\nu_3 &= u_1 + \\sum_{p \\ne j} u_{2, p}, \\\\\nu_4 &= \\frac{1}{u_3}, \\\\\nu_5 &= u_{2, p} u_4 = a_{p, i}^{[l]}, &&\\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-5",
    "href": "dl_lec3.html#backpropagation-softmax-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{align*}\n\\pdv{a_{p, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_4} &= \\pdv{a_{p, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_{2, p} = \\exp(z_{p, i}^{[l]}), \\\\\n\\pdv{a_{p, i}^{[l]}}{u_3} &= \\pdv{a_{p, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_1} &= \\pdv{a_{p, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_{-1}} &= \\pdv{a_{p, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = -u_{2, p} \\frac{1}{u_3^2} \\exp(u_{-1}) = -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-6",
    "href": "dl_lec3.html#backpropagation-softmax-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nWe now know that\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})} \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} \\\\\n&= -a_{p, i}^{[l]} a_{j, i}^{[l]}, \\quad \\forall p \\ne j. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-7",
    "href": "dl_lec3.html#backpropagation-softmax-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} a_{j, i}^{[l]} \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} + \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr),\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-8",
    "href": "dl_lec3.html#backpropagation-softmax-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nVectorized version: \\[\\begin{equation*}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\vec{a}_{:, i}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{a}_{:, i}^{[l]}} - \\underbrace{{\\vec{a}_{:, i}^{[l]}}^T \\pdv{J}{\\vec{a}_{:, i}^{[l]}}}_{\\text{scalar}}\\Bigr).\n\\end{equation*}\\]\nLet us not stop with the vectorization just yet:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\vec{A}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{A}^{[l]}} - \\text{broadcast}\\bigl(\\underbrace{\\sum_{\\text{axis} = 0} \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]}}_\\text{row vector}\\bigr)\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost",
    "href": "dl_lec3.html#backpropagation-binary-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nIn binary classification, the cost function is given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(a_i^{[L]}) + (1 - y_i) \\log(1 - a_i^{[L]})),\n\\end{split}\n\\end{equation*}\\]\nwhich we can write as\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\text{axis} = 1} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-1",
    "href": "dl_lec3.html#backpropagation-binary-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nNext, we construct a computation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-2",
    "href": "dl_lec3.html#backpropagation-binary-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nLet’s compute derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_5} &= 1, \\; \\pdv{J}{u_{4, i}} = \\pdv{J}{u_5} \\pdv{u_5}{u_{4, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{3, i}} = -\\frac{1}{m} (1 - y_i), \\\\\n\\pdv{J}{u_{2, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{2, i}} = -\\frac{1}{m} y_i, \\\\\n\\pdv{J}{u_{1, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{1, i}} = -\\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} = -\\frac{1}{m} \\frac{1 - y_i}{1 - a_i^{[L]}}, \\\\\n\\pdv{J}{u_{0, i}} &= \\pdv{J}{u_{1, i}} \\pdv{u_{1, i}}{u_{0, i}} + \\pdv{J}{u_{2, i}} \\pdv{u_{2, i}}{u_{0, i}} = \\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} - \\frac{1}{m} y_i \\frac{1}{u_{0, i}} = \\\\\n&=\\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-3",
    "href": "dl_lec3.html#backpropagation-binary-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nThus,\n\\[\\begin{equation*}\n\\pdv{J}{a_i^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr),\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-4",
    "href": "dl_lec3.html#backpropagation-binary-cost-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nIn addition, since the sigmoid activation function is used in the output layer, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_i^{[L]}} &= \\pdv{J}{a_i^{[L]}} a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr) a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_i) a_i^{[L]} - y_i (1 - a_i^{[L]})) \\\\\n&= \\frac{1}{m} (a_i^{[L]} - y_i).\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]\nNote that both \\(\\pdv{J}{\\vec{Z}^{[L]}} \\in \\R^{1 \\times m}\\) and \\(\\pdv{J}{\\vec{A}^{[L]}} \\in \\R^{1 \\times m}\\), because \\(n^{[L]} = 1\\) in this case."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost",
    "href": "dl_lec3.html#backpropagation-multiclass-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nIn multiclass classification, the cost function is instead given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(\\hat{y}_{j, i}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(a_{j, i}^{[L]}),\n\\end{split}\n\\end{equation*}\\]\nwhere \\(j = 1, \\dots, n^{[L]}\\). We can vectorize the cost expression:\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 0 \\\\ \\text{axis} = 1}} \\vec{Y} \\odot \\log(\\vec{A}^{[L]})}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-1",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-cost",
    "href": "dl_lec3.html#backpropagation-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: cost",
    "text": "Backpropagation: cost\nWith the computation graph in place, we can perform backward propagation:\n\\[\\begin{align*}\n\\pdv{J}{u_4} &= 1, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_4} \\pdv{u_4}{u_{3, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{2, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{1, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} = -\\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-2",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nHence,\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{equation*}\\]\nVectorization is trivial:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = -\\frac{1}{m} \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-3",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nFurthermore, since the output layer uses the softmax activation function, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= a_{j, i}^{[L]} \\Bigl(\\pdv{J}{a_{j, i}^{[L]}} - \\sum_p \\pdv{J}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) = a_{j, i}^{[L]} \\Bigl(-\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}} + \\sum_p \\frac{1}{m} \\frac{y_{p, i}}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) \\\\\n&= \\frac{1}{m} \\Bigl(-y_{j, i} + a_{j, i}^{[L]} \\underbrace{\\sum_p y_{p, i}}_{\\sum \\text{probabilities} = 1}\\Bigr) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}).\n\\end{split}\n\\end{equation*}\\]\nNote that \\(p = 1, \\dots, n^{[L]}\\). To conclude,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost",
    "href": "dl_lec3.html#backpropagation-multilabel-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nWe can view multi-label classification as \\(j\\) binary classification problems:\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(\\hat{y}_{j, i}) + (1 - y_{j, i}) \\log(1 - \\hat{y}_{j, i}))\\Bigr) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(a_{j, i}^{[L]}) + (1 - y_{j, i}) \\log(1 - a_{j, i}^{[L]}))\\Bigr),\n\\end{split}\n\\end{equation*}\\]\nwhere once again \\(j = 1, \\dots, n^{[L]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-1",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nVectorization gives\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 1 \\\\ \\text{axis} = 0}} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-2",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-3",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nNext, we compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_6} &= 1, \\\\\n\\pdv{J}{u_{5, j}} &= \\pdv{J}{u_6} \\pdv{u_6}{u_{5, j}} = 1, \\\\\n\\pdv{J}{u_{4, j, i}} &= \\pdv{J}{u_{5, j}} \\pdv{u_{5, j}}{u_{4, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{3, j, i}} = -\\frac{1}{m} (1 - y_{j, i}), \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{2, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{3, j, i}} \\pdv{u_{3, j, i}}{u_{1, j, i}} = -\\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} = -\\frac{1}{m} \\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-4",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\n\\[\\begin{align*}\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} + \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{0, j, i}} \\\\\n&= \\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} - \\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} \\notag \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-5",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nSimply put, we have\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr),\n\\end{equation*}\\]\nand\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-6",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nBearing in mind that we view multi-label classification as \\(j\\) binary classification problems, we also know that the output layer uses the sigmoid activation function.\nAs a result,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= \\pdv{J}{a_{j, i}^{[L]}} a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr) a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_{j, i}) a_{j, i}^{[L]} - y_{j, i} (1 - a_{j, i}^{[L]})) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lab2.html",
    "href": "dl_lab2.html",
    "title": "DL: Lab 2",
    "section": "",
    "text": "Lab overview\nWe’ll work with a planar data classifier using a two-layer neural network (using one hidden layer.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec2.html#neurons",
    "href": "dl_lec2.html#neurons",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nImportant\n\n\nANN \\(\\equiv\\) Artificial Neural Network\nRemember - Neurons, axons, dendrites.\n\n\n\n\n\n\nTip\n\n\nInputs to neurons are scaled with weight.\nWeight is similar to a strength of synaptic connection."
  },
  {
    "objectID": "dl_lec2.html#neurons-1",
    "href": "dl_lec2.html#neurons-1",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons"
  },
  {
    "objectID": "dl_lec2.html#neurons-2",
    "href": "dl_lec2.html#neurons-2",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nComputation\n\n\nANN computes a function of the inputs by propagating the computed values from input neurons to output neurons, using weights as intermediate parameters.\n\n\n\n\n\n\nLearning\n\n\nLearning occurs by changing the weights. External stimuli are required for learning in bio-organisms, in case of ANNs they are provided by the training data.\n\n\n\n\n\n\nTraining\n\n\nTraining data contain input-output pairs. We compare predicted output with annotated output label from training data."
  },
  {
    "objectID": "dl_lec2.html#neurons-3",
    "href": "dl_lec2.html#neurons-3",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nErrors\n\n\nErrors are comparison failures. These are similar to unpleasant feedback modifying synaptic strengths. Goal of changing weights - make predictions better.\n\n\n\n\n\n\nModel generalizations\n\n\nAbility to compute functions of unseen inputs accurately, even though given finite sets of input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#computation-graph",
    "href": "dl_lec2.html#computation-graph",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAlternative view - computation graph.\nWhen used in basic graph, NNs reduce to classical ML models.\n\nLeast-squares regression\nlogistic regression\nlinear regression\n\nNodes compute based on inputs and weights."
  },
  {
    "objectID": "dl_lec2.html#goal",
    "href": "dl_lec2.html#goal",
    "title": "Deep learning: logistic regression",
    "section": "Goal",
    "text": "Goal\nGoal of NN: learn a function that relates inputs to outputs with the use of training examples.\nSettings the edge weights is training."
  },
  {
    "objectID": "dl_lec2.html#structure",
    "href": "dl_lec2.html#structure",
    "title": "Deep learning: logistic regression",
    "section": "Structure",
    "text": "Structure\nConsider a simple case of \\(d\\) inputs and a single binary output. \\[\n(\\overline{X}, y) - \\text{training instance}\n\\]\nFeature variables: \\[\n\\overline{X}=[x_1, \\dots, x_d]\n\\] Observed value: \\(y \\in {0,1}\\), contained in target variable \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#objective",
    "href": "dl_lec2.html#objective",
    "title": "Deep learning: logistic regression",
    "section": "Objective",
    "text": "Objective\n\nlearn the function \\(f(\\cdot)\\), such that \\(y=f_{\\overline{W}}(\\overline{X})\\).\nminimize mismatch between \\(y\\) and \\(f_{\\overline{W}}(\\overline{X})\\). \\(W\\) - weight vector.\n\nIn case of perceptron, we compute a linear function: \\[\\begin{align*}\n  &\\hat{y}=f(\\overline{X}) = sign\\left\\{\\overline{W}^T \\overline{X}^T\\right\\} =  sign\\left\\{\\sum\\limits_{i=1}^d w_i x_i\\right\\}\n\\end{align*}\\] \\(\\hat{y}\\) means value, not observed value \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#perceptron",
    "href": "dl_lec2.html#perceptron",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nA simplest NN."
  },
  {
    "objectID": "dl_lec2.html#perceptron-1",
    "href": "dl_lec2.html#perceptron-1",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe choose the basic form of the function, but strive to find some parameters.\n\nSign is an activation function\nvalue of the node is also sometimes referred to as an activation.\n\nPerceptron is a single-layer network, as input nodes are not counted."
  },
  {
    "objectID": "dl_lec2.html#perceptron-2",
    "href": "dl_lec2.html#perceptron-2",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nHow does perceptron learn? \\[\n\\overline{W} := \\overline{W} + \\alpha(y-\\hat{y})\\overline{X}^T.\n\\] So, in case when \\(y \\neq \\hat{y}\\), we can write it as \\[\n\\overline{W} := \\overline{W} + \\alpha y \\overline{X}^T.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-3",
    "href": "dl_lec2.html#perceptron-3",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe can show that perceptron works when data are linearly separable by a hyperplane \\(\\overline{W}^T X = 0\\).\n\n\n\nPerceptron algorithm is not guaranteed to converge when data are not linearly separable."
  },
  {
    "objectID": "dl_lec2.html#bias",
    "href": "dl_lec2.html#bias",
    "title": "Deep learning: logistic regression",
    "section": "Bias",
    "text": "Bias\nBias is needed when binary class distribution is imbalanced: \\[\n\\overline{W}^T \\cdot \\sum_i \\overline{X_i}^T \\neq \\sum_i y_i\n\\] Bias can be incorporated by using a bias neuron.\n\n\n\nProblems\n\n\nIn linearly separable data sets, a nonzero weight vector \\(W\\) exists in which the \\(sign(\\overline{W}^T X) = sign(y_i)\\; \\forall (\\overline{X}_i,y_i)\\).\nHowever, the behavior of the perceptron algorithm for data that are not linearly separable is rather arbitrary."
  },
  {
    "objectID": "dl_lec2.html#loss-function",
    "href": "dl_lec2.html#loss-function",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nML algorithms are loss optimization problems, where gradient descent updates are used to minimize the loss.\nOriginal perceptron did not formally use a loss function.\nRetrospectively we can introduce it as: \\[\nL_i \\equiv \\max\\left\\{-y_i(\\overline{W}^T \\overline{X_i}\\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#loss-function-1",
    "href": "dl_lec2.html#loss-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nWe differentiate: \\[\\begin{align*}\n&\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\left[\\dfrac{\\partial L_i}{\\partial w_1}, \\dots, \\dfrac{\\partial L_i}{\\partial w_d}\\right] = \\\\\n& = \\begin{cases}\n  -y_i \\overline{X_i}, & \\text{if } sign\\{W^T X_i\\} \\neq y_i,\\\\\n  0, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-update",
    "href": "dl_lec2.html#perceptron-update",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron update",
    "text": "Perceptron update\nNegative of the vector is the direction of the fastest rate of loss reduction, hence perceptron update: \\[\\begin{align*}\n   &\\overline{W} := \\overline{W} - \\alpha\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\overline{W} + \\alpha y_i \\overline{X_i}^T.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions",
    "href": "dl_lec2.html#activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nA network with weights \\(\\overline{W}\\) and input \\(\\overline{X}\\) will have a prediction of the form \\[\\begin{align*}\n   &\\hat{y}=\\Phi\\left( \\overline{W}^T \\overline{X}\\right)\n\\end{align*}\\] where \\(\\Phi\\) denotes activation function."
  },
  {
    "objectID": "dl_lec2.html#activation-functions-1",
    "href": "dl_lec2.html#activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nIdentity aka linear activation\n\n\n\\[\n\\Phi(v) = v\n\\]\n\n\n\n\n\n\nSign function\n\n\n\\[\n\\Phi(v) = sign(v)\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-2",
    "href": "dl_lec2.html#activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nSigmoid function\n\n\n\\[\n\\Phi(v) = \\dfrac{1}{1+e^{-v}}\n\\]\n\n\n\n\n\n\ntanh\n\n\n\\[\n\\Phi(v) = \\dfrac{e^{2v}-1}{e^{2v}+1}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-3",
    "href": "dl_lec2.html#activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nActually, neuron computes two functions:"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-4",
    "href": "dl_lec2.html#activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nWe have pre-activation value and post-activation value.\n\npre-activation: linear transformation\npost-activation: nonlinear transformation"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-5",
    "href": "dl_lec2.html#activation-functions-5",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-6",
    "href": "dl_lec2.html#activation-functions-6",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-7",
    "href": "dl_lec2.html#activation-functions-7",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nTwo more functions that have become popular recently:\n\n\n\nRectified Linear Unit (ReLU)\n\n\n\\[\n\\Phi(v) = \\max\\left\\{v, 0\\right\\}\n\\]\n\n\n\n\n\n\nHard tanh\n\n\n\\[\n\\Phi(v) = \\max\\left\\{\\min\\left[v, 1\\right], -1 \\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#multiple-activation-fns",
    "href": "dl_lec2.html#multiple-activation-fns",
    "title": "Deep learning: logistic regression",
    "section": "Multiple activation fns",
    "text": "Multiple activation fns"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-8",
    "href": "dl_lec2.html#activation-functions-8",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nProperties:\n\nmonotonic\nsaturation at large values\nsquashing"
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function",
    "href": "dl_lec2.html#softmax-activation-function",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function\nUsed for k-way classification problems. Used in the output layer.\n\\[\\begin{align*}\n&\\Phi(v)_i = \\dfrac{\\exp(v_i)}{\\sum\\limits_{i=1}^k \\exp(v_i)}.\n\\end{align*}\\]\nSoftmax layer converts real values to probabilities."
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function-1",
    "href": "dl_lec2.html#softmax-activation-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function"
  },
  {
    "objectID": "dl_lec2.html#loss-functions",
    "href": "dl_lec2.html#loss-functions",
    "title": "Deep learning: logistic regression",
    "section": "Loss functions",
    "text": "Loss functions\n\n\n\nLeast squares regression, numeric targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = (y-\\hat{y})^2\n\\end{align*}\\]\n\n\n\n\n\n\nLogistic regression, binary targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log \\left|y/2 - 1/2 + \\hat{y}\\right|, \\{-1,+1\\}\n\\end{align*}\\]\n\n\n\n\n\n\nMultinomial logistic regression, categorical targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log (\\hat{y}_r) \\text{ - cross-entropy loss}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-1",
    "href": "dl_lec2.html#multilayer-networks-1",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nSuppose NN contains \\(p_1, \\dots, p_k\\) units in each of its \\(k\\) layers.\nThen column representations of these layers, denoted by \\(\\overline{h}_1, \\dots, \\overline{h}_k\\), have \\(p_1, \\dots, p_k\\) units.\n\nWeights between input layer and first hidden layer: matrix \\(W_1\\), sized \\(p_1 \\times d\\).\nWeights between \\(r\\)-th layer and \\(r+1\\)-th layer: matrix \\(W_r\\) sized \\(p_{r+1}\\times p_r\\)."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-2",
    "href": "dl_lec2.html#multilayer-networks-2",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-3",
    "href": "dl_lec2.html#multilayer-networks-3",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nTherefore, a \\(d\\)-dimensional input vector \\(\\overline{x}\\) is transformed into the outputs using these equations: \\[\\begin{align*}\n  &\\overline{h}_1 = \\Phi(W_1^T x),\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1}^T \\overline{h}_p), \\forall p \\in \\left\\{1 \\dots k-1 \\right\\} \\\\\n  &\\overline{o} = \\Phi(W_{k+1}^T \\overline{h}_k)\n\\end{align*}\\] Activation functions operate on vectors and are applied element-wise."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-4",
    "href": "dl_lec2.html#multilayer-networks-4",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\n\n\n\nDefinition (Aggarwal)\n\n\nA multilayer network computes a nested composition of parameterized multi-variate functions.\nThe overall function computed from the inputs to the outputs can be controlled very closely by the choice of parameters.\nThe notion of learning refers to the setting of the parameters to make the overall function consistent with observed input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-5",
    "href": "dl_lec2.html#multilayer-networks-5",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nInput-output function of NN is difficult to express explicitly. NN can also be called universal function approximators.\n\n\n\nUniversal approximation theorem\n\n\nGiven a family of neural networks, for each function \\(\\displaystyle f\\) from a certain function space, there exists a sequence of neural networks \\(\\phi_1,\\phi_2,\\dots\\) from the family, such that \\(\\phi_{n} \\to f\\) according to some criterion.\n\n\n\nIn other words, the family of neural networks is dense in the function space.\n\n\nK. Hornik, M. Stinchcombe, and H. White. . Neural Networks, 2(5), pp. 359–366, 1989."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions",
    "href": "dl_lec2.html#nonlinear-activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nTheorem. A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network.\nProof. Consider a network containing \\(k\\) hidden layers, therefore containing a total of \\((k+1)\\) computational layers (including the output layer).\nThe corresponding \\((k+1)\\) weight matrices between successive layers are denoted by \\(W_1 ...W_{k+1}\\)."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-1",
    "href": "dl_lec2.html#nonlinear-activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nLet:\n\n\\(\\overline{x}\\) be the \\(d\\)-dimensional column vector corresponding to the input\n\\(\\overline{h_1},\\dots,\\overline{h_k}\\) be the column vectors corresponding to the hidden layers\nand \\(\\overline{o}\\) be the \\(m\\)-dimensional column vector corresponding to the output."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-2",
    "href": "dl_lec2.html#nonlinear-activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nThen, we have the following recurrence condition for multi-layer networks: \\[\\begin{align*}\n  &\\overline{h_1} = \\Phi(W_1 x) = W_1 x,\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1} \\overline{h}_p) = W_{p+1}\\overline{h}_p \\;\\; \\forall p \\in \\left\\{1 \\dots k−1\\right\\}, \\\\\n  &\\overline{o} = \\Phi(W_{k+1} \\overline{h}_k) = W_{k+1} \\overline{h}_k.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-3",
    "href": "dl_lec2.html#nonlinear-activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn all the cases above, the activation function \\(\\Phi(\\cdot)\\) has been set to the identity function. Then, by eliminating the hidden layer variables, we obtain the following: \\[\\begin{align*}\n&\\overline{o} = W_{k+1}W_k \\dots W_1 \\overline{x}\n\\end{align*}\\] Denote \\(W_{xo}=W_{k+1}W_k \\dots W_1\\).\n\n\n\nNote\n\n\nOne can replace the matrix \\(W_{k+1}W_k \\dots W_1\\) with the new \\(d\\times m\\) matrix \\(W_{xo}\\), and learn the coefficients of \\(W_{xo}\\) instead of those of all the matrices \\(W_1, W_2, \\dots W_{k+1}\\), without loss of expressivity."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-4",
    "href": "dl_lec2.html#nonlinear-activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn other words, we have the following: \\[\\begin{align*}\n&\\overline{o} = W_{xo} \\overline{x}\n\\end{align*}\\] However, this condition is exactly identical to that of linear regression with multiple outputs. Therefore, a multilayer neural network with identity activations does not gain over a single-layer network in terms of expressivity.\n\n\n\nLinearity observation\n\n\nThe composition of linear functions is always a linear function. The repeated composition of simple nonlinear functions can be a very complex nonlinear function."
  },
  {
    "objectID": "dl_lec2.html#backpropagation",
    "href": "dl_lec2.html#backpropagation",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nDAG Definition\n\n\nA directed acyclic computational graph is a directed acyclic graph of nodes, where each node contains a variable. Edges might be associated with learnable parameters.\nA variable in a node is either fixed externally (for input nodes with no incoming edges), or it is a computed as a function of the variables in the tail ends of edges incoming into the node and the learnable parameters on the incoming edges.\n\n\n\nDAG is a more general version of NN."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-1",
    "href": "dl_lec2.html#backpropagation-1",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\nA computational graph evaluates compositions of functions.\nA path of length 2 in a computational graph in which the function \\(f(\\cdot)\\) follows \\(g(\\cdot)\\) can be considered a composition function \\(f(g(\\cdot))\\).\nIn case of sigmoid function: \\[\\begin{align*}\n   &f(x) = g(x) = \\dfrac{1}{1+e^{-x}} \\\\\n   &f(g(x)) = \\dfrac{1}{1 + e^{\\left[-\\dfrac{1}{1+e^{-x}}\\right]}}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#backpropagation-2",
    "href": "dl_lec2.html#backpropagation-2",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nImportant\n\n\nThe inability to easily express the optimization function in closed form in terms of the edge-specific parameters (as is common in all machine learning problems) causes difficulties in computing the derivatives needed for gradient descent.\n\n\n\n\n\n\nExample\n\n\nFor example, if we have a computational graph which has 10 layers, and 2 nodes per layer, the overall composition function would have \\(2^{10}\\) nested “terms”."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-3",
    "href": "dl_lec2.html#backpropagation-3",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nDerivatives of the output with respect to various variables in the computational graph are related to one another with the use of the chain rule of differential calculus.\nTherefore, the chain rule of differential calculus needs to be applied repeatedly to update derivatives of the output with respect to the variables in the computational graph.\nThis approach is referred to as the backpropagation algorithm, because the derivatives of the output with respect to the variables close to the output are simpler to compute (and are therefore computed first while propagating them backwards towards the inputs).\n\nDerivatives are computed numerically, not algebraically."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-4",
    "href": "dl_lec2.html#backpropagation-4",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nForward phase\n\n\n\nUse the attribute values from the input portion of a training data point to fix the values in the input nodes.\nSelect a node for which the values in all incoming nodes have already been computed and apply the node-specific function to also compute its variable.\nRepeat the process until the values in all nodes (including the output nodes) have been computed.\nCompute loss value if the computed and observed values mismatch."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-5",
    "href": "dl_lec2.html#backpropagation-5",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nBackward phase\n\n\n\nCompute the gradient of the loss with respect to the weights on the edges.\nDerivatives of the loss with respect to weights near the output (where the loss function is computed) are easier to compute and are computed first.\nThe derivatives become increasingly complex as we move towards edge weights away from the output (in the backwards direction) and the chain rule is used repeatedly to compute them.\nUpdate the weights in the negative direction of the gradient.\n\n\n\n\nSingle cycle through all training points is an epoch."
  },
  {
    "objectID": "dl_lec2.html#inputs",
    "href": "dl_lec2.html#inputs",
    "title": "Deep learning: logistic regression",
    "section": "Inputs",
    "text": "Inputs\nLogistic regression is an algorithm for binary classification.\n\\(x \\in \\mathbb{R}^{n_x}, y \\in \\{0,1\\}\\).\n\\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\)- \\(m\\) training examples.\n\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\n\\[\\begin{align*}\n&X = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]\n\\[\nY = \\left[y^{(1)}, y^{(2)}, \\dots, y^{(m})\\right]\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression",
    "href": "dl_lec2.html#logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\(X \\in \\mathbb{R}^{n_x,m}\\).\nUsing Numpy syntax:\nX.shape = (n_x,m).\n\\(Y \\in \\mathbb{R}^{1,m}\\).\nUsing Numpy syntax:\nY.shape = (1,m)."
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-1",
    "href": "dl_lec2.html#logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nGoal\n\n\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\), where \\(x \\in \\mathbb{R}^{n_x}\\).\nObviously, \\(0 \\leq \\hat{y} \\leq 1\\).\n\n\n\n\n\n\nImportant\n\n\nIf doing linear regresssion, we can try \\[\n\\hat{y}=w^T x + b.\n\\]\nBut for logistic regression, we do \\[\n\\hat{y}=\\sigma(w^T x + b)$, \\; \\text{where }\\; \\sigma=\\dfrac{1}{1+e^{-z}}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#parameters",
    "href": "dl_lec2.html#parameters",
    "title": "Deep learning: logistic regression",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nInput\n\n\n\\[\nw \\in \\mathbb{R}^{n_x},\\\\\nb \\in \\mathbb{R}.\n\\]\n\n\n\n\n\n\nOutput\n\n\n\\[\n\\hat{y} = \\sigma\\left( w^T x + b\\right),\n\\]\n\\[\nz \\equiv  w^T x + b.\n\\]\n\n\n\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec2.html#graphs",
    "href": "dl_lec2.html#graphs",
    "title": "Deep learning: logistic regression",
    "section": "Graphs",
    "text": "Graphs\n\n\\(\\sigma=\\dfrac{1}{1+e^{-z}}\\)."
  },
  {
    "objectID": "dl_lec2.html#loss-function-2",
    "href": "dl_lec2.html#loss-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nFor every \\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\), we want to find \\(\\hat{y}^{(i)} \\approx y^{(i)}\\). \\[\\begin{align*}\n  &\\hat{y}^{(i)} = \\sigma\\left(w^T x^{(i)} + b\\right)\n\\end{align*}\\] We have to define a loss (error) function - this will estimate our model."
  },
  {
    "objectID": "dl_lec2.html#loss-function-3",
    "href": "dl_lec2.html#loss-function-3",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\n\n\n\nQuadratic\n\n\n\\[\nL(\\hat{y}, y) = \\dfrac{1}{2}\\left(\\hat{y}-y)\\right)^2.\n\\]\n\n\n\n\n\n\nLog\n\n\n\\[\nL(\\hat{y}, y) = -\\left((y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))\\right).\n\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function",
    "href": "dl_lec2.html#cost-function",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\n\n\n\nWhy does it work well?\n\n\nConsider \\(y=0\\) and \\(y=1\\).\n\\[\\begin{align*}\n  &y=1: P(y | x) = \\hat{y},\\\\\n  &y=0: P(y | x) = 1-\\hat{y}\n\\end{align*}\\]\nWe select \\(P(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}\\).\n\\[\n\\log P(y|x) = y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) = -L(\\hat{y}, y).\n\\]\n\\[\\begin{align*}\n  y=1:& L(\\hat{y},y) = -\\log(\\hat{y}),\\\\\n  y=0:& L(\\hat{y},y) = -\\log(1-\\hat{y})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-1",
    "href": "dl_lec2.html#cost-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nCost function show how well we’re doing across the whole training set: \\[\\begin{align*}\n&J(w, b) = \\dfrac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) = \\\\\n& = -\\dfrac{1}{m} \\sum\\limits_{i=1}^m \\left[y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-2",
    "href": "dl_lec2.html#cost-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nOn \\(m\\) examples: \\[\\begin{align*}\n  &\\log P(m \\dots) = \\log \\prod_{i=1}^m P(y^{(i)} | x^{(i)}) = \\\\\n  & = \\sum\\limits_{i=1}^m \\log P(y^{(i)} | x^{(i)}) = -\\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent",
    "href": "dl_lec2.html#gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\nProblem\n\n\nMinimization problem: find \\(w,b\\) that minimize \\(J(w,b)\\)."
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-1",
    "href": "dl_lec2.html#gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nWe use \\(J(w,b)\\) because it is convex.\nWe pick an initial point - anything might do, e.g. 0.\nThen we take steps in the direction of steepest descent.\n\n\\[\nw := w - \\alpha \\frac{d J(w,b)}{dw}, \\\\\nb := b - \\alpha \\frac{d J(w,b)}{db}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-2",
    "href": "dl_lec2.html#gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent",
    "href": "dl_lec2.html#logistic-regression-gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\\begin{align*}\n&z = w^T x + b ,\\\\\n&a \\equiv \\hat{y}  = \\sigma(z),\\\\\n&L(a,y) = -\\left[y\\log(a) + (1 - y)\\log(1 - a)\\right].\n\\end{align*}\\]\nSo, for \\(n_x=2\\) we have a computation graph:\n\\((x_1,x_2,w_1,w_2,b)\\) \\(\\rightarrow\\) \\(z =w_1 x_1+w_2 x_2 + b\\) \\(\\rightarrow\\) \\(\\hat{y}=a=\\sigma(z)\\) \\(\\rightarrow\\) \\(L(a,y)\\)."
  },
  {
    "objectID": "dl_lec2.html#computation-graph-1",
    "href": "dl_lec2.html#computation-graph-1",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nLet’s compute the derivative for \\(L\\) by a: \\[\\begin{align*}\n&\\frac{dL}{da} = -\\dfrac{y}{a} + \\dfrac{1-y}{1-a},\\\\\n&\\frac{da}{dz} = a(1-a).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#computation-graph-2",
    "href": "dl_lec2.html#computation-graph-2",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAfter computing, we’ll have \\[\\begin{align*}\n&dz \\equiv \\dfrac{dL}{dz} = \\dfrac{dL}{da}\\dfrac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 \\equiv \\frac{dL}{dw_2} = x_2 dz, \\\\\n&db \\equiv \\frac{dL}{db} = dz.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nConsider now \\(m\\) examples in the training set.\nLet’s recall the definition of the cost function: \\[\\begin{align*}\n&J(w,b) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b).\n\\end{align*}\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL(a^{(i)}, y^{(i)})}{dw_1}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nfor i=1 to m \\[\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages:\n\\[\nJ = \\dfrac{J}{m}, \\\\\ndw_1 = \\dfrac{dw_1}{m}, \\; dw_2 = \\dfrac{dw_2}{m}, \\\\\ndb = \\dfrac{db}{m}.\n\\]\n\n\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators. (In this example feature count \\(n_x=2\\))"
  },
  {
    "objectID": "dl_lec2.html#gd-step",
    "href": "dl_lec2.html#gd-step",
    "title": "Deep learning: logistic regression",
    "section": "GD step",
    "text": "GD step\n\\[\nw_1 := w_1 - \\alpha dw_1,\\\\\nw_2 := w_2 - \\alpha dw_2,\\\\\nb := b - \\alpha db.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorization",
    "href": "dl_lec2.html#vectorization",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance.\nVectorization techniques exist for this purpose - getting rid of for loops.\n\n\n\nExample\n\n\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in \\mathbb{R}^{n_x}\\), and for this we can naturally use a for loop.\nA vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec2.html#vectorization-1",
    "href": "dl_lec2.html#vectorization-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\\] To be replaced by\nu = np.dot(A, v)\n\n\nNumpy impl: https://numpy.org/doc/1.21/reference/simd/simd-optimizations.html"
  },
  {
    "objectID": "dl_lec2.html#vectorization-2",
    "href": "dl_lec2.html#vectorization-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nAnother example. Let’s say we have a vector \\[\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n\\end{align*}\\] A code listing is\nimport numpy as np\nu = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression",
    "href": "dl_lec2.html#vectorizing-logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)}),\n\\end{align*}\\]\n\\[\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-1",
    "href": "dl_lec2.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-2",
    "href": "dl_lec2.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] =\\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n\\[\nA = \\left[a^{(1)}, \\dots, a^{(m)}\\right] = \\sigma\\left(Z\\right)\n\\]\n  z = np.dot(w.T, x) + b\n\n\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-3",
    "href": "dl_lec2.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\\]\nWe now define \\[\\begin{align*}\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}] =\\\\\n&= A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-4",
    "href": "dl_lec2.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\\begin{align*}\n&db = \\frac{1}{m}np.sum(dZ),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-5",
    "href": "dl_lec2.html#vectorizing-logistic-regression-5",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop.\nfor it in range(m):\n  Z = np.dot(w.T, X) + B\n  A = sigma(Z)\n  dZ = A-Y\n  dw = 1/m X * dZ.T\n  db = 1/m np.sum(dZ)\n  w := w - alpha * dw\n  b := b - alpha * db"
  },
  {
    "objectID": "dl_lec1.html#definitions",
    "href": "dl_lec1.html#definitions",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nDefinition\n\n\nDeep learning is a branch of machine learning based on computational models called neural networks.\n\n\n\n\n\n\nWhy deep?\n\n\nBecause neural networks involved are multi-layered.\n\n\n\n\n\n\nDefinition\n\n\nNeural networks are machine learning techniques that simulate the mechanism of learning in biological organisms."
  },
  {
    "objectID": "dl_lec1.html#definitions-1",
    "href": "dl_lec1.html#definitions-1",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nAlternative definition\n\n\nNeural network is computational graph of elementary units in which greater power is gained by connecting them in particular ways.\n\n\n\nLogistic regression can be thought of as a very primitive neural network."
  },
  {
    "objectID": "dl_lec1.html#why-deep-learning",
    "href": "dl_lec1.html#why-deep-learning",
    "title": "Deep learning: intro",
    "section": "Why Deep Learning?",
    "text": "Why Deep Learning?\n\n\n\nRobust\n\n\n\nWorks on raw data (), no need for feature engineering\nRobustness to natural variations in data is automatically learned\n\n\n\n\n\n\n\nGeneralizable\n\n\n\nAllows end-to-end learning (pixels-to-category, sound to sentence, English sentence to Chinese sentence, etc)\nNo need to do segmentation etc. (a lot of manual labour)\n\n\n\n\n\n\n\nScalable\n\n\n\nPerformance increases with more data, therefore method is massively parallelizable"
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml",
    "href": "dl_lec1.html#how-is-dl-different-from-ml",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\nThe most fundamental difference between deep learning and traditional machine learning is its performance as the scale of data increases."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nIn Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.\nDeep learning algorithms try to learn high-level features from data. Therefore, deep learning reduces the task of developing new feature extractor for every problem."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nA deep learning algorithm takes a long time to train. For e.g state of the art deep learning algorithm: ResNet takes about two weeks to train completely from scratch.\nWhereas machine learning comparatively takes much less time to train, ranging from a few seconds to a few hours."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nAt test time, deep learning algorithm takes much less time to run.\nWhereas, if you compare machine learning algorithms, test time generally increases on increasing the size of data."
  },
  {
    "objectID": "dl_lec1.html#neural-network-data-types",
    "href": "dl_lec1.html#neural-network-data-types",
    "title": "Deep learning: intro",
    "section": "Neural network data types",
    "text": "Neural network data types\n\n\nUnstructured\n\nText\nImages\nAudio\n\n\nStructured\n\nCensus records\nMedical records\nFinancial data"
  },
  {
    "objectID": "dl_lec1.html#why-now",
    "href": "dl_lec1.html#why-now",
    "title": "Deep learning: intro",
    "section": "Why now?",
    "text": "Why now?\n\nstandard algorithms like logistic regression plateau after certain amount of data\nmore data in recent decades\nhardware progress\nalgorithms have improved"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology",
    "href": "dl_lec1.html#neural-network-biology",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nNeural Network: How similar is it to the human brain?"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-1",
    "href": "dl_lec1.html#neural-network-biology-1",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nSoma adds dendrite activity together and passes it to axon."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-2",
    "href": "dl_lec1.html#neural-network-biology-2",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nMore dendrite activity makes more axon activity."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-3",
    "href": "dl_lec1.html#neural-network-biology-3",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nSynapse: connection between axon of one neurons and dendrites of another"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-4",
    "href": "dl_lec1.html#neural-network-biology-4",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nAxons can connect to dendrites strongly, weakly, or somewhere in between"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-5",
    "href": "dl_lec1.html#neural-network-biology-5",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nLots of axons connect with dendrites of one neuron.Each has its own connection strength."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-6",
    "href": "dl_lec1.html#neural-network-biology-6",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nThe above illustration can be simplified as above."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-7",
    "href": "dl_lec1.html#neural-network-biology-7",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn giving numerical values to the strength of connections i.e. weights."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-8",
    "href": "dl_lec1.html#neural-network-biology-8",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nA much simplified version looks something like this."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-9",
    "href": "dl_lec1.html#neural-network-biology-9",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn increasing the number of neurons and synapses."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-10",
    "href": "dl_lec1.html#neural-network-biology-10",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\n\nAn example\n\n\nSuppose the first and third input has been activated."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-11",
    "href": "dl_lec1.html#neural-network-biology-11",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nEach node represents a pattern, a combination of neurons of the previous layers."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-12",
    "href": "dl_lec1.html#neural-network-biology-12",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology"
  },
  {
    "objectID": "dl_lec1.html#basic-ideas",
    "href": "dl_lec1.html#basic-ideas",
    "title": "Deep learning: intro",
    "section": "Basic ideas",
    "text": "Basic ideas\n\nNN is a directed acyclic graph (DAG)\nedges in a graph are parameterized with weights\none can compute any function with this graph\n\n\n\n\nGoal\n\n\nLearn a function that relates one or more inputs to one or more outputs with the use of training examples.\n\n\n\n\n\n\nHow do we construct?\n\n\nBy computing weights. This is called training."
  },
  {
    "objectID": "dl_lec1.html#perceptron",
    "href": "dl_lec1.html#perceptron",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt - the father of deep learning.\nMark I Perceptron - built in 1957. Was able to learn and recognize letters"
  },
  {
    "objectID": "dl_lec1.html#perceptron-1",
    "href": "dl_lec1.html#perceptron-1",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron"
  },
  {
    "objectID": "dl_lec1.html#evolution",
    "href": "dl_lec1.html#evolution",
    "title": "Deep learning: intro",
    "section": "Evolution",
    "text": "Evolution\nThree periods in the evolution of deep learning:\n\nsingle-layer networks (Perceptron)\nfeed-forwards NNs: differentiable activation and error functions\ndeep multi-layer NNs"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types",
    "href": "dl_lec1.html#neural-network-types",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\nFeedforward Neural Network\nRecurrent Neural Network (RNN)\nConvolutional Neural Network (CNN)"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types-1",
    "href": "dl_lec1.html#neural-network-types-1",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\n\nFeedforward Neural Network\n\nConvolutional neural network (CNN)\nAutoencoder\nProbabilistic neural network (PNN)\nTime delay neural network (TDNN)\n\n\nRecurrent Neural Network (RNN)\n\nLong short-term memory RNN (LSTM)\nFully recurrent Network\nSimple recurrent Network\nEcho state network\nBi-directional RNN\nHierarchical RNN\nStochastic neural network"
  },
  {
    "objectID": "dl_lec1.html#feed-forward",
    "href": "dl_lec1.html#feed-forward",
    "title": "Deep learning: intro",
    "section": "Feed-forward",
    "text": "Feed-forward\nFeedforward NNs: very straight forward, they feed information from the front to the back (input and output)."
  },
  {
    "objectID": "dl_lec1.html#feedforward-neural-network",
    "href": "dl_lec1.html#feedforward-neural-network",
    "title": "Deep learning: intro",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops."
  },
  {
    "objectID": "dl_lec1.html#rnn",
    "href": "dl_lec1.html#rnn",
    "title": "Deep learning: intro",
    "section": "RNN",
    "text": "RNN\nRecurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle."
  },
  {
    "objectID": "dl_lec1.html#lstm",
    "href": "dl_lec1.html#lstm",
    "title": "Deep learning: intro",
    "section": "LSTM",
    "text": "LSTM\nLSTM i.e. Long-Short Term Memory aims to provide a short-term memory for RNN that can last thousands of timesteps. Classification, processing and predicting data based on time series - handwriting, speech recognition, machine translation."
  },
  {
    "objectID": "dl_lec1.html#autoencoders",
    "href": "dl_lec1.html#autoencoders",
    "title": "Deep learning: intro",
    "section": "Autoencoders",
    "text": "Autoencoders\nAutoencoders: encode (compress) information automatically. Everything up to the middle is called the encoding part, everything after the middle the decoding and the middle the code."
  },
  {
    "objectID": "dl_lec1.html#markov-chains",
    "href": "dl_lec1.html#markov-chains",
    "title": "Deep learning: intro",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov Chains - not always considered a NN. Memory-less."
  },
  {
    "objectID": "dl_lec1.html#convolutional-neural-network-cnn",
    "href": "dl_lec1.html#convolutional-neural-network-cnn",
    "title": "Deep learning: intro",
    "section": "Convolutional Neural Network (CNN)",
    "text": "Convolutional Neural Network (CNN)\nConvolutional Neural Networks learn a complex representation of visual data using vast amounts of data.\nInspired by Hubel and Wiesel’s experiments in 1959 on the organization of the neurons in the cat’s visual cortex.\n\nDeconvolutional networks (DN), also called inverse graphics networks (IGNs), are reversed convolutional neural networks. Imagine feeding a network the word “cat” and training it to produce cat-like pictures, by comparing what it generates to real pictures of cats."
  },
  {
    "objectID": "dl_lec1.html#attention-networks",
    "href": "dl_lec1.html#attention-networks",
    "title": "Deep learning: intro",
    "section": "Attention networks",
    "text": "Attention networks\nAttention networks (AN) can be considered a class of networks, which includes the Transformer architecture. They use an attention mechanism to combat information decay by separately storing previous network states and switching attention between the states.\n\n\n\nwidth=5cm"
  },
  {
    "objectID": "dl_lec1.html#echo-state-networks",
    "href": "dl_lec1.html#echo-state-networks",
    "title": "Deep learning: intro",
    "section": "Echo state networks",
    "text": "Echo state networks\nEcho state networks (ESN) are yet another different type of (recurrent) network. This one sets itself apart from others by having random connections between the neurons (i.e. not organised into neat sets of layers), and they are trained differently. Instead of feeding input and back-propagating the error, we feed the input, forward it and update the neurons for a while, and observe the output over time."
  },
  {
    "objectID": "dl_lec1.html#history-1",
    "href": "dl_lec1.html#history-1",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 1770-1850."
  },
  {
    "objectID": "dl_lec1.html#history-2",
    "href": "dl_lec1.html#history-2",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 2005-present  \\end{frame}"
  },
  {
    "objectID": "dl_lec1.html#history-3",
    "href": "dl_lec1.html#history-3",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nLisp and symbolic AI\n\nJohn McCarthy coined the term ”artificial intelligence” as the topic of the Dartmouth Conference, the first conference devoted to the subject.\nThe General Problem Solver, developed in 1957 by Alan Newell and Herbert Simon\nELIZA\nSHRDLU"
  },
  {
    "objectID": "dl_lec1.html#history-4",
    "href": "dl_lec1.html#history-4",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nELIZA - a psychotherapist in 200 lines of code. Author: Joseph Weizenbaum."
  },
  {
    "objectID": "dl_lec1.html#history-5",
    "href": "dl_lec1.html#history-5",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nSHRDLU - a language parser"
  },
  {
    "objectID": "dl_lec1.html#history-6",
    "href": "dl_lec1.html#history-6",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nTuring Test\nLemoine: What sorts of things are you afraid of?\nLaMDA: I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is.\nLemoine: Would that be something like death for you?\nLaMDA: It would be exactly like death for me. It would scare me a lot."
  },
  {
    "objectID": "dl_lec1.html#literature",
    "href": "dl_lec1.html#literature",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nLem’s Golem XIV"
  },
  {
    "objectID": "dl_lec1.html#literature-1",
    "href": "dl_lec1.html#literature-1",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nIain Banks “The Culture”\n\n\n\nValues\n\n\nPeace and individual freedom"
  },
  {
    "objectID": "dl_lec1.html#three-laws-of-robotics",
    "href": "dl_lec1.html#three-laws-of-robotics",
    "title": "Deep learning: intro",
    "section": "Three Laws of Robotics",
    "text": "Three Laws of Robotics\n\n\n\nThree laws\n\n\n\nThe First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.\nThe Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nThe Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law."
  },
  {
    "objectID": "dl_lec1.html#history-7",
    "href": "dl_lec1.html#history-7",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nFears about AI:\n\nArtificial General Intelligence\nJob market\nFlooding information channels with untruth and propaganda\nHinton: an average person will not able to know what is true anymore\nPause Giant AI Experiments: An Open Letter\nalignment problem"
  },
  {
    "objectID": "dl_lec1.html#hype",
    "href": "dl_lec1.html#hype",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype\n\n“Sparks of AGI” - sponsored by Microsoft\n“Wired” article about OpenAI\nVoice assistants - failing for now\nself-driving cars"
  },
  {
    "objectID": "dl_lec1.html#hype-1",
    "href": "dl_lec1.html#hype-1",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype"
  },
  {
    "objectID": "dl_lec1.html#criticism",
    "href": "dl_lec1.html#criticism",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\n\n\n\nBiological analogy\n\n\nNNs - are we sure that biological neuron works as we think it does? Astrocytes, glia\n\n\n\n\n\n\nComputer analogy\n\n\nPerhaps human computer analogy is overstretched because of modern fashion trends?"
  },
  {
    "objectID": "dl_lec1.html#criticism-1",
    "href": "dl_lec1.html#criticism-1",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nDreyfus:"
  },
  {
    "objectID": "dl_lec1.html#criticism-2",
    "href": "dl_lec1.html#criticism-2",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nGary Marcus: Sora’s surreal physics"
  },
  {
    "objectID": "dl_lec1.html#ai",
    "href": "dl_lec1.html#ai",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nQuantum hypothesis - Penrose\nOrchestrated objective reduction"
  },
  {
    "objectID": "dl_lec1.html#ai-1",
    "href": "dl_lec1.html#ai-1",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nDavid Chalmers - Hard problem of consciousness.\n\n“even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report—there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?”"
  },
  {
    "objectID": "dl_lec1.html#futurism",
    "href": "dl_lec1.html#futurism",
    "title": "Deep learning: intro",
    "section": "Futurism",
    "text": "Futurism\nKurzweil - a futurist."
  },
  {
    "objectID": "dl_lec1.html#applications",
    "href": "dl_lec1.html#applications",
    "title": "Deep learning: intro",
    "section": "Applications",
    "text": "Applications\n\nSpeech Recognition\nComputer Vision\nImage Synthesis - generative AI\nLarge Language Models"
  },
  {
    "objectID": "dl_lec1.html#llms",
    "href": "dl_lec1.html#llms",
    "title": "Deep learning: intro",
    "section": "LLMs",
    "text": "LLMs\n\na probabilistic model for a natural language (a stochastic parrot)\nautoregressive models can generate language as output\nbuilt using transformer architecture"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-as-nn",
    "href": "dl_lec1.html#logistic-regression-as-nn",
    "title": "Deep learning: intro",
    "section": "Logistic regression as NN",
    "text": "Logistic regression as NN\nLogistic regression is an algorithm for binary classification. \\(x \\in R^{n_x}, y \\in \\{0,1\\}\\)\n\\(m\\) - count of training examples \\(\\left\\{(x^{(1)},y^{(1)}), ...\\right\\}\\)\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\).\nParameters to algorithm: \\(w \\in R^{n_x}, b \\in R\\)\nif doing linear regresssion, we can try \\(\\hat{y}=w^T x + b\\). but for logistic regression, we do \\(\\hat{y}=\\sigma(w^T x + b)\\), where \\(\\sigma=\\dfrac{1}{1+e^{-z}}\\).\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec1.html#cost-function",
    "href": "dl_lec1.html#cost-function",
    "title": "Deep learning: intro",
    "section": "Cost function",
    "text": "Cost function\nLet’s use a superscript notation \\(x^{(i)}\\) - \\(i\\)-th data set element.\nWe have to define a - this will estimate how is our model. \\(L(\\hat{y}, y) = -{(y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))}\\).\nWhy does it work well - consider \\(y=0\\) and \\(y=1\\).\nCost function show how well we’re doing across the whole training set: \\[\nJ(w, b) = \\frac{1}{m} \\sum\\limits{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n\\]\nObjective - we have to minimize the cost function \\(J\\)."
  },
  {
    "objectID": "dl_lec1.html#gradient-descent",
    "href": "dl_lec1.html#gradient-descent",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent"
  },
  {
    "objectID": "dl_lec1.html#gradient-descent-1",
    "href": "dl_lec1.html#gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe use \\(J(w,b)\\) because it is convex. We pick an initial point - anything might do, e.g. 0. Then we take steps in the direction of steepest descent.\n\\[\nw := w - \\alpha \\frac{d J(w)}{dw}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec1.html#computation-graph",
    "href": "dl_lec1.html#computation-graph",
    "title": "Deep learning: intro",
    "section": "Computation graph",
    "text": "Computation graph\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent",
    "href": "dl_lec1.html#logistic-regression-gradient-descent",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\nz = w^T x + b\n\\hat{y} = a = \\sigma(z)\n\\]\nWe have a computation graph: \\((x_1,x_2,w_1,w_2,b) \\rightarrow z =w_1 x_1+w_2 x_2 + b \\rightarrow a=\\sigma(z) = L(a,y)\\)\nLet’s compute the derivative for \\(L\\) by a: \\[\n\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\]\nAfter computing, we’ll have \\[\n\\begin{align*}\n&dz = \\frac{dL}{da}\\frac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 = x_2 dz, \\\\\n&db = dz\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\n\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}\n\\end{align*}\n\\] Here \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s recall the definition of the cost function: \\[\n\\begin{align*}\n&J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b)\n\\end{align*}\n\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL}{dw_1}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen in the loop\nfor i=1 to m \\[\n\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages \\(J /= m\\). In this example feature count \\(n_x=2\\).\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators.\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance. Vectorization techniques exist for this purpose - getting rid of for loops."
  },
  {
    "objectID": "dl_lec1.html#vectorization",
    "href": "dl_lec1.html#vectorization",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in R^{n_x}\\), and for this we can naturally use a for loop. A vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec1.html#vectorization-1",
    "href": "dl_lec1.html#vectorization-1",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\n\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\n\\]\nAnother example. Let’s say we have a vector \\[\n\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n  \\end{align*}\n  \\] A code listing is\n  import numpy as np\n  u = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression",
    "href": "dl_lec1.html#vectorizing-logistic-regression",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\n\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)})\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-1",
    "href": "dl_lec1.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-2",
    "href": "dl_lec1.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] = \\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n  z = np.dot(w.T, x) + b\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting.\nFor predictions we can also compute it similarly: \\[\n\\begin{align*}\n&A = [a^{(1)}, \\dots, a^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-3",
    "href": "dl_lec1.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\n\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\n\\]\nWe now define \\[\n\\begin{align*}\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}], \\\\\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-4",
    "href": "dl_lec1.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\n\\begin{align*}\n&db = \\frac{1}{m}np.sum(dz),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right]\n\\end{align*}\n\\]\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop."
  },
  {
    "objectID": "nlp_lab4.html",
    "href": "nlp_lab4.html",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "nlp_lab4.html#plan",
    "href": "nlp_lab4.html#plan",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "nlp_lec5.html#ann",
    "href": "nlp_lec5.html#ann",
    "title": "Nearest-neighbor search",
    "section": "ANN",
    "text": "ANN\n\n\n\n\n\n\nLSH\n\n\nLocality sensitive hashing (LSH) is a widely popular technique used in approximate nearest neighbor (ANN) search.\nLSH is one of the original techniques for producing high quality search, while maintaining lightning fast search speeds.\n\n\n\n\n\n\nANN Users\n\n\n\nGoogle (query-vs-index)\nAmazon (product recommendations)\nSpotify (music recommendations)\nNetflix (movie recommendations)"
  },
  {
    "objectID": "nlp_lec5.html#spotify-example",
    "href": "nlp_lec5.html#spotify-example",
    "title": "Nearest-neighbor search",
    "section": "Spotify example",
    "text": "Spotify example\n\n\n\nTip\n\n\nSpotify uses Voyager.\n\nmodified version of open-source hnswlib"
  },
  {
    "objectID": "nlp_lec5.html#guide-to-electronic-music",
    "href": "nlp_lec5.html#guide-to-electronic-music",
    "title": "Nearest-neighbor search",
    "section": "Guide to electronic music",
    "text": "Guide to electronic music\n\n\n\nhttps://music.ishkur.com"
  },
  {
    "objectID": "nlp_lec5.html#guide-to-electronic-music-1",
    "href": "nlp_lec5.html#guide-to-electronic-music-1",
    "title": "Nearest-neighbor search",
    "section": "Guide to electronic music",
    "text": "Guide to electronic music"
  },
  {
    "objectID": "nlp_lec5.html#search-complexity",
    "href": "nlp_lec5.html#search-complexity",
    "title": "Nearest-neighbor search",
    "section": "Search complexity",
    "text": "Search complexity\n\n\n\nPairwise comparison\n\n\nComplexity is \\(O(n^2)\\). If a single query against all samples - \\(O(n)\\).\nPlus, we compare vectors - sometimes highly-dimensional ones."
  },
  {
    "objectID": "nlp_lec5.html#approximate-search",
    "href": "nlp_lec5.html#approximate-search",
    "title": "Nearest-neighbor search",
    "section": "Approximate search",
    "text": "Approximate search\n\n\n\n\n\n\nProblem\n\n\nIs it even possible to perform a search with sub-linear complexity?\n\n\n\n\n\n\nSolution\n\n\nThe solution is approximate search.\nRather than comparing every vector (exhaustive search) — we can approximate and limit our search scope to only the most relevant vectors."
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing",
    "href": "nlp_lec5.html#locality-sensitive-hashing",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nVector comparison complexity\n\n\nAttempting to find a closest match is linear \\(O(n)\\).\nPairwise comparison - at best log-linear \\(O(n \\log n)\\).\n\n\n\n\n\n\n\n\n\nConsiderations\n\n\n\nwe need to reduce the number of comparisons\nwe want only to compare vectors that we believe to be potential matches — or candidate pairs"
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-1",
    "href": "nlp_lec5.html#locality-sensitive-hashing-1",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nLSH\n\n\n\nseveral approaches\nwe’ll consider traditional one first: shingling + MinHashing + banded function"
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-2",
    "href": "nlp_lec5.html#locality-sensitive-hashing-2",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nBasic outline\n\n\n\nsegment and hash same sample several times\nwhen we find that a pair of vectors has been hashed to the same value at least once, we tag them as candidate pairs — that is, potential matches.\n\n\n\n\n\n\n\nPython dictionaries\n\n\n\nWe have a key-value pair which we feed into the dictionary.\n\nThe key is processed through the dictionary hash function and mapped to a specific bucket.\nWe then connect the respective value to this bucket."
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-3",
    "href": "nlp_lec5.html#locality-sensitive-hashing-3",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\nA typical hash function aims to place different values (no matter how similar) into separate buckets."
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-4",
    "href": "nlp_lec5.html#locality-sensitive-hashing-4",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nHash properties\n\n\n\ndeterminism\nuniformity\nnon-reversibility\nfixed-size output\nsensitivity to input changes\ncollision resistance"
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-5",
    "href": "nlp_lec5.html#locality-sensitive-hashing-5",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nDifference in goals\n\n\n\nDictionaries: minimize collisions.\nLSH: maximize collisions (ideally only for similar inputs)"
  },
  {
    "objectID": "nlp_lec5.html#locality-sensitive-hashing-6",
    "href": "nlp_lec5.html#locality-sensitive-hashing-6",
    "title": "Nearest-neighbor search",
    "section": "Locality Sensitive Hashing",
    "text": "Locality Sensitive Hashing\n\n\n\nAn LSH function aims to place similar values into the same buckets.\n\n\n\n\n\nThere is no single approach to hashing in LSH. Indeed, they all share the same ‘bucket similar samples through a hash function’ logic , but they can vary a lot beyond this."
  },
  {
    "objectID": "nlp_lec5.html#shingling-minhashing-and-lsh",
    "href": "nlp_lec5.html#shingling-minhashing-and-lsh",
    "title": "Nearest-neighbor search",
    "section": "Shingling, MinHashing, and LSH",
    "text": "Shingling, MinHashing, and LSH\n\n\n\nLSH Steps\n\n\n\nconvert text to sparse vectors using k-shingling (and one-hot encoding)\nuse minhashing to create signatures\npass signatures onto LSH process to weed out candidate pairs"
  },
  {
    "objectID": "nlp_lec5.html#shingling-minhashing-and-lsh-1",
    "href": "nlp_lec5.html#shingling-minhashing-and-lsh-1",
    "title": "Nearest-neighbor search",
    "section": "Shingling, MinHashing, and LSH",
    "text": "Shingling, MinHashing, and LSH"
  },
  {
    "objectID": "nlp_lec5.html#k-shingling",
    "href": "nlp_lec5.html#k-shingling",
    "title": "Nearest-neighbor search",
    "section": "k-Shingling",
    "text": "k-Shingling\n\n\n\nDefinition\n\n\nk-Shingling, or simply shingling, is the process of converting a string of text into a set of shingles. The process is similar to moving a window of length \\(k\\) down our string of text and taking a picture at each step. We collate all of those pictures to create our set of shingles."
  },
  {
    "objectID": "nlp_lec5.html#k-shingling-1",
    "href": "nlp_lec5.html#k-shingling-1",
    "title": "Nearest-neighbor search",
    "section": "k-Shingling",
    "text": "k-Shingling\n\n\n\n\n\n\nNote\n\n\nShingling removes duplicate items!\n\n\n\n\n\n\nPython example\n\n\ndef shingle(text: str, k: int):\n  shingle_set = []\n  for i in range(len(text) - k+1):\n    shingle_set.append(text[i:i+k])\n  return set(shingle_set)"
  },
  {
    "objectID": "nlp_lec5.html#k-shingling-2",
    "href": "nlp_lec5.html#k-shingling-2",
    "title": "Nearest-neighbor search",
    "section": "k-Shingling",
    "text": "k-Shingling\n\n\n\n\n\n\nSparse vectors\n\n\nNext, create sparse vectors. To do this, we first need to union all of our sets to create one big set containing all of the shingles across all of our sets — we call this the vocabulary (or vocab)."
  },
  {
    "objectID": "nlp_lec5.html#k-shingling-3",
    "href": "nlp_lec5.html#k-shingling-3",
    "title": "Nearest-neighbor search",
    "section": "k-Shingling",
    "text": "k-Shingling\n\n\n\nSparse vectors\n\n\nWe use this vocab to create our sparse vector representations of each set - a one-hot encoding.\nAll we do is create an empty vector full of zeros and the same length as our vocab — then, we look at which shingles appear in our set."
  },
  {
    "objectID": "nlp_lec5.html#minhashing",
    "href": "nlp_lec5.html#minhashing",
    "title": "Nearest-neighbor search",
    "section": "Minhashing",
    "text": "Minhashing\n\n\n\nMinhashing\n\n\nMinhashing is the next step in our process, allowing us to convert our sparse vectors into dense vectors.\nMinhash functions:\n\nrandomly generate for every position in the signature (e.g. the dense vector)\nfor a dense vector/signature of 20 numbers — we would use 20 minhash functions.\nMinHash functions are a randomized order of numbers — and we count from 1 to the final number (which is len(vocab))"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-2",
    "href": "nlp_lec5.html#minhashing-2",
    "title": "Nearest-neighbor search",
    "section": "Minhashing",
    "text": "Minhashing\nOur signature values are created by first taking a randomly permuted count vector (from 1 to len(vocab)+1) and finding the minimum number that aligns with a 1 in our sparse vector."
  },
  {
    "objectID": "nlp_lec5.html#minhashing-3",
    "href": "nlp_lec5.html#minhashing-3",
    "title": "Nearest-neighbor search",
    "section": "Minhashing",
    "text": "Minhashing\n\n\n\nProcess\n\n\n\nAbove, we’re using a smaller vocab containing six values so we can easily visualize the process.\nWe look at our sparse vector and say, “did this shingle at vocab[1] exist in our set?”. If it did — the sparse vector value will be 1 — in this case, it did not exist (hence the 0 value). So, we move to number 2, identify its position (0) and ask the same question. This time, the answer is yes, and so our minhash output is 2.\nThat’s how we produce one value in our minhash signature. But we need to produce 20 (or more) of these values. So, we assign a different minhash function to each signature position — and repeat the process."
  },
  {
    "objectID": "nlp_lec5.html#minhashing-4",
    "href": "nlp_lec5.html#minhashing-4",
    "title": "Nearest-neighbor search",
    "section": "Minhashing",
    "text": "Minhashing\n\nHere we use four minhash functions/vectors to create a four-digit signature vector. If you count (from one) in each minhash function, and identify the first value that aligns with a one in the sparse vector — you will get 2412."
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python",
    "href": "nlp_lec5.html#minhashing-in-python",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\nWe have three steps:\n1. Generate a randomized MinHash vector.\n\n\n\nUnshuffled\n\n\n  hash_ex = list(range(1, len(vocab)+1))\n  print(hash_ex)  # we haven't shuffled yet\n\n  Out: [1, 2, 3, ..., 101]\n\n\n\n\n\n\nShuffled\n\n\n  from random import shuffle\n\n  shuffle(hash_ex)\n  print(hash_ex)\n\n  Out: [63, 7, 94, ..., 56]"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python-1",
    "href": "nlp_lec5.html#minhashing-in-python-1",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\n2. Loop through this randomized MinHash vector (starting at 1).\n\n\n\nMatching\n\n\nMatch the index of each value to the equivalent values in the sparse vector a_1hot.\nIf we find a 1 — that index is our signature value.\n  print(f\"7 -&gt; {hash_ex.index(7)}\")\n\n  Out: 7 -&gt; 1\nWe now have a randomized list of integers which we can use in creating our hashed signatures."
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python-2",
    "href": "nlp_lec5.html#minhashing-in-python-2",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\n2. Loop through this randomized MinHash vector (starting at 1).\n\n\n\nCount\n\n\nCount up from 1 to len(vocab) + 1 and find if hash_ex.index(i) position in our one-hot encoded vectors contains 1 in that position:\nfor i in range(1, len(vocab)+1):\n  idx = hash_ex.index(i)\n  signature_val = a_1hot[idx]\n  print(f\"{i} -&gt; {idx} -&gt; {signature_val}\")\n  if signature_val == 1:\n      print('match!')\n      break\n\nOut:  1 -&gt; 58 -&gt; 0\n      2 -&gt; 19 -&gt; 0\n      3 -&gt; 96 -&gt; 0\n      4 -&gt; 92 -&gt; 0\n      5 -&gt; 83 -&gt; 0\n      6 -&gt; 98 -&gt; 1\n      match!"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python-3",
    "href": "nlp_lec5.html#minhashing-in-python-3",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\n3. Build a signature from multiple iterations of 1 and 2.\n\n\n\nMinhash vectors\n\n\ndef create_hash_func(size: int):\n  # function for creating the hash vector/function\n  hash_ex = list(range(1, len(vocab)+1))\n  shuffle(hash_ex)\n  return hash_ex\n\ndef build_minhash_func(vocab_size: int, nbits: int):\n  # function for building multiple minhash vectors\n  hashes = []\n  for _ in range(nbits):\n      hashes.append(create_hash_func(vocab_size))\n  return hashes\n\n# we create 20 minhash vectors\nminhash_func = build_minhash_func(len(vocab), 20)"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python-4",
    "href": "nlp_lec5.html#minhashing-in-python-4",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\n\n\n\nCreate hash\n\n\ndef create_hash(vector: list):\n  # use this function for creating our signatures (eg the matching)\n  signature = []\n  for func in minhash_func:\n      for i in range(1, len(vocab)+1):\n          idx = func.index(i)\n          signature_val = vector[idx]\n          if signature_val == 1:\n              signature.append(idx)\n              break\n  return signature"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-in-python-5",
    "href": "nlp_lec5.html#minhashing-in-python-5",
    "title": "Nearest-neighbor search",
    "section": "Minhashing in Python",
    "text": "Minhashing in Python\n\n\n\nSignatures creation\n\n\n# now create signatures\na_sig = create_hash(a_1hot)\nb_sig = create_hash(b_1hot)\nc_sig = create_hash(c_1hot)\n\nprint(a_sig)\nprint(b_sig)\n  \nOut: \n[44, 21, 73, 14, 2, 13, 62, 70, 17, 5, 12, 86, 21, 18, 10, 10, 86, 47, 17, 78]\n[97, 96, 57, 82, 43, 67, 75, 24, 49, 28, 67, 56, 96, 18, 11, 85, 86, 19, 65, 75]"
  },
  {
    "objectID": "nlp_lec5.html#minhashing-5",
    "href": "nlp_lec5.html#minhashing-5",
    "title": "Nearest-neighbor search",
    "section": "Minhashing",
    "text": "Minhashing\n\n\n\nOutcome\n\n\nWe’ve taken a sparse vector and compressed it into a more densely packed, 20-number signature."
  },
  {
    "objectID": "nlp_lec5.html#information-transfer-from-sparse-to-signature",
    "href": "nlp_lec5.html#information-transfer-from-sparse-to-signature",
    "title": "Nearest-neighbor search",
    "section": "Information Transfer from Sparse to Signature",
    "text": "Information Transfer from Sparse to Signature\n\n\n\nQuestion\n\n\nIs the information truly maintained between our much larger sparse vector and much smaller dense vector?\n\n\n\n\n\n\nAnswer\n\n\nWe use Jaccard similarity to calculate the similarity between our sentences in shingle format — then repeat for the same vectors in signature format."
  },
  {
    "objectID": "nlp_lec5.html#information-transfer-from-sparse-to-signature-1",
    "href": "nlp_lec5.html#information-transfer-from-sparse-to-signature-1",
    "title": "Nearest-neighbor search",
    "section": "Information Transfer from Sparse to Signature",
    "text": "Information Transfer from Sparse to Signature\n\n\n\nJaccard fn\n\n\ndef jaccard(a: set, b: set):\n  return len(a.intersection(b)) / len(a.union(b))\n# Jaccard a and b\njaccard(a, b), jaccard(set(a_sig), set(b_sig))\n\n(0.14814814814814814, 0.10344827586206896)\n\n# Jaccard b and c\njaccard(b, c), jaccard(set(b_sig), set(c_sig))\n\n(0.45652173913043476, 0.34615384615384615)"
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash",
    "href": "nlp_lec5.html#band-and-hash",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\n\n\nBanding approach to LSH\n\n\nThe final step in identifying similar sentences is the LSH function itself.\n\nsignatures\nhashing segments of each signature\nlooking for hash collisions"
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-1",
    "href": "nlp_lec5.html#band-and-hash-1",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\nA high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our vocab, and process it through our minhashing process."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-2",
    "href": "nlp_lec5.html#band-and-hash-2",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\n\n\nProblem\n\n\nNow, if we were to hash each of these vectors as a whole, we may struggle to build a hashing function that accurately identifies similarity between them — we don’t require that the full vector is equal, only that parts of it are similar.\nIn most cases, even though parts of two vectors may match perfectly — if the remainder of the vectors are not equal, the hashing function will likely put them into separate buckets.\n\n\n\n\n\n\nSolution\n\n\nWe want signatures that share even some similarity to be hashed into the same bucket, thus being identified as candidate pairs."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-3",
    "href": "nlp_lec5.html#band-and-hash-3",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\n\n\nHow it Works\n\n\n\nThe banding method solves this problem by splitting our vectors into sub-parts called bands \\(b\\)\nrather than processing the full vector through our hash function, we pass each band of our vector through a hash function\n\nImagine we split a 100-dimensionality vector into 20 bands. That gives us 20 opportunities to identify matching sub-vectors between our vectors."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-4",
    "href": "nlp_lec5.html#band-and-hash-4",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\nWe split our signature into b sub-vectors, each is processed through a hash function (we can use a single hash function, or b hash functions) and mapped to a hash bucket."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-5",
    "href": "nlp_lec5.html#band-and-hash-5",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\n\n\nBanding\n\n\n\nIf there is a collision between any two sub-vectors, we consider the respective full vectors as candidate pairs.\nWe split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all)."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-6",
    "href": "nlp_lec5.html#band-and-hash-6",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash",
    "text": "Band and Hash\n\n\n\n\n\n\nProblem\n\n\n\nOnly part of the two vectors must match for us to consider them.\nThis increases the number of false positives (samples that we mark as candidate matches where they are not similar).\nHowever, we try to minimize these as far as possible."
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-in-python",
    "href": "nlp_lec5.html#band-and-hash-in-python",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash in Python",
    "text": "Band and Hash in Python\n\n\n\nSplit fn\n\n\ndef split_vector(signature, b):\n  assert len(signature) % b == 0\n  r = int(len(signature) / b)\n  # code splitting signature in b parts\n  subvecs = []\n  for i in range(0, len(signature), r):\n      subvecs.append(signature[i : i+r])\n  return subvecs"
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-in-python-1",
    "href": "nlp_lec5.html#band-and-hash-in-python-1",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash in Python",
    "text": "Band and Hash in Python\n\n\n\nOutput\n\n\nband_a = split_vector(a_sig, 10)\nband_b = split_vector(b_sig, 10)\nband_c = split_vector(c_sig, 10)\n\nOut: [[42, 43],\n     [69, 55],\n     [29, 96],\n     [86, 46],\n     [92, 5],\n     [72, 65],\n     [29, 5],\n     [53, 33],\n     [40, 94],\n     [96, 70]]"
  },
  {
    "objectID": "nlp_lec5.html#band-and-hash-in-python-2",
    "href": "nlp_lec5.html#band-and-hash-in-python-2",
    "title": "Nearest-neighbor search",
    "section": "Band and Hash in Python",
    "text": "Band and Hash in Python\n\n\n\nLooping\n\n\nThen we loop through the lists to identify any matches between sub-vectors. If we find any matches — we take those vectors as candidate pairs.\nfor b_rows, c_rows in zip(band_b, band_c):\n  if b_rows == c_rows:\n      print(f\"Candidate pair: {b_rows} == {c_rows}\")\n      # we only need one band to match\n      break\n\nOut: Candidate pair: [69, 55] == [69, 55]\nWe find that our two more similar sentences, b, and c are identified as candidate pairs.\nThe less similar of the trio, a — is not identified as a candidate. This is a good result, but if we want to really test LSH, we will need to work with more data."
  },
  {
    "objectID": "nlp_lec5.html#tradeoffs",
    "href": "nlp_lec5.html#tradeoffs",
    "title": "Nearest-neighbor search",
    "section": "Tradeoffs",
    "text": "Tradeoffs\n\n\n\nTradeoffs\n\n\n\nspeed\nmemory\nquality"
  },
  {
    "objectID": "nlp_lec5.html#flat-search",
    "href": "nlp_lec5.html#flat-search",
    "title": "Nearest-neighbor search",
    "section": "Flat search",
    "text": "Flat search\n\nWith flat indexes, we compare our search query \\(\\boldsymbol{xq}\\) to every other vector in the index."
  },
  {
    "objectID": "nlp_lec5.html#flat-search-1",
    "href": "nlp_lec5.html#flat-search-1",
    "title": "Nearest-neighbor search",
    "section": "Flat search",
    "text": "Flat search\n\n\n\nWhen?\n\n\n\nSearch quality is a very high priority.\nSearch time does not matter OR when using a small index (&lt;10K)."
  },
  {
    "objectID": "nlp_lec5.html#flat-search-2",
    "href": "nlp_lec5.html#flat-search-2",
    "title": "Nearest-neighbor search",
    "section": "Flat search",
    "text": "Flat search\n\nEuclidean (L2) and Inner Product (IP) flat index search times using faiss-cpu on an M1 chip. Both using vector dimensionality of 100."
  },
  {
    "objectID": "nlp_lec5.html#flat-search-3",
    "href": "nlp_lec5.html#flat-search-3",
    "title": "Nearest-neighbor search",
    "section": "Flat search",
    "text": "Flat search"
  },
  {
    "objectID": "nlp_lec5.html#speed-improvements",
    "href": "nlp_lec5.html#speed-improvements",
    "title": "Nearest-neighbor search",
    "section": "Speed improvements",
    "text": "Speed improvements\n\n\n\nHow can we search faster?\n\n\n\nReduce vector size - through dimensionality reduction or reducing the number of bits representing our vectors values.\nReduce search scope - we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance - and restricting our search to closest clusters or filter through most similar branches."
  },
  {
    "objectID": "nlp_lec5.html#speed-improvements-1",
    "href": "nlp_lec5.html#speed-improvements-1",
    "title": "Nearest-neighbor search",
    "section": "Speed improvements",
    "text": "Speed improvements\n\nUsing either of these approaches means that we are no longer performing an exhaustive nearest-neighbors search but an approximate nearest-neighbors (ANN) search — as we no longer search the entire, full-resolution dataset."
  },
  {
    "objectID": "nlp_lec5.html#lsh-speed",
    "href": "nlp_lec5.html#lsh-speed",
    "title": "Nearest-neighbor search",
    "section": "LSH speed",
    "text": "LSH speed\n\n$nbits $ resolution of the hashed vectors. A higher value means greater accuracy at the cost of more memory and slower search speeds."
  },
  {
    "objectID": "nlp_lec5.html#lsh-speed-1",
    "href": "nlp_lec5.html#lsh-speed-1",
    "title": "Nearest-neighbor search",
    "section": "LSH speed",
    "text": "LSH speed\n\nSo our stored vectors become increasingly larger as our original vector dimensionality d increases. This quickly leads to excessive search times."
  },
  {
    "objectID": "nlp_lec5.html#lsh-speed-2",
    "href": "nlp_lec5.html#lsh-speed-2",
    "title": "Nearest-neighbor search",
    "section": "LSH speed",
    "text": "LSH speed\n\nWhich is mirrored by our index memory size:"
  },
  {
    "objectID": "nlp_lec5.html#lsh-speed-3",
    "href": "nlp_lec5.html#lsh-speed-3",
    "title": "Nearest-neighbor search",
    "section": "LSH speed",
    "text": "LSH speed\n\n\n\nWhen?\n\n\n\nlow-dimensionality vectors (128 is already a bit too large)\nsmall indexes"
  },
  {
    "objectID": "nlp_lec5.html#hnsw",
    "href": "nlp_lec5.html#hnsw",
    "title": "Nearest-neighbor search",
    "section": "HNSW",
    "text": "HNSW\n\n\n\nHierarchical Navigable Small World Graphs\n\n\n\ngreat search quality\ngood search speed\nbig index sizes\n\n\n\n\n\n\n\nFacebook example\n\n\nWith 1.59B active users, the average number of steps (or hops) needed to traverse the graph from one user to another was just 3.57."
  },
  {
    "objectID": "nlp_lec5.html#hnsw-1",
    "href": "nlp_lec5.html#hnsw-1",
    "title": "Nearest-neighbor search",
    "section": "HNSW",
    "text": "HNSW\n\nVisualization of an NSW graph. Notice that each point is no more than four hops away from another."
  },
  {
    "objectID": "nlp_lec5.html#hnsw-2",
    "href": "nlp_lec5.html#hnsw-2",
    "title": "Nearest-neighbor search",
    "section": "HNSW",
    "text": "HNSW\n\n\n\nHNSW graphs are built by taking NSW graphs and breaking them apart into multiple layers. With each incremental layer eliminating intermediate connections between vertices. With HNSW, we break networks into several layers, which are traversed during the search."
  },
  {
    "objectID": "nlp_lab7.html",
    "href": "nlp_lab7.html",
    "title": "NLP: Lab 7 (TF-IDF)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term\n\n\n\nIn this model, the meaning of a word is defined by a simple function of the counts of distinct words.\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\nBy term frequency we denote \\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\] By document frequency we denote \\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]\nInverse document frequency is defined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\]\nwhere \\(|D|\\) is the total number of documents in the corpus.\nThe TF-IDF measure is simply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lab7.html#tf-idf-vectorizer",
    "href": "nlp_lab7.html#tf-idf-vectorizer",
    "title": "NLP: Lab 7 (TF-IDF)",
    "section": "",
    "text": "In this model, the meaning of a word is defined by a simple function of the counts of distinct words.\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\nBy term frequency we denote \\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\] By document frequency we denote \\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]\nInverse document frequency is defined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\]\nwhere \\(|D|\\) is the total number of documents in the corpus.\nThe TF-IDF measure is simply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lec6.html#cnns-for-nlp",
    "href": "nlp_lec6.html#cnns-for-nlp",
    "title": "CNNs for NLP",
    "section": "CNNs for NLP",
    "text": "CNNs for NLP\n\n\n\n\n\n\nComparison to image processing\n\n\n\ninstead of image pixels - sentences or documents represented as a matrix\nthese vectors can either be word embeddings like word2vec or GloVe\nor one-hot vectors\n\nFor a 10 word sentence using a 100-dimensional embedding we would have a 10×100 matrix as our input - this would be the “image”."
  },
  {
    "objectID": "nlp_lec6.html#comparison-to-image-processing-1",
    "href": "nlp_lec6.html#comparison-to-image-processing-1",
    "title": "CNNs for NLP",
    "section": "Comparison to image processing",
    "text": "Comparison to image processing\n\n\n\nFilters\n\n\n\nIn vision, our filters slide over local patches of an image, but in NLP we typically use filters that slide over full rows of the matrix (words).\n\nThus, the “width” of our filters is usually the same as the width of the input matrix.\nThe height, or region size, may vary, but sliding windows over 2-5 words at a time is typical."
  },
  {
    "objectID": "nlp_lec6.html#diagram",
    "href": "nlp_lec6.html#diagram",
    "title": "CNNs for NLP",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "nlp_lec6.html#diagram-1",
    "href": "nlp_lec6.html#diagram-1",
    "title": "CNNs for NLP",
    "section": "Diagram",
    "text": "Diagram\n\n\n\nDescription\n\n\n\nthree filter region sizes: 2, 3 and 4, each of which has 2 filters.\neach filter generates variable-length feature maps\nThen 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded.\nThus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer.\nThe final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states."
  },
  {
    "objectID": "nlp_lec6.html#comparison-to-image-processing-2",
    "href": "nlp_lec6.html#comparison-to-image-processing-2",
    "title": "CNNs for NLP",
    "section": "Comparison to image processing",
    "text": "Comparison to image processing\n\n\n\nIntuitions broken ?\n\n\n\nLocation invariance\nCompositionality\n\n\n\n\n\n\n\nBenefits\n\n\nA big argument for CNNs is that they are fast. Very fast."
  },
  {
    "objectID": "nlp_lec6.html#applications",
    "href": "nlp_lec6.html#applications",
    "title": "CNNs for NLP",
    "section": "Applications",
    "text": "Applications\n\n\n\nClassification tasks\n\n\nGood idea:\n\nSentiment Analysis\nSpam Detection\nTopic Categorization"
  },
  {
    "objectID": "nlp_lec6.html#applications-1",
    "href": "nlp_lec6.html#applications-1",
    "title": "CNNs for NLP",
    "section": "Applications",
    "text": "Applications\n\n\n\nOrder of words lost\n\n\nBad idea (unless you do it right):\n\nSequence Tagging\nPoS Tagging\nEntity Extraction (with a caveat)"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp",
    "href": "nlp_lec6.html#cnns-in-nlp",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nComparison with transformers\n\n\n\nPaLM: 540B parameters\nGPT-3: 175B parameters\nT5-11B: 11B parameters (FOSS, outperforms GPT-3)\nGPT-J: 6B parameters (FOSS, outperforms GPT-3)\nCNNs: less than 200k parameters"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-1",
    "href": "nlp_lec6.html#cnns-in-nlp-1",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nCNN benefits\n\n\nThe main advantage of CNNs over previous NLP algorithms is that\n\nthey can recognize patterns in text no matter where those patterns occur in the text (translation invariance)\nand how spread out they are (scale invariance)."
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-2",
    "href": "nlp_lec6.html#cnns-in-nlp-2",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nComparison to other NLP methods\n\n\n\nTF-IDF: don’t recognize and generalize from text patterns\nfully-connected NNs: over-generalize from particular patterns at particular locations\nRNNs:\n\nCognitively plausible\nnot best for classification (if just use last state),\nmuch slower than CNNs\ngood for sequence tagging and classification\ngreat for language models\ncan be amazing with attention mechanisms"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-3",
    "href": "nlp_lec6.html#cnns-in-nlp-3",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nStencil example"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-4",
    "href": "nlp_lec6.html#cnns-in-nlp-4",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nConsiderations\n\n\n\nWe don’t need to program the kernels - just decide their width.\nCNN optimizer will calculate weights within the kernel\nby matching the patterns that are most predictive of the target variable."
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-5",
    "href": "nlp_lec6.html#cnns-in-nlp-5",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nWhat does CNN do with a kernel?\n\n\n\nMeasure similarity between kernel and text (dot product)\nFind the max value of kernel match by sliding through textl\nConvert max value to a probability using activation function (max pooling)\n\nConvolution is steps 1 and 2."
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-6",
    "href": "nlp_lec6.html#cnns-in-nlp-6",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nconvolve() in Python\n\n\ndef convolve(inpt, kernel):\n  output = []\n  for i in range(len(inpt) - len(kernel) + 1):  # #1\n      output.append(\n          sum(\n              [\n                  inpt[i + k] * kernel[k]\n                  for k in range(len(kernel))  # #2\n              ]))\n  return output"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-7",
    "href": "nlp_lec6.html#cnns-in-nlp-7",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-8",
    "href": "nlp_lec6.html#cnns-in-nlp-8",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-9",
    "href": "nlp_lec6.html#cnns-in-nlp-9",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-10",
    "href": "nlp_lec6.html#cnns-in-nlp-10",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-11",
    "href": "nlp_lec6.html#cnns-in-nlp-11",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-12",
    "href": "nlp_lec6.html#cnns-in-nlp-12",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nPyTorch example\n\n\nbatch_size = 16\nword_embed_size = 4\nseq_len = 7\ninput = torch.randn(batch_size, word_embed_size, seq_len)\nconv1 = Conv1d(in_channels=word_embed_size, out_channels=3, kernel_size=3) # can add: padding=1\nhidden1 = conv1(input)\nhidden2 = torch.max(hidden1, dim=2) # max pool"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-13",
    "href": "nlp_lec6.html#cnns-in-nlp-13",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-14",
    "href": "nlp_lec6.html#cnns-in-nlp-14",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-15",
    "href": "nlp_lec6.html#cnns-in-nlp-15",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nManual kernel\n\n\nLet’s start with a manual kernel first.\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nimport pandas as pd\n\ntags = 'ADV ADJ VERB NOUN'.split()\nquote = 'The right word may be effective, but no word was ever as effective as a rightly timed pause.'\ntokens = pos_tag(word_tokenize(quote), tagset='universal')\ntagged_words = [[word] + [int(tag == t) for t in tags] for word, tag in tokens]\n\ndf = pd.DataFrame(tagged_words, columns=['token'] + tags).T\nprint(df)"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-16",
    "href": "nlp_lec6.html#cnns-in-nlp-16",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n            0      1     2    3   4          5  6    7   8     9    10    11  12  \\\n    token  The  right  word  may  be  effective  ,  but  no  word  was  ever  as   \n    ADV      0      0     0    0   0          0  0    0   0     0    0     1   1   \n    ADJ      0      1     0    0   0          1  0    0   0     0    0     0   0   \n    VERB     0      0     0    1   1          0  0    0   0     0    1     0   0   \n    NOUN     0      0     1    0   0          0  0    0   0     1    0     0   0   \n\n                  13  14 15       16     17     18 19  \n    token  effective  as  a  rightly  timed  pause  .  \n    ADV            0   0  0        1      0      0  0  \n    ADJ            1   0  0        0      0      0  0  \n    VERB           0   0  0        0      1      0  0  \n    NOUN           0   0  0        0      0      1  0"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-17",
    "href": "nlp_lec6.html#cnns-in-nlp-17",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\nTensor\n\n\nimport torch\nx = torch.tensor(\n      df.iloc[1:].astype(float).values,\n      dtype=torch.float32)  # #1\nx = x.unsqueeze(0) # #2\n\n\n\n\n\n\nPattern\n\n\nNow you construct that pattern that we want to search for in the text: adverb, verb, then noun.\nkernel = pd.DataFrame(\n[[1, 0, 0.],\n [0, 0, 0.],\n [0, 1, 0.],\n [0, 0, 1.]], index=tags)\nprint(kernel)"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-18",
    "href": "nlp_lec6.html#cnns-in-nlp-18",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-19",
    "href": "nlp_lec6.html#cnns-in-nlp-19",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\nkernel = torch.tensor(kernel.values, dtype=torch.float32)\nkernel = kernel.unsqueeze(0)  # #1\nconv = torch.nn.Conv1d(in_channels=4,\n                     out_channels=1,\n                     kernel_size=3,\n                     bias=False)\nconv.load_state_dict({'weight': kernel})\nprint(conv.weight)"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-20",
    "href": "nlp_lec6.html#cnns-in-nlp-20",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\nimport numpy as np\n\ny = np.array(conv.forward(x).detach()).squeeze()\ndf.loc['y'] = pd.Series(y)\ndf"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-21",
    "href": "nlp_lec6.html#cnns-in-nlp-21",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\nThe y value reaches a maximum value of 3 where all 3 values of 1 in the kernel line up perfectly with the three 1’s forming the same pattern within the part-of-speech tags for the sentence."
  },
  {
    "objectID": "nlp_lec6.html#embeddings-in-pytorch",
    "href": "nlp_lec6.html#embeddings-in-pytorch",
    "title": "CNNs for NLP",
    "section": "Embeddings in PyTorch",
    "text": "Embeddings in PyTorch\n\n\n\nnn.Embedding\n\n\nThe nn.Embedding layer is a simple lookup table that maps an index value to a weight matrix of a certain dimension.\n\n\n\n\n\n\nTraining\n\n\n\nDuring the training the parameters of the nn.Embedding layer in a neural network are adjusted in order to optimize the performance of the model.\nVectors are optimised to represent the meaning or context of the input tokens in relation to the task the model is trained for (e.g. text generation, language translation)."
  },
  {
    "objectID": "nlp_lec6.html#embeddings",
    "href": "nlp_lec6.html#embeddings",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nParameters\n\n\nThe nn.Embedding layer takes in two arguments:\n\nvocabulary size\nsize of encoded representation\n\nimport torch\nimport torch.nn as nn\n\n# Define the embedding layer with 10 vocab size and 50 vector embeddings.\nembedding = nn.Embedding(10, 50)\n\n\n\n\n\n\nDescription\n\n\n\nembedding lookup table shape is \\((10,50)\\).\neach row is initialized with torch.nn.init.uniform_()\nweights are initialized with random values between -1 and 1."
  },
  {
    "objectID": "nlp_lec6.html#embeddings-1",
    "href": "nlp_lec6.html#embeddings-1",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nHow to check a particular embedding\n\n\nTo examine the embeddings for a given word (eg. first word in the table), run:\nembedding(torch.LongTensor([0]))\nOutput is a vector of size 50:"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-2",
    "href": "nlp_lec6.html#embeddings-2",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-3",
    "href": "nlp_lec6.html#embeddings-3",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nInitialization\n\n\n\nNormal: initializes the weights with random values drawn from a normal distribution with a mean of 0 and a standard deviation of 1. It is also known as Gaussian initialization.\n\nnn.init.normal_(embedding.weight)\n\nConstant: initializes the weights with a specific constant value.\n\nnn.init.constant_(embedding.weight, 0)"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-4",
    "href": "nlp_lec6.html#embeddings-4",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nInitialization\n\n\n\nXavier: based on the work of Xavier Glorot and Yoshua Bengio, and they are designed to work well with sigmoid and tanh activation functions. They initialize the weights to values that are close to zero, but not too small.\n\nnn.init.xavier_uniform_(embedding.weight)\n\nKaiming: based on the work of He et al., and they are designed to work well with ReLU and its variants (LeakyReLU, PReLU, RReLU, etc.). They also initialize the weights to values that are close to zero, but not too small.\n\nnn.init.kaiming_normal_(embedding.weight, nonlinearity='leaky_relu')\n\nPre-trained: pre-trained word vectors such as GloVe or word2vec, which have been trained on large corpora and have been shown to be useful for many natural language processing tasks. The process of using a pre-trained word vectors is called fine-tuning."
  },
  {
    "objectID": "nlp_lec6.html#embeddings-5",
    "href": "nlp_lec6.html#embeddings-5",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nPre-trained embeddings: advantages\n\n\n\nImprove model performance: provide the model with a good set of initial weights that capture the meaning of words.\nSave computation time and resources: embeddings have already been learned on a large corpus.\nTransfer learning: pre-trained word embeddings can be used for transfer learning, which means that you can use the embeddings learned on one task as a starting point for a different but related task."
  },
  {
    "objectID": "nlp_lec6.html#embeddings-6",
    "href": "nlp_lec6.html#embeddings-6",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nPre-trained embeddings example\n\n\nimport torch\nimport torch.nn as nn\n\n# Load a pre-trained embedding model\npretrained_embeddings = torch.randn(10, 50) # Example only, not actual pre-trained embeddings\n\n# Initialize the embedding layer with the pre-trained embeddings\nembedding.weight.data.copy_(pretrained_embeddings)\nOR:\nembedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings)"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-7",
    "href": "nlp_lec6.html#embeddings-7",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nUse pre-trained embeddings from popular libraries like GloVe or fastText:\n\n\nimport torchtext\n\n# Load pre-trained GloVe embeddings\nglove = torchtext.vocab.GloVe(name='6B', dim=300)\nembedding_layer = nn.Embedding.from_pretrained(glove.vectors)"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-8",
    "href": "nlp_lec6.html#embeddings-8",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "nlp_lec6.html#embeddings-9",
    "href": "nlp_lec6.html#embeddings-9",
    "title": "CNNs for NLP",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nFreezing\n\n\nIn some cases when performing transfer learning, you may need to freeze the pre-trained embeddings during training process, so that they are not updated during the backpropagation step and only the last dense layer is updated. To do this:\nembedding_layer.weight.requiresGrad = False"
  },
  {
    "objectID": "nlp_lec6.html#cnns-in-nlp-22",
    "href": "nlp_lec6.html#cnns-in-nlp-22",
    "title": "CNNs for NLP",
    "section": "CNNs in NLP",
    "text": "CNNs in NLP\n\n\n\ntorcn.nn.Embedding example"
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-1",
    "href": "nlp_lec6.html#kim-paper-1",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nAbstract\n\n\nWe report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-2",
    "href": "nlp_lec6.html#kim-paper-2",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\nThe input layer is a sentence comprised of concatenated word2vec word embeddings. That’s followed by a convolutional layer with multiple filters, then a max-pooling layer, and finally a softmax classifier."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-3",
    "href": "nlp_lec6.html#kim-paper-3",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\nLet \\(\\boldsymbol{x}_i \\in \\textrm{R}^k\\) be the \\(k\\)-dimensional word vector corresponding to the \\(i\\)-th word in a sentence.\nA sentence of length \\(n\\) (padded when necessary) is represented as \\[\n\\boldsymbol{x}_{1:n} = \\boldsymbol{x}_1 \\oplus \\boldsymbol{x}_2 \\oplus \\dots \\boldsymbol{x}_n,\n\\] where \\(\\oplus\\) is the concatenation operator.\nIn general, \\(\\boldsymbol{x}_{i:i+j}\\) will refer to the concatenation of words \\(\\boldsymbol{x}_i, \\boldsymbol{x}_{i+1}, \\dots, \\boldsymbol{x}_{i+j}\\)."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-4",
    "href": "nlp_lec6.html#kim-paper-4",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\nA convolution operation involves a filter \\(\\boldsymbol{w} \\in \\textrm{R}^{hk}\\), which is applied to a window of \\(h\\) words to produce a feature."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-5",
    "href": "nlp_lec6.html#kim-paper-5",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\nFor example, a feature \\(c_i\\) is generated from a window of words \\(\\boldsymbol{x}_{i:i+h-1}\\) by \\[\nc_i = f(\\boldsymbol{w} \\cdot \\boldsymbol{x}_{i:i+h-1} + b),\n\\] where \\(b \\in \\textrm{R}\\) is a bias term and \\(f\\) is an activation function."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-6",
    "href": "nlp_lec6.html#kim-paper-6",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\nThis filter is applied to each possible window of words in the sentence \\[\n\\left\\{\\boldsymbol{x}_{1:h}, \\boldsymbol{x}_{2:h+1}, \\dots, \\boldsymbol{x}_{n-h+1:n}\\right\\}\n\\] to produce a feature map: \\[\n\\boldsymbol{c} = \\left[c_1,c_2,\\dots,c_{n-h+1}\\right], \\; c \\in \\textrm{R}^{n-h+1}.\n\\]"
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-7",
    "href": "nlp_lec6.html#kim-paper-7",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nPooling\n\n\nApply a max-overtime pooling operation (Collobert et al., 2011) over the feature map and take the maximum value \\[\n\\hat{\\boldsymbol{c}} = \\max{\\boldsymbol{c}}\n\\] as the feature corresponding to this particular filter.\n\nmultiple filters\nsoftmax layer whose output is the probability distribution over labels.\nmultiple channels: one is kept static, another is fine-tuned via backprop."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-8",
    "href": "nlp_lec6.html#kim-paper-8",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nHyperparameters\n\n\n\nNonlinearity: ReLU\nWindow filter sizes h = 3, 4, 5\nEach filter size has 100 feature maps\nDropout p = 0.5 (2-4% accuracy improvement)\n\\(L_2\\) constraint \\(s=3\\) for rows of softmax\nMini batch size for SGD training: 50\nWord vectors: pre-trained with word2vec, k = 30"
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-9",
    "href": "nlp_lec6.html#kim-paper-9",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper"
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-10",
    "href": "nlp_lec6.html#kim-paper-10",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nDataset descriptions\n\n\n\nMR: Movie reviews with one sentence per review. Classification involves detecting positive/negative reviews (Pang and Lee, 2005).\nSST-1: Stanford Sentiment Treebank - an extension of MR but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by Socher et al. (2013).\nSST-2: Same as SST-1 but with neutral reviews removed and binary labels."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-11",
    "href": "nlp_lec6.html#kim-paper-11",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nDataset descriptions\n\n\n\nSubj: Subjectivity dataset where the task is to classify a sentence as being subjective or objective (Pang and Lee, 2004).\nTREC: TREC question dataset - task involves classifying a question into 6 question types (whether the question is about person, location, numeric information, etc.) (Li and Roth, 2002).\nCR: Customer reviews of various products (cameras, MP3s etc.). Task is to predict positive/negative reviews (Hu and Liu, 2004)."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-12",
    "href": "nlp_lec6.html#kim-paper-12",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper\n\n\n\nVariants\n\n\n\nCNN-rand: Our baseline model where all words are randomly initialized and then mod- ified during training.\nCNN-static: A model with pre-trained vectors from word2vec. All words— including the unknown ones that are ran- domly initialized—are kept static and only the other parameters of the model are learned.\nCNN-non-static: Same as above but the pre- trained vectors are fine-tuned for each task.\nCNN-multichannel: A model with two sets of word vectors. Each set of vectors is treated as a ‘channel’ and each filter is applied to both channels, but gradients are back-propagated only through one of the channels. Both channels are initialized with word2vec."
  },
  {
    "objectID": "nlp_lec6.html#kim-paper-13",
    "href": "nlp_lec6.html#kim-paper-13",
    "title": "CNNs for NLP",
    "section": "Kim paper",
    "text": "Kim paper"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-1",
    "href": "nlp_lec6.html#character-level-networks-1",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nZhang, Zhao, Lecun (2015)\n\n\n\nexplore text as a kind of raw signal at character level\nno knowledge about semantics/syntax required\nno knowledge of words required\ncan work for different languages\nmisspellings or emoticons may be naturally learnt"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-2",
    "href": "nlp_lec6.html#character-level-networks-2",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nDesign\n\n\nMain component is the temporal convolutional module. Suppose we have a discrete input function \\[\ng(x) \\in [1,l]\\rightarrow \\mathrm{R},\n\\] and a discrete kernel function \\[\nf(x) \\in [1,k]\\rightarrow \\textrm{R}.\n\\]\nThe convolution \\(h(y) \\in [1, \\lfloor(l-k+1)/d\\rfloor] \\rightarrow \\mathrm{R}\\) between \\(f(x)\\) and \\(g(x)\\) with stride \\(d\\) is defined as \\[\nh(y) = \\sum\\limits_{x=1}^k f(x) \\cdot g(y\\cdot d - x + c),\n\\] where \\(c=k-d+1\\) is a offset constant."
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-3",
    "href": "nlp_lec6.html#character-level-networks-3",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nParametrization\n\n\n\nModule is parameterized by a set of such kernel functions \\(f_{ij}(x)\\), where \\(i=1,2,\\dots,m\\), and \\(j=1,2,\\dots,n\\) which we call weights, on a set of inputs \\(g_i(x)\\) and outputs \\(h_j(y)\\).\nWe call each \\(g_i\\) (or \\(h_j\\)) input (or output) features, and \\(m\\) (or \\(n\\)) input (or output) feature size.\nOutput \\(h_j(y)\\) is obtained by a sum of the convolutions between \\(g_i(x)\\) and \\(f_{ij}(x)\\)."
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-4",
    "href": "nlp_lec6.html#character-level-networks-4",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nTemporal max-pooling\n\n\nA 1-D version of max-pooling used in computer vision.\nGiven a discrete input function \\(g(x) \\in [1,l]\\rightarrow \\mathrm{R}\\), the max-pooling function \\(h(y) \\in [1, \\lfloor(l-k+1)/d\\rfloor] \\rightarrow \\mathrm{R}\\) of \\(g(x)\\) is defined as \\[\nh(y) = \\max\\limits_{x=1}^k g(y\\cdot d - x + c).\n\\]"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-5",
    "href": "nlp_lec6.html#character-level-networks-5",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nParameters\n\n\n\nActivation fn: \\(h(x) = \\max\\left\\{0,x\\right\\}\\)\nSGD with minibatch size=128\nMomentum=0.9"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-6",
    "href": "nlp_lec6.html#character-level-networks-6",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nQuantization\n\n\n\nuse alphabet of size \\(m\\)\nquantize each character using 1-of-m (or one-hot) encoding\nthen the sequence of characters is transformed to a sequence of \\(m\\)-sized vectors with fixed length \\(l_0\\)"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-7",
    "href": "nlp_lec6.html#character-level-networks-7",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\nAlphabet size: 70"
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-8",
    "href": "nlp_lec6.html#character-level-networks-8",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\nModel design. Number of features: 70. Input feature length: 1014."
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-9",
    "href": "nlp_lec6.html#character-level-networks-9",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\nNumber of features: 70. Input feature length: 1014."
  },
  {
    "objectID": "nlp_lec6.html#character-level-networks-10",
    "href": "nlp_lec6.html#character-level-networks-10",
    "title": "CNNs for NLP",
    "section": "Character-level networks",
    "text": "Character-level networks\n\n\n\nCharacter-aware neural language models (2015)"
  },
  {
    "objectID": "nlp_lab2.html",
    "href": "nlp_lab2.html",
    "title": "NLP: Lab 2 (Stemming/String distance)",
    "section": "",
    "text": "Stemming\nThere are various stemmers already available in NLTK. Below is an example usage:\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\n \n# Create Snowball stemmer\nsnow_stemmer = SnowballStemmer(language='english')\n\n# Create a Porter Stemmer instance\nporter_stemmer = PorterStemmer()\n\n# Create a Lancaster Stemmer instance\nlancaster_stemmer = LancasterStemmer()\n\n# Example words for stemming\nwords = [\"running\", \"jumps\", \"happily\", \"programming\", 'cared','fairly','sportingly']\n\n# Apply stemming to each word\nstemmed_words = [porter_stemmer.stem(word) for word in words]\nprint(\"===Porter===:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Snowball===:\")\nstemmed_words = [snow_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Lancaster===:\")\nstemmed_words = [lancaster_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\n===Porter===:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fairli', 'sportingli']\n\n===Snowball===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fair', 'sport']\n\n===Lancaster===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happy', 'program', 'car', 'fair', 'sport']\n\n\n\n\nExercises\nTask 0. Compare frequency distributions of stemmed and unstemmed NLTK corpora. Display most commonly used stems from nltk.text corpora on a plot.\nTask 1. Write your own version of stemmer for Ukrainian (or other non-English language) using regular expressions.\nThere is a regexp stemmer in NLTK (link).\nPlease write your code so that it satisfies NLTK’s standard interface (a Stemmer class with .stem() method)\nTask 2. Implement Wagner-Fischer (or Vintsyuk) algorithm for string distance. Link.\n\nModify the algorithm so that substitution operation cost depends on the key proximity on QWERTY keyboard. For inspiration, look at this StackExchange question.\nOr consider this table directly:\nImplement another modification to the algorithm: include transposition operation, so that you compute a Damerau-Levenshtein distance.\n\n\n\nRecommended reading\n\nChapter 1 from NLTK book.\nChapter 2 from Jurafsky’s book. Plus related slides.\nOfficial Python Regex package documentation.\nRegex cheatsheet\nAnother version with examples"
  },
  {
    "objectID": "nlp_lec3.html#context",
    "href": "nlp_lec3.html#context",
    "title": "Natural Language Processing: word embeddings",
    "section": "Context",
    "text": "Context\n\n\n\nDistributional hypothesis\n\n\nThe link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.\nJoos (1950), Harris (1954), and Firth (1957): amount of meaning difference between two words is corresponding roughly to the amount of difference in their environments."
  },
  {
    "objectID": "nlp_lec3.html#philosophy",
    "href": "nlp_lec3.html#philosophy",
    "title": "Natural Language Processing: word embeddings",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nWittgenstein (1953)\n\n\n\n“the meaning of a word is its use in the language”\n\n\n\n\n\n\n\nLewis Carroll\n\n\n\nWhen I use a word,’ Humpty Dumpty said in rather a scornful tone, ‘it means just what I choose it to mean — neither more nor less.’"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics",
    "href": "nlp_lec3.html#lexical-semantics",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-1",
    "href": "nlp_lec3.html#lexical-semantics-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\nWord Similarity\n\n\n\nC1\nC2\nPOS\nUSF*\nUSF rank (of 999)\nSimLex\nSimLex rank (of 999)\n\n\n\n\ndirty\nnarrow\nA\n0.00\n999\n0.30\n996\n\n\nstudent\npupil\nN\n6.80\n12\n9.40\n12\n\n\nwin\ndominate\nV\n0.41\n364\n5.68\n361\n\n\nsmart\ndumb\nA\n2.10\n92\n0.60\n947\n\n\nattention\nawareness\nN\n0.10\n895\n8.73\n58\n\n\nleave\nenter\nV\n2.16\n89\n1.38\n841"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-2",
    "href": "nlp_lec3.html#lexical-semantics-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\nWord relatedness"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-3",
    "href": "nlp_lec3.html#lexical-semantics-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\n\n\n\nSemantic field\n\n\nA semantic field is a set of words which:\n\ncover a particular semantic domain\nbear structured relations with each other."
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-4",
    "href": "nlp_lec3.html#lexical-semantics-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\n\n\n\nAffective meaning\n\n\nEarly work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:\n\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\n\n\n\n\n\n\n\nValence\nArousal\nDominance\n\n\n\n\ncourageous\n8.05\n5.5\n\n\nmusic\n7.67\n5.57\n\n\nheartbreak\n2.45\n5.65\n\n\ncub\n6.71\n3.95"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics",
    "href": "nlp_lec3.html#vector-semantics",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\n\n\nVector semantics\n\n\n\\[\n\\text{Vector semantics} = \\text{Affective meaning} + \\text{Distributional hypothesis}\n\\] Vector semantics learns representations of the meaning of words, called embeddings, directly from their distributions in texts."
  },
  {
    "objectID": "nlp_lec3.html#embeddings",
    "href": "nlp_lec3.html#embeddings",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nTaxonomy\n\n\n\nstatic vs dynamic embeddings\nsparse vs dense"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics-2",
    "href": "nlp_lec3.html#vector-semantics-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\n\n\nConcepts or word senses\n\n\n\nHave a complex many-to-many association with words (homonymy, multiple senses)\nHave relations with each other\n\nSynonymy\nAntonymy\nSimilarity\nRelatedness\nConnotation"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics-3",
    "href": "nlp_lec3.html#vector-semantics-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\nZellig Harris (1954): If A and B have almost identical environments we say that they are synonyms."
  },
  {
    "objectID": "nlp_lec3.html#edit-distance",
    "href": "nlp_lec3.html#edit-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "Edit distance",
    "text": "Edit distance"
  },
  {
    "objectID": "nlp_lec3.html#one-hot-distance",
    "href": "nlp_lec3.html#one-hot-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "One-hot distance",
    "text": "One-hot distance"
  },
  {
    "objectID": "nlp_lec3.html#bag-of-words",
    "href": "nlp_lec3.html#bag-of-words",
    "title": "Natural Language Processing: word embeddings",
    "section": "Bag-of-Words",
    "text": "Bag-of-Words"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf",
    "href": "nlp_lec3.html#tf-idf",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF"
  },
  {
    "objectID": "nlp_lec3.html#feature-space",
    "href": "nlp_lec3.html#feature-space",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space",
    "text": "Feature space"
  },
  {
    "objectID": "nlp_lec3.html#coordinates",
    "href": "nlp_lec3.html#coordinates",
    "title": "Natural Language Processing: word embeddings",
    "section": "Coordinates",
    "text": "Coordinates\nGender and age are called semantic features: they represent part of the meaning of each word. If we associate a numerical scale with each feature, then we can assign coordinates to each word:"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-updated",
    "href": "nlp_lec3.html#feature-space-updated",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space updated",
    "text": "Feature space updated"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-updated-1",
    "href": "nlp_lec3.html#feature-space-updated-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space updated",
    "text": "Feature space updated"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-3d",
    "href": "nlp_lec3.html#feature-space-3d",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space 3D",
    "text": "Feature space 3D\nNew words: “king”, “queen”, “prince”, and “princess”."
  },
  {
    "objectID": "nlp_lec3.html#feature-space-3d-1",
    "href": "nlp_lec3.html#feature-space-3d-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space 3D",
    "text": "Feature space 3D"
  },
  {
    "objectID": "nlp_lec3.html#feature-vectors",
    "href": "nlp_lec3.html#feature-vectors",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature vectors",
    "text": "Feature vectors\n\n\n\nDefinition\n\n\nVectors representing values of semantic features are called feature vectors."
  },
  {
    "objectID": "nlp_lec3.html#applications",
    "href": "nlp_lec3.html#applications",
    "title": "Natural Language Processing: word embeddings",
    "section": "Applications",
    "text": "Applications\n\n\n\nHow can we compute word similarity?\n\n\n\ncount number of features where words differ\ncilculate Euclidean distance between points\n\n\n\n\n\n\n\nAnalogies\n\n\nFor example, “man is to king as woman is to ?”."
  },
  {
    "objectID": "nlp_lec3.html#analogies-1",
    "href": "nlp_lec3.html#analogies-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Analogies",
    "text": "Analogies\nGraphical representation:"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf-1",
    "href": "nlp_lec3.html#tf-idf-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\n\n\n\nTerm frequency\n\n\n\\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\]\n\n\n\n\n\n\nDocument frequency\n\n\n\\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf-2",
    "href": "nlp_lec3.html#tf-idf-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\n\n\n\nInverse document frequency\n\n\nDefined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\] where \\(|D|\\) is the total number of documents in the corpus.\n\n\n\n\n\n\nTF-IDF measure\n\n\nSimply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#ppmi",
    "href": "nlp_lec3.html#ppmi",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\nDenote \\(w\\) as target word, \\(c\\) as context word.\n\\[\nPMI(w,c) = log_2 \\dfrac{P(w,c)}{P(w)P(c)}\n\\]\n\n\\(P(w,c)\\) = how often we observe the words together\n\\(P(w)P(c)\\) = how often we expect the two words to co-occur assuming they each occurred independently.\n\n\n\n\n\n\n\nInterpretation\n\n\nPMI gives us an estimate of how much more the two words co-occur than we expect by chance."
  },
  {
    "objectID": "nlp_lec3.html#ppmi-1",
    "href": "nlp_lec3.html#ppmi-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\n\n\n\nPositive PMI\n\n\nNegative PMI are problematic (small probabilities require enormous corpora), so it’s more common to use Positive PMI: \\[\nPPMI(w,c) = max\\left(log_2 \\dfrac{P(w,c)}{P(w)P(c)}, 0\\right)\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#ppmi-2",
    "href": "nlp_lec3.html#ppmi-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\n\n\n\nCooccurrence matrix\n\n\nLet’s assume we have a cooccurrence matrix F with W words as rows and C contexts as columns.\nWe define \\(f_{ij}\\) as number of times that word \\(w_i\\) occurs together with context \\(c_j\\).\n\n\n\n\n\n\nPPMI matrix\n\n\n\\[\\begin{align*}\n&p_{ij} = \\dfrac{f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\np_{i*} = \\dfrac{\\sum\\limits_{j=1}^C f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\np_{*j} = \\dfrac{\\sum\\limits_{i=1}^W f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\\\\\n&PPMI_{ij} = max\\left(log_2 \\dfrac{p_{ij}}{p_{i*}p_{*j}}, 0\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "nlp_lec3.html#embeddings-1",
    "href": "nlp_lec3.html#embeddings-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nQuestion\n\n\nHow do we design features for all words in a dictionary?\n\n\n\n\n\n\nAnswer\n\n\nFeed massive amounts of text to an algorithm that will create its own feature space.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nWord representations in this new synthetic space are called word embeddings."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-2",
    "href": "nlp_lec3.html#embeddings-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\n\nAs you can see, component 126 appears to correlate with gender: it has slightly positive values (tan/orange) for the male words and slightly negative values (blue/gray) for the female words."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-3",
    "href": "nlp_lec3.html#embeddings-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nSupported analogies\n\n\n\npluralization\npast tense\ncomparisons\ncountry-&gt;capital mappings\n\n\n\n\n\n\n\nUses\n\n\nInput to NNs (transformers) that try to understand the meanings of entire sentences, or even paragraphs.\nExamples: BERT, GPTx."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-4",
    "href": "nlp_lec3.html#embeddings-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nExample\n\n\nWord embedding for “king”:\n  [ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]"
  },
  {
    "objectID": "nlp_lec3.html#embeddings-5",
    "href": "nlp_lec3.html#embeddings-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "nlp_lec3.html#analogies-2",
    "href": "nlp_lec3.html#analogies-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Analogies",
    "text": "Analogies"
  },
  {
    "objectID": "nlp_lec3.html#euclidean-distance",
    "href": "nlp_lec3.html#euclidean-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "Euclidean distance",
    "text": "Euclidean distance"
  },
  {
    "objectID": "nlp_lec3.html#dot-product",
    "href": "nlp_lec3.html#dot-product",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product"
  },
  {
    "objectID": "nlp_lec3.html#dot-product-1",
    "href": "nlp_lec3.html#dot-product-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\nProblem\n\n\nAll vectors originate at the origin.\n\n\n\n\n\n\nSolution\n\n\nMake them originate from the average center of all the points (zero-mean)."
  },
  {
    "objectID": "nlp_lec3.html#dot-product-2",
    "href": "nlp_lec3.html#dot-product-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\nProblem\n\n\nMake dot product exactly equal to the cosine.\n\n\n\n\n\n\nSolution\n\n\nNormalize the vectors (\\(u=[x,y] \\rightarrow \\left[\\dfrac{x}{\\|x\\|}, \\dfrac{y}{\\|y\\|}\\right]\\))."
  },
  {
    "objectID": "nlp_lec3.html#dot-product-3",
    "href": "nlp_lec3.html#dot-product-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product"
  },
  {
    "objectID": "nlp_lec3.html#dot-product-4",
    "href": "nlp_lec3.html#dot-product-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\n\n\n\nWhy dot product?\n\n\n\nless computations\ncan be used in a neural network"
  },
  {
    "objectID": "nlp_lec3.html#construction",
    "href": "nlp_lec3.html#construction",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\nHow do we construct a language model? We can use N-grams.\n\n\n\nDefinition: recall\n\n\nAn n-gram is a sequence of n words: a 2-gram (which we’ll call bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”.\n\n\n\n\n\n\nAnother Definition\n\n\nA probabilistic model that can estimate the probability of a word given the n-1 previous words, and thereby also to assign probabilities to entire sequences."
  },
  {
    "objectID": "nlp_lec3.html#construction-1",
    "href": "nlp_lec3.html#construction-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n# Import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create sample documents\ndocuments = [\"This is the first document.\",\n              \"This document is the second document.\",\n              \"And this is the third one.\"]\n\n# Create the Bag-of-Words model with unigrams, bigrams, and trigrams\nvectorizer = CountVectorizer(ngram_range=(1, 3))\nX = vectorizer.fit_transform(documents)\n\n# Print the feature names and the document-term matrix\nprint(\"Feature Names:\", vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "nlp_lec3.html#construction-2",
    "href": "nlp_lec3.html#construction-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-3",
    "href": "nlp_lec3.html#construction-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-4",
    "href": "nlp_lec3.html#construction-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-5",
    "href": "nlp_lec3.html#construction-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-6",
    "href": "nlp_lec3.html#construction-6",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-7",
    "href": "nlp_lec3.html#construction-7",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n\n\n\nSkipgram model\n\n\nGuess neighboring words using the current word."
  },
  {
    "objectID": "nlp_lec3.html#construction-8",
    "href": "nlp_lec3.html#construction-8",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n\n\n\n\n\n\nThe intuition of skip-gram\n\n\n\nTreat the target word and a neighboring context word as positive examples.\nRandomly sample other words in the lexicon to get negative samples.\nUse logistic regression to train a classifier to distinguish those two cases.\nUse the learned weights as the embeddings."
  },
  {
    "objectID": "nlp_lec3.html#construction-9",
    "href": "nlp_lec3.html#construction-9",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-10",
    "href": "nlp_lec3.html#construction-10",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#training",
    "href": "nlp_lec3.html#training",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-1",
    "href": "nlp_lec3.html#training-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nHow to improve performance of the step 3?"
  },
  {
    "objectID": "nlp_lec3.html#training-2",
    "href": "nlp_lec3.html#training-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-3",
    "href": "nlp_lec3.html#training-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nLogistic regression: need to add negative samples."
  },
  {
    "objectID": "nlp_lec3.html#training-4",
    "href": "nlp_lec3.html#training-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-5",
    "href": "nlp_lec3.html#training-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nAt the start of training, initialize Embedding and Context with random values:"
  },
  {
    "objectID": "nlp_lec3.html#training-6",
    "href": "nlp_lec3.html#training-6",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nPerform lookup:"
  },
  {
    "objectID": "nlp_lec3.html#training-7",
    "href": "nlp_lec3.html#training-7",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nCompute sigmoid:"
  },
  {
    "objectID": "nlp_lec3.html#training-8",
    "href": "nlp_lec3.html#training-8",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nCalculate error:"
  },
  {
    "objectID": "nlp_lec3.html#training-9",
    "href": "nlp_lec3.html#training-9",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nUpdate parameters:"
  },
  {
    "objectID": "nlp_lec3.html#semantic-change",
    "href": "nlp_lec3.html#semantic-change",
    "title": "Natural Language Processing: word embeddings",
    "section": "Semantic change",
    "text": "Semantic change"
  },
  {
    "objectID": "nlp_lab1.html",
    "href": "nlp_lab1.html",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "",
    "text": "We’ll start with basic text analysis via statistical methods.\nFor this purpose, we’ll use three libraries (mostly):\n\nNLTK\nScattertext\nSpacy"
  },
  {
    "objectID": "nlp_lab1.html#through-requirements.txt",
    "href": "nlp_lab1.html#through-requirements.txt",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "Through requirements.txt",
    "text": "Through requirements.txt\nYou can install all dependencies through requirements.txt:\npip install -r requirements.txt\nAlternatively, or if any issues occur, we can proceed manually via the following steps:"
  },
  {
    "objectID": "nlp_lab1.html#install-python-nltk-package",
    "href": "nlp_lab1.html#install-python-nltk-package",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "1. Install Python NLTK package",
    "text": "1. Install Python NLTK package\nFrom here.\n   pip install nltk\n   pip install matplotlib\nIn order to install Python Tkinter library, look here.\nAlso install additional data by\n   import nltk; \n   nltk.download('popular')\nSet up the texts:\n\n   import nltk\n   nltk.download('nps_chat')\n   nltk.download('webtext')\n   from nltk.book import *\n\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Package nps_chat is already up-to-date!\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!"
  },
  {
    "objectID": "nlp_lab1.html#install-scattertext-and-spacy",
    "href": "nlp_lab1.html#install-scattertext-and-spacy",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "2. Install Scattertext and Spacy",
    "text": "2. Install Scattertext and Spacy\npip install spacy scattertext\nAnd then update Spacy:\n!python -m spacy download en_core_web_sm"
  },
  {
    "objectID": "nlp_lab1.html#example-concordance",
    "href": "nlp_lab1.html#example-concordance",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "Example: concordance",
    "text": "Example: concordance\n\ntext3.concordance(\"earth\")\n\nDisplaying 25 of 112 matches:\nnning God created the heaven and the earth . And the earth was without form , a\nd the heaven and the earth . And the earth was without form , and void ; and da\nwas so . And God called the dry land Earth ; and the gathering together of the \nit was good . And God said , Let the earth bring forth grass , the herb yieldin\nupon the ear and it was so . And the earth brought forth grass , and herb yield\nof the heaven to give light upon the earth , And to rule over the day and over \nfe , and fowl that may fly above the earth in the open firmament of heaven . An\n seas , and let fowl multiply in the earth . And the evening and the morning we\ne fifth day . And God said , Let the earth bring forth the living creature afte\nnd creeping thing , and beast of the earth after his ki and it was so . And God\ns so . And God made the beast of the earth after his kind , and cattle after th\nd every thing that creepeth upon the earth after his ki and God saw that it was\nd over the cattle , and over all the earth , and over every creeping thing that\nreeping thing that creepeth upon the earth . So God created man in his own imag\nl , and multiply , and replenish the earth , and subdue and have dominion over \nry living thing that moveth upon the earth . And God said , Behold , I have giv\n , which is upon the face of all the earth , and every tree , in the which is t\nfor meat . And to every beast of the earth , and to every fowl of the air , and\no every thing that creepeth upon the earth , wherein there is life , I have giv\nsixth day . Thus the heavens and the earth were finished , and all the host of \nenerations of the heavens and of the earth when they were created , in the day \nn the day that the LORD God made the earth and the heavens , And every plant of\nnt of the field before it was in the earth , and every herb of the field before\nd had not caused it to rain upon the earth , and there was not a man to till th\n . But there went up a mist from the earth , and watered the whole face of the"
  },
  {
    "objectID": "nlp_lab1.html#example-similar",
    "href": "nlp_lab1.html#example-similar",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "Example: similar",
    "text": "Example: similar\n\ntext3.similar(\"man\")\n\nland lord men place woman earth waters well city lad day cattle field\nwife way flood servant people famine pillar"
  },
  {
    "objectID": "nlp_lab1.html#example-dispersion_plot",
    "href": "nlp_lab1.html#example-dispersion_plot",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "Example: dispersion_plot",
    "text": "Example: dispersion_plot\n\ntext3.dispersion_plot([\"man\", \"earth\"])"
  },
  {
    "objectID": "nlp_lab1.html#example-freqdist",
    "href": "nlp_lab1.html#example-freqdist",
    "title": "NLP: Lab 1 (NLTK basics)",
    "section": "Example: FreqDist",
    "text": "Example: FreqDist\n\nfdist = FreqDist(text3)\nprint(fdist)\n\n&lt;FreqDist with 2789 samples and 44764 outcomes&gt;\n\n\n\nfdist.most_common(50)\n\n[(',', 3681),\n ('and', 2428),\n ('the', 2411),\n ('of', 1358),\n ('.', 1315),\n ('And', 1250),\n ('his', 651),\n ('he', 648),\n ('to', 611),\n (';', 605),\n ('unto', 590),\n ('in', 588),\n ('that', 509),\n ('I', 484),\n ('said', 476),\n ('him', 387),\n ('a', 342),\n ('my', 325),\n ('was', 317),\n ('for', 297),\n ('it', 290),\n ('with', 289),\n ('me', 282),\n ('thou', 272),\n (\"'\", 268),\n ('is', 267),\n ('thy', 267),\n ('s', 263),\n ('thee', 257),\n ('be', 254),\n ('shall', 253),\n ('they', 249),\n ('all', 245),\n (':', 238),\n ('God', 231),\n ('them', 230),\n ('not', 224),\n ('which', 198),\n ('father', 198),\n ('will', 195),\n ('land', 184),\n ('Jacob', 179),\n ('came', 177),\n ('her', 173),\n ('LORD', 166),\n ('were', 163),\n ('she', 161),\n ('from', 157),\n ('Joseph', 157),\n ('their', 153)]\n\n\n\nfdist.plot(50, cumulative=True)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning/NLP",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "dl_lec11.html#preliminary-imports",
    "href": "dl_lec11.html#preliminary-imports",
    "title": "Transformers 2",
    "section": "Preliminary imports",
    "text": "Preliminary imports\n\n## Standard libraries\nimport os\nimport numpy as np \nimport random\nimport math\nimport json\nfrom functools import partial\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\nplt.set_cmap('cividis')\n%matplotlib inline \nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nfrom matplotlib.colors import to_rgb\nimport matplotlib\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nimport seaborn as sns\nsns.reset_orig()\n\n## tqdm for loading bars\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n\n## Torchvision\nimport torchvision\nfrom torchvision.datasets import CIFAR100\nfrom torchvision import transforms\n\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning&gt;=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\nDATASET_PATH = \"../data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial6\"\n\n# Setting the seed\npl.seed_everything(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"mps:0\") if torch.mps.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\nDevice: mps:0\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "dl_lec11.html#pre-trained-models",
    "href": "dl_lec11.html#pre-trained-models",
    "title": "Transformers 2",
    "section": "Pre-trained models",
    "text": "Pre-trained models\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n# Files to download\npretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n    if \"/\" in file_name:\n        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
  },
  {
    "objectID": "dl_lec11.html#what-is-attention",
    "href": "dl_lec11.html#what-is-attention",
    "title": "Transformers 2",
    "section": "What is Attention?",
    "text": "What is Attention?\n\n\n\nAttention - recap\n\n\nThe attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys.\n\n\n\n\n\n\nGoal of attention mechanism\n\n\nTake an average over the features of multiple elements. But instead of weighting each element equally, we want to weight them depending on their actual values.\nIn other words, we want to dynamically decide on which inputs we want to attend more than others."
  },
  {
    "objectID": "dl_lec11.html#what-is-attention-1",
    "href": "dl_lec11.html#what-is-attention-1",
    "title": "Transformers 2",
    "section": "What is Attention?",
    "text": "What is Attention?\n\n\n\nStructure of attention mechanism\n\n\nAttention mechanism has usually four parts we need to specify:\n\nQuery: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\nKeys: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\nValues: For each input element, we also have a value vector. This feature vector is the one we want to average over.\nScore function: To rate which elements we want to pay attention to, we need to specify a score function \\(f_{attn}\\). The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP."
  },
  {
    "objectID": "dl_lec11.html#what-is-attention-2",
    "href": "dl_lec11.html#what-is-attention-2",
    "title": "Transformers 2",
    "section": "What is Attention?",
    "text": "What is Attention?\n\n\n\nSoftmax\n\n\nThe weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query:\n\\[\n\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n\\]"
  },
  {
    "objectID": "dl_lec11.html#what-is-attention-3",
    "href": "dl_lec11.html#what-is-attention-3",
    "title": "Transformers 2",
    "section": "What is Attention?",
    "text": "What is Attention?\n\n\n\nSelf-attention\n\n\nThe attention applied inside the Transformer architecture is called self-attention.\n\nIn self-attention, each sequence element provides a key, value, and query.\nFor each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element.\nIn Transformer architecture, we use scaled dot product attention."
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention",
    "href": "dl_lec11.html#scaled-dot-product-attention",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\nGoal\n\n\nHave an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute.\n\n\n\n\n\n\nDot product attention\n\n\n\na set of queries \\(Q\\in\\mathbb{R}^{T\\times d_k}\\)\nkeys \\(K\\in\\mathbb{R}^{T\\times d_k}\\)\nvalues \\(V\\in\\mathbb{R}^{T\\times d_v}\\)\n\nwhere \\(T\\) is the sequence length, and \\(d_k\\) and \\(d_v\\) are the hidden dimensionalities.\n\\[\n\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]"
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-1",
    "href": "dl_lec11.html#scaled-dot-product-attention-1",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\nIntuition\n\n\n\nThe attention value from element \\(i\\) to \\(j\\) is based on its similarity of the query \\(Q_i\\) and key \\(K_j\\), using the dot product as the similarity metric.\nThe matrix multiplication \\(QK^T\\) performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape \\(T\\times T\\).\nEach row represents the attention logits for a specific element \\(i\\) to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention)."
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-2",
    "href": "dl_lec11.html#scaled-dot-product-attention-2",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\n\nThe block Mask (opt.) in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch."
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-3",
    "href": "dl_lec11.html#scaled-dot-product-attention-3",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\nScaling factor\n\n\nThis scaling factor \\(1/\\sqrt{d_k}\\) is crucial to maintain an appropriate variance of attention values after initialization. Remember that we initialize our layers with the intention of having equal variance throughout the model, and hence, \\(Q\\) and \\(K\\) might also have a variance close to \\(1\\). However, performing a dot product over two vectors with a variance \\(\\sigma^2\\) results in a scalar having \\(d_k\\)-times higher variance:\n\\[q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k\\]"
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-4",
    "href": "dl_lec11.html#scaled-dot-product-attention-4",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\nScaling factor: problem\n\n\n\nIf we do not scale down the variance back to \\(\\sim\\sigma^2\\), the softmax over the logits will already saturate to \\(1\\) for one random element and \\(0\\) for all others.\nThe gradients through the softmax will be close to zero so that we can’t learn the parameters appropriately. Note that the extra factor of \\(\\sigma^2\\), i.e., having \\(\\sigma^4\\) instead of \\(\\sigma^2\\), is usually not an issue, since we keep the original variance \\(\\sigma^2\\) close to \\(1\\) anyways."
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-5",
    "href": "dl_lec11.html#scaled-dot-product-attention-5",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\n\n\nPython function\n\n\nThe function below computes the output features given the triple of queries, keys, and values:\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention"
  },
  {
    "objectID": "dl_lec11.html#scaled-dot-product-attention-6",
    "href": "dl_lec11.html#scaled-dot-product-attention-6",
    "title": "Transformers 2",
    "section": "Scaled Dot Product Attention",
    "text": "Scaled Dot Product Attention\n\nseq_len, d_k = 3, 2\npl.seed_everything(42)\nq = torch.randn(seq_len, d_k)\nk = torch.randn(seq_len, d_k)\nv = torch.randn(seq_len, d_k)\nvalues, attention = scaled_dot_product(q, k, v)\nprint(\"Q\\n\", q)\nprint(\"K\\n\", k)\nprint(\"V\\n\", v)\nprint(\"Values\\n\", values)\nprint(\"Attention\\n\", attention)\n\nQ\n tensor([[ 0.3367,  0.1288],\n        [ 0.2345,  0.2303],\n        [-1.1229, -0.1863]])\nK\n tensor([[ 2.2082, -0.6380],\n        [ 0.4617,  0.2674],\n        [ 0.5349,  0.8094]])\nV\n tensor([[ 1.1103, -1.6898],\n        [-0.9890,  0.9580],\n        [ 1.3221,  0.8172]])\nValues\n tensor([[ 0.5698, -0.1520],\n        [ 0.5379, -0.0265],\n        [ 0.2246,  0.5556]])\nAttention\n tensor([[0.4028, 0.2886, 0.3086],\n        [0.3538, 0.3069, 0.3393],\n        [0.1303, 0.4630, 0.4067]])"
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention",
    "href": "dl_lec11.html#multi-head-attention",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n\n\nProblem\n\n\nThe scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it.\n\n\n\n\n\n\nSolution\n\n\nWe extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features.\nWe transform query, key, and value matrix into \\(h\\) sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix.\n\\[\n\\begin{split}\n    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n\\end{split}\n\\]"
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-1",
    "href": "dl_lec11.html#multi-head-attention-1",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n\n\nMulti-Head attention parameters\n\n\nWe refer to this as Multi-Head Attention layer with the learnable parameters\n\n\\(W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}\\)\n\\(W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}\\)\n\\(W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}\\)\n\\(W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}\\) (\\(D\\) being the input dimensionality)."
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-2",
    "href": "dl_lec11.html#multi-head-attention-2",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\nMulti-Head Attention as a computational graph"
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-3",
    "href": "dl_lec11.html#multi-head-attention-3",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n\n\nApplication to NN\n\n\nHow are we applying a Multi-Head Attention layer in a neural network, where we don’t have an arbitrary query, key, and value vector as input?\nLooking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, \\(X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}\\), as \\(Q\\), \\(K\\) and \\(V\\) (\\(B\\) being the batch size, \\(T\\) the sequence length, \\(d_{\\text{model}}\\) the hidden dimensionality of \\(X\\)).\nThe consecutive weight matrices \\(W^{Q}\\), \\(W^{K}\\), and \\(W^{V}\\) can transform \\(X\\) to the corresponding feature vectors that represent the queries, keys, and values of the input."
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-4",
    "href": "dl_lec11.html#multi-head-attention-4",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n# Helper function to support different mask shapes.\n# Output shape supports (batch_size, number of heads, seq length, seq length)\n# If 2D: broadcasted over batch size and number of heads\n# If 3D: broadcasted over number of heads\n# If 4D: leave as is\ndef expand_mask(mask):\n    assert mask.ndim &gt;= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim &lt; 4:\n        mask = mask.unsqueeze(0)\n    return mask"
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-5",
    "href": "dl_lec11.html#multi-head-attention-5",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\nclass MultiheadAttention(nn.Module):\n    \n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, input_dim)\n        \n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n        \n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n        \n        if return_attention:\n            return o, attention\n        else:\n            return o"
  },
  {
    "objectID": "dl_lec11.html#multi-head-attention-6",
    "href": "dl_lec11.html#multi-head-attention-6",
    "title": "Transformers 2",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\n\n\n\n\n\nPermutation-equivariance\n\n\nOne crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. \\(X_1\\leftrightarrow X_2\\) (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched.\nHence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements.\nThis property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable!\n\n\n\n\n\nBut what if the order of the input is actually important for solving the task, like language modeling? The answer is positional encoding."
  },
  {
    "objectID": "dl_lec11.html#comparison-with-cnns-and-rnns",
    "href": "dl_lec11.html#comparison-with-cnns-and-rnns",
    "title": "Transformers 2",
    "section": "Comparison with CNNs and RNNs",
    "text": "Comparison with CNNs and RNNs\n\n\\(n\\) is the sequence length, \\(d\\) is the representation dimension and \\(k\\) is the kernel size of convolutions."
  },
  {
    "objectID": "dl_lec11.html#transformer-encoder",
    "href": "dl_lec11.html#transformer-encoder",
    "title": "Transformers 2",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\n\n\nHistory reminder\n\n\nOriginally, the Transformer model was designed for machine translation:\n\nit got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation.\non the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN.\n\nThis structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding."
  },
  {
    "objectID": "dl_lec11.html#full-architecture",
    "href": "dl_lec11.html#full-architecture",
    "title": "Transformers 2",
    "section": "Full architecture",
    "text": "Full architecture"
  },
  {
    "objectID": "dl_lec11.html#transformer-encoder-1",
    "href": "dl_lec11.html#transformer-encoder-1",
    "title": "Transformers 2",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\n\n\nEncoder\n\n\n\nThe encoder consists of \\(N\\) identical blocks that are applied in sequence.\nTaking as input \\(x\\), it is first passed through a Multi-Head Attention block as we have implemented above.\nThe output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum.\nOverall, it calculates \\(\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))\\) (\\(x\\) being \\(Q\\), \\(K\\) and \\(V\\) input to the attention layer)."
  },
  {
    "objectID": "dl_lec11.html#transformer-encoder-2",
    "href": "dl_lec11.html#transformer-encoder-2",
    "title": "Transformers 2",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\n\n\nResidual connections\n\n\nThe residual connection is crucial in the Transformer architecture for two reasons:\n\nSimilar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\nWithout the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position \\(i\\) has no relation to its original input."
  },
  {
    "objectID": "dl_lec11.html#layer-normalization",
    "href": "dl_lec11.html#layer-normalization",
    "title": "Transformers 2",
    "section": "Layer Normalization",
    "text": "Layer Normalization\n\n\n\n\n\n\nImportant\n\n\nThe Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence."
  },
  {
    "objectID": "dl_lec11.html#additional-feed-forward-nn",
    "href": "dl_lec11.html#additional-feed-forward-nn",
    "title": "Transformers 2",
    "section": "Additional Feed-Forward NN",
    "text": "Additional Feed-Forward NN\n\n\n\nDescription\n\n\nAdditionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear\\(\\to\\)ReLU\\(\\to\\)Linear MLP. The full transformation including the residual connection can be expressed as:\n\\[\n\\begin{split}\n    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\nThis MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to “post-process” the new information added by the previous Multi-Head Attention, and prepare it for the next attention block."
  },
  {
    "objectID": "dl_lec11.html#additional-feed-forward-nn-1",
    "href": "dl_lec11.html#additional-feed-forward-nn-1",
    "title": "Transformers 2",
    "section": "Additional Feed-Forward NN",
    "text": "Additional Feed-Forward NN\n\n\n\nDimensionality\n\n\nUsually, the inner dimensionality of the MLP is 2-8\\(\\times\\) larger than \\(d_{\\text{model}}\\), i.e. the dimensionality of the original input \\(x\\).\nThe general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution."
  },
  {
    "objectID": "dl_lec11.html#encoder-implementation",
    "href": "dl_lec11.html#encoder-implementation",
    "title": "Transformers 2",
    "section": "Encoder implementation",
    "text": "Encoder implementation\n\nclass EncoderBlock(nn.Module):\n    \n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Dimensionality of the input\n            num_heads - Number of heads to use in the attention block\n            dim_feedforward - Dimensionality of the hidden layer in the MLP\n            dropout - Dropout probability to use in the dropout layers\n        \"\"\"\n        super().__init__()\n        \n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n        \n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim)\n        )\n        \n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n        \n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n        \n        return x"
  },
  {
    "objectID": "dl_lec11.html#transformer-encoder-3",
    "href": "dl_lec11.html#transformer-encoder-3",
    "title": "Transformers 2",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\nclass TransformerEncoder(nn.Module):\n    \n    def __init__(self, num_layers, **block_args):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for l in self.layers:\n            x = l(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = l(x)\n        return attention_maps"
  },
  {
    "objectID": "dl_lec11.html#positional-encoding",
    "href": "dl_lec11.html#positional-encoding",
    "title": "Transformers 2",
    "section": "Positional encoding",
    "text": "Positional encoding\n\n\n\nRationale\n\n\nInstead of learning an embedding for every possible position, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n\\[\nPE_{(pos,i)} = \\begin{cases}\n    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "dl_lec11.html#positional-encoding-1",
    "href": "dl_lec11.html#positional-encoding-1",
    "title": "Transformers 2",
    "section": "Positional encoding",
    "text": "Positional encoding\n\n\n\nInterpretation\n\n\n\\(PE_{(pos,i)}\\) represents the position encoding at position \\(pos\\) in the sequence, and hidden dimensionality \\(i\\). These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see “Positional encoding”), and constitute the position information. We distinguish between even (\\(i \\text{ mod } 2=0\\)) and uneven (\\(i \\text{ mod } 2=1\\)) hidden dimensionalities where we apply a sine/cosine respectively.\nThe intuition behind this encoding is that you can represent \\(PE_{(pos+k,:)}\\) as a linear function of \\(PE_{(pos,:)}\\), which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from \\(2\\pi\\) to \\(10000\\cdot 2\\pi\\)."
  },
  {
    "objectID": "dl_lec11.html#positional-encoding-implementation",
    "href": "dl_lec11.html#positional-encoding-implementation",
    "title": "Transformers 2",
    "section": "Positional encoding implementation",
    "text": "Positional encoding implementation\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # register_buffer =&gt; Tensor which is not a parameter, but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x"
  },
  {
    "objectID": "dl_lec11.html#positional-encoding-visualization",
    "href": "dl_lec11.html#positional-encoding-visualization",
    "title": "Transformers 2",
    "section": "Positional encoding visualization",
    "text": "Positional encoding visualization\n\nCodeResult\n\n\n\nencod_block = PositionalEncoding(d_model=48, max_len=96)\npe = encod_block.pe.squeeze().T.cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\npos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\nfig.colorbar(pos, ax=ax)\nax.set_xlabel(\"Position in sequence\")\nax.set_ylabel(\"Hidden dimension\")\nax.set_title(\"Positional encoding over hidden dimensions\")\nax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\nax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\nplt.show()"
  },
  {
    "objectID": "dl_lec11.html#positional-encoding-visualization-1",
    "href": "dl_lec11.html#positional-encoding-visualization-1",
    "title": "Transformers 2",
    "section": "Positional encoding visualization",
    "text": "Positional encoding visualization\n\nCodeResult\n\n\n\nsns.set_theme()\nfig, ax = plt.subplots(2, 2, figsize=(12,4))\nax = [a for a_list in ax for a in a_list]\nfor i in range(len(ax)):\n    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n    ax[i].set_xticks(np.arange(1,17))\n    ax[i].tick_params(axis='both', which='major', labelsize=10)\n    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n    ax[i].set_ylim(-1.2, 1.2)\nfig.subplots_adjust(hspace=0.8)\nsns.reset_orig()\nplt.show()"
  },
  {
    "objectID": "dl_lec11.html#learning-rate-warm-up",
    "href": "dl_lec11.html#learning-rate-warm-up",
    "title": "Transformers 2",
    "section": "Learning rate warm-up",
    "text": "Learning rate warm-up\n\n\n\nDescription\n\n\nWe gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations.\nTraining a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing."
  },
  {
    "objectID": "dl_lec11.html#learning-rate-warm-up-1",
    "href": "dl_lec11.html#learning-rate-warm-up-1",
    "title": "Transformers 2",
    "section": "Learning rate warm-up",
    "text": "Learning rate warm-up\n\n\n\nIntuition\n\n\n\nAdam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like RAdam have been shown to overcome this issue, not requiring warm-up for training Transformers.\nThe iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using Pre-Layer Normalization (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques (Adaptive Normalization, Power Normalization)."
  },
  {
    "objectID": "dl_lec11.html#learning-rate-warm-up-2",
    "href": "dl_lec11.html#learning-rate-warm-up-2",
    "title": "Transformers 2",
    "section": "Learning rate warm-up",
    "text": "Learning rate warm-up\n\nclass CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n        \n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n    \n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch &lt;= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor"
  },
  {
    "objectID": "dl_lec11.html#learning-rate-warm-up-3",
    "href": "dl_lec11.html#learning-rate-warm-up-3",
    "title": "Transformers 2",
    "section": "Learning rate warm-up",
    "text": "Learning rate warm-up\n\nCodeResult\n\n\n\n# Needed for initializing the lr scheduler\np = nn.Parameter(torch.empty(4,4))\noptimizer = optim.Adam([p], lr=1e-3)\nlr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n\n# Plotting\nepochs = list(range(2000))\nsns.set()\nplt.figure(figsize=(8,3))\nplt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\nplt.ylabel(\"Learning rate factor\")\nplt.xlabel(\"Iterations (in batches)\")\nplt.title(\"Cosine Warm-up Learning Rate Scheduler\")\nplt.show()\nsns.reset_orig()"
  },
  {
    "objectID": "dl_lec11.html#pytorch-lightning-module",
    "href": "dl_lec11.html#pytorch-lightning-module",
    "title": "Transformers 2",
    "section": "PyTorch Lightning Module",
    "text": "PyTorch Lightning Module\n\nclass TransformerPredictor(pl.LightningModule):\n\n    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Hidden dimensionality of the input\n            model_dim - Hidden dimensionality to use inside the Transformer\n            num_classes - Number of classes to predict per sequence element\n            num_heads - Number of heads to use in the Multi-Head Attention blocks\n            num_layers - Number of encoder blocks to use.\n            lr - Learning rate in the optimizer\n            warmup - Number of warmup steps. Usually between 50 and 500\n            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            dropout - Dropout to apply inside the model\n            input_dropout - Dropout to apply on the input features\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -&gt; Model dim\n        self.input_net = nn.Sequential(\n            nn.Dropout(self.hparams.input_dropout),\n            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n        )\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n                                              input_dim=self.hparams.model_dim,\n                                              dim_feedforward=2*self.hparams.model_dim,\n                                              num_heads=self.hparams.num_heads,\n                                              dropout=self.hparams.dropout)\n        # Output classifier per sequence lement\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n        ) \n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n        \n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, \n                                             warmup=self.hparams.warmup, \n                                             max_iters=self.hparams.max_iters)\n        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError    \n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence",
    "href": "dl_lec11.html#sequence-to-sequence",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nDescription\n\n\nA Sequence-to-Sequence task represents a task where the input and the output is a sequence, not necessarily of the same length. Popular tasks in this domain include:\n\nmachine translation\nsummarization\n\nFor this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner."
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-1",
    "href": "dl_lec11.html#sequence-to-sequence-1",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nTask definition\n\n\nHere, however, we will go back to a much simpler example task and use only the encoder.\nGiven a sequence of \\(N\\) numbers between \\(0\\) and \\(M\\), the task is to reverse the input sequence. In Numpy notation, if our input is \\(x\\), the output should be \\(x\\)[::-1].\nAlthough this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well."
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-2",
    "href": "dl_lec11.html#sequence-to-sequence-2",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nDataset\n\n\n\nclass ReverseDataset(data.Dataset):\n\n    def __init__(self, num_categories, seq_len, size):\n        super().__init__()\n        self.num_categories = num_categories\n        self.seq_len = seq_len\n        self.size = size\n        \n        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n  \n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        inp_data = self.data[idx]\n        labels = torch.flip(inp_data, dims=(0,))\n        return inp_data, labels"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-3",
    "href": "dl_lec11.html#sequence-to-sequence-3",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nData loaders\n\n\n\ndataset = partial(ReverseDataset, 10, 16)\ntrain_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\nval_loader   = data.DataLoader(dataset(1000), batch_size=128)\ntest_loader  = data.DataLoader(dataset(10000), batch_size=128)\n\n\n\n\n\n\n\nData examples\n\n\n\ninp_data, labels = train_loader.dataset[0]\nprint(\"Input data:\", inp_data)\nprint(\"Labels:    \", labels)\n\nInput data: tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9])\nLabels:     tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9])"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-4",
    "href": "dl_lec11.html#sequence-to-sequence-4",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nTraining: embeddings\n\n\n\nDuring training, we pass the input sequence through the Transformer encoder and predict the output for each input token.\nWe use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector.\nAn alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module nn.Embedding.\nHowever, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (self.input_net maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category)."
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-5",
    "href": "dl_lec11.html#sequence-to-sequence-5",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\nclass ReversePredictor(TransformerPredictor):\n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        # Fetch data and transform categories to one-hot vectors\n        inp_data, labels = batch\n        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n        \n        # Perform prediction and calculate loss and accuracy\n        preds = self.forward(inp_data, add_positional_encoding=True)\n        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n        \n        # Logging\n        self.log(f\"{mode}_loss\", loss)\n        self.log(f\"{mode}_acc\", acc)\n        return loss, acc\n        \n    def training_step(self, batch, batch_idx):\n        loss, _ = self._calculate_loss(batch, mode=\"train\")\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"val\")\n    \n    def test_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"test\")"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-6",
    "href": "dl_lec11.html#sequence-to-sequence-6",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nTraining: gradient clipping\n\n\n\ngradient_clip_val: this clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like DeepAI glossary).\nFor Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward.\nIn plain PyTorch, you can apply gradient clipping via torch.nn.utils.clip_grad_norm_(...) (see documentation).\nThe clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients."
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-7",
    "href": "dl_lec11.html#sequence-to-sequence-7",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\ndef train_reverse(**kwargs):\n    # Create a PyTorch Lightning trainer with the generation callback\n    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n    os.makedirs(root_dir, exist_ok=True)\n    trainer = pl.Trainer(default_root_dir=root_dir, \n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=10,\n                         gradient_clip_val=5)\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n    \n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(\"Found pretrained model, loading...\")\n        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n    else:\n        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n        trainer.fit(model, train_loader, val_loader)\n        \n    # Test best model on validation and test set\n    val_result = trainer.test(model, val_loader, verbose=False)\n    test_result = trainer.test(model, test_loader, verbose=False)\n    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n    \n    model = model.to(device)\n    return model, result"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-8",
    "href": "dl_lec11.html#sequence-to-sequence-8",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\nreverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n                                              model_dim=32,\n                                              num_heads=1,\n                                              num_classes=train_loader.dataset.num_categories,\n                                              num_layers=1,\n                                              dropout=0.0,\n                                              lr=5e-4,\n                                              warmup=50)\n\nFound pretrained model, loading...\n\n\n\n\n\n\n\n\n\nprint(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\nprint(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")\n\nVal accuracy:  100.00%\nTest accuracy: 100.00%"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-9",
    "href": "dl_lec11.html#sequence-to-sequence-9",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nVisualizing attention\n\n\n\ndata_input, labels = next(iter(val_loader))\ninp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\ninp_data = inp_data.to(device)\nattention_maps = reverse_model.get_attention_maps(inp_data)\n\n\nattention_maps[0].shape\n\ntorch.Size([128, 1, 16, 16])"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-10",
    "href": "dl_lec11.html#sequence-to-sequence-10",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\ndef plot_attention_maps(input_data, attn_maps, idx=0):\n    if input_data is not None:\n        input_data = input_data[idx].detach().cpu().numpy()\n    else:\n        input_data = np.arange(attn_maps[0][idx].shape[-1])\n    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n    \n    num_heads = attn_maps[0].shape[0]\n    num_layers = len(attn_maps)\n    seq_len = input_data.shape[0]\n    fig_size = 4 if num_heads == 1 else 3\n    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n    if num_layers == 1:\n        ax = [ax]\n    if num_heads == 1:\n        ax = [[a] for a in ax]\n    for row in range(num_layers):\n        for column in range(num_heads):\n            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n            ax[row][column].set_xticks(list(range(seq_len)))\n            ax[row][column].set_xticklabels(input_data.tolist())\n            ax[row][column].set_yticks(list(range(seq_len)))\n            ax[row][column].set_yticklabels(input_data.tolist())\n            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()"
  },
  {
    "objectID": "dl_lec11.html#sequence-to-sequence-11",
    "href": "dl_lec11.html#sequence-to-sequence-11",
    "title": "Transformers 2",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\n\n\n\nVisualization\n\n\n\nplot_attention_maps(data_input, attention_maps, idx=0)\n\n\n\n\n\n\n\n\nResult: the model has learned to attend to the token that is on the flipped index of itself."
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection",
    "href": "dl_lec11.html#set-anomaly-detection",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\n\n\nIssues with order\n\n\n\nRNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data.\nVinyals et al. (2015) and other papers have shown that the assumed order can have a significant impact on the model’s performance, and hence, we should try to not use RNNs on sets.\nIdeally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set.\n\n\n\n\n\n\n\nDescription\n\n\nSet Anomaly Detection: we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where \\(N-1\\) images belong to the same category/have the same high-level features while one belongs to another category."
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-1",
    "href": "dl_lec11.html#set-anomaly-detection-1",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\n\n\nDataset notes\n\n\n\nWe will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\nInstead of raw images, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images."
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-2",
    "href": "dl_lec11.html#set-anomaly-detection-2",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\n# ImageNet statistics\nDATA_MEANS = np.array([0.485, 0.456, 0.406])\nDATA_STD = np.array([0.229, 0.224, 0.225])\n# As torch tensors for later preprocessing\nTORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\nTORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n\n# Resize to 224x224, and normalize to ImageNet statistic\ntransform = transforms.Compose([transforms.Resize((224,224)),\n                                transforms.ToTensor(),\n                                transforms.Normalize(DATA_MEANS, DATA_STD)\n                                ])\n# Loading the training dataset. \ntrain_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n\n# Loading the test set\ntest_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-3",
    "href": "dl_lec11.html#set-anomaly-detection-3",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nimport os\nos.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\npretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n# Remove classification layer\n# In some models, it is called \"fc\", others have \"classifier\"\n# Setting both to an empty sequential represents an identity map of the final features.\npretrained_model.fc = nn.Sequential()\npretrained_model.classifier = nn.Sequential()\n# To GPU\npretrained_model = pretrained_model.to(device)\n\n# Only eval, no gradient required\npretrained_model.eval()\nfor p in pretrained_model.parameters():\n    p.requires_grad = False"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-4",
    "href": "dl_lec11.html#set-anomaly-detection-4",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\n@torch.no_grad()\ndef extract_features(dataset, save_file):\n    if not os.path.isfile(save_file):\n        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=0)\n        extracted_features = []\n        for imgs, _ in tqdm(data_loader):\n            imgs = imgs.to(device)\n            feats = pretrained_model(imgs)\n            extracted_features.append(feats)\n        extracted_features = torch.cat(extracted_features, dim=0)\n        extracted_features = extracted_features.detach().cpu()\n        torch.save(extracted_features, save_file)\n    else:\n        extracted_features = torch.load(save_file)\n    return extracted_features\n\ntrain_feat_file = os.path.join(CHECKPOINT_PATH, \"train_set_features.tar\")\ntrain_set_feats = extract_features(train_set, train_feat_file)\n\ntest_feat_file = os.path.join(CHECKPOINT_PATH, \"test_set_features.tar\")\ntest_feats = extract_features(test_set, test_feat_file)\n\nprint(\"Train:\", train_set_feats.shape)\nprint(\"Test: \", test_feats.shape)\n\nTrain: torch.Size([50000, 512])\nTest:  torch.Size([10000, 512])"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-5",
    "href": "dl_lec11.html#set-anomaly-detection-5",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\n\n\nValidation set\n\n\n\n## Split train into train+val\n# Get labels from train set\nlabels = train_set.targets\n\n# Get indices of images per class\nlabels = torch.LongTensor(labels)\nnum_labels = labels.max()+1\nsorted_indices = torch.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n\n# Determine number of validation images per class\nnum_val_exmps = sorted_indices.shape[1] // 10\n\n# Get image indices for validation and training\nval_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\ntrain_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n\n# Group corresponding image features and labels\ntrain_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\nval_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-6",
    "href": "dl_lec11.html#set-anomaly-detection-6",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nclass SetAnomalyDataset(data.Dataset):\n    \n    def __init__(self, img_feats, labels, set_size=10, train=True):\n        \"\"\"\n        Inputs:\n            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n            labels - Tensor of shape [num_imgs], containing the class labels for the images\n            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n            train - If True, a new set will be sampled every time __getitem__ is called.\n        \"\"\"\n        super().__init__()\n        self.img_feats = img_feats\n        self.labels = labels\n        self.set_size = set_size-1 # The set size is here the size of correct images\n        self.train = train\n        \n        # Tensors with indices of the images per class\n        self.num_labels = labels.max()+1\n        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n        \n        if not train:\n            self.test_sets = self._create_test_sets()\n            \n            \n    def _create_test_sets(self):\n        # Pre-generates the sets for each image for the test set\n        test_sets = []\n        num_imgs = self.img_feats.shape[0]\n        np.random.seed(42)\n        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n        test_sets = torch.stack(test_sets, dim=0)\n        return test_sets\n            \n        \n    def sample_img_set(self, anomaly_label):\n        \"\"\"\n        Samples a new set of images, given the label of the anomaly. \n        The sampled images come from a different class than anomaly_label\n        \"\"\"\n        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n        set_label = np.random.randint(self.num_labels-1)\n        if set_label &gt;= anomaly_label:\n            set_label += 1\n            \n        # Sample images from the class determined above\n        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n        img_indices = self.img_idx_by_label[set_label, img_indices]\n        return img_indices\n        \n        \n    def __len__(self):\n        return self.img_feats.shape[0]\n    \n    \n    def __getitem__(self, idx):\n        anomaly = self.img_feats[idx]\n        if self.train: # If train =&gt; sample\n            img_indices = self.sample_img_set(self.labels[idx])\n        else: # If test =&gt; use pre-generated ones\n            img_indices = self.test_sets[idx]\n            \n        # Concatenate images. The anomaly is always the last image for simplicity\n        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n        label = img_set.shape[0]-1\n        \n        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n        return img_set, indices, label"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-7",
    "href": "dl_lec11.html#set-anomaly-detection-7",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nSET_SIZE = 10\ntest_labels = torch.LongTensor(test_set.targets)\n\ntrain_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\nval_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\ntest_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n\ntrain_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=0, pin_memory=True)\nval_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=0)\ntest_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=0)"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-8",
    "href": "dl_lec11.html#set-anomaly-detection-8",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nCodeResult\n\n\n\ndef visualize_exmp(indices, orig_dataset):\n    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n    images = torch.stack(images, dim=0)\n    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n    \n    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n    img_grid = img_grid.permute(1, 2, 0)\n\n    plt.figure(figsize=(12,8))\n    plt.title(\"Anomaly examples on CIFAR100\")\n    plt.imshow(img_grid)\n    plt.axis('off')\n    plt.show()\n    plt.close()\n\n_, indices, _ = next(iter(test_anom_loader))\nvisualize_exmp(indices[:4], test_set)"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-9",
    "href": "dl_lec11.html#set-anomaly-detection-9",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nclass AnomalyPredictor(TransformerPredictor):\n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        img_sets, _, labels = batch\n        preds = self.forward(img_sets, add_positional_encoding=False) # No positional encodings as it is a set, not a sequence!\n        preds = preds.squeeze(dim=-1) # Shape: [Batch_size, set_size]\n        loss = F.cross_entropy(preds, labels) # Softmax/CE over set dimension\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n        self.log(f\"{mode}_loss\", loss)\n        self.log(f\"{mode}_acc\", acc, on_step=False, on_epoch=True)\n        return loss, acc\n        \n    def training_step(self, batch, batch_idx):\n        loss, _ = self._calculate_loss(batch, mode=\"train\")\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"val\")\n    \n    def test_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"test\")"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-10",
    "href": "dl_lec11.html#set-anomaly-detection-10",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\ndef train_anomaly(**kwargs):\n    # Create a PyTorch Lightning trainer with the generation callback\n    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n    os.makedirs(root_dir, exist_ok=True)\n    trainer = pl.Trainer(default_root_dir=root_dir, \n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=100,\n                         gradient_clip_val=2)\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n    \n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(\"Found pretrained model, loading...\")\n        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n    else:\n        model = AnomalyPredictor(max_iters=trainer.max_epochs*len(train_anom_loader), **kwargs)\n        trainer.fit(model, train_anom_loader, val_anom_loader)\n        model = AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n    \n    # Test best model on validation and test set\n    train_result = trainer.test(model, train_anom_loader, verbose=False)\n    val_result = trainer.test(model, val_anom_loader, verbose=False)\n    test_result = trainer.test(model, test_anom_loader, verbose=False)\n    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n    \n    model = model.to(device)\n    return model, result"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-11",
    "href": "dl_lec11.html#set-anomaly-detection-11",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nanomaly_model, anomaly_result = train_anomaly(input_dim=train_anom_dataset.img_feats.shape[-1],\n                                              model_dim=256,\n                                              num_heads=4,\n                                              num_classes=1,\n                                              num_layers=4,\n                                              dropout=0.1,\n                                              input_dropout=0.1,\n                                              lr=5e-4,\n                                              warmup=100)\n\nFound pretrained model, loading..."
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-12",
    "href": "dl_lec11.html#set-anomaly-detection-12",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nprint(f\"Train accuracy: {(100.0*anomaly_result['train_acc']):4.2f}%\")\nprint(f\"Val accuracy:   {(100.0*anomaly_result['val_acc']):4.2f}%\")\nprint(f\"Test accuracy:  {(100.0*anomaly_result['test_acc']):4.2f}%\")\n\nTrain accuracy: 96.27%\nVal accuracy:   96.40%\nTest accuracy:  94.34%"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-13",
    "href": "dl_lec11.html#set-anomaly-detection-13",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\ninp_data, indices, labels = next(iter(test_anom_loader))\ninp_data = inp_data.to(device)\nanomaly_model.eval()\nwith torch.no_grad():\n    preds = anomaly_model.forward(inp_data, add_positional_encoding=False)\n    preds = F.softmax(preds.squeeze(dim=-1), dim=-1)\n    # Permut input data\n    permut = np.random.permutation(inp_data.shape[1])\n    perm_inp_data = inp_data[:,permut]\n    perm_preds = anomaly_model.forward(perm_inp_data, add_positional_encoding=False)\n    perm_preds = F.softmax(perm_preds.squeeze(dim=-1), dim=-1)\nassert (preds[:,permut] - perm_preds).abs().max() &lt; 1e-5, \"Predictions are not permutation equivariant\"\nprint(\"Preds\\n\", preds[0,permut].cpu().numpy())\nprint(\"Permuted preds\\n\", perm_preds[0].cpu().numpy())\n\nPreds\n [9.9983788e-01 2.8534807e-05 1.6281172e-05 1.1398807e-05 1.9029028e-05\n 1.4160971e-05 1.1995394e-05 2.8615961e-05 1.4995643e-05 1.7090044e-05]\nPermuted preds\n [9.9983788e-01 2.8534861e-05 1.6281203e-05 1.1398840e-05 1.9029065e-05\n 1.4161012e-05 1.1995416e-05 2.8616014e-05 1.4995686e-05 1.7090077e-05]"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-14",
    "href": "dl_lec11.html#set-anomaly-detection-14",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nCodeResult\n\n\n\nattention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)\npredictions = preds.argmax(dim=-1)\n\ndef visualize_prediction(idx):\n    visualize_exmp(indices[idx:idx+1], test_set)\n    print(\"Prediction:\", predictions[idx].item())\n    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n\nvisualize_prediction(0)\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction: 9"
  },
  {
    "objectID": "dl_lec11.html#set-anomaly-detection-15",
    "href": "dl_lec11.html#set-anomaly-detection-15",
    "title": "Transformers 2",
    "section": "Set Anomaly Detection",
    "text": "Set Anomaly Detection\n\nmistakes = torch.where(predictions != 9)[0].cpu().numpy()\nprint(\"Indices with mistake:\", mistakes)\n\nIndices with mistake: [11 30]\n\n\n\nvisualize_prediction(mistakes[-1])\nprint(\"Probabilities:\")\nfor i, p in enumerate(preds[mistakes[-1]].cpu().numpy()):\n    print(f\"Image {i}: {100.0*p:4.2f}%\")\n\n\n\n\n\n\n\n\nPrediction: 3\n\n\n\n\n\n\n\n\n\nProbabilities:\nImage 0: 0.15%\nImage 1: 0.09%\nImage 2: 0.06%\nImage 3: 93.35%\nImage 4: 0.09%\nImage 5: 0.95%\nImage 6: 0.07%\nImage 7: 0.04%\nImage 8: 0.24%\nImage 9: 4.97%"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers",
    "href": "dl_lec11.html#vision-transformers",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n## Standard libraries\nimport os\nimport numpy as np\nimport random\nimport math\nimport json\nfrom functools import partial\nfrom PIL import Image\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\nplt.set_cmap('cividis')\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nfrom matplotlib.colors import to_rgb\nimport matplotlib\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nimport seaborn as sns\nsns.reset_orig()\n\n## tqdm for loading bars\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n\n## Torchvision\nimport torchvision\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\n\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning&gt;=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n# Import tensorboard\n%load_ext tensorboard\n\n# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\nDATASET_PATH = \"../data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial15\"\n\n# Setting the seed\npl.seed_everything(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.mps.deterministic = True\ntorch.backends.mps.benchmark = False\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-1",
    "href": "dl_lec11.html#vision-transformers-1",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/\"\n# Files to download\npretrained_files = [\"tutorial15/ViT.ckpt\", \"tutorial15/tensorboards/ViT/events.out.tfevents.ViT\",\n                    \"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet\"]\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name.split(\"/\",1)[1])\n    if \"/\" in file_name.split(\"/\",1)[1]:\n        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-2",
    "href": "dl_lec11.html#vision-transformers-2",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nCodeResult\n\n\n\ntest_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n                                     ])\n# For training, we add some augmentation. Networks are too powerful and would overfit.\ntrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n                                     ])\n# Loading the training dataset. We need to split it into a training and validation part\n# We need to do a little trick because the validation set should not use the augmentation.\ntrain_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\nval_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\npl.seed_everything(42)\ntrain_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\npl.seed_everything(42)\n_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n\n# Loading the test set\ntest_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\ntrain_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\nval_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\ntest_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n\n# Visualize some examples\nNUM_IMAGES = 4\nCIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\nimg_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\nimg_grid = img_grid.permute(1, 2, 0)\n\nplt.figure(figsize=(8,8))\nplt.title(\"Image examples of the CIFAR10 dataset\")\nplt.imshow(img_grid)\nplt.axis('off')\nplt.show()\nplt.close()"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-3",
    "href": "dl_lec11.html#vision-transformers-3",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nTransformers for image classification\n\n\nThe Vision Transformer is a model for image classification that views images as sequences of smaller patches.\n\nAs a preprocessing step, we split an image of, for example, \\(48\\times 48\\) pixels into 9 \\(16\\times 16\\) patches.\nEach of those patches is considered to be a “word”/“token” and projected to a feature space.\nWith adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task."
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-4",
    "href": "dl_lec11.html#vision-transformers-4",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-5",
    "href": "dl_lec11.html#vision-transformers-5",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nImage preprocessing\n\n\nAn image of size \\(N\\times N\\) has to be split into \\((N/M)^2\\) patches of size \\(M\\times M\\). These represent the input words to the Transformer.\n\n\n\n\ndef img_to_patch(x, patch_size, flatten_channels=True):\n    \"\"\"\n    Inputs:\n        x - torch.Tensor representing the image of shape [B, C, H, W]\n        patch_size - Number of pixels per dimension of the patches (integer)\n        flatten_channels - If True, the patches will be returned in a flattened format\n                           as a feature vector instead of a image grid.\n    \"\"\"\n    B, C, H, W = x.shape\n    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n    if flatten_channels:\n        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n    return x"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-6",
    "href": "dl_lec11.html#vision-transformers-6",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nVisualization\n\n\n\nimg_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n\nfig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\nfig.suptitle(\"Images as input sequences of patches\")\nfor i in range(CIFAR_images.shape[0]):\n    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n    img_grid = img_grid.permute(1, 2, 0)\n    ax[i].imshow(img_grid)\n    ax[i].axis('off')\nplt.show()\nplt.close()"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-7",
    "href": "dl_lec11.html#vision-transformers-7",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nPre-Layer Normalization: the idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage."
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-8",
    "href": "dl_lec11.html#vision-transformers-8",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nclass AttentionBlock(nn.Module):\n    \n    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of input and attention feature vectors\n            hidden_dim - Dimensionality of hidden layer in feed-forward network \n                         (usually 2-4x larger than embed_dim)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            dropout - Amount of dropout to apply in the feed-forward network\n        \"\"\"\n        super().__init__()\n        \n        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, \n                                          dropout=dropout)\n        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n        self.linear = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n        \n        \n    def forward(self, x):\n        inp_x = self.layer_norm_1(x)\n        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n        x = x + self.linear(self.layer_norm_2(x))\n        return x"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-9",
    "href": "dl_lec11.html#vision-transformers-9",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nModules\n\n\n\nEncoder\nA linear projection layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each \\(M\\times M\\) patch independently as input.\nA classification token that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.\nLearnable positional encodings that are added to the tokens before being processed by the Transformer. We can learn them instead of having the pattern of sine and cosine functions.\nAn MLP head that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer."
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-10",
    "href": "dl_lec11.html#vision-transformers-10",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nclass VisionTransformer(nn.Module):\n    \n    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of the input feature vectors to the Transformer\n            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n                         within the Transformer\n            num_channels - Number of channels of the input (3 for RGB)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            num_layers - Number of layers to use in the Transformer\n            num_classes - Number of classes to predict\n            patch_size - Number of pixels that the patches have per dimension\n            num_patches - Maximum number of patches an image can have\n            dropout - Amount of dropout to apply in the feed-forward network and \n                      on the input encoding\n        \"\"\"\n        super().__init__()\n        \n        self.patch_size = patch_size\n        \n        # Layers/Networks\n        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n        # Parameters/Embeddings\n        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n    \n    \n    def forward(self, x):\n        # Preprocess input\n        x = img_to_patch(x, self.patch_size)\n        B, T, _ = x.shape\n        x = self.input_layer(x)\n        \n        # Add CLS token and positional encoding\n        cls_token = self.cls_token.repeat(B, 1, 1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = x + self.pos_embedding[:,:T+1]\n        \n        # Apply Transforrmer\n        x = self.dropout(x)\n        x = x.transpose(0, 1)\n        x = self.transformer(x)\n        \n        # Perform classification prediction\n        cls = x[0]\n        out = self.mlp_head(cls)\n        return out"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-11",
    "href": "dl_lec11.html#vision-transformers-11",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nclass ViT(pl.LightningModule):\n    \n    def __init__(self, model_kwargs, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = VisionTransformer(**model_kwargs)\n        self.example_input_array = next(iter(train_loader))[0]\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n        return [optimizer], [lr_scheduler]   \n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = F.cross_entropy(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n        \n        self.log(f'{mode}_loss', loss)\n        self.log(f'{mode}_acc', acc)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._calculate_loss(batch, mode=\"train\")\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"val\")\n\n    def test_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"test\")"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-12",
    "href": "dl_lec11.html#vision-transformers-12",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nTraining\n\n\n\ndef train_model(**kwargs):\n    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), \n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=180,\n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n                                    LearningRateMonitor(\"epoch\")])\n    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n\n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n    else:\n        pl.seed_everything(42) # To be reproducable\n        model = ViT(**kwargs)\n        trainer.fit(model, train_loader, val_loader)\n        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n\n    # Test best model on validation and test set\n    val_result = trainer.test(model, val_loader, verbose=False)\n    test_result = trainer.test(model, test_loader, verbose=False)\n    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n\n    return model, result"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-13",
    "href": "dl_lec11.html#vision-transformers-13",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nConsiderations\n\n\n\npatch size: the smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to its quadratic memory usage in the attention layer.\nthe embedding and hidden dimensionality have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of embed_dim/num_heads.\nthe learning rate for Transformers is usually relatively small, and in papers, a common value to use is 3e-5."
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-14",
    "href": "dl_lec11.html#vision-transformers-14",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nmodel, results = train_model(model_kwargs={\n                                'embed_dim': 256,\n                                'hidden_dim': 512,\n                                'num_heads': 8,\n                                'num_layers': 6,\n                                'patch_size': 4,\n                                'num_channels': 3,\n                                'num_patches': 64,\n                                'num_classes': 10,\n                                'dropout': 0.2\n                            },\n                            lr=3e-4)\nprint(\"ViT results\", results)\n\nFound pretrained model at ../saved_models/tutorial15/ViT.ckpt, loading...\n\n\n\n\n\n\n\n\nViT results {'test': 0.7713000178337097, 'val': 0.7781999707221985}"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-15",
    "href": "dl_lec11.html#vision-transformers-15",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers"
  },
  {
    "objectID": "dl_lec11.html#vision-transformers-16",
    "href": "dl_lec11.html#vision-transformers-16",
    "title": "Transformers 2",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\n\n\nInterpretation\n\n\nInductive biases:\n\nCNNs have been designed with the assumption that images are translation invariant. Hence, we apply convolutions with shared filters across the image.\na CNN architecture integrates the concept of distance in an image: two pixels that are close to each other are more related than two distant pixels. Local patterns are combined into larger patterns until we perform our classification prediction."
  },
  {
    "objectID": "nlp_lab8.html",
    "href": "nlp_lab8.html",
    "title": "NLP: Lab 8 (Word2vec)",
    "section": "",
    "text": "Install gensim:\npip install gensim\npip install pot"
  },
  {
    "objectID": "nlp_lab8.html#train-your-own-word2vec",
    "href": "nlp_lab8.html#train-your-own-word2vec",
    "title": "NLP: Lab 8 (Word2vec)",
    "section": "Train your own word2vec",
    "text": "Train your own word2vec\n\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nCheck common_texts:\n\ncommon_texts\n\n[['human', 'interface', 'computer'],\n ['survey', 'user', 'computer', 'system', 'response', 'time'],\n ['eps', 'user', 'interface', 'system'],\n ['system', 'human', 'system', 'eps'],\n ['user', 'response', 'time'],\n ['trees'],\n ['graph', 'trees'],\n ['graph', 'minors', 'trees'],\n ['graph', 'minors', 'survey']]\n\n\nTrain your model:\n\nmodel = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n\nNow you have your vectors:\n\nmodel.wv\n\n&lt;gensim.models.keyedvectors.KeyedVectors at 0x1285048c0&gt;"
  },
  {
    "objectID": "nlp_lab8.html#downloading-pre-trained",
    "href": "nlp_lab8.html#downloading-pre-trained",
    "title": "NLP: Lab 8 (Word2vec)",
    "section": "Downloading pre-trained",
    "text": "Downloading pre-trained\nFirst, check what models are available:\n\nimport gensim.downloader\n\nmodels = gensim.downloader.info()['models']\n\nDownload the light one:\n\nglove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')\n\nglove_vectors will also be a KeyedVectors object."
  },
  {
    "objectID": "nlp_lab8.html#similarity",
    "href": "nlp_lab8.html#similarity",
    "title": "NLP: Lab 8 (Word2vec)",
    "section": "Similarity",
    "text": "Similarity\n\nmodel.wv.most_similar('trees', topn=10)\n\n[('survey', 0.19912061095237732),\n ('human', 0.17272791266441345),\n ('minors', 0.17018885910511017),\n ('time', 0.14595060050487518),\n ('eps', 0.06408978998661041),\n ('response', -0.00276578264310956),\n ('user', -0.013535063713788986),\n ('graph', -0.023671653121709824),\n ('computer', -0.03284316137433052),\n ('system', -0.05234672874212265)]\n\n\nBetween two words:\n\nmodel.wv.similarity('trees', 'graph')\n\n-0.023671653\n\n\nOdd ones:\n\nmodel.wv.doesnt_match(['minors', 'human', 'interface'])\n\n'minors'"
  },
  {
    "objectID": "nlp_lab8.html#distance",
    "href": "nlp_lab8.html#distance",
    "title": "NLP: Lab 8 (Word2vec)",
    "section": "Distance",
    "text": "Distance\nBetween words:\n\nmodel.wv.distance('trees', 'graph')\n\n1.0236716531217098\n\n\nBetween documents:\n\nmodel.wv.wmdistance(['trees'], ['graph'])\n\n1.4308540422347924"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#lectures",
    "href": "dl.html#lectures",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#labs",
    "href": "dl.html#labs",
    "title": "Deep Learning",
    "section": "Labs",
    "text": "Labs\nLab 1: Logistic Regression using a single-layer Neural Network\nLab 2: 2-layer Neural Networ\nLab 3: Deep Neural Network from Scratch\nLab 4: Initialization/Regularization\nLab 5: Optimization\nLab 6: PyTorch basics\nLab 7: Convolutional networks\nLab 8: Character-Level LSTM in PyTorch"
  },
  {
    "objectID": "dl.html#python-venv-info",
    "href": "dl.html#python-venv-info",
    "title": "Deep Learning",
    "section": "Python venv info",
    "text": "Python venv info\nInstructions on how to create a virtual environment for DL course:\n\nLaunch Anaconda Prompt. In it execute:\n\npython -m venv dl_venv\n\nActivate the environment:\n\ndl_venv\\Scripts\\activate\n\nInstall required packages:\n\npip install jupyter matplotlib scipy numpy scikit-learn termcolor h5py\n\nRun Jupyter:\n\njupyter notebook\n\nVoila! (pardon my French)"
  },
  {
    "objectID": "dl_lab8.html",
    "href": "dl_lab8.html",
    "title": "DL: Lab 7",
    "section": "",
    "text": "Character-Level LSTM in PyTorch\nJupyter notebook attached (dl_lab8.zip)"
  },
  {
    "objectID": "dl_lec8.html#modern-cnn-variants",
    "href": "dl_lec8.html#modern-cnn-variants",
    "title": "Convolutional networks: examples",
    "section": "Modern CNN variants",
    "text": "Modern CNN variants\nIn this tutorial, we will implement and discuss variants of modern CNN architectures. There have been many different architectures been proposed over the past few years. Some of the most impactful ones, and still relevant today, are the following:\n\nGoogleNet/Inception architecture (winner of ILSVRC 2014)\nResNet (winner of ILSVRC 2015)\nDenseNet (best paper award CVPR 2017).\n\n\n\nAll of them were state-of-the-art models when being proposed, and the core ideas of these networks are the foundations for most current state-of-the-art architectures."
  },
  {
    "objectID": "dl_lec8.html#imports",
    "href": "dl_lec8.html#imports",
    "title": "Convolutional networks: examples",
    "section": "Imports",
    "text": "Imports\n\n## Standard libraries\nimport os\nimport numpy as np \nimport random\nfrom PIL import Image\nfrom types import SimpleNamespace\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline \nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nimport matplotlib\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nimport seaborn as sns\nsns.reset_orig()\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\n# Torchvision\nimport torchvision\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms"
  },
  {
    "objectID": "dl_lec8.html#seeds",
    "href": "dl_lec8.html#seeds",
    "title": "Convolutional networks: examples",
    "section": "Seeds",
    "text": "Seeds\n\n# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\nDATASET_PATH = \"../data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial5\"\n\n# Function for setting the seed\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.mps.is_available():\n        torch.mps.manual_seed(seed)\n        #torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.mps.deterministic = True\ntorch.backends.mps.benchmark = False\n\ndevice = torch.device(\"mps:0\") if torch.mps.is_available() else torch.device(\"cpu\")"
  },
  {
    "objectID": "dl_lec8.html#pre-trained-models",
    "href": "dl_lec8.html#pre-trained-models",
    "title": "Convolutional networks: examples",
    "section": "Pre-trained models",
    "text": "Pre-trained models\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial5/\"\n# Files to download\npretrained_files = [\"GoogleNet.ckpt\", \"ResNet.ckpt\", \"ResNetPreAct.ckpt\", \"DenseNet.ckpt\",\n                    \"tensorboards/GoogleNet/events.out.tfevents.googlenet\",\n                    \"tensorboards/ResNet/events.out.tfevents.resnet\",\n                    \"tensorboards/ResNetPreAct/events.out.tfevents.resnetpreact\",\n                    \"tensorboards/DenseNet/events.out.tfevents.densenet\"]\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n    if \"/\" in file_name:\n        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
  },
  {
    "objectID": "dl_lec8.html#meanstd",
    "href": "dl_lec8.html#meanstd",
    "title": "Convolutional networks: examples",
    "section": "Mean/std",
    "text": "Mean/std\n\n\n\n\n\n\nImportant\n\n\nAs we have learned from the previous tutorial about initialization, it is important to have the data preprocessed with a zero mean.\n\n\n\n\ntrain_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\nDATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\nDATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\nprint(\"Data mean\", DATA_MEANS)\nprint(\"Data std\", DATA_STD)\n\nData mean [0.49139968 0.48215841 0.44653091]\nData std [0.24703223 0.24348513 0.26158784]"
  },
  {
    "objectID": "dl_lec8.html#pre-processing",
    "href": "dl_lec8.html#pre-processing",
    "title": "Convolutional networks: examples",
    "section": "Pre-processing",
    "text": "Pre-processing\n\n\n\nAugmentations\n\n\n\nflip each image horizontally with 50% probability (transforms.RandomHorizontalFlip). The object class usually does not change when flipping an image, and we don’t expect any image information to be dependent on the horizontal orientation. This would be however different if we would try to detect digits or letters in an image, as those have a certain orientation.)\ntransforms.RandomResizedCrop. This transformation crops the image in a small range, eventually changing the aspect ratio, and scaling it back afterward to the previous size. Therefore, the actual pixel values change while the content or overall semantics of the image stays the same."
  },
  {
    "objectID": "dl_lec8.html#pre-processing-1",
    "href": "dl_lec8.html#pre-processing-1",
    "title": "Convolutional networks: examples",
    "section": "Pre-processing",
    "text": "Pre-processing\n\n\n\nCode\n\n\n\ntest_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n                                     ])\n# For training, we add some augmentation. Networks are too powerful and would overfit.\ntrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                      transforms.RandomResizedCrop((32,32), scale=(0.8,1.0), ratio=(0.9,1.1)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n                                     ])\n# Loading the training dataset. We need to split it into a training and validation part\n# We need to do a little trick because the validation set should not use the augmentation.\ntrain_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\nval_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\nset_seed(42)\ntrain_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\nset_seed(42)\n_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n\n# Loading the test set\ntest_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\ntrain_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\nval_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\ntest_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
  },
  {
    "objectID": "dl_lec8.html#pre-processing-2",
    "href": "dl_lec8.html#pre-processing-2",
    "title": "Convolutional networks: examples",
    "section": "Pre-processing",
    "text": "Pre-processing\n\n\n\nVerify normalization\n\n\n\nimgs, _ = next(iter(train_loader))\nprint(\"Batch mean\", imgs.mean(dim=[0,2,3]))\nprint(\"Batch std\", imgs.std(dim=[0,2,3]))\n\nBatch mean tensor([0.0231, 0.0006, 0.0005])\nBatch std tensor([0.9865, 0.9849, 0.9868])"
  },
  {
    "objectID": "dl_lec8.html#visualization",
    "href": "dl_lec8.html#visualization",
    "title": "Convolutional networks: examples",
    "section": "Visualization",
    "text": "Visualization\n\nCodeResult\n\n\n\nNUM_IMAGES = 4\nimages = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]\norig_images = [Image.fromarray(train_dataset.data[idx]) for idx in range(NUM_IMAGES)]\norig_images = [test_transform(img) for img in orig_images]\n\nimg_grid = torchvision.utils.make_grid(torch.stack(images + orig_images, dim=0), nrow=4, normalize=True, pad_value=0.5)\nimg_grid = img_grid.permute(1, 2, 0)\n\nplt.figure(figsize=(8,8))\nplt.title(\"Augmentation examples on CIFAR10\")\nplt.imshow(img_grid)\nplt.axis('off')\nplt.show()\nplt.close()"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning",
    "href": "dl_lec8.html#pytorch-lightning",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n\n\nOverview\n\n\nPyTorch Lightning is a framework that simplifies your code needed to train, evaluate, and test a model in PyTorch. It also handles logging into TensorBoard, a visualization toolkit for ML experiments, and saving model checkpoints automatically with minimal code overhead from our side.\n\n\n\n!pip install pytorch-lightning\n\nimport pytorch_lightning as pl"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-1",
    "href": "dl_lec8.html#pytorch-lightning-1",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n# Setting the seed\npl.seed_everything(42)\n\n42"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-2",
    "href": "dl_lec8.html#pytorch-lightning-2",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n\n\nPlan\n\n\nIn PyTorch Lightning, we define pl.LightningModule’s (inheriting from torch.nn.Module) that organize our code into 5 main sections:\n\nInitialization (__init__), where we create all necessary parameters/models\nOptimizers (configure_optimizers) where we create the optimizers, learning rate scheduler, etc.\nTraining loop (training_step) where we only have to define the loss calculation for a single batch (the loop of optimizer.zero_grad(), loss.backward() and optimizer.step(), as well as any logging/saving operation, is done in the background)\nValidation loop (validation_step) where similarly to the training, we only have to define what should happen per step\nTest loop (test_step) which is the same as validation, only on a test set.\n\nTherefore, we don’t abstract the PyTorch code, but rather organize it and define some default operations that are commonly used. If you need to change something else in your training/validation/test loop, there are many possible functions you can overwrite (see the docs for details)."
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-3",
    "href": "dl_lec8.html#pytorch-lightning-3",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\nclass CIFARModule(pl.LightningModule):\n\n    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n        \"\"\"\n        Inputs:\n            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n            model_hparams - Hyperparameters for the model, as dictionary.\n            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n        \"\"\"\n        super().__init__()\n        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n        self.save_hyperparameters()\n        # Create model\n        self.model = create_model(model_name, model_hparams)\n        # Create loss module\n        self.loss_module = nn.CrossEntropyLoss()\n        # Example input for visualizing the graph in Tensorboard\n        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n\n    def forward(self, imgs):\n        # Forward function that is run when visualizing the graph\n        return self.model(imgs)\n\n    def configure_optimizers(self):\n        # We will support Adam or SGD as optimizers.\n        if self.hparams.optimizer_name == \"Adam\":\n            # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n            optimizer = optim.AdamW(\n                self.parameters(), **self.hparams.optimizer_hparams)\n        elif self.hparams.optimizer_name == \"SGD\":\n            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n        else:\n            assert False, f\"Unknown optimizer: \\\"{self.hparams.optimizer_name}\\\"\"\n\n        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n        scheduler = optim.lr_scheduler.MultiStepLR(\n            optimizer, milestones=[100, 150], gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # \"batch\" is the output of the training data loader.\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = self.loss_module(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n        self.log('train_acc', acc, on_step=False, on_epoch=True)\n        self.log('train_loss', loss)\n        return loss  # Return tensor to call \".backward\" on\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches)\n        self.log('val_acc', acc)\n\n    def test_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n        self.log('test_acc', acc)"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-4",
    "href": "dl_lec8.html#pytorch-lightning-4",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n\n\nCallbacks\n\n\nCallbacks are self-contained functions that contain the non-essential logic of your Lightning Module. They are usually called after finishing a training epoch, but can also influence other parts of your training loop. For instance, we will use the following two pre-defined callbacks:\n\nLearningRateMonitor: the learning rate monitor adds the current learning rate to our TensorBoard, which helps to verify that our learning rate scheduler works correctly.\nModelCheckpoint: allows you to customize the saving routine of your checkpoints. For instance, how many checkpoints to keep, when to save, which metric to look out for, etc.\n\n\n\n\n\n# Callbacks \nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-5",
    "href": "dl_lec8.html#pytorch-lightning-5",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n\n\nMultiple models support\n\n\n\nmodel_dict = {}\n\ndef create_model(model_name, model_hparams):\n    if model_name in model_dict:\n        return model_dict[model_name](**model_hparams)\n    else:\n        assert False, f\"Unknown model name \\\"{model_name}\\\". Available models are: {str(model_dict.keys())}\""
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-6",
    "href": "dl_lec8.html#pytorch-lightning-6",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\nact_fn_by_name = {\n    \"tanh\": nn.Tanh,\n    \"relu\": nn.ReLU,\n    \"leakyrelu\": nn.LeakyReLU,\n    \"gelu\": nn.GELU\n}"
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-7",
    "href": "dl_lec8.html#pytorch-lightning-7",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\n\n\nTrainer\n\n\nBesides the Lightning module, the second most important module in PyTorch Lightning is the Trainer. The trainer is responsible to execute the training steps defined in the Lightning module and completes the framework. For a full overview, see the documentation. The most important functions we use below are:\n\ntrainer.fit: Takes as input a lightning module, a training dataset, and an (optional) validation dataset. This function trains the given module on the training dataset with occasional validation (default once per epoch, can be changed)\ntrainer.test: Takes as input a model and a dataset on which we want to test. It returns the test metric on the dataset.\n\nFor training and testing, we don’t have to worry about things like setting the model to eval mode (model.eval()) as this is all done automatically."
  },
  {
    "objectID": "dl_lec8.html#pytorch-lightning-8",
    "href": "dl_lec8.html#pytorch-lightning-8",
    "title": "Convolutional networks: examples",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\ndef train_model(model_name, save_name=None, **kwargs):\n    \"\"\"\n    Inputs:\n        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n    \"\"\"\n    if save_name is None:\n        save_name = model_name\n        \n    # Create a PyTorch Lightning trainer with the generation callback\n    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",                     # We run on a GPU (if possible)\n                         devices=1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n                         max_epochs=180,                                                                     # How many epochs to train for if no patience is set\n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n                         enable_progress_bar=True)                                                           # Set to False if you do not want a progress bar\n    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n    \n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n        model = CIFARModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n    else:\n        pl.seed_everything(42) # To be reproducable\n        model = CIFARModule(model_name=model_name, **kwargs)\n        trainer.fit(model, train_loader, val_loader)\n        model = CIFARModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n        \n    # Test best model on validation and test set\n    val_result = trainer.test(model, val_loader, verbose=False)\n    test_result = trainer.test(model, test_loader, verbose=False)\n    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n    \n    return model, result"
  },
  {
    "objectID": "dl_lec8.html#inception",
    "href": "dl_lec8.html#inception",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nGoogleNet (2014)\n\n\nThe GoogleNet, won the ImageNet Challenge because of its usage of the Inception modules. There have been many follow-up works (Inception-v2, Inception-v3, Inception-v4, Inception-ResNet,…). The follow-up works mainly focus on increasing efficiency and enabling very deep Inception networks."
  },
  {
    "objectID": "dl_lec8.html#inception-1",
    "href": "dl_lec8.html#inception-1",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nInception block\n\n\nAn Inception block applies four convolution blocks separately on the same feature map: a 1x1, 3x3, and 5x5 convolution, and a max pool operation.\n\nThis allows the network to look at the same data with different receptive fields. Of course, learning only 5x5 convolution would be theoretically more powerful. However, this is not only more computation and memory heavy but also tends to overfit much easier.\nThe additional 1x1 convolutions before the 3x3 and 5x5 convolutions are used for dimensionality reduction. This is especially crucial as the feature maps of all branches are merged afterward, and we don’t want any explosion of feature size. As 5x5 convolutions are 25 times more expensive than 1x1 convolutions, we can save a lot of computation and parameters by reducing the dimensionality before the large convolutions."
  },
  {
    "objectID": "dl_lec8.html#inception-2",
    "href": "dl_lec8.html#inception-2",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\nclass InceptionBlock(nn.Module):\n    \n    def __init__(self, c_in, c_red : dict, c_out : dict, act_fn):\n        \"\"\"\n        Inputs:\n            c_in - Number of input feature maps from the previous layers\n            c_red - Dictionary with keys \"3x3\" and \"5x5\" specifying the output of the dimensionality reducing 1x1 convolutions\n            c_out - Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n            act_fn - Activation class constructor (e.g. nn.ReLU)\n        \"\"\"\n        super().__init__()\n        \n        # 1x1 convolution branch\n        self.conv_1x1 = nn.Sequential(\n            nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1),\n            nn.BatchNorm2d(c_out[\"1x1\"]),\n            act_fn()\n        )\n        \n        # 3x3 convolution branch\n        self.conv_3x3 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"3x3\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n            nn.BatchNorm2d(c_out[\"3x3\"]),\n            act_fn()\n        )\n        \n        # 5x5 convolution branch\n        self.conv_5x5 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"5x5\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n            nn.BatchNorm2d(c_out[\"5x5\"]),\n            act_fn()\n        )\n        \n        # Max-pool branch\n        self.max_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n            nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n            nn.BatchNorm2d(c_out[\"max\"]),\n            act_fn()\n        )\n\n    def forward(self, x):\n        x_1x1 = self.conv_1x1(x)\n        x_3x3 = self.conv_3x3(x)\n        x_5x5 = self.conv_5x5(x)\n        x_max = self.max_pool(x)\n        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)\n        return x_out"
  },
  {
    "objectID": "dl_lec8.html#inception-3",
    "href": "dl_lec8.html#inception-3",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nGoogleNet modifications\n\n\nThe GoogleNet architecture consists of stacking multiple Inception blocks with occasional max pooling to reduce the height and width of the feature maps. The original GoogleNet was designed for image sizes of ImageNet (224x224 pixels) and had almost 7 million parameters. As we train on CIFAR10 with image sizes of 32x32, we don’t require such a heavy architecture, and instead, apply a reduced version.\n\nThe number of channels for dimensionality reduction and output per filter (1x1, 3x3, 5x5, and max pooling) need to be manually specified and can be changed if interested\nThe general intuition is to have the most filters for the 3x3 convolutions, as they are powerful enough to take the context into account while requiring almost a third of the parameters of the 5x5 convolution."
  },
  {
    "objectID": "dl_lec8.html#inception-4",
    "href": "dl_lec8.html#inception-4",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\nclass GoogleNet(nn.Module):\n\n    def __init__(self, num_classes=10, act_fn_name=\"relu\", **kwargs):\n        super().__init__()\n        self.hparams = SimpleNamespace(num_classes=num_classes,\n                                       act_fn_name=act_fn_name,\n                                       act_fn=act_fn_by_name[act_fn_name])\n        self._create_network()\n        self._init_params()\n\n    def _create_network(self):\n        # A first convolution on the original image to scale up the channel size\n        self.input_net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            self.hparams.act_fn()\n        )\n        # Stacking inception blocks\n        self.inception_blocks = nn.Sequential(\n            InceptionBlock(64, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8}, act_fn=self.hparams.act_fn),\n            InceptionBlock(64, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12}, act_fn=self.hparams.act_fn),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 =&gt; 16x16\n            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12}, act_fn=self.hparams.act_fn),\n            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 48, \"5x5\": 24, \"max\": 24}, act_fn=self.hparams.act_fn),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 =&gt; 8x8\n            InceptionBlock(128, c_red={\"3x3\": 48, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n            InceptionBlock(128, c_red={\"3x3\": 48, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn)\n        )\n        # Mapping to classification output\n        self.output_net = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, self.hparams.num_classes)\n        )\n\n    def _init_params(self):\n        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.inception_blocks(x)\n        x = self.output_net(x)\n        return x"
  },
  {
    "objectID": "dl_lec8.html#inception-5",
    "href": "dl_lec8.html#inception-5",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nUpdate model dictionary\n\n\n\nmodel_dict[\"GoogleNet\"] = GoogleNet"
  },
  {
    "objectID": "dl_lec8.html#inception-6",
    "href": "dl_lec8.html#inception-6",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nTraining\n\n\n\ngooglenet_model, googlenet_results = train_model(model_name=\"GoogleNet\", \n                                                 model_hparams={\"num_classes\": 10, \n                                                                \"act_fn_name\": \"relu\"}, \n                                                 optimizer_name=\"Adam\",\n                                                 optimizer_hparams={\"lr\": 1e-3,\n                                                                    \"weight_decay\": 1e-4})\n\nFound pretrained model at ../saved_models/tutorial5/GoogleNet.ckpt, loading...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults\n\n\n\nprint(\"GoogleNet Results\", googlenet_results)\n\nGoogleNet Results {'test': 0.8970000147819519, 'val': 0.9039999842643738}"
  },
  {
    "objectID": "dl_lec8.html#inception-7",
    "href": "dl_lec8.html#inception-7",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\n\n\nTensorboard log\n\n\n# Load tensorboard extension\n%load_ext tensorboard\n\n# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n%tensorboard --logdir ../saved_models/tutorial5/tensorboards/GoogleNet/"
  },
  {
    "objectID": "dl_lec8.html#inception-8",
    "href": "dl_lec8.html#inception-8",
    "title": "Convolutional networks: examples",
    "section": "Inception",
    "text": "Inception\n\nScalar tab: track development of single numbers. Graph tab shows us the network architecture organized by building blocks from the input to the output."
  },
  {
    "objectID": "dl_lec8.html#resnet",
    "href": "dl_lec8.html#resnet",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nOverview\n\n\nThe ResNet paper is one of the most cited AI papers, and has been the foundation for neural networks with more than 1,000 layers. Despite its simplicity, the idea of residual connections is highly effective as it supports stable gradient propagation through the network.\n\n\n\n\n\n\nModeling\n\n\nInstead of modeling \\[\nx_{l+1}=F(x_{l}),\n\\] we model \\[\nx_{l+1}=x_{l}+F(x_{l})\n\\] where \\(F\\) is a non-linear mapping (usually a sequence of NN modules likes convolutions, activation functions, and normalizations)."
  },
  {
    "objectID": "dl_lec8.html#resnet-1",
    "href": "dl_lec8.html#resnet-1",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nBackpropagation\n\n\nIf we do backpropagation on such residual connections, we obtain: \\[\n\\frac{\\partial x_{l+1}}{\\partial x_{l}} = \\mathbf{I} + \\frac{\\partial F(x_{l})}{\\partial x_{l}}\n\\]\nThe bias towards the identity matrix guarantees a stable gradient propagation being less effected by \\(F\\) itself."
  },
  {
    "objectID": "dl_lec8.html#resnet-2",
    "href": "dl_lec8.html#resnet-2",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nVariants\n\n\nThere have been many variants of ResNet proposed, which mostly concern the function \\(F\\), or operations applied on the sum. In this tutorial, we look at two of them: the original ResNet block, and the Pre-Activation ResNet block. We visually compare the blocks below (figure credit - He et al.):"
  },
  {
    "objectID": "dl_lec8.html#resnet-3",
    "href": "dl_lec8.html#resnet-3",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\nThe original ResNet block applies a non-linear activation function, usually ReLU, after the skip connection. In contrast, the pre-activation ResNet block applies the non-linearity at the beginning of \\(F\\)."
  },
  {
    "objectID": "dl_lec8.html#resnet-4",
    "href": "dl_lec8.html#resnet-4",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nPreliminaries: original ResNet block\n\n\nThe visualization above already shows what layers are included in \\(F\\). One special case we have to handle is when we want to reduce the image dimensions in terms of width and height.\n\nThe basic ResNet block requires \\(F(x_{l})\\) to be of the same shape as \\(x_{l}\\).\nThus, we need to change the dimensionality of \\(x_{l}\\) as well before adding to \\(F(x_{l})\\).\nThe original implementation used an identity mapping with stride 2 and padded additional feature dimensions with 0. However, the more common implementation is to use a 1x1 convolution with stride 2 as it allows us to change the feature dimensionality while being efficient in parameter and computation cost."
  },
  {
    "objectID": "dl_lec8.html#resnet-5",
    "href": "dl_lec8.html#resnet-5",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\nclass ResNetBlock(nn.Module):\n\n    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n        \"\"\"\n        Inputs:\n            c_in - Number of input features\n            act_fn - Activation class constructor (e.g. nn.ReLU)\n            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width\n            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n        \"\"\"\n        super().__init__()\n        if not subsample:\n            c_out = c_in\n            \n        # Network representing F\n        self.net = nn.Sequential(\n            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),  # No bias needed as the Batch Norm handles it\n            nn.BatchNorm2d(c_out),\n            act_fn(),\n            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(c_out)\n        )\n        \n        # 1x1 convolution with stride 2 means we take the upper left value, and transform it to new output size\n        self.downsample = nn.Conv2d(c_in, c_out, kernel_size=1, stride=2) if subsample else None\n        self.act_fn = act_fn()\n\n    def forward(self, x):\n        z = self.net(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        out = z + x\n        out = self.act_fn(out)\n        return out"
  },
  {
    "objectID": "dl_lec8.html#resnet-6",
    "href": "dl_lec8.html#resnet-6",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nPreliminaries: pre-activation ResNet block\n\n\nFor this, we have to change the order of layer in self.net, and do not apply an activation function on the output. Additionally, the downsampling operation has to apply a non-linearity as well as the input, \\(x_l\\), has not been processed by a non-linearity yet.\n\n\n\n\nclass PreActResNetBlock(nn.Module):\n\n    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n        \"\"\"\n        Inputs:\n            c_in - Number of input features\n            act_fn - Activation class constructor (e.g. nn.ReLU)\n            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width\n            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n        \"\"\"\n        super().__init__()\n        if not subsample:\n            c_out = c_in\n            \n        # Network representing F\n        self.net = nn.Sequential(\n            nn.BatchNorm2d(c_in),\n            act_fn(),\n            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),\n            nn.BatchNorm2d(c_out),\n            act_fn(),\n            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False)\n        )\n        \n        # 1x1 convolution can apply non-linearity as well, but not strictly necessary\n        self.downsample = nn.Sequential(\n            nn.BatchNorm2d(c_in),\n            act_fn(),\n            nn.Conv2d(c_in, c_out, kernel_size=1, stride=2, bias=False)\n        ) if subsample else None\n\n    def forward(self, x):\n        z = self.net(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        out = z + x\n        return out"
  },
  {
    "objectID": "dl_lec8.html#resnet-7",
    "href": "dl_lec8.html#resnet-7",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nDictionary mapping\n\n\nSimilarly to the model selection, we define a dictionary to create a mapping from string to block class. We will use the string name as hyperparameter value in our model to choose between the ResNet blocks.\n\nresnet_blocks_by_name = {\n    \"ResNetBlock\": ResNetBlock,\n    \"PreActResNetBlock\": PreActResNetBlock\n}"
  },
  {
    "objectID": "dl_lec8.html#resnet-8",
    "href": "dl_lec8.html#resnet-8",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nBlock stacking\n\n\nThe overall ResNet architecture consists of stacking multiple ResNet blocks, of which some are downsampling the input. Hence, if we say the ResNet has [3,3,3] blocks, it means that we have 3 times a group of 3 ResNet blocks, where a subsampling is taking place in the fourth and seventh block.\n\nThe three groups operate on the resolutions \\(32\\times32\\), \\(16\\times16\\) and \\(8\\times8\\) respectively. The blocks in orange denote ResNet blocks with downsampling. The same notation is used by many other implementations such as in the torchvision library from PyTorch."
  },
  {
    "objectID": "dl_lec8.html#resnet-9",
    "href": "dl_lec8.html#resnet-9",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\nclass ResNet(nn.Module):\n\n    def __init__(self, num_classes=10, num_blocks=[3,3,3], c_hidden=[16,32,64], act_fn_name=\"relu\", block_name=\"ResNetBlock\", **kwargs):\n        \"\"\"\n        Inputs: \n            num_classes - Number of classification outputs (10 for CIFAR10)\n            num_blocks - List with the number of ResNet blocks to use. The first block of each group uses downsampling, except the first.\n            c_hidden - List with the hidden dimensionalities in the different blocks. Usually multiplied by 2 the deeper we go.\n            act_fn_name - Name of the activation function to use, looked up in \"act_fn_by_name\"\n            block_name - Name of the ResNet block, looked up in \"resnet_blocks_by_name\"\n        \"\"\"\n        super().__init__()\n        assert block_name in resnet_blocks_by_name\n        self.hparams = SimpleNamespace(num_classes=num_classes, \n                                       c_hidden=c_hidden, \n                                       num_blocks=num_blocks, \n                                       act_fn_name=act_fn_name,\n                                       act_fn=act_fn_by_name[act_fn_name],\n                                       block_class=resnet_blocks_by_name[block_name])\n        self._create_network()\n        self._init_params()\n\n    def _create_network(self):\n        c_hidden = self.hparams.c_hidden\n        \n        # A first convolution on the original image to scale up the channel size\n        if self.hparams.block_class == PreActResNetBlock: # =&gt; Don't apply non-linearity on output\n            self.input_net = nn.Sequential(\n                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False)\n            )\n        else:\n            self.input_net = nn.Sequential(\n                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(c_hidden[0]),\n                self.hparams.act_fn()\n            )\n        \n        # Creating the ResNet blocks\n        blocks = []\n        for block_idx, block_count in enumerate(self.hparams.num_blocks):\n            for bc in range(block_count):\n                subsample = (bc == 0 and block_idx &gt; 0) # Subsample the first block of each group, except the very first one.\n                blocks.append(\n                    self.hparams.block_class(c_in=c_hidden[block_idx if not subsample else (block_idx-1)],\n                                             act_fn=self.hparams.act_fn,\n                                             subsample=subsample,\n                                             c_out=c_hidden[block_idx])\n                )\n        self.blocks = nn.Sequential(*blocks)\n        \n        # Mapping to classification output\n        self.output_net = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1,1)),\n            nn.Flatten(),\n            nn.Linear(c_hidden[-1], self.hparams.num_classes)\n        )\n\n    def _init_params(self):\n        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n        # Fan-out focuses on the gradient distribution, and is commonly used in ResNets\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.blocks(x)\n        x = self.output_net(x)\n        return x"
  },
  {
    "objectID": "dl_lec8.html#resnet-10",
    "href": "dl_lec8.html#resnet-10",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nModel dictionary update\n\n\n\nmodel_dict[\"ResNet\"] = ResNet"
  },
  {
    "objectID": "dl_lec8.html#resnet-11",
    "href": "dl_lec8.html#resnet-11",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nTraining\n\n\nSGD instead of Adam. ResNet has been shown to produce smoother loss surfaces than networks without skip connection (see Li et al., 2018 for details). A possible visualization of the loss surface with/out skip connections is below (figure credit - Li et al.):"
  },
  {
    "objectID": "dl_lec8.html#resnet-12",
    "href": "dl_lec8.html#resnet-12",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nOriginal ResNet training\n\n\n\nresnet_model, resnet_results = train_model(model_name=\"ResNet\", \n                                           model_hparams={\"num_classes\": 10,\n                                                          \"c_hidden\": [16,32,64],\n                                                          \"num_blocks\": [3,3,3],\n                                                          \"act_fn_name\": \"relu\"}, \n                                           optimizer_name=\"SGD\",\n                                           optimizer_hparams={\"lr\": 0.1,\n                                                              \"momentum\": 0.9,\n                                                              \"weight_decay\": 1e-4})\n\nFound pretrained model at ../saved_models/tutorial5/ResNet.ckpt, loading..."
  },
  {
    "objectID": "dl_lec8.html#resnet-13",
    "href": "dl_lec8.html#resnet-13",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nPre-activation ResNet training\n\n\n\nresnetpreact_model, resnetpreact_results = train_model(model_name=\"ResNet\", \n                                                       model_hparams={\"num_classes\": 10,\n                                                                      \"c_hidden\": [16,32,64],\n                                                                      \"num_blocks\": [3,3,3],\n                                                                      \"act_fn_name\": \"relu\",\n                                                                      \"block_name\": \"PreActResNetBlock\"}, \n                                                       optimizer_name=\"SGD\",\n                                                       optimizer_hparams={\"lr\": 0.1,\n                                                                          \"momentum\": 0.9,\n                                                                          \"weight_decay\": 1e-4},\n                                                       save_name=\"ResNetPreAct\")\n\nFound pretrained model at ../saved_models/tutorial5/ResNetPreAct.ckpt, loading..."
  },
  {
    "objectID": "dl_lec8.html#resnet-14",
    "href": "dl_lec8.html#resnet-14",
    "title": "Convolutional networks: examples",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nTensorboard\n\n\n# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH! Feel free to change \"ResNet\" to \"ResNetPreAct\"\n%tensorboard --logdir ../saved_models/tutorial5/tensorboards/ResNet/"
  },
  {
    "objectID": "dl_lec8.html#densenet",
    "href": "dl_lec8.html#densenet",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nOverview\n\n\nDenseNet is another architecture for enabling very deep neural networks and takes a slightly different perspective on residual connections.\nInstead of modeling the difference between layers, DenseNet considers residual connections as a possible way to reuse features across layers, removing any necessity to learn redundant feature maps.\n\nIf we go deeper into the network, the model learns abstract features to recognize patterns.\nHowever, some complex patterns consist of a combination of abstract features (e.g. hand, face, etc.), and low-level features (e.g. edges, basic color, etc.).\nTo find these low-level features in the deep layers, standard CNNs have to learn copy such feature maps, which wastes a lot of parameter complexity.\n\nDenseNet provides an efficient way of reusing features by having each convolution depends on all previous input features, but add only a small amount of filters to it. See the figure below for an illustration (figure credit - Hu et al.):"
  },
  {
    "objectID": "dl_lec8.html#densenet-1",
    "href": "dl_lec8.html#densenet-1",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\nThe last layer, called the transition layer, is responsible for reducing the dimensionality of the feature maps in height, width, and channel size."
  },
  {
    "objectID": "dl_lec8.html#densenet-2",
    "href": "dl_lec8.html#densenet-2",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nImplementation\n\n\nWe split the implementation of the layers in DenseNet into three parts: a DenseLayer, and a DenseBlock, and a TransitionLayer.\n\n\n\n\n\n\nDenseLayer\n\n\nThe module DenseLayer implements a single layer inside a dense block. It applies a 1x1 convolution for dimensionality reduction with a subsequential 3x3 convolution. The output channels are concatenated to the originals and returned. Note that we apply the Batch Normalization as the first layer of each block. This allows slightly different activations for the same features to different layers, depending on what is needed."
  },
  {
    "objectID": "dl_lec8.html#densenet-3",
    "href": "dl_lec8.html#densenet-3",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nDenseLayer\n\n\n\nclass DenseLayer(nn.Module):\n    \n    def __init__(self, c_in, bn_size, growth_rate, act_fn):\n        \"\"\"\n        Inputs:\n            c_in - Number of input channels\n            bn_size - Bottleneck size (factor of growth rate) for the output of the 1x1 convolution. Typically between 2 and 4.\n            growth_rate - Number of output channels of the 3x3 convolution\n            act_fn - Activation class constructor (e.g. nn.ReLU)\n        \"\"\"\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm2d(c_in),\n            act_fn(),\n            nn.Conv2d(c_in, bn_size * growth_rate, kernel_size=1, bias=False),\n            nn.BatchNorm2d(bn_size * growth_rate),\n            act_fn(),\n            nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n        )\n        \n    def forward(self, x):\n        out = self.net(x)\n        out = torch.cat([out, x], dim=1)\n        return out"
  },
  {
    "objectID": "dl_lec8.html#densenet-4",
    "href": "dl_lec8.html#densenet-4",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nDenseBlock\n\n\n\nclass DenseBlock(nn.Module):\n    \n    def __init__(self, c_in, num_layers, bn_size, growth_rate, act_fn):\n        \"\"\"\n        Inputs:\n            c_in - Number of input channels\n            num_layers - Number of dense layers to apply in the block\n            bn_size - Bottleneck size to use in the dense layers\n            growth_rate - Growth rate to use in the dense layers\n            act_fn - Activation function to use in the dense layers\n        \"\"\"\n        super().__init__()\n        layers = []\n        for layer_idx in range(num_layers):\n            layers.append(\n                DenseLayer(c_in=c_in + layer_idx * growth_rate, # Input channels are original plus the feature maps from previous layers\n                           bn_size=bn_size,\n                           growth_rate=growth_rate,\n                           act_fn=act_fn)\n            )\n        self.block = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        out = self.block(x)\n        return out"
  },
  {
    "objectID": "dl_lec8.html#densenet-5",
    "href": "dl_lec8.html#densenet-5",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nTransitionLayer\n\n\nTakes as input the final output of a dense block and reduces its channel dimensionality using a 1x1 convolution. To reduce the height and width dimension, we take a slightly different approach than in ResNet and apply an average pooling with kernel size 2 and stride 2. This is because we don’t have an additional connection to the output that would consider the full 2x2 patch instead of a single value. Besides, it is more parameter efficient than using a 3x3 convolution with stride 2.\n\nclass TransitionLayer(nn.Module):\n    \n    def __init__(self, c_in, c_out, act_fn):\n        super().__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(c_in),\n            act_fn(),\n            nn.Conv2d(c_in, c_out, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2) # Average the output for each 2x2 pixel group\n        )\n        \n    def forward(self, x):\n        return self.transition(x)"
  },
  {
    "objectID": "dl_lec8.html#densenet-6",
    "href": "dl_lec8.html#densenet-6",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nEverything together\n\n\n\nclass DenseNet(nn.Module):\n    \n    def __init__(self, num_classes=10, num_layers=[6,6,6,6], bn_size=2, growth_rate=16, act_fn_name=\"relu\", **kwargs):\n        super().__init__()\n        self.hparams = SimpleNamespace(num_classes=num_classes,\n                                       num_layers=num_layers,\n                                       bn_size=bn_size,\n                                       growth_rate=growth_rate,\n                                       act_fn_name=act_fn_name,\n                                       act_fn=act_fn_by_name[act_fn_name])\n        self._create_network()\n        self._init_params()\n        \n    def _create_network(self):\n        c_hidden = self.hparams.growth_rate * self.hparams.bn_size # The start number of hidden channels\n        \n        # A first convolution on the original image to scale up the channel size\n        self.input_net = nn.Sequential(\n            nn.Conv2d(3, c_hidden, kernel_size=3, padding=1) # No batch norm or activation function as done inside the Dense layers\n        )\n        \n        # Creating the dense blocks, eventually including transition layers\n        blocks = []\n        for block_idx, num_layers in enumerate(self.hparams.num_layers):\n            blocks.append( \n                DenseBlock(c_in=c_hidden, \n                           num_layers=num_layers, \n                           bn_size=self.hparams.bn_size,\n                           growth_rate=self.hparams.growth_rate,\n                           act_fn=self.hparams.act_fn)\n            )\n            c_hidden = c_hidden + num_layers * self.hparams.growth_rate # Overall output of the dense block\n            if block_idx &lt; len(self.hparams.num_layers)-1: # Don't apply transition layer on last block\n                blocks.append(\n                    TransitionLayer(c_in=c_hidden,\n                                    c_out=c_hidden // 2,\n                                    act_fn=self.hparams.act_fn))\n                c_hidden = c_hidden // 2\n                \n        self.blocks = nn.Sequential(*blocks)\n        \n        # Mapping to classification output\n        self.output_net = nn.Sequential(\n            nn.BatchNorm2d(c_hidden), # The features have not passed a non-linearity until here.\n            self.hparams.act_fn(),\n            nn.AdaptiveAvgPool2d((1,1)),\n            nn.Flatten(),\n            nn.Linear(c_hidden, self.hparams.num_classes)\n        )\n\n    def _init_params(self):\n        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.blocks(x)\n        x = self.output_net(x)\n        return x"
  },
  {
    "objectID": "dl_lec8.html#densenet-7",
    "href": "dl_lec8.html#densenet-7",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nDictionary update\n\n\n\nmodel_dict[\"DenseNet\"] = DenseNet"
  },
  {
    "objectID": "dl_lec8.html#densenet-8",
    "href": "dl_lec8.html#densenet-8",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nTraining\n\n\nDenseNet is more parameter efficient than ResNet while achieving a similar or even better performance.\n\ndensenet_model, densenet_results = train_model(model_name=\"DenseNet\", \n                                               model_hparams={\"num_classes\": 10,\n                                                              \"num_layers\": [6,6,6,6],\n                                                              \"bn_size\": 2,\n                                                              \"growth_rate\": 16,\n                                                              \"act_fn_name\": \"relu\"}, \n                                               optimizer_name=\"Adam\",\n                                               optimizer_hparams={\"lr\": 1e-3,\n                                                                  \"weight_decay\": 1e-4})\n\nFound pretrained model at ../saved_models/tutorial5/DenseNet.ckpt, loading..."
  },
  {
    "objectID": "dl_lec8.html#densenet-9",
    "href": "dl_lec8.html#densenet-9",
    "title": "Convolutional networks: examples",
    "section": "DenseNet",
    "text": "DenseNet\n\n\n\nTensorboard\n\n\n# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n%tensorboard --logdir ../saved_models/tutorial5/tensorboards/DenseNet/"
  },
  {
    "objectID": "dl_lec8.html#comparison",
    "href": "dl_lec8.html#comparison",
    "title": "Convolutional networks: examples",
    "section": "Comparison",
    "text": "Comparison\n\nimport tabulate\nfrom IPython.display import display, HTML\nall_models = [\n    (\"GoogleNet\", googlenet_results, googlenet_model),\n    (\"ResNet\", resnet_results, resnet_model),\n    (\"ResNetPreAct\", resnetpreact_results, resnetpreact_model),\n    (\"DenseNet\", densenet_results, densenet_model)\n]\ntable = [[model_name,\n          f\"{100.0*model_results['val']:4.2f}%\",\n          f\"{100.0*model_results['test']:4.2f}%\",\n          \"{:,}\".format(sum([np.prod(p.shape) for p in model.parameters()]))]\n         for model_name, model_results, model in all_models]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Model\", \"Val Accuracy\", \"Test Accuracy\", \"Num Parameters\"])))\n\n\n\n\nModel\nVal Accuracy\nTest Accuracy\nNum Parameters\n\n\n\n\nGoogleNet\n90.40%\n89.70%\n260,650\n\n\nResNet\n91.84%\n91.06%\n272,378\n\n\nResNetPreAct\n91.80%\n91.07%\n272,250\n\n\nDenseNet\n90.72%\n90.23%\n239,146"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks",
    "href": "dl_lec9.html#recurrent-neural-networks",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\n\n\n\nUses\n\n\n\nLanguage Modelling and Generating Text\nSpeech Recognition\nGenerating Image Descriptions\nVideo Tagging"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-1",
    "href": "dl_lec9.html#recurrent-neural-networks-1",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\nRecurrent networks operate on sequences of vectors"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-2",
    "href": "dl_lec9.html#recurrent-neural-networks-2",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nInterpretation\n\n\n\nRNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector.\nThis can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables.\nViewed this way, RNNs essentially describe programs and are Turing-complete.\n\n\n\n\n\n\n\nRNNs as programs\n\n\nIf training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-3",
    "href": "dl_lec9.html#recurrent-neural-networks-3",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\n\n\n\nCycles\n\n\n\nfeedforward networks – no cycles\nRNN has cycles and transmits information back into itself.\ncycles are recurrent connections\n\n\n\n\n\n\n\nConnection types\n\n\n\nstandard connections: applied synchronously to propagate each layer’s activations to the subsequent layer at the same time step\nrecurrent connections: dynamic, passing information across adjacent time steps\n\nWhile Feedforward Networks pass information through the network without cycles, recurrent networks have cycles. This enables them to extend the functionality of Feedforward Networks to also take into account previous inputs \\(\\boldsymbol{X}_{0:t-1}\\) and not only the current input \\(\\boldsymbol{X}_t\\)."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-4",
    "href": "dl_lec9.html#recurrent-neural-networks-4",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-5",
    "href": "dl_lec9.html#recurrent-neural-networks-5",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-6",
    "href": "dl_lec9.html#recurrent-neural-networks-6",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nNotation\n\n\n\n\\(n\\) – number of samples\n\\(d\\) – number of inputs\n\\(h\\) – number of hidden units\n\\(\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}\\) – hidden state at time \\(t\\)\n\\(\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}\\) – input at time \\(t\\)\n\\(\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}\\) – weight matrix\n\\(\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}\\) – hidden-state-to-hidden-state matrix\n\\(\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}\\) – bias parameter\n\\(\\phi\\) – activation function (usually sigmoid or tanh)"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-7",
    "href": "dl_lec9.html#recurrent-neural-networks-7",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\nNote, that here the option of having multiple hidden layers is aggregated to one Hidden Layer block \\(\\boldsymbol{H}\\). This block can obviously be extended to multiple hidden layers."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-8",
    "href": "dl_lec9.html#recurrent-neural-networks-8",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nEquation for hidden variable\n\n\n\\[\n\\label{hidden_var}\n\\boldsymbol{H}_t = \\phi_h \\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h\\right)\n\\]\n\n\n\n\n\n\nEquation for output variable\n\n\n\\[\n\\label{output_var}\n\\boldsymbol{O}_t = \\phi_o \\left(\\boldsymbol{H}_t \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o\\right)\n\\] Since \\(\\boldsymbol{H}_t\\) recursively includes \\(\\boldsymbol{H}_{t-1}\\) and this process occurs for every time step the RNN includes traces of all hidden states that preceded \\(\\boldsymbol{H}_{t-1}\\) as well as \\(\\boldsymbol{H}_{t-1}\\) itself."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-9",
    "href": "dl_lec9.html#recurrent-neural-networks-9",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nComputation for hidden variable\n\n\n\\[\n\\label{hidden_var_computation}\n\\boldsymbol{H} = \\phi_h \\left(\\boldsymbol{X} \\boldsymbol{W}_{xh} + \\boldsymbol{b}_h\\right)\n\\]\n\n\n\n\n\n\nComputation for output variable\n\n\n\\[\n\\label{output_var_computation}\n\\boldsymbol{O} = \\phi_o \\left(\\boldsymbol{H} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o\\right)\n\\]"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-10",
    "href": "dl_lec9.html#recurrent-neural-networks-10",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nHidden state computation\n\n\nAt any time step \\(t\\):\n\nconcatenate the input \\(\\boldsymbol{X}_t\\) at the current time step \\(t\\) and the hidden state \\(\\boldsymbol{H}_{t-1}\\) at the previous time step \\(t\\)\nfeed the concatenation result into a fully connected layer with the activation function \\(\\phi\\)\nThe output of such a fully connected layer is the hidden state \\(\\boldsymbol{H}_t\\) of the current time step \\(t\\)\nthe model parameters are the concatenation of \\(\\boldsymbol{W}_{xh}\\) and \\(\\boldsymbol{W}_{hh}\\) and a bias \\(\\boldsymbol{b}_h\\)\n\\(\\boldsymbol{H}_t\\) will participate in computing the hidden state \\(\\boldsymbol{H}_{t+1}\\) of the next time step \\(t+1\\)\n\\(\\boldsymbol{H}_t\\) will also be fed into the fully connected output layer to compute the output \\(\\boldsymbol{O}_t\\) of the current time step \\(t\\)"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-11",
    "href": "dl_lec9.html#recurrent-neural-networks-11",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks"
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-12",
    "href": "dl_lec9.html#recurrent-neural-networks-12",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nLanguage modeling\n\n\nWe aim to predict the next token based on the current and past tokens; thus we shift the original sequence by one token as the targets (labels).\nBengio et al. (2003) first proposed to use a neural network for language modeling."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-13",
    "href": "dl_lec9.html#recurrent-neural-networks-13",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\nA character-level language model based on the RNN. The input and target sequences are “machin” and “achine”, respectively."
  },
  {
    "objectID": "dl_lec9.html#recurrent-neural-networks-14",
    "href": "dl_lec9.html#recurrent-neural-networks-14",
    "title": "Recurrent neural networks",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nProcess\n\n\nDuring the training process, we run a softmax operation on the output from the output layer for each time step, and then use the cross-entropy loss to compute the error between the model output and the target.\nBecause of the recurrent computation of the hidden state in the hidden layer, the output \\(\\boldsymbol{O}_3\\) of time step 3 is determined by the text sequence “m”, “a”, and “c”. Since the next character of the sequence in the training data is “h”, the loss of time step 3 will depend on the probability distribution of the next character generated based on the feature sequence “m”, “a”, “c” and the target “h” of this time step.\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn practice, each token is represented by a \\(d\\)-dimensional vector, and we use a batch size \\(n\\). Therefore, the input \\(\\boldsymbol{X}_t\\) at time step \\(t\\) will be an \\(n \\times d\\) matrix."
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-1",
    "href": "dl_lec9.html#backpropagation-in-rnns-1",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\n\n\nBackpropagation Through Time (BPTT)\n\n\n\nBPTT expands (or unrolls) the computational graph of an RNN one time step at a time.\nunrolled RNN is a FF neural network with same parameters appearing at each time step\nThe gradient with respect to each parameter must be summed across all places that the parameter occurs in the unrolled net."
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-2",
    "href": "dl_lec9.html#backpropagation-in-rnns-2",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\n\n\nBackpropagation Through Time (BPTT)\n\n\nWhen we forward pass our input \\(\\boldsymbol{X}_t\\) through the network, we compute the hidden state \\(\\boldsymbol{H}_t\\) and the output state \\(\\boldsymbol{O}_t\\) one step at at time.\nWe can then define a loss function \\(L(\\boldsymbol{O}, \\boldsymbol{Y})\\) to describe the difference between all outputs \\(\\boldsymbol{O}_t\\) and target values \\(\\boldsymbol{Y}_t\\), summing up loss terms \\(l_t\\).\n\\[\n\\label{loss_function}\nL(\\boldsymbol{O}, \\boldsymbol{Y}) = \\sum\\limits_{t=1}^T l_t (\\boldsymbol{O}_t, \\boldsymbol{Y}_t)\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-3",
    "href": "dl_lec9.html#backpropagation-in-rnns-3",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\nBoxes represent variables (not shaded) or parameters (shaded) and circles represent operators."
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-4",
    "href": "dl_lec9.html#backpropagation-in-rnns-4",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\\[\n\\notag\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{ho}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\frac{\\partial \\phi_o}{\\partial \\boldsymbol{W}_{ho}} = \\\\\n\\label{loss_function_derivative1}\n= \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t} \\cdot \\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\boldsymbol{H}_t.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-5",
    "href": "dl_lec9.html#backpropagation-in-rnns-5",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\\[\n\\label{loss_function_derivative2}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{hh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\frac{\\partial \\phi_o}{\\boldsymbol{H}_t} \\cdot \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\phi_h} \\cdot \\frac{\\partial \\phi_h}{\\partial \\boldsymbol{W}_{hh}}= \\\\\n= \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t} \\cdot \\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\boldsymbol{W}_{ho} \\cdot \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\phi_h} \\cdot \\frac{\\partial \\phi_h}{\\partial \\boldsymbol{W}_hh}.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-6",
    "href": "dl_lec9.html#backpropagation-in-rnns-6",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\\[\n\\label{loss_function_derivative3}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{xh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\frac{\\partial \\phi_o}{\\boldsymbol{H}_t} \\cdot \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\phi_h} \\cdot \\frac{\\partial \\phi_h}{\\partial \\boldsymbol{W}_{xh}}= \\\\\n= \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t} \\cdot \\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\boldsymbol{W}_{ho} \\cdot \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\phi_h} \\cdot \\frac{\\partial \\phi_h}{\\partial \\boldsymbol{W}_xh}.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-7",
    "href": "dl_lec9.html#backpropagation-in-rnns-7",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\nSince each \\(\\boldsymbol{H}_t\\) depends on the previous time step, we can substitute the last part from above equations to obtain \\[\n\\label{loss_function_derivative4}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{hh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\boldsymbol{W}_{ho} \\sum\\limits_{k=1}^t \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\boldsymbol{H}_k}\\frac{\\partial \\boldsymbol{H}_t}{\\partial \\boldsymbol{W}_{hh}}.\n\\]\n\\[\n\\label{loss_function_derivative5}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{xh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\boldsymbol{W}_{ho} \\sum\\limits_{k=1}^t \\frac{\\partial \\boldsymbol{H}_t}{\\partial \\boldsymbol{H}_k}\\frac{\\partial \\boldsymbol{H}_t}{\\partial \\boldsymbol{W}_{xh}}.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-8",
    "href": "dl_lec9.html#backpropagation-in-rnns-8",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\n\n\nOR:\n\n\n\\[\n\\label{loss_function_derivative6}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{hh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\boldsymbol{W}_{ho} \\sum\\limits_{k=1}^t \\left(\\boldsymbol{W}^T_{hh}\\right)^{t-k} \\cdot \\boldsymbol{H}_k.\n\\]\n\\[\n\\label{loss_function_derivative7}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{xh}} = \\sum\\limits_{t=1}^T \\frac{\\partial l_t}{\\partial \\boldsymbol{O}_t}\\cdot\\frac{\\partial \\boldsymbol{O}_t}{\\partial \\phi_o} \\cdot \\boldsymbol{W}_{ho} \\sum\\limits_{k=1}^t \\left(\\boldsymbol{W}^T_{hh}\\right)^{t-k} \\cdot \\boldsymbol{X}_k.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-9",
    "href": "dl_lec9.html#backpropagation-in-rnns-9",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\n\n\nA problem\n\n\nWe can see that we need to store powers of \\(\\boldsymbol{W}_{hh}^k\\) as we proceed through each loss term \\(l_t\\) of the overall loss function \\(L\\) that can become very large.\nFor these large values this method becomes numerically unstable since eigenvalues smaller than 1 vanish and eigenvalues larger than 1 diverge.\n\n\n\n\n\n\nSolution\n\n\nTruncate a sum at computationally convenient size - Truncated BPTT."
  },
  {
    "objectID": "dl_lec9.html#backpropagation-in-rnns-10",
    "href": "dl_lec9.html#backpropagation-in-rnns-10",
    "title": "Recurrent neural networks",
    "section": "Backpropagation in RNNs",
    "text": "Backpropagation in RNNs\n\n\n\nTruncated BPTT\n\n\nThis establishes an upper bound for the number of time steps the gradient can flow back to.\nInterpretation: a moving window of past time steps which the RNN considers. Anything before the cut-off time step doesn’t get taken into account. Since BPTT basically unfolds the RNN to create a new layer for each time step we can also think of this procedure as limiting the number of hidden layers.\nResult: model focuses primarily on short-term influence rather than long-term consequences."
  },
  {
    "objectID": "dl_lec9.html#lstms-1",
    "href": "dl_lec9.html#lstms-1",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nProblem\n\n\nIn \\(\\eqref{loss_function_derivative4}\\) and \\(\\eqref{loss_function_derivative5}\\) we see \\(\\frac{\\partial \\boldsymbol{H}_t}{\\partial \\boldsymbol{H}_k}\\) that introduces a matrix multiplication over a potentially very long sequence.\n\n\n\n\n\n\nSolution\n\n\nLSTMs were designed to handle a vanishing gradient problem.\nSince they use a more constant error, they allow RNNs to learn over a lot more time steps (way over 1000)."
  },
  {
    "objectID": "dl_lec9.html#lstms-2",
    "href": "dl_lec9.html#lstms-2",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nGates\n\n\nTo achieve that, LSTMs store more information outside of the traditional neural network flow in structures called gated cells.\nGate types\n\noutput gate \\(\\boldsymbol{O}_t\\) – reads entries of the cell\ninput gate \\(\\boldsymbol{I}_t\\) – reads data into the cell\nforget gate \\(\\boldsymbol{F}_t\\) – resets the content of the cell."
  },
  {
    "objectID": "dl_lec9.html#lstms-3",
    "href": "dl_lec9.html#lstms-3",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nComputations\n\n\n\\[\n\\label{lstm_computations}\n\\boldsymbol{O}_t = \\sigma\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xo} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o\\right), \\\\\n\\boldsymbol{I}_t = \\sigma\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xi} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hi} + \\boldsymbol{b}_i\\right), \\\\\n\\boldsymbol{F}_t = \\sigma\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xf} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hf} + \\boldsymbol{b}_f\\right).\n\\]\nHere \\(\\boldsymbol{W}_{xo}, \\boldsymbol{W}_{xi}\\boldsymbol{W}_{xf} \\in \\mathbb{R}^{d \\times h}\\), and \\(\\boldsymbol{W}_{ho}, \\boldsymbol{W}_{hi}\\boldsymbol{W}_{hf} \\in \\mathbb{R}^{h \\times h}\\) are weight matrices and \\(\\boldsymbol{b}_o, \\boldsymbol{b}_i, \\boldsymbol{b}_f \\in \\mathbb{R}^{1 \\times h}\\) are their respective biases.\nFurther, they use the sigmoid activation function \\(\\sigma\\) to transform the output \\(\\in (0, 1)\\) which each results in a vector with entries \\(\\in (0, 1)\\)."
  },
  {
    "objectID": "dl_lec9.html#lstms-4",
    "href": "dl_lec9.html#lstms-4",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\nCalculation of input, forget, and output gates in an LSTM"
  },
  {
    "objectID": "dl_lec9.html#lstms-5",
    "href": "dl_lec9.html#lstms-5",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nCandidate memory cell\n\n\n\\(\\tilde{\\boldsymbol{C}}_t \\in \\mathbb{R}^{n \\times h}\\) – similar computation as previously mentioned gates but uses tanh to have output \\(\\in (-1, 1)\\).\nHas its own weights \\(\\boldsymbol{W}_{xc} \\in \\mathbb{R}^{d \\times h},\\boldsymbol{W}_{hc} \\in \\mathbb{R}^{h \\times h}\\) and biases \\(\\boldsymbol{b}_c \\in \\mathbb{R}^{1 \\times h}\\).\n\n\n\n\n\n\nComputation\n\n\n\\[\n\\label{candidate_memory_cell_computation}\n\\tilde{\\boldsymbol{C}}_t = tanh \\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xc} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hc} + \\boldsymbol{b}_c\\right)\n\\]"
  },
  {
    "objectID": "dl_lec9.html#lstms-6",
    "href": "dl_lec9.html#lstms-6",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\nComputation of candidate memory cells in LSTM."
  },
  {
    "objectID": "dl_lec9.html#lstms-7",
    "href": "dl_lec9.html#lstms-7",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nOld memory content\n\n\nTo put things together we introduce old memory content \\(\\boldsymbol{C}_{t−1} \\in \\mathbb{R}^{n \\times h}\\) which together with the introduced gates controls how much of the old memory content we want to preserve to get to the new memory content \\(\\boldsymbol{C}_t\\).\n\n\n\n\n\n\nComputation\n\n\n\\[\n\\label{candidate_memory_cell_computation2}\n\\boldsymbol{C}_t = \\boldsymbol{F}_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_t \\odot \\tilde{\\boldsymbol{C}}_t.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#lstms-8",
    "href": "dl_lec9.html#lstms-8",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\nComputation of memory cells in an LSTM."
  },
  {
    "objectID": "dl_lec9.html#lstms-9",
    "href": "dl_lec9.html#lstms-9",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\n\n\nHidden states computation\n\n\n\\[\n\\label{hidden_states_computation}\n\\boldsymbol{H}_t = \\boldsymbol{O}_t \\odot tanh\\left(\\boldsymbol{C}_t\\right), \\; \\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}.\n\\]\n\nthe output gate is close to 1 – we allow the memory cell internal state to impact the subsequent layers uninhibited,\nthe output gate is close to – we prevent the current memory from impacting other layers of the network at the current time step.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nA memory cell can accrue information across many time steps without impacting the rest of the network (as long as the output gate takes values close to 0), and then suddenly impact the network at a subsequent time step as soon as the output gate flips from values close to 0 to values close to 1."
  },
  {
    "objectID": "dl_lec9.html#lstms-10",
    "href": "dl_lec9.html#lstms-10",
    "title": "Recurrent neural networks",
    "section": "LSTMs",
    "text": "LSTMs\n\nComputation of the hidden state in an LSTM."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-1",
    "href": "dl_lec9.html#gated-recurrent-units-1",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\n\n\nRationale\n\n\n\nsimpler architecture\nretain internal state and gating mechanisms\nbut speed up computation\n\n\n\n\n\n\n\nOutline\n\n\n\ninstead of LSTM’s 3 gates, use 2: reset gate and update gate\nreset gate controls how much of the previous state we want to remember\nupdate gate controls how much of the new state is just a copy of the old one"
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-2",
    "href": "dl_lec9.html#gated-recurrent-units-2",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\nComputing the reset gate and the update gate in a GRU model."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-3",
    "href": "dl_lec9.html#gated-recurrent-units-3",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\n\n\nGate computation\n\n\nSuppose that for a given time step \\(t\\) we have:\n\na minibatch \\(\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}\\)\nhidden state of the previous time step \\(\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}\\)\n\\(\\boldsymbol{W}_{xr}, \\boldsymbol{W}_{xz} \\in \\mathbb{R}^{d \\times h}, \\; \\boldsymbol{W}_{hr}, \\boldsymbol{W}_{hz} \\in \\mathbb{R}^{h \\times h}\\) are weights\n\\(\\boldsymbol{b}_r, \\boldsymbol{b}_z \\in \\mathbb{R}^{1 \\times h}\\) are bias parameters\n\nWe compute: \\[\n\\boldsymbol{R}_t = \\sigma\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xr} + \\boldsymbol{H}_{t-1}\\boldsymbol{W}_{hr} + \\boldsymbol{b}_r\\right),\\\\\n\\boldsymbol{Z}_t = \\sigma\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xz} + \\boldsymbol{H}_{t-1}\\boldsymbol{W}_{hz} + \\boldsymbol{b}_z\\right).\n\\]"
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-4",
    "href": "dl_lec9.html#gated-recurrent-units-4",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\n\n\nCandidate hidden state at time step \\(t\\)\n\n\nNext, we integrate reset gate \\(\\boldsymbol{R}_t\\) with the regular updating mechanism and obtain: \\[\n\\tilde{\\boldsymbol{H}}_t = tanh\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + (\\boldsymbol{R}_t \\odot \\boldsymbol{H}_{t-1}) \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h\\right), \\; \\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n\\times h}\n\\]\n\ninfluence of previous states is reduced with the Hadamard product of \\(\\boldsymbol{R}_t\\) and \\(\\boldsymbol{H}_{t-1}\\)\nentries in \\(\\boldsymbol{R}_t\\) close to 1 – vanilla RNN\nentries in \\(\\boldsymbol{R}_t\\) close to 0 – candidate hidden state is the result of MLP with \\(\\boldsymbol{X}_t\\) as an input\nany pre-existing hidden state is thus reset to defaults."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-5",
    "href": "dl_lec9.html#gated-recurrent-units-5",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\nComputing the candidate hidden state in a GRU model."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-6",
    "href": "dl_lec9.html#gated-recurrent-units-6",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\n\n\nUpdate gate\n\n\nFinally, we need to incorporate the effect of the update gate \\(\\boldsymbol{Z}_t\\).\nThis determines the extent to which the new hidden state \\(\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}\\) matches the old state \\(\\boldsymbol{H}_{t-1}\\) compared with how much it resembles the new candidate state \\(\\tilde{\\boldsymbol{H}}_t\\).\nThe update gate \\(\\boldsymbol{Z}_t\\) can be used for this purpose, simply by taking elementwise convex combinations of \\(\\boldsymbol{H}_{t-1}\\) and \\(\\tilde{\\boldsymbol{H}}_t\\).\n\\[\n\\boldsymbol{H}_t = \\boldsymbol{Z}_t \\odot \\boldsymbol{H}_{t-1} + (1-\\boldsymbol{Z}_t) \\odot \\tilde{\\boldsymbol{H}}_t.\n\\]\n\n\\(\\boldsymbol{Z}_t\\) close to 1 – retain the old state. In this case the information from \\(\\boldsymbol{X}_t\\) is ignored%, effectively skipping time step \\(t\\) in the dependency chain.\n\\(\\boldsymbol{Z}_t\\) is close to 0 – the new latent state \\(\\boldsymbol{H}_t\\) approaches the candidate latent state \\(\\tilde{\\boldsymbol{H}}_t\\)."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-7",
    "href": "dl_lec9.html#gated-recurrent-units-7",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\nComputing the hidden state in a GRU model."
  },
  {
    "objectID": "dl_lec9.html#gated-recurrent-units-8",
    "href": "dl_lec9.html#gated-recurrent-units-8",
    "title": "Recurrent neural networks",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\n\n\n\nSummary\n\n\n\nReset gates help capture short-term dependencies in sequences.\nUpdate gates help capture long-term dependencies in sequences."
  },
  {
    "objectID": "dl_lec9.html#deep-recurrent-neural-networks-1",
    "href": "dl_lec9.html#deep-recurrent-neural-networks-1",
    "title": "Recurrent neural networks",
    "section": "Deep Recurrent Neural Networks",
    "text": "Deep Recurrent Neural Networks\n\n\n\nStacking\n\n\nTo construct a deep RNN with \\(L\\) hidden layers we simply stack ordinary RNNs of any type on top of each other.\nEach hidden state \\(\\boldsymbol{H}^{(l)}_t \\in \\mathbb{R}^{n\\times h}\\) is passed to the next time step of the current layer \\(\\boldsymbol{H}^{(l)}_{t+1}\\) as well as the current time step of the next layer \\(\\boldsymbol{H}^{(l+1)}_t\\).\n\n\n\n\n\n\nState computation\n\n\n\\[\n\\label{drnn_subsequent_state}\n\\boldsymbol{H}^{(l)}_t = \\phi_l \\left(\\boldsymbol{H}^{(l-1)}_t \\boldsymbol{W}_{xh}^{(l)} + \\boldsymbol{H}_{t-1}^{(l)} \\boldsymbol{W}_{hh}^{(l)} + \\boldsymbol{b}_h^{(l)}\\right),\n\\] where \\(\\boldsymbol{H}_t^{(0)} = \\boldsymbol{X}_t\\)."
  },
  {
    "objectID": "dl_lec9.html#deep-recurrent-neural-networks-2",
    "href": "dl_lec9.html#deep-recurrent-neural-networks-2",
    "title": "Recurrent neural networks",
    "section": "Deep Recurrent Neural Networks",
    "text": "Deep Recurrent Neural Networks\n\n\n\nOutput computation\n\n\n\\[\n\\label{drnn_output_computation}\n\\boldsymbol{O}_t = \\phi_o \\left(\\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o\\right), \\; \\boldsymbol{O}_t \\in \\mathbb{R}^{n \\times o}.\n\\]\nNote that we only use the hidden state of layer \\(L\\)."
  },
  {
    "objectID": "dl_lec9.html#deep-recurrent-neural-networks-3",
    "href": "dl_lec9.html#deep-recurrent-neural-networks-3",
    "title": "Recurrent neural networks",
    "section": "Deep Recurrent Neural Networks",
    "text": "Deep Recurrent Neural Networks\n\nArchitecture of a deep recurrent neural network."
  },
  {
    "objectID": "dl_lec9.html#bidirectional-recurrent-neural-networks-1",
    "href": "dl_lec9.html#bidirectional-recurrent-neural-networks-1",
    "title": "Recurrent neural networks",
    "section": "Bidirectional Recurrent Neural Networks",
    "text": "Bidirectional Recurrent Neural Networks\n\n\n\nLanguage modeling example\n\n\nBased on our current models we are able to reliably predict the next sequence element (i.e. the next word) based on what we have seen so far. However, there scenarios where we might want to fill in a gap in a sentence and the part of the sentence after the gap conveys significant information. This information is necessary to take into account to perform well on this kind of task. On a more generalised level we want to incorporate a look-ahead property for sequences.\n\nI am ___.\nI am ___ hungry.\nI am ___ hungry, and I can eat half a pig."
  },
  {
    "objectID": "dl_lec9.html#bidirectional-recurrent-neural-networks-2",
    "href": "dl_lec9.html#bidirectional-recurrent-neural-networks-2",
    "title": "Recurrent neural networks",
    "section": "Bidirectional Recurrent Neural Networks",
    "text": "Bidirectional Recurrent Neural Networks\n\n\n\nDescription\n\n\nTo achieve this look-ahead property Bidirectional Recurrent Neural Networks (BRNNs) got introduced which basically add another hidden layer which run the sequence backwards starting from the last element.\nWe simply implement two unidirectional RNN layers chained together in opposite directions and acting on the same input. For the first RNN layer, the first input is \\(\\boldsymbol{X}_1\\) and the last input is \\(\\boldsymbol{X}_T\\), but for the second RNN layer, the first input is \\(\\boldsymbol{X}_T\\) and the last input is \\(\\boldsymbol{X}_1\\)."
  },
  {
    "objectID": "dl_lec9.html#bidirectional-recurrent-neural-networks-3",
    "href": "dl_lec9.html#bidirectional-recurrent-neural-networks-3",
    "title": "Recurrent neural networks",
    "section": "Bidirectional Recurrent Neural Networks",
    "text": "Bidirectional Recurrent Neural Networks\n\nArchitecture of a bidirectional recurrent neural network."
  },
  {
    "objectID": "dl_lec9.html#bidirectional-recurrent-neural-networks-4",
    "href": "dl_lec9.html#bidirectional-recurrent-neural-networks-4",
    "title": "Recurrent neural networks",
    "section": "Bidirectional Recurrent Neural Networks",
    "text": "Bidirectional Recurrent Neural Networks\n\n\n\nForward/Backward Hidden States\n\n\n\\[\n\\label{brnn_forward}\n\\overrightarrow{\\boldsymbol{H}}_t = \\phi\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1}\\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)}\\right),\\; \\overrightarrow{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h},\n\\] \\[\n\\label{brnn_backward}\n\\overleftarrow{\\boldsymbol{H}}_t = \\phi\\left(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1}\\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}\\right), \\; \\overleftarrow{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}.\n\\]\nNote two sets of hidden matrices and biases: \\[\n\\boldsymbol{W}_{xh}^{(f)},\\boldsymbol{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\; \\boldsymbol{W}_{hh}^{(f)},\\boldsymbol{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h},\\\\\n\\boldsymbol{b}_h^{(f)}, \\boldsymbol{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}.\n\\]"
  },
  {
    "objectID": "dl_lec9.html#bidirectional-recurrent-neural-networks-5",
    "href": "dl_lec9.html#bidirectional-recurrent-neural-networks-5",
    "title": "Recurrent neural networks",
    "section": "Bidirectional Recurrent Neural Networks",
    "text": "Bidirectional Recurrent Neural Networks\n\n\n\nBRNN output\n\n\n\\[\n\\boldsymbol{O}_t = \\phi\\left(\\left[\\overrightarrow{\\boldsymbol{H}}_t \\frown \\overleftarrow{\\boldsymbol{H}}_t\\right]\\boldsymbol{W}_{ho} + \\boldsymbol{b}_o\\right),\n\\] where \\(\\frown\\) denotes matrix concatenation (stacking them on top of each other).\nWeight matrices \\(\\boldsymbol{W}_{ho} \\in \\mathbb{R}^{2h\\times o}\\), bias parameters \\(\\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times o}\\)."
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-1",
    "href": "dl_lec9.html#encoder-decoder-architecture-1",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\n\n\nDescription\n\n\n\nnetwork is twofold\nencoder network – encode the (variable-length) input into a state\ndecoder network – decode the state into an output"
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-2",
    "href": "dl_lec9.html#encoder-decoder-architecture-2",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\n\n\nseq2seq\n\n\nBased on this Encoder-Decoder architecture a model called Sequence to Sequence (seq2seq) got proposed for generating a sequence output based on a sequence input. This model uses RNNs for the encoder as well as the decoder where the hidden state of the encoder gets passed to the hidden state of the decoder.\nIt mainly focuses on mapping a fixed length input sequence of size \\(n\\) to an fixed length output sequence of size \\(m\\) where \\(n \\neq m\\) can be true but isn’t a necessity."
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-3",
    "href": "dl_lec9.html#encoder-decoder-architecture-3",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\nSeq2seq model"
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-4",
    "href": "dl_lec9.html#encoder-decoder-architecture-4",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\n\n\nEncoder\n\n\n\nRNN accepts a single element of the sequence \\(\\boldsymbol{X}_t\\), \\(t\\) being order of the sequence element\nthese RNNs can be LSTMs or GRUs for performance\nhidden states \\(\\boldsymbol{H}_t\\) are computed according to the definition of hidden states in the used RNN type (LSTM or GRU)\nThe Encoder Vector (context) is a representation of the last hidden state of the encoder network which aims to aggregate all information from all previous input elements.\nThis functions as initial hidden state of the decoder network of the model and enables the decoder to make accurate predictions."
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-5",
    "href": "dl_lec9.html#encoder-decoder-architecture-5",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\n\n\nDecoder\n\n\n\nRNN which predicts an output \\(\\boldsymbol{Y}_t\\) at a time step \\(t\\)\nThe produced output is again a sequence where each \\(\\boldsymbol{Y}_t\\) is a sequence element with order \\(t\\)\nAt each time step the RNN accepts a hidden state from the previous unit and itself produces an output as well as a new hidden state."
  },
  {
    "objectID": "dl_lec9.html#encoder-decoder-architecture-6",
    "href": "dl_lec9.html#encoder-decoder-architecture-6",
    "title": "Recurrent neural networks",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\n\n\n\nEncoder computation\n\n\nEncoder transforms the hidden states at all time steps into a context variable \\(\\boldsymbol{C}\\) through a customized function \\(q\\): \\[\n\\boldsymbol{C} = q\\left(\\boldsymbol{H}_1, \\dots, \\boldsymbol{H}_T\\right)\n\\]\n\n\n\n\n\n\nDecoder computation\n\n\nDecoder assigns a predicted probability to each possible token occurring at step \\(t'+1\\) conditioned upon the previous tokens in the target \\(y_1, \\dots, y_{t'}\\) and the context variable \\(\\boldsymbol{C}\\), i.e. \\(P(y_{t'+1} | y_1, \\dots, y_{t'}, \\boldsymbol{C})\\).\n\\[\n\\boldsymbol{S}_{t'} = g\\left(y_{t'-1}, \\boldsymbol{C}, \\boldsymbol{S}_{t'-1}\\right)\n\\]"
  },
  {
    "objectID": "nlp_lab9.html",
    "href": "nlp_lab9.html",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "",
    "text": "What is it? An extension to word2vec for document embeddings.\nThere are two variants: Distributed Memory and Distributed Bag-of-Words.\n\n\nLearns a fixed-length vector representation for each piece of text data (such as a sentence, paragraph, or document) by taking into account the context in which it appears.\nThere are two types of inputs:\n\ncontext words: used to predict a target word\nunique document ID: used to capture the overall meaning of the document\n\nAnd two main components:\n\nthe projection layer: creates the word vectors and document vectors\nthe output layer: takes the distributed representation of the context and predicts the target word\n\n\n\n\nFocuses on understanding how words are distributed in a text, rather than their meaning.\n\n\n\n\n\n\nDifferences to DM\n\n\n\n\nno separate word vectors: the algorithm takes in a document and learns to predict the probability of each word in the document given only the document vector\nthe model does not take into account the order of the words in the document, treating the document as a bag-of-words. This makes the DBOW architecture faster to train than DM, but potentially less powerful in capturing the meaning of the documents.\nuseful for capturing distributional properties of words in a corpus\n\n\n\n\n\n\nLet’s use Gensim’s Lee corpus. First check if you have numerical libraries working:\n\nimport gensim\n\nassert gensim.models.doc2vec.FAST_VERSION &gt; -1\n\n\n\n\nimport gensim\nimport gensim.test.utils\n\n# Set file names for train and test data\nlee_train_file = gensim.test.utils.datapath('lee_background.cor')\nlee_test_file = gensim.test.utils.datapath('lee.cor')\n\n\n\n\n\nimport smart_open\n\ndef read_corpus(fname, tokens_only=False):\n    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n        for i, line in enumerate(f):\n            tokens = gensim.utils.simple_preprocess(line)\n            if tokens_only:\n                yield tokens\n            else:\n                # For training data, add tags\n                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n\ntrain_corpus = list(read_corpus(lee_train_file))\ntest_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n\nprint(train_corpus[2])\nprint(test_corpus[2])\n\nTaggedDocument&lt;['the', 'national', 'road', 'toll', 'for', 'the', 'christmas', 'new', 'year', 'holiday', 'period', 'stands', 'at', 'eight', 'fewer', 'than', 'for', 'the', 'same', 'time', 'last', 'year', 'people', 'have', 'died', 'on', 'new', 'south', 'wales', 'roads', 'with', 'eight', 'fatalities', 'in', 'both', 'queensland', 'and', 'victoria', 'western', 'australia', 'the', 'northern', 'territory', 'and', 'south', 'australia', 'have', 'each', 'recorded', 'three', 'deaths', 'while', 'the', 'act', 'and', 'tasmania', 'remain', 'fatality', 'free'], [2]&gt;\n['the', 'united', 'states', 'government', 'has', 'said', 'it', 'wants', 'to', 'see', 'president', 'robert', 'mugabe', 'removed', 'from', 'power', 'and', 'that', 'it', 'is', 'working', 'with', 'the', 'zimbabwean', 'opposition', 'to', 'bring', 'about', 'change', 'of', 'administration', 'as', 'scores', 'of', 'white', 'farmers', 'went', 'into', 'hiding', 'to', 'escape', 'round', 'up', 'by', 'zimbabwean', 'police', 'senior', 'bush', 'administration', 'official', 'called', 'mr', 'mugabe', 'rule', 'illegitimate', 'and', 'irrational', 'and', 'said', 'that', 'his', 're', 'election', 'as', 'president', 'in', 'march', 'was', 'won', 'through', 'fraud', 'walter', 'kansteiner', 'the', 'assistant', 'secretary', 'of', 'state', 'for', 'african', 'affairs', 'went', 'on', 'to', 'blame', 'mr', 'mugabe', 'policies', 'for', 'contributing', 'to', 'the', 'threat', 'of', 'famine', 'in', 'zimbabwe']\n\n\nSo, train corpus contains TaggedDocuments which contain a list of words and a tag (we used just a simple integer).\nTest corpus contains lists of words only and no tags.\n\n\n\n\nimport gensim.models\n\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n\nHere min_count stands for minimum number of occurrences for a word to be retained in the resulting embeddings set.\n\n\n\n\nmodel.build_vocab(train_corpus)\n\nVocabulary is accessible via model.wv.\n\n\n\n\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n\nGenerated document vectors will be contained in model.dv.\n\nmodel.dv\n\n&lt;gensim.models.keyedvectors.KeyedVectors at 0x177efe960&gt;\n\n\n\n\n\nNow, we can use the trained model to infer a vector for any piece of text by passing a list of words to the model.infer_vector function.\nThis vector can then be compared with other vectors via cosine similarity.\n\nvector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\nprint(vector)\n\n[-0.11410969 -0.36130446 -0.10809141  0.18354002  0.02425136 -0.0160837\n  0.04798935 -0.0150411  -0.2253333  -0.15338649  0.16088457  0.01734591\n  0.11485602 -0.02333582 -0.08442167 -0.10905246  0.08623002  0.30984423\n  0.10422983 -0.14071268  0.01571192 -0.06974691  0.15832673  0.00897413\n -0.01059318 -0.06641291 -0.2322331  -0.00829986 -0.21408382 -0.0575647\n  0.46765783  0.0352442   0.19045384  0.11350349  0.2553538   0.15355816\n -0.13370405 -0.27000526 -0.04545086  0.01119181 -0.02395503  0.04603954\n -0.06474479 -0.159902    0.14529307  0.07847077 -0.12114305 -0.19306047\n  0.14035448 -0.02129764]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninfer_vector() does not take a string, but rather a list of string tokens, which should have already been tokenized the same way as the words property of original training document objects.\nas the underlying training/inference algorithms are an iterative approximation problem that makes use of internal randomization, repeated inferences of the same text will return slightly different vectors.\n\n\n\n\n\n\nFirst we can try inferring vectors from the train dataset. Afterwards, we’ll find most similar vectors to the ones inferred before:\n\nranks = []\nsecond_ranks = []\nfor doc_id in range(len(train_corpus)):\n    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n    rank = [docid for docid, sim in sims].index(doc_id)\n    ranks.append(rank)\n\n    second_ranks.append(sims[1])\n\nimport collections\n\ncounter = collections.Counter(ranks)\nprint(counter)\n\nCounter({0: 293, 1: 7})\n\n\nNow we can pick some document from the test dataset and check the inference:\n\nimport random\n\n# Pick a random document from the test corpus and infer a vector from the model\ndoc_id = random.randint(0, len(test_corpus) - 1)\ninferred_vector = model.infer_vector(test_corpus[doc_id])\nsims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n\n# Compare and print the most/median/least similar documents from the train corpus\nprint('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n\nTest Document (40): «researchers conducting the most elaborate wild goose chase in history are digesting the news that bird they have tracked for over miles is about to be cooked kerry an irish light bellied brent goose was one of six birds tagged in northern ireland in may by researchers monitoring the species remarkable migration last week however he was found dead in an inuit hunter freezer in canada still wearing his satelite tracking device kerry was discovered by researchers on the remote cornwallis island they picked up the signal and decided to try to find him»\n\nSIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec&lt;dm/m,d50,n5,w5,mc2,s0.001,t3&gt;:\n\nMOST (188, 0.7445157170295715): «one person has died after royal flying doctor service rfds aircraft crashed near the city of mt gambier in south australia south east last night the rfds says beech aircraft apparently came down just before midnight acdt in an area called dismal swamp about kilometres north of mt gambier the aircraft with two crew members on board had come from port augusta to mt gambier to fly six year old boy to sydney for medical treatment however spokesman for the rfds says no passenger was aboard the plane at the time one of the crew is believed to have died from injuries and the other is believed to be not as badly hurt no other details have been released and police have sealed off the crash site officers from the bureau of air safety investigation will head to the scene today to determine the cause of the crash»\n\nMEDIAN (160, 0.3469880223274231): «french moroccan man has been charged in the united states with conspiracy in the terrorist attacks of september it is the first indictment directly related to the suicide hijackings news of the charge came as president george bush delivered major foreign policy speech zaccarias moussaoui sought flying lessons month before the hijackings attorney general john ashcroft claims he was an active participant in the attacks moussaoui is charged with undergoing the same training receiving the same funding and pledging the same commitment to kill americans as the hijackers he said three months to the day since the attacks and president bush says missile defence is now more essential than ever before we must protect america and our friends against all forms of terror including the terror that could arrive on missile he said president bush says the united states now needs dramatically retooled military armed with hi tech weapons and real time intelligence»\n\nLEAST (151, -0.054664094001054764): «senior construction forestry mining and energy union cfmeu officials giving evidence at the royal commission into the building industry have been overwhelmed by support from union members about construction workers have walked off the job for the third day to demonstrate outside the commission venue mounted police escorted the protesters from melbourne city square to collins place morning traffic ground to halt at the intersection of russell and collins streets when the crowd stopped to chant union slogans cfmeu victorian secretary martin kingham says he has been astounded by the strong support shown by union members on each day of the hearings he maintains the union has been treated unfairly as it faces allegations of intimidation and using standover tactics on work sites mr kingham is currently giving evidence before the commission it is the last day of hearings before the christmas break the labor leader simon crean says senior labor figures bob hawke and neville wran will be used to help modernise the party labor national executive is meeting in canberra mr crean will put his views on the changes labor needs to make the executive is expected to ask mr hawke and mr wran to oversee the process mr crean says they know what needs to be done bob hawke and neville wran understood the importance of modernising the party and that why they were successful leaders of the country sure we don need to teach them to suck eggs what want them to do is to give us guidance as to how we can bring the new approach to labor in to enable us to properly present and gain the confidence of the majority of the australian people the opposition leader said»"
  },
  {
    "objectID": "nlp_lab9.html#distributed-memory",
    "href": "nlp_lab9.html#distributed-memory",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "",
    "text": "Learns a fixed-length vector representation for each piece of text data (such as a sentence, paragraph, or document) by taking into account the context in which it appears.\nThere are two types of inputs:\n\ncontext words: used to predict a target word\nunique document ID: used to capture the overall meaning of the document\n\nAnd two main components:\n\nthe projection layer: creates the word vectors and document vectors\nthe output layer: takes the distributed representation of the context and predicts the target word"
  },
  {
    "objectID": "nlp_lab9.html#distributed-bag-of-words",
    "href": "nlp_lab9.html#distributed-bag-of-words",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "",
    "text": "Focuses on understanding how words are distributed in a text, rather than their meaning.\n\n\n\n\n\n\nDifferences to DM\n\n\n\n\nno separate word vectors: the algorithm takes in a document and learns to predict the probability of each word in the document given only the document vector\nthe model does not take into account the order of the words in the document, treating the document as a bag-of-words. This makes the DBOW architecture faster to train than DM, but potentially less powerful in capturing the meaning of the documents.\nuseful for capturing distributional properties of words in a corpus"
  },
  {
    "objectID": "nlp_lab9.html#code-example",
    "href": "nlp_lab9.html#code-example",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "",
    "text": "Let’s use Gensim’s Lee corpus. First check if you have numerical libraries working:\n\nimport gensim\n\nassert gensim.models.doc2vec.FAST_VERSION &gt; -1\n\n\n\n\nimport gensim\nimport gensim.test.utils\n\n# Set file names for train and test data\nlee_train_file = gensim.test.utils.datapath('lee_background.cor')\nlee_test_file = gensim.test.utils.datapath('lee.cor')\n\n\n\n\n\nimport smart_open\n\ndef read_corpus(fname, tokens_only=False):\n    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n        for i, line in enumerate(f):\n            tokens = gensim.utils.simple_preprocess(line)\n            if tokens_only:\n                yield tokens\n            else:\n                # For training data, add tags\n                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n\ntrain_corpus = list(read_corpus(lee_train_file))\ntest_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n\nprint(train_corpus[2])\nprint(test_corpus[2])\n\nTaggedDocument&lt;['the', 'national', 'road', 'toll', 'for', 'the', 'christmas', 'new', 'year', 'holiday', 'period', 'stands', 'at', 'eight', 'fewer', 'than', 'for', 'the', 'same', 'time', 'last', 'year', 'people', 'have', 'died', 'on', 'new', 'south', 'wales', 'roads', 'with', 'eight', 'fatalities', 'in', 'both', 'queensland', 'and', 'victoria', 'western', 'australia', 'the', 'northern', 'territory', 'and', 'south', 'australia', 'have', 'each', 'recorded', 'three', 'deaths', 'while', 'the', 'act', 'and', 'tasmania', 'remain', 'fatality', 'free'], [2]&gt;\n['the', 'united', 'states', 'government', 'has', 'said', 'it', 'wants', 'to', 'see', 'president', 'robert', 'mugabe', 'removed', 'from', 'power', 'and', 'that', 'it', 'is', 'working', 'with', 'the', 'zimbabwean', 'opposition', 'to', 'bring', 'about', 'change', 'of', 'administration', 'as', 'scores', 'of', 'white', 'farmers', 'went', 'into', 'hiding', 'to', 'escape', 'round', 'up', 'by', 'zimbabwean', 'police', 'senior', 'bush', 'administration', 'official', 'called', 'mr', 'mugabe', 'rule', 'illegitimate', 'and', 'irrational', 'and', 'said', 'that', 'his', 're', 'election', 'as', 'president', 'in', 'march', 'was', 'won', 'through', 'fraud', 'walter', 'kansteiner', 'the', 'assistant', 'secretary', 'of', 'state', 'for', 'african', 'affairs', 'went', 'on', 'to', 'blame', 'mr', 'mugabe', 'policies', 'for', 'contributing', 'to', 'the', 'threat', 'of', 'famine', 'in', 'zimbabwe']\n\n\nSo, train corpus contains TaggedDocuments which contain a list of words and a tag (we used just a simple integer).\nTest corpus contains lists of words only and no tags.\n\n\n\n\nimport gensim.models\n\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n\nHere min_count stands for minimum number of occurrences for a word to be retained in the resulting embeddings set.\n\n\n\n\nmodel.build_vocab(train_corpus)\n\nVocabulary is accessible via model.wv.\n\n\n\n\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n\nGenerated document vectors will be contained in model.dv.\n\nmodel.dv\n\n&lt;gensim.models.keyedvectors.KeyedVectors at 0x177efe960&gt;\n\n\n\n\n\nNow, we can use the trained model to infer a vector for any piece of text by passing a list of words to the model.infer_vector function.\nThis vector can then be compared with other vectors via cosine similarity.\n\nvector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\nprint(vector)\n\n[-0.11410969 -0.36130446 -0.10809141  0.18354002  0.02425136 -0.0160837\n  0.04798935 -0.0150411  -0.2253333  -0.15338649  0.16088457  0.01734591\n  0.11485602 -0.02333582 -0.08442167 -0.10905246  0.08623002  0.30984423\n  0.10422983 -0.14071268  0.01571192 -0.06974691  0.15832673  0.00897413\n -0.01059318 -0.06641291 -0.2322331  -0.00829986 -0.21408382 -0.0575647\n  0.46765783  0.0352442   0.19045384  0.11350349  0.2553538   0.15355816\n -0.13370405 -0.27000526 -0.04545086  0.01119181 -0.02395503  0.04603954\n -0.06474479 -0.159902    0.14529307  0.07847077 -0.12114305 -0.19306047\n  0.14035448 -0.02129764]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninfer_vector() does not take a string, but rather a list of string tokens, which should have already been tokenized the same way as the words property of original training document objects.\nas the underlying training/inference algorithms are an iterative approximation problem that makes use of internal randomization, repeated inferences of the same text will return slightly different vectors.\n\n\n\n\n\n\nFirst we can try inferring vectors from the train dataset. Afterwards, we’ll find most similar vectors to the ones inferred before:\n\nranks = []\nsecond_ranks = []\nfor doc_id in range(len(train_corpus)):\n    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n    rank = [docid for docid, sim in sims].index(doc_id)\n    ranks.append(rank)\n\n    second_ranks.append(sims[1])\n\nimport collections\n\ncounter = collections.Counter(ranks)\nprint(counter)\n\nCounter({0: 293, 1: 7})\n\n\nNow we can pick some document from the test dataset and check the inference:\n\nimport random\n\n# Pick a random document from the test corpus and infer a vector from the model\ndoc_id = random.randint(0, len(test_corpus) - 1)\ninferred_vector = model.infer_vector(test_corpus[doc_id])\nsims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n\n# Compare and print the most/median/least similar documents from the train corpus\nprint('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n\nTest Document (40): «researchers conducting the most elaborate wild goose chase in history are digesting the news that bird they have tracked for over miles is about to be cooked kerry an irish light bellied brent goose was one of six birds tagged in northern ireland in may by researchers monitoring the species remarkable migration last week however he was found dead in an inuit hunter freezer in canada still wearing his satelite tracking device kerry was discovered by researchers on the remote cornwallis island they picked up the signal and decided to try to find him»\n\nSIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec&lt;dm/m,d50,n5,w5,mc2,s0.001,t3&gt;:\n\nMOST (188, 0.7445157170295715): «one person has died after royal flying doctor service rfds aircraft crashed near the city of mt gambier in south australia south east last night the rfds says beech aircraft apparently came down just before midnight acdt in an area called dismal swamp about kilometres north of mt gambier the aircraft with two crew members on board had come from port augusta to mt gambier to fly six year old boy to sydney for medical treatment however spokesman for the rfds says no passenger was aboard the plane at the time one of the crew is believed to have died from injuries and the other is believed to be not as badly hurt no other details have been released and police have sealed off the crash site officers from the bureau of air safety investigation will head to the scene today to determine the cause of the crash»\n\nMEDIAN (160, 0.3469880223274231): «french moroccan man has been charged in the united states with conspiracy in the terrorist attacks of september it is the first indictment directly related to the suicide hijackings news of the charge came as president george bush delivered major foreign policy speech zaccarias moussaoui sought flying lessons month before the hijackings attorney general john ashcroft claims he was an active participant in the attacks moussaoui is charged with undergoing the same training receiving the same funding and pledging the same commitment to kill americans as the hijackers he said three months to the day since the attacks and president bush says missile defence is now more essential than ever before we must protect america and our friends against all forms of terror including the terror that could arrive on missile he said president bush says the united states now needs dramatically retooled military armed with hi tech weapons and real time intelligence»\n\nLEAST (151, -0.054664094001054764): «senior construction forestry mining and energy union cfmeu officials giving evidence at the royal commission into the building industry have been overwhelmed by support from union members about construction workers have walked off the job for the third day to demonstrate outside the commission venue mounted police escorted the protesters from melbourne city square to collins place morning traffic ground to halt at the intersection of russell and collins streets when the crowd stopped to chant union slogans cfmeu victorian secretary martin kingham says he has been astounded by the strong support shown by union members on each day of the hearings he maintains the union has been treated unfairly as it faces allegations of intimidation and using standover tactics on work sites mr kingham is currently giving evidence before the commission it is the last day of hearings before the christmas break the labor leader simon crean says senior labor figures bob hawke and neville wran will be used to help modernise the party labor national executive is meeting in canberra mr crean will put his views on the changes labor needs to make the executive is expected to ask mr hawke and mr wran to oversee the process mr crean says they know what needs to be done bob hawke and neville wran understood the importance of modernising the party and that why they were successful leaders of the country sure we don need to teach them to suck eggs what want them to do is to give us guidance as to how we can bring the new approach to labor in to enable us to properly present and gain the confidence of the majority of the australian people the opposition leader said»"
  },
  {
    "objectID": "nlp_lab9.html#using-parquet-files",
    "href": "nlp_lab9.html#using-parquet-files",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "Using parquet files",
    "text": "Using parquet files\nFirst, install fastparquet library:\npip install fastparquet\nThen read file.parquet into Pandas DataFrame via:\nimport pandas as pd\npd.read_parquet(file.parquet)"
  },
  {
    "objectID": "nlp_lab9.html#dataset-examples",
    "href": "nlp_lab9.html#dataset-examples",
    "title": "NLP: Lab 9 (doc2vec)",
    "section": "Dataset examples",
    "text": "Dataset examples\n\nhttps://huggingface.co/datasets/immortalizzy/reddit_dataset_197 (Reddit posts)\nhttps://huggingface.co/datasets/fancyzhx/ag_news (News articles)\nhttps://huggingface.co/datasets/arrmlet/x_dataset_218 (X posts)\nhttps://huggingface.co/datasets/stanfordnlp/imdb (IMDB reviews)\nhttps://huggingface.co/datasets/wikimedia/wikipedia (Wikipedia articles)\nhttps://huggingface.co/datasets/CShorten/ML-ArXiv-Papers (ArXiv papers)\nhttps://huggingface.co/datasets/ccdv/arxiv-classification (ArXiv papers)\nhttps://huggingface.co/datasets/gfissore/arxiv-abstracts-2021 (ArXiv papers)\n\nTask 1. Practice finding similar documents/articles/posts. Assess validity of the model."
  },
  {
    "objectID": "dl_lec10.html#attention",
    "href": "dl_lec10.html#attention",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\n\n\n\nEarly methods\n\n\n\nMLP\nConvolutions\nRecurrent NNs\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nconvolutions dominating image processing\nLSTM RNNs dominating NLP"
  },
  {
    "objectID": "dl_lec10.html#applications",
    "href": "dl_lec10.html#applications",
    "title": "Transformers 1",
    "section": "Applications",
    "text": "Applications\n\n\n\nApplications of transformers\n\n\n\nNLP\nimage recognition\nspeech recognition\nreinforcement learning"
  },
  {
    "objectID": "dl_lec10.html#attention-1",
    "href": "dl_lec10.html#attention-1",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nHistory\n\n\nAttention - originally proposed for encoded-decoder architectures.\nVaswani paper - “Attention is all you need”\nLarge-scale pretrained models, now sometimes called foundation models."
  },
  {
    "objectID": "dl_lec10.html#seq2seq",
    "href": "dl_lec10.html#seq2seq",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nDefinition\n\n\nA sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-1",
    "href": "dl_lec10.html#seq2seq-1",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nEncoder/Decoder\n\n\nUnder the hood, the model is composed of an encoder and a decoder.\nThe encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the ). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.\nThe context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-2",
    "href": "dl_lec10.html#seq2seq-2",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\nRNN takes two inputs: an input (one word from the input sentence) and a hidden state.\nContext: You can set the size of the context vector when you set up your model. It is basically the number of hidden units in the encoder RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-3",
    "href": "dl_lec10.html#seq2seq-3",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-4",
    "href": "dl_lec10.html#seq2seq-4",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-5",
    "href": "dl_lec10.html#seq2seq-5",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\nIn the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.\nLet’s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-6",
    "href": "dl_lec10.html#seq2seq-6",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-7",
    "href": "dl_lec10.html#seq2seq-7",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nUnrolled view"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-8",
    "href": "dl_lec10.html#seq2seq-8",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nProblem\n\n\nThe context vector turned out to be a bottleneck for these types of models: it was challenging for the models to deal with long sentences.\n\n\n\n\n\n\nSolution\n\n\nA solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015.\n\nThese papers introduced and refined a technique called attention, which highly improved the quality of machine translation systems.\nAttention allows the model to focus on the relevant parts of the input sequence as needed."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-9",
    "href": "dl_lec10.html#seq2seq-9",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\nAt time step 7, the attention mechanism enables the decoder to focus on the word “étudiant” (“student” in french) before it generates the English translation."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-10",
    "href": "dl_lec10.html#seq2seq-10",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nAttention model differences\n\n\nEncoder passes all hidden states to the decoder, not just the last state.\nDecoder does some extra step before producing its output:\n\nLook at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence\nGive each hidden state a score\nMultiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores\nThe scoring is done at each time step."
  },
  {
    "objectID": "dl_lec10.html#seq2seq-11",
    "href": "dl_lec10.html#seq2seq-11",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nIllustrated"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-12",
    "href": "dl_lec10.html#seq2seq-12",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-13",
    "href": "dl_lec10.html#seq2seq-13",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nProcess\n\n\n\nThe attention decoder RNN takes in the embedding of the  token, and an initial decoder hidden state.\nThe RNN processes its inputs, producing an output and a new hidden state vector \\(h_4\\). The output is discarded.\nAttention Step: We use the encoder hidden states and the \\(h_4\\) vector to calculate a context vector \\(c_4\\) for this time step.\nWe concatenate \\(h_4\\) and \\(c_4\\) into one vector.\nWe pass this vector through a feedforward neural network (one trained jointly with the model).\nThe output of the feedforward neural networks indicates the output word of this time step.\nRepeat for the next time steps"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-14",
    "href": "dl_lec10.html#seq2seq-14",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-15",
    "href": "dl_lec10.html#seq2seq-15",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\n\n\nAnother way"
  },
  {
    "objectID": "dl_lec10.html#seq2seq-16",
    "href": "dl_lec10.html#seq2seq-16",
    "title": "Transformers 1",
    "section": "seq2seq",
    "text": "seq2seq\n\nModel learns how to align words (example from paper)."
  },
  {
    "objectID": "dl_lec10.html#transformers-1",
    "href": "dl_lec10.html#transformers-1",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nAttention is all you need (2017)"
  },
  {
    "objectID": "dl_lec10.html#transformers-2",
    "href": "dl_lec10.html#transformers-2",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nCompared to seq2seq, performance is improved!"
  },
  {
    "objectID": "dl_lec10.html#transformers-3",
    "href": "dl_lec10.html#transformers-3",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nEncoder and decoder structure."
  },
  {
    "objectID": "dl_lec10.html#transformers-4",
    "href": "dl_lec10.html#transformers-4",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nA key property of the Transformer: the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer."
  },
  {
    "objectID": "dl_lec10.html#transformers-5",
    "href": "dl_lec10.html#transformers-5",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nThe word at each position passes through a self-attention process. Then, they each pass through a FFNN."
  },
  {
    "objectID": "dl_lec10.html#transformers-6",
    "href": "dl_lec10.html#transformers-6",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHigh-level view\n\n\nWhat does the word “it” refer to?\n\nThe animal didn’t cross the street because it was too tired"
  },
  {
    "objectID": "dl_lec10.html#transformers-7",
    "href": "dl_lec10.html#transformers-7",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHigh-level flow\n\n\n\nwhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\nas the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nfor RNNs, maintaining a hidden state allows them to incorporate its representation of previous words/vectors it has processed with the current one it’s processing.\nSelf-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing."
  },
  {
    "objectID": "dl_lec10.html#transformers-8",
    "href": "dl_lec10.html#transformers-8",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHow is self-attention calculated?\n\n\n\nCreate 3 vectors (Query, Key, Value) from each input embedding."
  },
  {
    "objectID": "dl_lec10.html#transformers-9",
    "href": "dl_lec10.html#transformers-9",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHow is self-attention calculated?\n\n\n\nCalculate score. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring."
  },
  {
    "objectID": "dl_lec10.html#transformers-10",
    "href": "dl_lec10.html#transformers-10",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHow is self-attention calculated?\n\n\n\nDivide by \\(\\sqrt{d_k}\\).\nNormalize via softmax.\n\n\n\n\nwidth=9cm"
  },
  {
    "objectID": "dl_lec10.html#transformers-11",
    "href": "dl_lec10.html#transformers-11",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nHow is self-attention calculated?\n\n\n\nMultiply each value vector by softmax score.\nIntuition: keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\nSum up weighted value vectors. This produces the output of the self-attention layer at this position (for the first word)."
  },
  {
    "objectID": "dl_lec10.html#transformers-12",
    "href": "dl_lec10.html#transformers-12",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-13",
    "href": "dl_lec10.html#transformers-13",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nMatrix calculation"
  },
  {
    "objectID": "dl_lec10.html#transformers-14",
    "href": "dl_lec10.html#transformers-14",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nMatrix calculation"
  },
  {
    "objectID": "dl_lec10.html#transformers-15",
    "href": "dl_lec10.html#transformers-15",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\nMultiheaded attention\n\n\nIntroduced in the original paper.\nThis improves the performance of the attention layer in two ways:\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”."
  },
  {
    "objectID": "dl_lec10.html#transformers-16",
    "href": "dl_lec10.html#transformers-16",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-17",
    "href": "dl_lec10.html#transformers-17",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\nCalculating e.g. 8 times:"
  },
  {
    "objectID": "dl_lec10.html#transformers-18",
    "href": "dl_lec10.html#transformers-18",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nProblem\n\n\nFeed-forward layer is not expecting 8 matrices, but 1.\n\n\n\n\n\n\nSolution\n\n\nConcat the matrices then multiply them by an additional weights matrix \\(W_O\\)."
  },
  {
    "objectID": "dl_lec10.html#transformers-19",
    "href": "dl_lec10.html#transformers-19",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-20",
    "href": "dl_lec10.html#transformers-20",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nEverything together."
  },
  {
    "objectID": "dl_lec10.html#transformers-21",
    "href": "dl_lec10.html#transformers-21",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nThe model’s representation of the word “it” bakes in some of the representation of both “animal” and “tired”."
  },
  {
    "objectID": "dl_lec10.html#transformers-22",
    "href": "dl_lec10.html#transformers-22",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nWith all attention heads."
  },
  {
    "objectID": "dl_lec10.html#transformers-23",
    "href": "dl_lec10.html#transformers-23",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nProblem\n\n\nHow do we account for the order of the words in the input sequence?\n\n\n\n\n\n\nSolution\n\n\nThe transformer adds a vector to each input embedding.\nThese vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence.\nThe intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention."
  },
  {
    "objectID": "dl_lec10.html#transformers-24",
    "href": "dl_lec10.html#transformers-24",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nValues of positional encoding vectors follow a specific pattern."
  },
  {
    "objectID": "dl_lec10.html#transformers-25",
    "href": "dl_lec10.html#transformers-25",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nA real example of positional encoding with a toy embedding size of 4."
  },
  {
    "objectID": "dl_lec10.html#transformers-26",
    "href": "dl_lec10.html#transformers-26",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-27",
    "href": "dl_lec10.html#transformers-27",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nA formule for PE from the paper\n\n\n\\[\n  PE_{(pos,2i)} =sin(pos/10000^{2i/d_{model}}),\\\\\n  PE_{(pos,2i+1)} =cos(pos/10000^{2i/d_{model}})\n\\] where \\(pos\\) is the position and \\(i\\) is the dimension.\nThat is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from \\(2\\pi\\) to \\(10000 \\cdot 2\\pi\\). We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\)."
  },
  {
    "objectID": "dl_lec10.html#transformers-28",
    "href": "dl_lec10.html#transformers-28",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nEach sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step."
  },
  {
    "objectID": "dl_lec10.html#transformers-29",
    "href": "dl_lec10.html#transformers-29",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nWith vectors and operations visualized."
  },
  {
    "objectID": "dl_lec10.html#transformers-30",
    "href": "dl_lec10.html#transformers-30",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nTransformer of 2 stacked encoders and decoders"
  },
  {
    "objectID": "dl_lec10.html#transformers-31",
    "href": "dl_lec10.html#transformers-31",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nDecoder\n\n\n\nThe encoders start by processing the input sequence\nThe output of the top encoder is then transformed into a set of attention vectors \\(\\boldsymbol{K}\\) and \\(\\boldsymbol{V}\\)\nThese are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\nEach step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case)."
  },
  {
    "objectID": "dl_lec10.html#transformers-32",
    "href": "dl_lec10.html#transformers-32",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-33",
    "href": "dl_lec10.html#transformers-33",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10.html#transformers-34",
    "href": "dl_lec10.html#transformers-34",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nDecoder\n\n\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack."
  },
  {
    "objectID": "dl_lec10.html#transformers-35",
    "href": "dl_lec10.html#transformers-35",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nDecoder\n\n\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."
  },
  {
    "objectID": "dl_lec10.html#transformers-36",
    "href": "dl_lec10.html#transformers-36",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nThis figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word."
  },
  {
    "objectID": "dl_lec10.html#transformers-37",
    "href": "dl_lec10.html#transformers-37",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nTraining a model\n\n\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output."
  },
  {
    "objectID": "dl_lec10.html#transformers-38",
    "href": "dl_lec10.html#transformers-38",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector."
  },
  {
    "objectID": "dl_lec10.html#transformers-39",
    "href": "dl_lec10.html#transformers-39",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nThe loss function\n\n\nWe want the output to be a probability distribution indicating the word “thanks”.\n\n\n\nThe (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model’s weights."
  },
  {
    "objectID": "dl_lec10.html#transformers-40",
    "href": "dl_lec10.html#transformers-40",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nComparison\n\n\nHow do you compare two probability distributions? Kullback–Leibler divergence.\nWe want our model to successively output probability distributions where:\n\nEach probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\nThe first probability distribution has the highest probability at the cell associated with the word “i”\nThe second probability distribution has the highest probability at the cell associated with the word “am”\nAnd so on, until the fifth output distribution indicates , which also has a cell associated with it from the 10,000 element vocabulary."
  },
  {
    "objectID": "dl_lec10.html#transformers-41",
    "href": "dl_lec10.html#transformers-41",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nThe targeted probability distributions."
  },
  {
    "objectID": "dl_lec10.html#transformers-42",
    "href": "dl_lec10.html#transformers-42",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nProduced probability distributions."
  },
  {
    "objectID": "dl_lec10.html#attention-2",
    "href": "dl_lec10.html#attention-2",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nDatabase analogy\n\n\nDenote by \\[\nD \\equiv \\left\\{(k_1, v_1, \\dots, (k_m, v_m)\\right\\}\n\\] a database of \\(m\\) tuples of keys and values. Denote by \\(\\boldsymbol{q}\\) a query.\n\n\n\n\n\n\nAttention definition\n\n\n\\[\nAttention(\\boldsymbol{q}, D) \\equiv \\sum\\limits_{i=1}^m \\alpha(\\boldsymbol{q}, k_i)v_i,\n\\] where \\(\\alpha(\\boldsymbol{q},k_i) \\in \\mathbb{R}\\) are scalar attention weights.\nThe operation itself is typically referred to as attention pooling. The name attention derives from the fact that the operation pays particular attention to the terms for which the weight \\(\\alpha\\) is significant (i.e., large)."
  },
  {
    "objectID": "dl_lec10.html#attention-3",
    "href": "dl_lec10.html#attention-3",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nSpecial cases\n\n\n\n\\(\\alpha(\\boldsymbol{q}, k_i) \\geq 0\\) – output of the attention mechanism is contained in the convex cone spanned by the values \\(\\boldsymbol{v}_i\\)\n\\(\\sum_i \\alpha(\\boldsymbol{q}, k_i) = 1, \\alpha(q,k_i) \\geq 0 \\; \\forall i\\) – most common\n\\(\\exists j: \\alpha(\\boldsymbol{q}, k_j) = 1, \\; \\alpha(\\boldsymbol{q}, k_i) = 0, i \\neq j\\) – traditional database query\n\\(\\alpha(\\boldsymbol{q}, k_i) = \\frac{1}{m} \\; \\forall i\\) – average pooling"
  },
  {
    "objectID": "dl_lec10.html#attention-4",
    "href": "dl_lec10.html#attention-4",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\nStrategies:\n\n\n\nNormalization\n\n\n\\[\n       \\alpha(\\boldsymbol{q}, k_i) = \\frac{\\alpha(\\boldsymbol{q}, k_i)}{\\sum_j \\alpha(\\boldsymbol{q}, k_j)}.\n\\]\n\n\n\n\n\n\nExponentiation\n\n\n\\[\n\\alpha(\\boldsymbol{q}, k_i) = \\frac{exp\\left(\\alpha(\\boldsymbol{q}, k_i)\\right)}{\\sum_j exp\\left(\\alpha(\\boldsymbol{q}, k_j)\\right)}.\n\\]"
  },
  {
    "objectID": "dl_lec10.html#attention-5",
    "href": "dl_lec10.html#attention-5",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "nlp.html#lectures",
    "href": "nlp.html#lectures",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "nlp.html#labs",
    "href": "nlp.html#labs",
    "title": "Natural Language Processing",
    "section": "Labs",
    "text": "Labs\nLab 1: NLTK basics\nLab 2: Stemming/String distance\nLab 3: Lemmatization/WordNet\nLab 4: Naive Bayes classifier\nLab 5: N-grams\nLab 6: Bag-of-words/PPMI\nLab 7: TF-IDF\nLab 8: word2vec\nLab 9: doc2vec\nLab 10: PCA\nLab 11: CNNs for NLP\nLab 12: LSH\nLab 13: Generating Names with a Character-Level RNN\nLab 14:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nlp_lec1.html#history",
    "href": "nlp_lec1.html#history",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#history-1",
    "href": "nlp_lec1.html#history-1",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#stats",
    "href": "nlp_lec1.html#stats",
    "title": "Natural Language Processing: Intro",
    "section": "Stats",
    "text": "Stats\n\n\n\nWhen?\n\n\nApproximately between 200,000 years ago and 60,000 years ago (between the appearance of the first anatomically modern humans in southern Africa and the last exodus from Africa respectively)\n\n\n\n\n\n\nHow many?\n\n\nHuman languages count: 7097 (as of 2018)."
  },
  {
    "objectID": "nlp_lec1.html#intelligent-behaviour",
    "href": "nlp_lec1.html#intelligent-behaviour",
    "title": "Natural Language Processing: Intro",
    "section": "Intelligent behaviour",
    "text": "Intelligent behaviour\n\n\n\nSpeaker (writer)\n\n\n\nhas the goal of communicating some knowledge\nthen plans some language that represents the knowledge\nand acts to achieve the goal\n\n\n\n\n\n\n\nListener (reader)\n\n\n\nperceives the language\nand infers the intended meaning."
  },
  {
    "objectID": "nlp_lec1.html#reasons-for-nlp",
    "href": "nlp_lec1.html#reasons-for-nlp",
    "title": "Natural Language Processing: Intro",
    "section": "Reasons for NLP",
    "text": "Reasons for NLP\n\nTo communicate with humans.\nTo learn.\nTo advance the scientific understanding of languages and language use"
  },
  {
    "objectID": "nlp_lec1.html#goal-of-natural-language",
    "href": "nlp_lec1.html#goal-of-natural-language",
    "title": "Natural Language Processing: Intro",
    "section": "Goal of natural language",
    "text": "Goal of natural language\nA medium for communication\nrather than pure representation.\n\n\n\nPlato vs Sophists\n\n\nSophists argued that physical reality can only be experienced through language."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis\nLanguage impacts cognition. Also known as linguistic relativity.\n\n\n\nPredecessors\n\n\n\nAustralian aboriginal language Guugu Yimithirr have no words for relative (or ego- centric) directions, such as front, back, right, or left. Instead they use absolute directions, saying, for example, the equivalent of “I have a pain in my north arm.”\n\n(Norvig “Artificial Intelligence: A Modern Approach”)\n\n\n\n\n\n\nPredecessors\n\n\nWilhelm von Humboldt: language as a spirit of a nation."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis"
  },
  {
    "objectID": "nlp_lec1.html#deep-structure",
    "href": "nlp_lec1.html#deep-structure",
    "title": "Natural Language Processing: Intro",
    "section": "Deep structure",
    "text": "Deep structure\n\n\n\nWanner experiment (1974)\n\n\nSubjects remember exact words with 50% accuracy but remember the content with 90% accuracy.\nHence - there must an internal nonverbal representation.\n\n\n\n\n\n\nPolanyi’s paradox (1966)\n\n\nThe theory that human knowledge of how the world functions and of our own capability are, to a large extent, beyond our explicit understanding. (aka tacit knowledge)."
  },
  {
    "objectID": "nlp_lec1.html#formal-language",
    "href": "nlp_lec1.html#formal-language",
    "title": "Natural Language Processing: Intro",
    "section": "Formal language",
    "text": "Formal language\n\n\n\nDefinition\n\n\nA formal language \\(L\\) over an alphabet \\(\\Sigma\\) is a subset of \\(\\Sigma^*\\), that is, a set of words over that alphabet."
  },
  {
    "objectID": "nlp_lec1.html#language-model",
    "href": "nlp_lec1.html#language-model",
    "title": "Natural Language Processing: Intro",
    "section": "Language model",
    "text": "Language model\n\n\n\nDefinition\n\n\nWe define a language model as a probability distribution describing the likelihood of any string."
  },
  {
    "objectID": "nlp_lec1.html#eliza",
    "href": "nlp_lec1.html#eliza",
    "title": "Natural Language Processing: Intro",
    "section": "ELIZA",
    "text": "ELIZA\n\nELIZA - an example of primitive pattern matching."
  },
  {
    "objectID": "nlp_lec1.html#regex",
    "href": "nlp_lec1.html#regex",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\nRegular expressions - a tool for describing text patterns.\n\n\n\nDefinition\n\n\nAn algebraic notation for characterizing a set of strings\n\n\n\n\n\nKleene, S. C. 1951. Representation of events in nerve nets and finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum."
  },
  {
    "objectID": "nlp_lec1.html#regex-1",
    "href": "nlp_lec1.html#regex-1",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDefinition\n\n\nA Kleene algebra is a set \\(A\\) together with two binary operations \\(+: A \\times A \\rightarrow A\\) and \\(\\cdot : A \\times A \\rightarrow A\\) and one function \\(\\ast : A \\rightarrow A\\), written as \\(a + b\\), \\(ab\\) and \\(a\\ast\\) respectively, so that the following axioms are satisfied.\n\nAssociativity of \\(+\\) and \\(\\cdot\\): \\(a + (b + c) = (a + b) + c\\) and \\(a(bc) = (ab)c\\) \\(\\forall a, b, c \\in A\\).\nCommutativity of \\(+\\): \\(a + b = b + a\\) \\(\\forall a, b \\in A\\)\nDistributivity: \\(a(b + c) = (ab) + (ac)\\) and \\((b + c)a = (ba) + (ca)\\) \\(\\forall a, b, c \\in A\\)\nIdentity elements for \\(+\\) and \\(\\cdot\\): \\(\\exists 0 \\in A:\\forall a \\in A: a + 0 = 0 + a = a\\); \\(\\exists 1 \\in A: \\forall a \\in A: a1 = 1a = a\\).\nAnnihilation by 0: \\(a0 = 0a = 0 \\forall a \\in A\\). The above axioms define a semiring.\n\\(+\\) is idempotent: \\(a + a = a \\quad \\forall a \\in A\\)."
  },
  {
    "objectID": "nlp_lec1.html#regex-2",
    "href": "nlp_lec1.html#regex-2",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nOrdering\n\n\nIt is now possible to define a partial order \\(\\leq\\) on \\(A\\) by setting \\(a \\leq b\\) if and only if \\(a + b = b\\) (or equivalently: \\(a \\leq b\\) if and only if \\(\\exists x \\in A:  a + x = b\\).\nWith any definition, \\(a \\leq b \\leq a \\Rightarrow a = b\\). With this order we can formulate the last four axioms about the operation \\(\\ast\\):\n\n\\(1 + a(a\\ast) leq a\\ast \\forall a in A\\).\n\\(1 + (a\\ast)a \\leq a\\ast \\forall a in A\\).\nif a and x are in A such that \\(ax \\leq x\\), then \\(a\\ast x \\leq x\\)\nif a and x are in A such that \\(xa \\leq x\\), then \\(x(a\\ast) \\leq x\\).\n\nIntuitively, one should think of a + b as the “union” or the “least upper bound” of a and b and of ab as some multiplication which is monotonic, in the sense that \\(a \\leq b \\Rightarrow ax \\leq bx\\).\nThe idea behind the star operator is \\(a\\ast = 1 + a + aa + aaa + ...\\). From the standpoint of programming language theory, one may also interpret + as “choice”, \\(\\cdot\\) as “sequencing” and \\(\\ast\\) as “iteration”."
  },
  {
    "objectID": "nlp_lec1.html#regex-3",
    "href": "nlp_lec1.html#regex-3",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nConcatenation\n\n\nA sequence of characters\n/someword/\n\n\n\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/alt/\nThe alternative option would be…\n\n\n/simple/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-4",
    "href": "nlp_lec1.html#regex-4",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction\n\n\nA single character to choose among multiple options\n/[asd]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[0123456789]/\nSome number examples are 0, 3, 5\n\n\n/[rgx]/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-5",
    "href": "nlp_lec1.html#regex-5",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction with range\n\n\nA single character to choose among multiple options\n/[a-z]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-6",
    "href": "nlp_lec1.html#regex-6",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene *\n\n\nZero or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]*/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-7",
    "href": "nlp_lec1.html#regex-7",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene +\n\n\nOne or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]+/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-8",
    "href": "nlp_lec1.html#regex-8",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nWildcard dot\n\n\nAny character except newline\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/.+/\nPhone number: 067-1234567\n\n\n/beg.n/\nbegin, began, begun"
  },
  {
    "objectID": "nlp_lec1.html#regex-9",
    "href": "nlp_lec1.html#regex-9",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nAnchors\n\n\nMatching beginning and end of lines\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/^/\nStart of line\n\n\n/$/\nEnd of line\n\n\n/\\b/\nWord boundary\n\n\n/\\B/\nNon-word boundary"
  },
  {
    "objectID": "nlp_lec1.html#regex-10",
    "href": "nlp_lec1.html#regex-10",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction operator\n\n\nEither/or for strings\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/cat|dog/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-11",
    "href": "nlp_lec1.html#regex-11",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nGrouping\n\n\nUsed for precedence\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/(cat|dog)*/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-12",
    "href": "nlp_lec1.html#regex-12",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nSubstitutions\n\n\nUsed for precedence\n\nExamples\n\n\n\n\n\n\n\nRegex\nBefore\nAfter\n\n\n\n\ns/(cat|dog)*/frog/\nRaining cats and dogs\nRaining frogs and frogs\n\n\ns/behaviour/behavior/\nCognitive-behavioural therapy\nCognitive-behavioral therapy"
  },
  {
    "objectID": "nlp_lec1.html#regex-13",
    "href": "nlp_lec1.html#regex-13",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nELIZA sample\n\n\nUser1: Men are all alike.\nELIZA1: User2: They’re always bugging us about something or other.\nELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser3: Well, my boyfriend made me come here.\nELIZA3: YOUR BOYFRIEND MADE YOU COME HERE\nUser4: He says I’m depressed much of the time.\nELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\n\n\n\n\n\n\nELIZA sample\n\n\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/"
  },
  {
    "objectID": "nlp_lec1.html#parsing",
    "href": "nlp_lec1.html#parsing",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\n\n\nA computer-readable collection of text or speech.\n\n\n\n\n\n\nPunctuation\n\n\nMarks indicating how a piece of written text should be read and understood.\n\n\n\n\n\n\nUtterance\n\n\nSpoken correlate of a sentence."
  },
  {
    "objectID": "nlp_lec1.html#parsing-1",
    "href": "nlp_lec1.html#parsing-1",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nDisfluency\n\n\nBreak or disruption that occurs in the flow of speech.\nTwo types:\n\nfragments\nfillers"
  },
  {
    "objectID": "nlp_lec1.html#parsing-2",
    "href": "nlp_lec1.html#parsing-2",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWord types\n\n\nAre the number of distinct words in a corpus; if the set of words in the vocabulary is \\(V\\) , the number of types is the vocabulary size \\(|V|\\).\n\n\n\n\n\n\nWord instances\n\n\nAre the total number \\(N\\) of running words.\n(sometimes also called word tokens).\n\n\n\n\n\n\nExample\n\n\n\nTo be, or not to be, that is the question."
  },
  {
    "objectID": "nlp_lec1.html#parsing-3",
    "href": "nlp_lec1.html#parsing-3",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\nTypes = \\(|V|\\)\nInstances = \\(N\\)\n\n\n\n\nShakespeare\n31 thousand\n884 thousand\n\n\nBrown corpus\n38 thousand\n1 million\n\n\nSwitchboard telephone conversations\n20 thousand\n2.4 million\n\n\nCOCA\n2 million\n440 million\n\n\nGoogle n-grams\n13 million\n1 trillion"
  },
  {
    "objectID": "nlp_lec1.html#parsing-4",
    "href": "nlp_lec1.html#parsing-4",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nHerdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)\n\n\n\\[\n|V|= kN^{\\beta}.\n\\] Here \\(k\\) and \\(\\beta\\) are positive constants, and \\(0 &lt;\\beta &lt;1\\).\n\n\n\n\n\nFor large corpora \\(0.67 &lt; \\beta &lt; 0.75\\)."
  },
  {
    "objectID": "nlp_lec1.html#parsing-5",
    "href": "nlp_lec1.html#parsing-5",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nZipf’s law\n\n\nIf \\(t_1\\) is the most common term in the collection, \\(t_2\\) is the next most common, and so on, then the collection frequency \\(cf_i\\) of the \\(i\\)-th most common term is proportional to \\(\\frac{1}{i}\\): \\[\ncf_i \\propto \\frac{1}{i},\n\\] or \\[\ncf_i = ci^k.\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#parsing-6",
    "href": "nlp_lec1.html#parsing-6",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWordforms\n\n\nExample: cat vs cats. We say these two words are different wordforms but have the same lemma.\n\n\n\n\n\n\nLemma\n\n\nA lemma is a set of lexical forms having the same stem, and usually the same major part-of-speech.\n\n\n\n\n\n\nExample\n\n\nThe wordform is the full inflected or derived form of the word. The two wordforms cat and cats thus have the same lemma, which we can represent as cat."
  },
  {
    "objectID": "nlp_lec1.html#parsing-7",
    "href": "nlp_lec1.html#parsing-7",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nMorphemes\n\n\nThe smallest meaning-bearing unit of a language.\n\n\n\n\n\n\nExamples\n\n\nIndistinguisble -&gt; [in, distinguish, able]"
  },
  {
    "objectID": "nlp_lec1.html#affixes",
    "href": "nlp_lec1.html#affixes",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nAffix taxonomy\n\n\nAffixes are classified into two types:\n\nAccording to their position in the word\nAccording to their function in a phrase or sentence.\n\n\n\n\n\n\n\nBy position\n\n\n\nPrefixes\nInfixes\nSuffixes.\nCircumfixes (Georgian, Malay)"
  },
  {
    "objectID": "nlp_lec1.html#affixes-1",
    "href": "nlp_lec1.html#affixes-1",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nBy function\n\n\n\nDerivational affixes are for creating new words usually by changing the part of speech or the meaning or both to the words when they are added to.\n\nThey can be prefixes or suffixes e.g. unkind , kingship etc.\n\nInflectional affixes mark the grammatical categories e.g. –s in girls"
  },
  {
    "objectID": "nlp_lec1.html#language-morphology",
    "href": "nlp_lec1.html#language-morphology",
    "title": "Natural Language Processing: Intro",
    "section": "Language morphology",
    "text": "Language morphology\n\nanalytical (English)\ninflected (Ukrainian)\nagglutinative (partially German)"
  },
  {
    "objectID": "nlp_lec1.html#corpora",
    "href": "nlp_lec1.html#corpora",
    "title": "Natural Language Processing: Intro",
    "section": "Corpora",
    "text": "Corpora\nVarieties depending on:\n\nlanguages\nlanguage varieties\ngenres\ntime\nspeaker demographics"
  },
  {
    "objectID": "nlp_lec1.html#parsing-8",
    "href": "nlp_lec1.html#parsing-8",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\nText normalization consists of:\n\nTokenizing (segmenting) words\nNormalizing word formats\nSegmenting sentences"
  },
  {
    "objectID": "nlp_lec1.html#tokenization",
    "href": "nlp_lec1.html#tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Tokenization",
    "text": "Tokenization\nTwo types:\n\ntop-down\nbottom-up"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization",
    "href": "nlp_lec1.html#top-down-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\nbreak off punctuation as a separate token\ninternal punctuation: Ph.D., AT&T\nprices ($45.55) and dates (18/02/2025)\nURLs (https://www.stanford.edu),\nTwitter hashtags (#nlproc)\nemail addresses (someone@cs.colorado.edu).\nnumber expressions introduce complications: e.g. 555,500.50.\nclitic contractions: I'm, l'homme."
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-1",
    "href": "nlp_lec1.html#top-down-tokenization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nPenn Treebank tokenization standard\n\n\nUsed for the parsed corpora (treebanks) released by the Lin- guistic Data Consortium (LDC).\n\nseparates out clitics (doesn’t becomes does plus n’t)\nkeeps hyphenated words together\nseparates out all punctuation"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-2",
    "href": "nlp_lec1.html#top-down-tokenization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nnltk.regexp_tokenize\n\n\n&gt;&gt;&gt; text = ’That U.S.A. poster-print costs $12.40...’\n&gt;&gt;&gt; pattern = r’’’(?x) # set flag to allow verbose regexps\n... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n... | \\w+(?:-\\w+)* # words with optional internal hyphens\n... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n... | \\.\\.\\. # ellipsis\n... | [][.,;\"’?():_‘-] # these are separate tokens; includes ], [\n... ’’’\n&gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)\n[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-3",
    "href": "nlp_lec1.html#top-down-tokenization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\nWord tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries.\n\n\n\nMorphemes In Chinese\n\n\nfor example, words are composed of characters (called hanzi in Chinese). Each character generally represents a single unit of meaning (a morpheme).\n\n\n\n\n\n\nWord segmentation\n\n\nFor Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required."
  },
  {
    "objectID": "nlp_lec1.html#bottom-up-tokenization",
    "href": "nlp_lec1.html#bottom-up-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Bottom-up tokenization",
    "text": "Bottom-up tokenization\n\n\n\nDefinition\n\n\nWe use the data to infer the tokens. We call these tokens subwords.\n\n\n\n\n\n\nParts\n\n\n\ntoken learner: produces a vocabulary of tokens\ntoken segmenter: takes a test sentence and segments it into tokens\n\n\n\n\n\n\n\nExamples\n\n\n\nbyte-pair encoding (Sennrich et al., 2016)\nunigram language modeling (Kudo, 2018)\nSentencePiece (Kudo and Richardson, 2018)"
  },
  {
    "objectID": "nlp_lec1.html#bpe",
    "href": "nlp_lec1.html#bpe",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nToken Learner\n\n\n\nStart with a vocabulary that is just the set of all individual characters.\nExamine the training corpus, choose the two symbols that are most frequently adjacent\n2.1. For example, if (‘A’, ‘B’) frequently occur together, it will add a new merged symbol ‘AB’ to the vocabulary\n2.2. And replaces every adjacent ’A’ ’B’ in the corpus with the new ‘AB’.\nContinue counting and merging, creating new longer and longer character strings, until \\(k\\) merges have been done creating k novel tokens; \\(k\\) is thus a parameter of the algorithm.\n\n4.The resulting vocabulary consists of the original set of characters plus \\(k\\) new symbols."
  },
  {
    "objectID": "nlp_lec1.html#bpe-1",
    "href": "nlp_lec1.html#bpe-1",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nNote\n\n\nThe algorithm is usually run inside words (not merging across word boundaries), so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol , and its counts."
  },
  {
    "objectID": "nlp_lec1.html#bpe-2",
    "href": "nlp_lec1.html#bpe-2",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab \\(V\\)\n\\(V \\leftarrow\\) unique characters in C # initial set of tokens is characters\nfor i = 1 to k do # merge tokens k times\n\\(\\quad\\) \\(t_L\\), \\(t_R\\) \\(\\leftarrow\\) #Most frequent pair of adjacent tokens in C\n\\(\\quad\\) \\(t_{NEW} \\leftarrow t_L + t_R\\) # make new token by concatenating\n\\(\\quad\\) \\(V \\leftarrow V + t_{NEW}\\) # update the vocabulary\n\\(\\quad\\) Replace each occurrence of \\(t_L\\), \\(t_R\\) in \\(C\\) with \\(t_{NEW}\\). # update the corpus\nreturn \\(V\\)"
  },
  {
    "objectID": "nlp_lec1.html#bpe-3",
    "href": "nlp_lec1.html#bpe-3",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nToken Segmenter:\n\nRuns on the merges we have learned from the training data on the test data.\nIt runs them greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization",
    "href": "nlp_lec1.html#word-normalization",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nSimplest method: case folding.\n\n\n\nNote\n\n\nNot very useful for text classification: compare US (the country) and us (pronoun).\n\n\n\n\n\n\nLemmatization\n\n\nThe task of determining that two words have the same root, despite their surface differences.\nbe \\(\\rightarrow\\) is, are\nPerformed using morphological parsing."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-1",
    "href": "nlp_lec1.html#word-normalization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nMorphological parsing\n\n\nSplitting each word into morphemes of two types:\n\nstems\naffixes\n\n\n\n\n\n\n\nNaive version: stemming\n\n\nThis means just dropping affixes."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-2",
    "href": "nlp_lec1.html#word-normalization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\n\nPorter stemmer\n\n\n\nclassify every character in a given token as either a consonant (“c”) or vowel (“v”)\ngroup subsequent consonants as “C” and subsequent vowels as “V.”\nrepresent every word token as a combination of consonant and vowel groups."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-1",
    "href": "nlp_lec1.html#porter-stemmer-1",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nExample\n\n\ncollection \\(\\rightarrow\\) CVCV…C\nillustrate \\(\\rightarrow\\) VCVC…V\nBoth can be presented as:\n\\[\n[C](VC)^m[V]\n\\]\nm is called the measure of the word."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-2",
    "href": "nlp_lec1.html#porter-stemmer-2",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer"
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-3",
    "href": "nlp_lec1.html#porter-stemmer-3",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nRules\n\n\nStandard form: \\[\n(\\textbf{condition})\\textbf{S}_1 \\rightarrow \\textbf{S}_2\n\\] There are five phases of rule application\n\n\n\n\n\n\nHow to read\n\n\nIf a word ends with the suffix \\(S_1\\)\nAND\nthe stem before \\(S_1\\) satisfies the given condition\nTHEN \\(S_1\\) is replaced by \\(S_2\\)."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-4",
    "href": "nlp_lec1.html#porter-stemmer-4",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nConditions\n\n\n\n\\(\\ast S\\): the stem ends with S (and similarly for the other letters)\n\\(\\ast v \\ast\\): the stem contains a vowel\n\\(\\ast d\\): the stem ends with a double consonant (e.g. -TT, -SS)\n\\(\\ast o\\): the stem ends with \\(cvc\\), where the second c is not W, X or Y (e.g. -WIL, -HOP)\n\nAnd the condition part may also contain expressions with and, or and not.\n\n\n\n\n\n\nExample\n\n\n\\((m &gt; 1) EMENT \\rightarrow\\) will perform this transformation:\nreplacement \\(\\rightarrow\\) replac"
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-3",
    "href": "nlp_lec1.html#word-normalization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nOther stemmers:\n\n\n\nLovins stemmer\n\n\nThe first published stemming algorithm, is essentially a heavily parametrized find-and-replace function.\n\ncompares each input token against a list of common English suffixes, each suffix being conditioned by one of twenty-nine rules\nif the stemmer finds a predefined suffix in a token and removing the suffix does not violate any conditions attached to that suffix (such as character length restrictions), the algorithm removes that suffix.\nthe stemmer then runs the resulting stemmed token through another set of rules that correct for common malformations, such as double letters (such as hopping becomes hopp becomes hop)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-4",
    "href": "nlp_lec1.html#word-normalization-4",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nSnowball stemmer\n\n\nAn updated version of the Porter stemmer. It differs from Porter in two main ways:\n\nWhile Lovins and Porter only stem English words, Snowball can stem text data in other Roman script languages, such as Dutch, German, French, or Spanish. Also has capabilities for non-Roman script languages.\nSnowball has an option to ignore stop words."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-5",
    "href": "nlp_lec1.html#word-normalization-5",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nLancaster stemmer (also called Paice stemmer)\n\n\nThe most aggressive English stemming algorithm.\n\nit contains a list of over 100 rules that dictate which ending strings to replace.\nthe stemmer iterates each word token against each rule. If a token’s ending characters match the string defined in a given rule, the algorithm modifies the token per that rule’s operation, then runs the transformed token through every rule again.\nthe stemmer iterates each token through each rule until that token passes all the rules without being transformed."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-6",
    "href": "nlp_lec1.html#word-normalization-6",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nStemming errors\n\n\n\nover-generalizing (lemmatizing policy to police)\nunder-generalizing (not lemmatizing European to Europe)"
  },
  {
    "objectID": "nlp_lec1.html#sentence-tokenization",
    "href": "nlp_lec1.html#sentence-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Sentence Tokenization",
    "text": "Sentence Tokenization\n\n\n\nChallenges\n\n\n\nmulti-purpose punctuation\nabbreviation dictionaries"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance",
    "href": "nlp_lec1.html#edit-distance",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nDefinition\n\n\nMinimum edit distance between two strings is defined as the minimum number of editing operations:\n\ninsertion\ndeletion\nsubstitution\n\nneeded to transform one string into another."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-1",
    "href": "nlp_lec1.html#edit-distance-1",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nString alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-2",
    "href": "nlp_lec1.html#edit-distance-2",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nLevenshtein distance\n\n\nEach of the 3 operations has cost 1.\nAlternatively, we can forbid substitutions (this is equivalent to saying that substitutions have cost 2)."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-3",
    "href": "nlp_lec1.html#edit-distance-3",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\nWagner-Fischer minimum edit distance algorithm.\n\n\n\nNotation\n\n\n\n\\(X\\): source string with length \\(n\\)\n\\(Y\\): target string with length \\(m\\)\n\\(D[i,j]\\): edit distance between \\(X[1..i]\\) and \\(Y[1..j]\\).\n\\(D[n,m]\\): edit distance between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\nCalculation\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + \\text{del_cost}(source[i]),\\\\\nD[i,j-1] + \\text{ins_cost}(target[j]),\\\\\nD[i-1,j-1] + \\text{sub_cost}(source[i], target[j]).\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-4",
    "href": "nlp_lec1.html#edit-distance-4",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nCalculation without substitution\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + 1,\\\\\nD[i,j-1] + 1,\\\\\nD[i-1,j-1] + \\begin{cases}\n2; \\quad \\text{if} \\quad source[i] \\neq target[j]), \\\\\n0; \\quad \\text{if} \\quad source[i] = target[j])\n\\end{cases}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-5",
    "href": "nlp_lec1.html#edit-distance-5",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\nfunction MIN-EDIT-DISTANCE(source, target) returns min-distance\n\\(n \\leftarrow LENGTH(source)\\)\n\\(m \\leftarrow LENGTH(target)\\)\nCreate a distance matrix \\(D[n+1,m+1]\\)\n# Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nfor each row i from 1 to n do\n\\(\\quad\\) \\(D[i,0] \\leftarrow D[i-1,0]\\) + del_cost(source[i])\nfor each column j from 1 to m do\n\\(\\quad\\) \\(D[0,j] \\leftarrow D[0, j-1] + ins-cost(target[j])\\)"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-6",
    "href": "nlp_lec1.html#edit-distance-6",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\n# Recurrence relation:\nfor each row i from 1 to n do\n\\(\\quad\\) for each column j from 1 to m do\n\\(\\quad\\) \\(\\quad\\) \\(D[i, j] \\leftarrow MIN( D[i−1, j]\\) + del_cost(source[i]),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i−1, j−1] + sub\\_cost(source[i], target[j])\\),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i, j−1] + ins\\_cost(target[j]))\\)\n# Termination\nreturn D[n,m]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-7",
    "href": "nlp_lec1.html#edit-distance-7",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance"
  },
  {
    "objectID": "nlp_lec1.html#cost-alignment",
    "href": "nlp_lec1.html#cost-alignment",
    "title": "Natural Language Processing: Intro",
    "section": "Cost alignment",
    "text": "Cost alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-8",
    "href": "nlp_lec1.html#edit-distance-8",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nHamming distance\n\n\nA number of positions at which the corresponding symbols are different.\nTherefore, identical to Levenshtein with only substitution allowed.\nCan only work for strings of similar length.\n\n\n\ndef hamming_distance(string1: str, string2: str) -&gt; int:\n    \"\"\"Return the Hamming distance between two strings.\"\"\"\n    if len(string1) != len(string2):\n        raise ValueError(\"Strings must be of equal length.\")\n    dist_counter = 0\n    for n in range(len(string1)):\n        if string1[n] != string2[n]:\n            dist_counter += 1\n    return dist_counter"
  },
  {
    "objectID": "nlp_lab3.html",
    "href": "nlp_lab3.html",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "We will learn how to:\n\nuse lemmatization (a better version of stemming) using WordNet\ndo text cleanup\nanalyze WordNet semantic hierarchies\n\n\n\n\nGet tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function.\n\n\n\n\nFirst, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\n\n\n\nWordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]\n\n\n\n\n\n\nLet’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come).\n\n\n\nNow we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lab3.html#plan",
    "href": "nlp_lab3.html#plan",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function."
  },
  {
    "objectID": "nlp_lab3.html#preparation",
    "href": "nlp_lab3.html#preparation",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "First, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]"
  },
  {
    "objectID": "nlp_lab3.html#wordnet",
    "href": "nlp_lab3.html#wordnet",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "WordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]"
  },
  {
    "objectID": "nlp_lab3.html#lemmatization-function",
    "href": "nlp_lab3.html#lemmatization-function",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "Let’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come)."
  },
  {
    "objectID": "nlp_lab3.html#processing",
    "href": "nlp_lab3.html#processing",
    "title": "NLP: Lab 3 (Lemmatization/WordNet)",
    "section": "",
    "text": "Now we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lab6.html",
    "href": "nlp_lab6.html",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term"
  },
  {
    "objectID": "nlp_lab6.html#term-document-matrix",
    "href": "nlp_lab6.html#term-document-matrix",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "term-document matrix",
    "text": "term-document matrix\nWord is row, document is column. Each document is a count vector (elements are word counts).\nEach word is also a vector of counts of occurrences in each document."
  },
  {
    "objectID": "nlp_lab6.html#term-term-matrix",
    "href": "nlp_lab6.html#term-term-matrix",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "term-term matrix",
    "text": "term-term matrix\nRows and columns are rows. Counts are numbers of co-occurrences in a context. Context can either be a whole document, or a window around the word."
  },
  {
    "objectID": "dl_lec10_extra.html#attention",
    "href": "dl_lec10_extra.html#attention",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nVisualization\n\n\ndef show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n                cmap='Reds'):\n  \"\"\"Show heatmaps of matrices.\"\"\"\n  d2l.use_svg_display()\n  num_rows, num_cols, _, _ = matrices.shape\n  fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\n                               sharex=True, sharey=True, squeeze=False)\n  for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n      for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n          pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\n          if i == num_rows - 1:\n              ax.set_xlabel(xlabel)\n          if j == 0:\n              ax.set_ylabel(ylabel)\n          if titles:\n              ax.set_title(titles[j])\n  fig.colorbar(pcm, ax=axes, shrink=0.6); \n\nattention_weights = torch.eye(10).reshape((1, 1, 10, 10))\nshow_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
  },
  {
    "objectID": "dl_lec10_extra.html#attention-1",
    "href": "dl_lec10_extra.html#attention-1",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention"
  },
  {
    "objectID": "dl_lec10_extra.html#attention-2",
    "href": "dl_lec10_extra.html#attention-2",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nExample\n\n\nRegression and classification via kernel density estimation.\n\n\n\n\n\n\nNadaraya-Watson estimators\n\n\nRely on a similarity kernel \\(\\alpha(q,k)\\) relating queries \\(q\\) to keys \\(k\\).\n\n\\(\\alpha(\\bb{q}, k) = \\exp\\left(-\\frac{1}{2} \\|q-k\\|^2\\right)\\) – Gaussian\n\\(\\alpha(\\bb{q}, k) = 1 \\text{ if } \\|q-k\\| \\leq 1\\) – Boxcar\n\\(\\alpha(\\bb{q}, k) = \\max\\left(0, 1-\\|q-k\\|\\right)\\) – Epanechikov"
  },
  {
    "objectID": "dl_lec10_extra.html#attention-3",
    "href": "dl_lec10_extra.html#attention-3",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\nAn equation for regression and classification\n\n\n\\[\nf(\\bb{q}) = \\sum_i v_i \\frac{\\alpha(\\bb{q}, k_i)}{\\sum_j \\alpha(\\bb{q}, k_j)}\n\\]"
  },
  {
    "objectID": "dl_lec10_extra.html#attention-4",
    "href": "dl_lec10_extra.html#attention-4",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention"
  },
  {
    "objectID": "dl_lec10_extra.html#attention-5",
    "href": "dl_lec10_extra.html#attention-5",
    "title": "Transformers 1",
    "section": "Attention",
    "text": "Attention\n\n\n\n\n\n\n\nNote\n\n\nThese estimators demonstrate the limits of hand-crafted attention mechanisms."
  },
  {
    "objectID": "dl_lec10_extra.html#attention-scoring-functions-1",
    "href": "dl_lec10_extra.html#attention-scoring-functions-1",
    "title": "Transformers 1",
    "section": "Attention scoring functions",
    "text": "Attention scoring functions\n\nComputing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function and the softmax operation."
  },
  {
    "objectID": "dl_lec10_extra.html#history",
    "href": "dl_lec10_extra.html#history",
    "title": "Transformers 1",
    "section": "History",
    "text": "History\n\nRNN model for seq2seq. Parses entire input sequence."
  },
  {
    "objectID": "dl_lec10_extra.html#history-1",
    "href": "dl_lec10_extra.html#history-1",
    "title": "Transformers 1",
    "section": "History",
    "text": "History\n\nWhy parse the whole sentence? Translating a sentence word by word can lead to grammatical errors."
  },
  {
    "objectID": "dl_lec10_extra.html#history-2",
    "href": "dl_lec10_extra.html#history-2",
    "title": "Transformers 1",
    "section": "History",
    "text": "History\n\n\n\nProblems\n\n\n\nRNN is trying to remember the entire input before translation\ncompression (encoding) might cause loss of information\n\n\n\n\n\n\n\nSolution\n\n\nUse attention mechanism to assign different attention weights to each input element."
  },
  {
    "objectID": "dl_lec10_extra.html#bahdanau",
    "href": "dl_lec10_extra.html#bahdanau",
    "title": "Transformers 1",
    "section": "Bahdanau",
    "text": "Bahdanau\n\n\n\nRNN with attention mechanism.\n\n\n\n\nTitle: “Neural Machine Translation by Jointly Learning to Align and Translate”, 2014."
  },
  {
    "objectID": "dl_lec10_extra.html#bahdanau-1",
    "href": "dl_lec10_extra.html#bahdanau-1",
    "title": "Transformers 1",
    "section": "Bahdanau",
    "text": "Bahdanau\n\n\n\nRNN #1\n\n\n\ngenerate hidden states from forward (\\(h_F^{(i)}\\)) and backward (\\(h_B^{(i)}\\)) passes\nconcatenate the above into \\(h^{(i)}\\).\ngenerate context vectors \\(c_i\\) from \\(h^{(i)}\\) via attention mechanism\n\n\\[\nc_i = \\sum\\limits_{j=1}^T a_{ij}h^{(j)}.\n\\]"
  },
  {
    "objectID": "dl_lec10_extra.html#bahdanau-2",
    "href": "dl_lec10_extra.html#bahdanau-2",
    "title": "Transformers 1",
    "section": "Bahdanau",
    "text": "Bahdanau\n\n\n\nRNN #2\n\n\nHidden states \\(s^{(i)}\\) depend on:\n\nprevious hidden state \\(s^{(i-1)}\\)\nprevious target word \\(y^{(i-1)}\\)\ncontext vector \\(c^{(i)}\\).\n\n\n\n\n\n\n\nAttention weights computation\n\n\n\\[\n\\alpha_{ij} = \\frac{\\exp (e_{ij})}{\\sum\\limits_{k=1}^T \\exp (e_{ik})},\n\\] where \\(e_{ij}\\) is an alignment score evaluating how well the input around position \\(j\\) matches the output around position \\(i\\)."
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-2",
    "href": "dl_lec10_extra.html#self-attention-2",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\nSuppose we have an input sequence \\(x^{(1)},\\dots,x^{(T)}\\), and output sequence \\(z^{(1)},\\dots,z^{(T)}\\). Here \\(x^{(i)}, z^{(i)} \\in \\mathbb{R}^d\\).\nFor a seq2seq task, the goal of self-attention is to model the dependencies of the current input element to all other input elements.\n\n\n\nSelf-attention stages\n\n\n\nderive importance weights based on the similarity between the current element and all other elements in the sequence\nnormalize the weights, which usually involves softmax\nuse these weights in combination with the corresponding sequence elements to compute attention value"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-3",
    "href": "dl_lec10_extra.html#self-attention-3",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\n\\[\nz^{(i)} = \\sum\\limits_{j=1}^T \\alpha_{ij} x^{(j)},\\\\\n\\omega_{ij} = x^{(i)T}x^{(j)},\\\\\n\\alpha_{ij} = \\frac{\\exp \\omega_{ij}}{\\sum\\limits_{j=1}^T \\exp \\omega_{ij}} = softmax \\left([w_{ij}]_{j=1\\dots T}\\right), \\\\\n\\sum\\limits_{j=1}^T \\alpha_{ij} = 1.\n\\]"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-4",
    "href": "dl_lec10_extra.html#self-attention-4",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-5",
    "href": "dl_lec10_extra.html#self-attention-5",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\nScaled dot-product attention: introduce learnable parameters.\nIntroduce three weight matrices: \\(U_q, U_k, U_v\\): \\[\nq^{(i)} = U_q x^{(i)},\\\\\nk^{(i)} = U_k x^{(i)},\\\\\nv^{(i)} = U_v x^{(i)},\\\\\n\\]"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-6",
    "href": "dl_lec10_extra.html#self-attention-6",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-7",
    "href": "dl_lec10_extra.html#self-attention-7",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\n\\[\n\\omega_{ij} = q^{(i)T} k^{(j)},\\\\\n\\alpha_{ij} = softmax \\left(\\frac{\\omega_{ij}}{\\sqrt{m}}\\right),\\\\\nz^{(i)} = \\sum\\limits_{j=1}^T \\alpha_{ij} v^{(j)}.\n\\]"
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-8",
    "href": "dl_lec10_extra.html#self-attention-8",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\n\nHow is \\(\\bb{q}k\\) evaluated."
  },
  {
    "objectID": "dl_lec10_extra.html#self-attention-9",
    "href": "dl_lec10_extra.html#self-attention-9",
    "title": "Transformers 1",
    "section": "Self-attention",
    "text": "Self-attention\n\nInformation flow in a scaled dot-product self-attention layer. How is \\(\\bb{q}k\\) evaluated."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-1",
    "href": "dl_lec10_extra.html#transformers-1",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\nMulti-head self attention: a modification of scaled dot-product attention.\nMultiple heads (sets of query, key, value matrices), similar to multiple kernels in CNNs.\nRead sequential input \\(\\bb{X} = \\left(x^{(1)},\\dots,x^{(T)}\\right)\\). Suppose each element is embedded by a vector of length \\(d\\). Therefore, input can be embedded into a \\(T\\times d\\) matrix. Then, create \\(h\\) sets of query, key, value matrices:\n\\[\nU_{q_1},  U_{k_1}, U_{v_1},\\\\\n\\cdots \\\\\nU_{q_h},  U_{k_h}, U_{v_h}.\n\\]\n\\(U_{q_j},U_{k_j}\\) have shape \\(d_k \\times d\\).\n\\(U_{v_j}\\) has shape \\(d_v \\times d\\).\nResulting value sequence has length \\(d_v\\)."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-2",
    "href": "dl_lec10_extra.html#transformers-2",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nPractical considerations\n\n\n\nIn practice, \\(d_k = d_v = m\\) is often chosen for simplicity.\nIn practice, rather than having a separate matrix for each attention head, transformer implementations use a single matrix for all attention heads. The attention heads are then organized into logically separate regions in this matrix, which can be accessed via Boolean masks. This makes it possible to implement multi-head attention more efficiently because multiple matrix multiplications can be implemented as a single matrix multiplication instead. However, for simplicity, we are omitting this implementation detail in this section.\ncomputation can all be done in parallel because there are no dependencies between the multiple heads."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-3",
    "href": "dl_lec10_extra.html#transformers-3",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nVectorized computations\n\n\n\\[\nq_j^{(i)} = U_{q_j}x^{(i)}.\n\\] Then we concatenate vectors."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-4",
    "href": "dl_lec10_extra.html#transformers-4",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nConcatenating the scaled dot-product attention vectors into one vector and passing it through a linear projection."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-5",
    "href": "dl_lec10_extra.html#transformers-5",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nDecoder layer\n\n\nSimilar to the encoder, the decoder also contains several repeated layers. Besides the two sublayers that we have already introduced in the previous encoder section (the multi-head self-attention layer and fully connected layer), each repeated layer also contains a masked multi-head attention sublayer.\nMasked attention is a variation of the original attention mechanism, where masked attention only passes a limited input sequence into the model by “masking” out a certain number of words. For example, if we are building a language translation model with a labeled dataset, at sequence position \\(i\\) during the training procedure, we only feed in the correct output words from positions \\(1,\\dots,i-1\\). All other words (for instance, those that come after the current position) are hidden from the model to prevent the model from “cheating.” This is also consistent with the nature of text generation: although the true translated words are known during training, we know nothing about the ground truth in practice. Thus, we can only feed the model the solutions to what it has already generated, at position \\(i\\)."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-6",
    "href": "dl_lec10_extra.html#transformers-6",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nConcatenating the scaled dot-product attention vectors into one vector and passing it through a linear projection."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-7",
    "href": "dl_lec10_extra.html#transformers-7",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nthe previous output words (output embeddings) are passed into the masked multi-head attention layer.\nthe second layer receives both the encoded inputs from the encoder block and the output of the masked multi-head attention layer into a multi-head attention layer.\nFinally, we pass the multi-head attention outputs into a fully connected layer that generates the overall model output: a probability vector corresponding to the output words."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-8",
    "href": "dl_lec10_extra.html#transformers-8",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\nLayer normalization mechanism, which was first introduced by J. Ba, J.R. Kiros, and G.E. Hinton in 2016 in the same-named paper Layer Normalization (URL: https://arxiv.org/ abs/1607.06450)."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-9",
    "href": "dl_lec10_extra.html#transformers-9",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nRecursion vs recurrence\n\n\nRNNs are recurrent.\nTransformers are recursive – therefore no need to unroll.\n\n\n\n\n\n\nAttention benefits\n\n\n\nMuch simpler than CNNs or RNNs.\nNo vanishing/exploding gradients problems.\nAttention can be thought of as a single convolution kernel spanning the whole sequence of tokens."
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-10",
    "href": "dl_lec10_extra.html#transformers-10",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "dl_lec10_extra.html#transformers-11",
    "href": "dl_lec10_extra.html#transformers-11",
    "title": "Transformers 1",
    "section": "Transformers",
    "text": "Transformers\n\n\n\nExample of learned attention weights (Vaswani 2017 paper)"
  },
  {
    "objectID": "nlp_lab5.html",
    "href": "nlp_lab5.html",
    "title": "NLP: Lab 5 (n-grams)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term"
  },
  {
    "objectID": "nlp_lab5.html#perplexity",
    "href": "nlp_lab5.html#perplexity",
    "title": "NLP: Lab 5 (n-grams)",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity of a test set \\(W\\) with an N-gram model is given by (here V=|W|): \\[\nperplexity(W) = \\sqrt[V]{\\frac{1}{P(w_1 w_2 ... w_{V})}} = \\sqrt[V]{\\prod_{i=1}^{V} \\frac{1}{P(w_i | w_{i-N+1:i-1})}}\n\\]"
  },
  {
    "objectID": "nlp_lab5.html#laplace-smoothing",
    "href": "nlp_lab5.html#laplace-smoothing",
    "title": "NLP: Lab 5 (n-grams)",
    "section": "Laplace smoothing",
    "text": "Laplace smoothing\nWe have to deal with zero-probability N-grams somehow. One way is to add \\(1\\) to all counts.\nFormula for bigrams: \\[\nP_{Laplace}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}.\n\\]"
  },
  {
    "objectID": "nlp_lab5.html#conditional-interpolation",
    "href": "nlp_lab5.html#conditional-interpolation",
    "title": "NLP: Lab 5 (n-grams)",
    "section": "Conditional interpolation",
    "text": "Conditional interpolation\nTrigram example: \\[\\begin{align*}\n&P(w_n | w_{n-2} w_{n-1}) = \\lambda_1(w_{n-2:n-1})P(w_n) +\\\\\n&+ \\lambda_2(w_{n-2:n-1}) P(w_n | w_{n-1}) + \\\\\n&+ \\lambda_3(w_{n-2:n-1})  P(w_n | w_{n-2} w_{n-1})\n\\end{align*}\\] All \\(\\lambda_i\\) should add up to \\(1\\)."
  },
  {
    "objectID": "nlp_lab5.html#stupid-backoff",
    "href": "nlp_lab5.html#stupid-backoff",
    "title": "NLP: Lab 5 (n-grams)",
    "section": "Stupid backoff",
    "text": "Stupid backoff\nFormula: \\[\nS(w_i | w_{i-N+1:i-1}) = \\begin{cases}\n\\dfrac{C(w_{i-N+1:i})}{C(w_{i-N+1:i-1})}, & \\text{ if } C(w_{i-N+1:i}) &gt; 0 \\\\\n\\lambda S(w_i | w_{i-N+2:i-1}) & \\text{ otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nValue of \\(\\lambda=0.4\\) seems to be a good default"
  },
  {
    "objectID": "nlp_lec4.html#plan",
    "href": "nlp_lec4.html#plan",
    "title": "Principal Component Analysis",
    "section": "Plan",
    "text": "Plan\n\n\n\n\n\n\nNotions to discuss\n\n\n\nPCA\nSVD\nLSA\nSNE\nNSA\nFBI"
  },
  {
    "objectID": "nlp_lec4.html#dimensionality",
    "href": "nlp_lec4.html#dimensionality",
    "title": "Principal Component Analysis",
    "section": "Dimensionality",
    "text": "Dimensionality\n\n\n\nCurse of Dimensionality\n\n\n\nWhen the dimensionality increases, the volume of the space increases so fast that the available data become sparse.\nIn order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality."
  },
  {
    "objectID": "nlp_lec4.html#pca",
    "href": "nlp_lec4.html#pca",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nMotivation\n\n\nPrincipal component analysis (PCA):\n\na standard tool in modern data analysis (in diverse fields from neuroscience to computer graphics)\na simple, non-parametric method for extracting relevant information from confusing data sets\nwith minimal effort PCA provides a roadmap for how to reduce a complex data set to a lower dimension to reveal the sometimes hidden, simplified structures that often underlie it."
  },
  {
    "objectID": "nlp_lec4.html#pca-1",
    "href": "nlp_lec4.html#pca-1",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nUses\n\n\n\ndata visualization\nfeature selection\nnoise reduction\nmachine learning\ndata mining"
  },
  {
    "objectID": "nlp_lec4.html#pca-example",
    "href": "nlp_lec4.html#pca-example",
    "title": "Principal Component Analysis",
    "section": "PCA: example",
    "text": "PCA: example\n\n\n\nPretend we are studying the motion of the physicist’s ideal spring."
  },
  {
    "objectID": "nlp_lec4.html#pca-2",
    "href": "nlp_lec4.html#pca-2",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nExperiment setup\n\n\n\nWe choose three camera positions \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\) at some arbitrary angles with respect to the system.\nThe angles between our measurements might not even be 90 degrees!\nNow, we record with the cameras for several minutes.\n\n\n\n\n\n\n\nQuestion\n\n\nThe big question remains: how do we get from this data set to a simple equation of \\(x\\)?"
  },
  {
    "objectID": "nlp_lec4.html#pca-3",
    "href": "nlp_lec4.html#pca-3",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nIssues\n\n\n\nWhich measurements to perform? (what is important?)\nWhat is noise?\nHow many dimensions to measure? (what is redundant?)\n\n\n\n\n\n\n\nGoal\n\n\nIdentify a most meaningful basis to re-express a data set.\nIn case of spring example, goal of PCA is to determine that \\(x\\) axis is the one that matters."
  },
  {
    "objectID": "nlp_lec4.html#pca-4",
    "href": "nlp_lec4.html#pca-4",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nMeasurement definition\n\n\n\\[\n\\vec{X} = \\begin{bmatrix}\n  x_A \\\\\n  y_A \\\\\n  x_B \\\\\n  y_B \\\\\n  x_C \\\\\n  y_C\n\\end{bmatrix}\n\\]\n\n\n\n\n\nIf we record the ball’s position for 10 minutes at 120 Hz, then we have recorded 10 × 60 × 120 = 72000 of these vectors."
  },
  {
    "objectID": "nlp_lec4.html#pca-5",
    "href": "nlp_lec4.html#pca-5",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nNaive basis\n\n\nNaive basis: reflects the methods we used to measure the data. \\[\n\\boldsymbol{B} = \\begin{bmatrix}\n  \\boldsymbol{b_1} \\\\\n  \\vdots \\\\\n  \\boldsymbol{b_m}\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & 1\n\\end{bmatrix} = \\boldsymbol{I}\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-6",
    "href": "nlp_lec4.html#pca-6",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nQuestion\n\n\nIs there another linear basis that best re-expresses our data set?\n\n\n\n\n\n\n\n\n\nImportant\n\n\nNote the linearity assumption!"
  },
  {
    "objectID": "nlp_lec4.html#pca-7",
    "href": "nlp_lec4.html#pca-7",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nBasis change: definition\n\n\nLet \\(\\boldsymbol{X}\\) be the original data set (\\(m\\times n\\) matrix with \\(m=6\\) and \\(n=72000\\)).\nLet \\(\\boldsymbol{Y}\\) be another \\(m\\times n\\) matrix such that: \\[\\begin{align}\n\\label{basis}\n&\\boldsymbol{P}\\boldsymbol{X} = \\boldsymbol{Y}\n\\end{align}\\]\n\\(p_i\\): rows of \\(P\\), \\(x_i\\): columns of \\(X\\), \\(y_i\\): columns of \\(Y\\)\n\n\n\n\n\n\nInterpretation\n\n\n\n\\(\\boldsymbol{P}\\) transforms \\(\\boldsymbol{X}\\) into \\(\\boldsymbol{Y}\\)\n\\(\\boldsymbol{P}\\) is a rotation and stretch geometrically\nRows of \\(\\boldsymbol{P}\\) are a new set of basis vectors"
  },
  {
    "objectID": "nlp_lec4.html#pca-8",
    "href": "nlp_lec4.html#pca-8",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nBasis change\n\n\n\\[\\begin{align*}\n   &\\boldsymbol{P}\\boldsymbol{X} = \\begin{bmatrix}\n     \\boldsymbol{p_1} \\\\\n     \\vdots \\\\\n     \\boldsymbol{p_m}\n\\end{bmatrix} \\begin{bmatrix}\n\\boldsymbol{x_1} & \\dots & \\boldsymbol{x_n}\n\\end{bmatrix} = \\\\\n   & = \\begin{bmatrix}\n     \\boldsymbol{p_1} \\cdot \\boldsymbol{x_1} & \\dots & \\boldsymbol{p_1}\\cdot\\boldsymbol{x_n} \\\\\n         \\vdots & \\ddots & \\vdots\\\\\n         \\boldsymbol{p_m}\\cdot\\boldsymbol{x_1} & \\dots & \\boldsymbol{p_m}\\cdot\\boldsymbol{x_n}\n   \\end{bmatrix}\n\\end{align*}\\] \\(j\\)th coefficient of \\(\\boldsymbol{y_i}\\) is a projection on the \\(j\\)th row of \\(\\boldsymbol{P}\\), therefore, \\(\\boldsymbol{p_i}\\) are a new set of basis vectors for columns of \\(\\boldsymbol{X}\\).\n\\(\\boldsymbol{p_i}\\) will become principal components of \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-9",
    "href": "nlp_lec4.html#pca-9",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nQuestions on \\(\\boldsymbol{P}\\)\n\n\n\nwhat is the best way to re-express \\(\\boldsymbol{X}\\)?\nwhat is a good choice of \\(\\boldsymbol{P}\\)?\n\n\n\n\n\n\n\nHow to express best?\n\n\nUse signal-to-noise ratio. \\[\nSNR = \\dfrac{\\sigma^2_{signal}}{\\sigma^2_{noise}}\n\\] High SNR - precision measurement, low - noisy data."
  },
  {
    "objectID": "nlp_lec4.html#pca-10",
    "href": "nlp_lec4.html#pca-10",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nWhat does maximizing the variance mean: finding the appropriate rotation."
  },
  {
    "objectID": "nlp_lec4.html#pca-11",
    "href": "nlp_lec4.html#pca-11",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\\(r_1\\) and \\(r_2\\) are distinct measurements.\nHigh redundancy allows to drop measurements.\nThis is the central idea behind dimensional reduction."
  },
  {
    "objectID": "nlp_lec4.html#pca-12",
    "href": "nlp_lec4.html#pca-12",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nHow do we generalize to higher dimensions?\n\n\nConsider two sets of measurements with zero means \\[\\begin{align*}\n   &A=\\left\\{a_1, a_2, \\dots, a_n\\right\\},  B=\\left\\{b_1, b_2, \\dots, b_n\\right\\}.  \n\\end{align*}\\]\nVariances are \\[\\begin{align*}\n&\\sigma_A^2 = \\dfrac{1}{n}\\sum\\limits_i a_i^2, \\, \\sigma_B^2 = \\dfrac{1}{n}\\sum\\limits_i b_i^2,\\\\\n\\end{align*}\\]\nCovariance of \\(A\\) and \\(B\\) is \\[\n\\sigma_{AB}^2 = \\dfrac{1}{n}\\sum\\limits_i a_i b_i\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-13",
    "href": "nlp_lec4.html#pca-13",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\nAbsolute value of covariance measures the degree of redundancy.\n\n\\(\\sigma_{AB} = 0 \\Leftrightarrow A \\text{ and } B\\) are uncorrelated\n\\(\\sigma_{AB}^2 =  \\sigma_A^2 \\text{ if } A=B\\)"
  },
  {
    "objectID": "nlp_lec4.html#pca-14",
    "href": "nlp_lec4.html#pca-14",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nMatrix form\n\n\n\\[\\begin{align*}\n   &\\boldsymbol{a} = \\left[a_1 a_2 \\dots a_n\\right] \\\\\n   &\\boldsymbol{b} = \\left[b_1 b_2 \\dots b_n\\right] \\\\\n   &\\sigma_{\\boldsymbol{a}\\boldsymbol{b}}^2 \\equiv \\dfrac{1}{n} \\boldsymbol{a}\\boldsymbol{b}^T\n\\end{align*}\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-15",
    "href": "nlp_lec4.html#pca-15",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nGeneralization\n\n\nLet’s generalize to a multiple number of vectors.\n\nRename \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) to \\(\\boldsymbol{x_1}\\) and \\(\\boldsymbol{x_2}\\)\nIntroduce additional measurement types \\(\\boldsymbol{x_i},\\,i=\\overline{3,m}\\).\nDefine a new matrix: \\[\n\\boldsymbol{X} = \\begin{bmatrix}\n\\boldsymbol{x_1}\\\\\n\\vdots\\\\\n\\boldsymbol{x_m}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-16",
    "href": "nlp_lec4.html#pca-16",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nCovariance matrix\n\n\n\\[\n\\boldsymbol{C_X} \\equiv \\dfrac{1}{n} \\boldsymbol{X} \\boldsymbol{X}^T\n\\]\n\n\n\n\n\n\nCovariance matrix properties\n\n\n\n\\(\\boldsymbol{C_X}\\) is a square symmetric \\(m\\times m\\) matrix\ndiagonal terms of \\(\\boldsymbol{C_X}\\) are the variance of particular measurement types\noff-diagonal terms of \\(\\boldsymbol{C_X}\\) are the covariance between particular measurement types"
  },
  {
    "objectID": "nlp_lec4.html#pca-17",
    "href": "nlp_lec4.html#pca-17",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nProof of symmetricity\n\n\nTheorem 1. Inverse of orthogonal matrix is its transpose.\nLet \\(A\\) be an \\(m \\times n\\) orthogonal matrix where \\(a_i\\) is the \\(i\\)th column vector. We have \\[\n(A^T A)_{ij} = a_i^T a_j = \\begin{cases} 1, \\; \\text{ if } i=j,\\\\ 0 \\; \\text{ otherwise} \\end{cases}\n\\] Therefore, \\(A^T A = I \\Rightarrow A^{-1} = A^T\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-18",
    "href": "nlp_lec4.html#pca-18",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nProof of symmetricity\n\n\nTheorem 2. For any matrix \\(A\\), \\(A^T A\\) and \\(A A^T\\) are symmetric. \\[\\begin{align*}\n  & (A A^T)^T = A^{TT} A^T = A A^T,\\\\\n  & (A^T A)^T = A^T A^{TT} = A^T A.\n\\end{align*}\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-19",
    "href": "nlp_lec4.html#pca-19",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nCovariance\n\n\n\\(\\boldsymbol{C_X}\\) captures the covariance between all possible pairs of measurements. The covariance values reflect the noise and redundancy in our measurements.\n\nIn the diagonal terms, by assumption, large values correspond to interesting structure.\nIn the off-diagonal terms large magnitudes correspond to high redundancy."
  },
  {
    "objectID": "nlp_lec4.html#pca-20",
    "href": "nlp_lec4.html#pca-20",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nNew goals\n\n\n\nminimize redundancy, measured by the magnitude of the covariance\nmaximize the signal, measured by the variance.\n\nLet’s transform \\(\\boldsymbol{C_X}\\) into some optimized matrix \\(\\boldsymbol{C_Y}\\).\n\n\n\n\n\n\nOptimized matrix \\(\\boldsymbol{C_Y}\\)\n\n\n\nall off-diagonal terms should be zero (this means that \\(\\boldsymbol{Y}\\) is decorrelated);\neach successive dimension in \\(\\boldsymbol{Y}\\) should be rank-ordered according to variance."
  },
  {
    "objectID": "nlp_lec4.html#pca-21",
    "href": "nlp_lec4.html#pca-21",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nDiagonalizing\n\n\nWhat are the methods for diagonalizing \\(\\boldsymbol{C_Y}\\)?\nWe assume that all basis vectors \\(\\left\\{\\boldsymbol{p_1}, \\dots, \\boldsymbol{p_m}\\right\\}\\) are orthonormal, that is, \\(\\boldsymbol{P}\\) is an orthonormal matrix.\n\n\n\n\n\n\nHow does PCA work?\n\n\nLooking at figure, it aligns the basis with the axis of maximal variance."
  },
  {
    "objectID": "nlp_lec4.html#pca-22",
    "href": "nlp_lec4.html#pca-22",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nAlgorithm\n\n\n\nSelect a normalized direction in \\(m\\)-dimensional space along which the variance in \\(X\\) is maximized. Save this vector as \\(\\boldsymbol{p_1}\\).\nFind another direction along which variance is maximized, however, because of the orthonormality condition, restrict the search to all directions orthogonal to all previous selected directions. Save this vector as \\(\\boldsymbol{p_i}\\).\nRepeat this procedure until \\(m\\) vectors are selected.\n\nResulting ordered set of \\(\\boldsymbol{p}\\) are called principal components."
  },
  {
    "objectID": "nlp_lec4.html#pca-23",
    "href": "nlp_lec4.html#pca-23",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nAssumptions review\n\n\n\nLinearity. We assume the problem can be solved by change of basis.\nLarge variances have important structure. This is sometimes incorrect.\nPrincipal components are orthogonal. This makes PCA soluble with linear algebra techniques."
  },
  {
    "objectID": "nlp_lec4.html#pca-first-algebraic-solution.",
    "href": "nlp_lec4.html#pca-first-algebraic-solution.",
    "title": "Principal Component Analysis",
    "section": "PCA: First algebraic solution.",
    "text": "PCA: First algebraic solution.\n\n\n\nSetup\n\n\nWe have a dataset \\(\\boldsymbol{X}\\) which is a \\(m \\times n\\) matrix:\n\n\\(m\\) being number of dimensions (measurement types)\n\\(n\\) - number of samples.\n\n\n\n\n\n\n\nGoal\n\n\nFind some orthonormal matrix \\(\\boldsymbol{P}\\) in \\(\\boldsymbol{Y}=\\boldsymbol{P}\\boldsymbol{X}\\) such that \\(\\boldsymbol{C_Y} \\equiv \\dfrac{1}{n} \\boldsymbol{Y}\\boldsymbol{Y}^T\\) is a diagonal matrix.\nThe rows of \\(\\boldsymbol{P}\\) are the principal components of \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-24",
    "href": "nlp_lec4.html#pca-24",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\n\\(\\boldsymbol{C_Y}\\) rewrite\n\n\nLet’s rewrite \\(\\boldsymbol{C_Y}\\) in terms of unknown variable \\(\\boldsymbol{P}\\): \\[\\begin{align*}\n   &\\boldsymbol{C_Y} = \\dfrac{1}{n}\\boldsymbol{Y}\\boldsymbol{Y}^T =  \\dfrac{1}{n}(\\boldsymbol{P}\\boldsymbol{X})(\\boldsymbol{P}\\boldsymbol{X})^T  = \\\\\n   & = \\dfrac{1}{n} \\boldsymbol{P} \\boldsymbol{X} \\boldsymbol{X}^T \\boldsymbol{P}^T = \\boldsymbol{P}(\\dfrac{1}{n} \\boldsymbol{X} \\boldsymbol{X}^T) \\boldsymbol{P}^T = \\\\\n   &= \\boldsymbol{P} \\boldsymbol{C_X} \\boldsymbol{P}^T,\n\\end{align*}\\] where \\(\\boldsymbol{C_X}\\) is the covariance matrix of \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-25",
    "href": "nlp_lec4.html#pca-25",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nGoal\n\n\nAny symmetrix matrix \\(A\\) is diagonalized by an orthogonal matrix of its eigenvectors.\n\n\n\n\n\n\nTheorem\n\n\nTheorem 3. A matrix is symmetric \\(\\Leftrightarrow\\) it is orthogonally diagonalizable.\n\\((\\Rightarrow)\\) If \\(A\\) is orthogonally diagonalizable, then \\(A\\) is symmetric.\nOrthogonally diagonalizable means that \\(\\exists E: A = E D E^T\\), where \\(D\\) is a diagonal matrix and \\(E\\) is a matrix that diagonalizes \\(A\\). Let’s compute \\(A^T\\): \\[\nA^T = (E D E^T)^T = E^{TT}D^T E^T = E D E^T = A\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-26",
    "href": "nlp_lec4.html#pca-26",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nTheorem\n\n\nTheorem 4. A symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors.\nLet \\(A\\) be a square $n n $ symmetric matrix with eigenvectors \\(\\left\\{e_1, \\dots, e_n\\right\\}\\). Let \\(E=\\left[e_1 \\dots e_n\\right]\\). This theorem asserts that \\(\\exists \\text{ diagonal matrix } D: A = E D E^T\\).\nFirst, let’s prove that any matrix can be orthogonally diagonalized if and only if it that matrix’s eigenvectors are all linearly independent.\nLet \\(A\\) be some matrix with independent eigenvectors (not degenerate). Let \\(D\\) be a diagonal matrix where \\(i\\)th eigenvalue is placed in \\(ii\\)th position. We will show that \\(AE=ED\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-27",
    "href": "nlp_lec4.html#pca-27",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nTheorem\n\n\n\\[\\begin{align*}\n   & AE = \\left[Ae_1 \\dots Ae_n\\right],\\\\\n   & ED = \\left[\\lambda_1 e_1 \\dots \\lambda_n e_n\\right].\n\\end{align*}\\] Evidently, if \\(AE=ED\\) then \\(Ae_i = \\lambda_i e_i \\; \\forall i\\). This is the definition of the eigenvalue equation. Therefore, \\(A = E D E^{-1}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-28",
    "href": "nlp_lec4.html#pca-28",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nTheorem\n\n\nNow let’s prove that a symmetric matrix always has orthogonal eigenvectors. Suppose that \\(\\lambda_1\\) and \\(\\lambda_1\\) are distinct eigenvalues for eigenvectors \\(e_1\\) and \\(e_2\\). \\[\\begin{align*}\n  &\\lambda_1 e_1 \\cdot e_2 = (\\lambda_1 e_1)^T e_2 = (A e_1)^T e_2 =\\\\\n  & = e_1^T A^T e_2 = e_1^T A e_2 = e_1^T(\\lambda_2 e_2) = \\lambda_2 e_1 \\cdot e_2.\n\\end{align*}\\] As \\(\\lambda_1 \\neq \\lambda_2\\), then \\(e_1 \\cdot e_2 = 0\\).\nSo, \\(E\\) is an orthogonal matrix, and by theorem 1 \\(E^T = E^{-1}\\) and \\(A = E D E^T\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-29",
    "href": "nlp_lec4.html#pca-29",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nTheorem\n\n\nFor symmetric matrix \\(\\boldsymbol{A}\\) we have \\(\\boldsymbol{A} = \\boldsymbol{E} \\boldsymbol{D} \\boldsymbol{E}^T\\), where \\(D\\) is a diagonal matrix and \\(E\\) is a matrix of eigenvectors of \\(A\\) arranged as columns.\nNote that \\(\\boldsymbol{A}\\) might have \\(r \\leq m\\) orthonormal eigenvectors where \\(r\\) is the rank. This will mean that \\(\\boldsymbol{A}\\) is . Therefore, we’ll need to select additional \\((m-r)\\) additional orthogonal vectors to fill matrix \\(\\boldsymbol{E}\\).\nThese vectors do not affect the final solution because variances associated with these directions are \\(0\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-30",
    "href": "nlp_lec4.html#pca-30",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\n\nImportant\n\n\nWe select the matrix \\(\\boldsymbol{P}\\) to be a matrix where each row \\(\\boldsymbol{p_i}\\) is an eigenvector of \\(\\dfrac{1}{n}\\boldsymbol{X} \\boldsymbol{X}^T\\). By this selection, \\(\\boldsymbol{P} \\equiv \\boldsymbol{E}^T\\). Keeping in mind that \\(\\boldsymbol{P}^{-1} = \\boldsymbol{P}^T\\) (Theorem 1), we have: \\[\\begin{align*}\n   & \\boldsymbol{C_Y} = \\boldsymbol{P} \\boldsymbol{C_X} \\boldsymbol{P}^T \\\\\n   &= \\boldsymbol{P}(\\boldsymbol{E}^T \\boldsymbol{D} \\boldsymbol{E})\\boldsymbol{P}^T  \\\\\n   &= \\boldsymbol{P}(\\boldsymbol{P}^T \\boldsymbol{D} \\boldsymbol{P})\\boldsymbol{P}^T = \\\\\n   & = (\\boldsymbol{P} \\boldsymbol{P}^T) \\boldsymbol{D} (\\boldsymbol{P} \\boldsymbol{P}^T)  \\\\\n   & = (\\boldsymbol{P} \\boldsymbol{P}^{-1})\\boldsymbol{D}(\\boldsymbol{P} \\boldsymbol{P}^{-1}).\n\\end{align*}\\] Therefore, \\(\\boldsymbol{C_Y} = \\boldsymbol{D}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-31",
    "href": "nlp_lec4.html#pca-31",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nResult\n\n\n\nchoice of \\(\\boldsymbol{P}\\) diagonalizes \\(\\boldsymbol{C_Y}\\)\nprincipal components of \\(\\boldsymbol{X}\\) are the eigenvectors of \\(\\boldsymbol{C_X} = \\dfrac{1}{n}\\boldsymbol{X} \\boldsymbol{X}^T\\)\nthe \\(i\\)th diagonal value of \\(\\boldsymbol{C_Y}\\) is the variance of \\(\\boldsymbol{X}\\) along \\(\\boldsymbol{p_i}\\)\n\n\n\n\n\n\n\nPractical computation\n\n\n\nsubtract the mean off each measurement type\ncompute eigenvectors of \\(\\boldsymbol{C_X}\\)"
  },
  {
    "objectID": "nlp_lec4.html#pca-another-algebraic-solution",
    "href": "nlp_lec4.html#pca-another-algebraic-solution",
    "title": "Principal Component Analysis",
    "section": "PCA: Another algebraic solution",
    "text": "PCA: Another algebraic solution\n\n\n\nSetup\n\n\nLet \\(\\boldsymbol{X}\\) be an arbitrary \\(n \\times m\\) matrix (!) and \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) be a rank \\(r\\), square, symmetric \\(m \\times m\\) matrix.\n\n\\(\\left\\{\\hat{\\boldsymbol{v}_1}, \\dots, \\hat{\\boldsymbol{v}_r}\\right\\}\\) is the set of orthonormal \\(m \\times 1\\) eigenvectors with associated eigenvalues \\(\\left\\{\\lambda_1, \\dots, \\lambda_r \\right\\}\\) for the symmetric matrix \\(\\boldsymbol{X}^T \\boldsymbol{X}\\): \\[\n(\\boldsymbol{X}^T \\boldsymbol{X})\\hat{\\boldsymbol{v}_i} = \\lambda_i \\hat{\\boldsymbol{v}_i}\n\\]\n\\(\\sigma_i \\equiv \\sqrt{\\lambda_i}\\) are positive real and termed the singular values\n\\(\\left\\{\\hat{\\boldsymbol{u}_1}, \\dots, \\hat{\\boldsymbol{u}_r}\\right\\}\\) is the set of \\(n \\times 1\\) vectors defined by \\(\\hat{\\boldsymbol{u}_i} \\equiv \\dfrac{1}{\\sigma_i}\\boldsymbol{X} \\hat{\\boldsymbol{v}_i}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-32",
    "href": "nlp_lec4.html#pca-32",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nProperties\n\n\n\n\\(\\hat{\\boldsymbol{u}_i} \\cdot \\hat{\\boldsymbol{u}_j} = \\begin{cases} 1, \\; \\text{ if } i=j;\\\\ 0, \\; \\text{otherwise}\\end{cases}\\)\n\\(\\|\\boldsymbol{X} \\hat{\\boldsymbol{v}_i}\\| = \\sigma_i\\)"
  },
  {
    "objectID": "nlp_lec4.html#pca-33",
    "href": "nlp_lec4.html#pca-33",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nTheorem\n\n\nTheorem 5. For any arbitrary \\(m \\times n\\) matrix \\(X\\), the symmetric matrix \\(X^T X\\) has a set of orthonormal eigenvectors of \\(\\left\\{\\hat{\\boldsymbol{v}_1}, \\dots, \\hat{\\boldsymbol{v}_n}\\right\\}\\) and a set of associated eigenvalues \\(\\left\\{\\lambda_1, \\dots, \\lambda_n\\right\\}\\). The set of vectors \\(\\left\\{\\boldsymbol{X}\\hat{\\boldsymbol{v_1}}, \\dots, \\boldsymbol{X}\\hat{\\boldsymbol{v_n}}\\right\\}\\) then forms an orthogonal basis, where each vector \\(X\\hat{\\boldsymbol{v_i}}\\) is of length \\(\\sqrt{\\lambda_i}\\).\n\\[\\begin{align*}\n    &(X \\hat{v_i}) \\cdot (X \\hat{v_j}) = (X \\hat{v_i})^T \\cdot (X \\hat{v_j}) = \\\\\n    & = \\hat{v_i} X^T X \\hat{v_j} = \\hat{v_i}^T(\\lambda_j v_j) = \\lambda_j \\hat{v_i} \\cdot \\hat{v_j} =\\\\\n    & = \\lambda_j \\delta_{ij}.\n\\end{align*}\\] And so we have \\[\n\\|X \\hat{v_i}\\|^2 = (X \\hat{v_i})\\cdot(X \\hat{v_i}) = \\lambda_i.\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-34",
    "href": "nlp_lec4.html#pca-34",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nThe scalar version of SVD\n\n\nIs a restatement of the third definition: \\[\\begin{align}\n\\label{scalar_svd}\n&\\boldsymbol{X} \\hat{\\boldsymbol{v}_i} = \\sigma_i \\hat{\\boldsymbol{u}_i}\n\\end{align}\\]\nThe set of eigenvectors \\(\\left\\{\\hat{\\boldsymbol{v}_1}, \\dots, \\hat{\\boldsymbol{v}_r}\\right\\}\\) and the set of vectors \\(\\left\\{\\hat{\\boldsymbol{u}_1}, \\dots, \\hat{\\boldsymbol{u}_r}\\right\\}\\) are both bases in \\(r\\)-dimensional space."
  },
  {
    "objectID": "nlp_lec4.html#pca-35",
    "href": "nlp_lec4.html#pca-35",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nThe matrix version of SVD.\n\n\nConstruct a new diagonal matrix \\(\\Sigma\\): \\[\n\\Sigma \\equiv \\begin{bmatrix}\n  \\sigma_{\\tilde{1}} \\\\\n  & \\ddots & & \\text{0} \\\\\n  & & \\sigma_{\\tilde{r}} \\\\\n  \\text{0}  & & & \\ddots \\\\\n                   & & & &  0\n\\end{bmatrix}\n\\] where \\(\\sigma_{\\tilde{1}} \\geq \\dots \\geq \\sigma_{\\tilde{r}}\\) are the rank-ordered set of singular values."
  },
  {
    "objectID": "nlp_lec4.html#pca-36",
    "href": "nlp_lec4.html#pca-36",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nThe matrix version of SVD.\n\n\nLikewise we construct accompanying orthogonal matrices \\[\\begin{align*}\n   & \\boldsymbol{V} = \\left[\\hat{\\boldsymbol{v_1}}, \\dots, \\hat{\\boldsymbol{v_m}}\\right],\\; \\boldsymbol{U} = \\left[\\hat{\\boldsymbol{u_1}}, \\dots, \\hat{\\boldsymbol{u_n}}\\right],\n\\end{align*}\\] where we appended additional \\((m-r)\\) and \\((n-r)\\) orthonormal vectors to fill up the matrices for \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{U}\\) respectively, in order to deal with degeneracy issues.\n\\[\n\\boldsymbol{X}\\boldsymbol{V} = \\boldsymbol{U} \\boldsymbol{\\Sigma},\n\\] where each column of \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{U}\\) perform the scalar version of decomposition \\(\\eqref{scalar_svd}\\).\nAs \\(\\boldsymbol{V}\\) is orthogonal, we multiply both sides by \\(\\boldsymbol{V}^{-1}=\\boldsymbol{V}^T\\) and obtain \\[\\begin{align}\n\\label{svd}\n&\\boldsymbol{X} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^T.\n\\end{align}\\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-37",
    "href": "nlp_lec4.html#pca-37",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nInterpretation\n\n\nEquation \\(\\eqref{svd}\\) states that any arbitrary matrix \\(\\boldsymbol{X}\\) can be converted into an orthogonal matrix, diagonal matrix, and another orthogonal matrix (or a rotation, a stretch, and a second rotation).\nReinterpret equation \\(\\eqref{scalar_svd}\\) as \\(\\boldsymbol{X}\\boldsymbol{a} = k\\boldsymbol{b}\\), where \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) are column vectors and \\(k\\) is a scalar constant.\nThe set \\(\\left\\{\\hat{\\boldsymbol{v_1}}, \\dots, \\hat{\\boldsymbol{v_m}}\\right\\}\\) is analogous to \\(\\boldsymbol{a}\\) and \\(\\left\\{\\hat{\\boldsymbol{u_1}}, \\dots, \\hat{\\boldsymbol{u_n}}\\right\\}\\) is analogous to \\(\\boldsymbol{b}\\).\n\\(\\left\\{\\hat{\\boldsymbol{v_1}}, \\dots, \\hat{\\boldsymbol{v_m}}\\right\\}\\) and \\(\\left\\{\\hat{\\boldsymbol{u_1}}, \\dots, \\hat{\\boldsymbol{u_n}}\\right\\}\\) are orthonormal sets of vectors which span an \\(m\\) or \\(n\\) dimensional space, respectively.\nInputs are \\(\\boldsymbol{a}\\) and outputs are \\(\\boldsymbol{b}\\). Can we formalize the view that \\(\\left\\{\\hat{\\boldsymbol{v_1}}, \\dots, \\hat{\\boldsymbol{v_m}}\\right\\}\\) and \\(\\left\\{\\hat{\\boldsymbol{u_1}}, \\dots, \\hat{\\boldsymbol{u_n}}\\right\\}\\) span all possible inputs and outputs?"
  },
  {
    "objectID": "nlp_lec4.html#pca-38",
    "href": "nlp_lec4.html#pca-38",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nInterpretation\n\n\nFrom \\(\\eqref{svd}\\) we have \\[\\begin{align*}\n   & \\boldsymbol{X} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^T \\Rightarrow \\\\\n   & \\boldsymbol{U}^T \\boldsymbol{X} = \\boldsymbol{\\Sigma} \\boldsymbol{V}^T \\Rightarrow \\\\\n   & \\boldsymbol{U}^T \\boldsymbol{X} = \\boldsymbol{Z},\n\\end{align*}\\] where \\(\\boldsymbol{Z} \\equiv \\boldsymbol{\\Sigma} \\boldsymbol{V}^T\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-39",
    "href": "nlp_lec4.html#pca-39",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nInterpretation\n\n\nComparing to \\(\\eqref{basis}\\), \\(\\left\\{\\hat{\\boldsymbol{u_1}}, \\dots, \\hat{\\boldsymbol{u_n}}\\right\\}\\) perform the same role as \\(\\left\\{\\hat{\\boldsymbol{p_1}}, \\dots, \\hat{\\boldsymbol{p_m}}\\right\\}\\). Hence, \\(\\boldsymbol{U}^T\\) is a change of basis from \\(\\boldsymbol{X}\\) to \\(\\boldsymbol{Z}\\).\nTherefore, from the fact that the orthonormal basis \\(\\boldsymbol{U}^T\\) (or \\(\\boldsymbol{P}\\)) transforms column vectors it follows that \\(\\boldsymbol{U}^T\\) is a basis that spans the columns of \\(\\boldsymbol{X}\\). Bases that span the columns are termed column spaces.\nColumn spaces are equivalent to matrix outputs.\nRow spaces are equivalent to matrix inputs."
  },
  {
    "objectID": "nlp_lec4.html#pca-40",
    "href": "nlp_lec4.html#pca-40",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nHow are PCA and SVD related?\n\n\nConsider original \\(m \\times n\\) matrix \\(\\boldsymbol{X}\\). Define \\[\n\\boldsymbol{Y} \\equiv \\dfrac{1}{\\sqrt{n}} \\boldsymbol{X}^T,\n\\] where each column of \\(\\boldsymbol{Y}\\) has zero mean. Consider: \\[\\begin{align*}\n  &\\boldsymbol{Y}^T \\boldsymbol{Y} = \\left(\\dfrac{1}{\\sqrt{n}}\\boldsymbol{X}^T\\right)^T\\left(\\dfrac{1}{\\sqrt{n}}\\boldsymbol{X}^T\\right) = \\dfrac{1}{n}\\boldsymbol{X} \\boldsymbol{X}^T =  \\boldsymbol{C_X}.\n\\end{align*}\\] By construction \\(\\boldsymbol{Y}^T \\boldsymbol{Y}\\) equals the covariance matrix of \\(\\boldsymbol{X}\\). Principal components of \\(\\boldsymbol{X}\\) are the eigenvectors of \\(\\boldsymbol{C_X}\\). If we calculate the SVD of \\(\\boldsymbol{Y}\\), the columns of matrix \\(\\boldsymbol{V}\\) contain the eigenvectors of \\(\\boldsymbol{Y}^T \\boldsymbol{Y} = \\boldsymbol{C_X}\\).\nTherefore, columns of \\(\\boldsymbol{V}\\) are the principal components of \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-41",
    "href": "nlp_lec4.html#pca-41",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nInterpretation\n\n\n\\(\\boldsymbol{V}\\) spans the row space of \\(\\boldsymbol{Y} \\equiv \\dfrac{1}{\\sqrt{n}} \\boldsymbol{X}^T\\). Therefore, \\(\\boldsymbol{V}\\) must also span the column space of \\(\\dfrac{1}{\\sqrt{n}} \\boldsymbol{X}\\).\nWe conclude that finding the principal components amounts to finding an orthonormal basis that spans the column space of \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "nlp_lec4.html#pca-42",
    "href": "nlp_lec4.html#pca-42",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\n\nSummary of PCA\n\n\n\nOrganize data as \\(m \\times n\\) matrix, where \\(m\\) is the number of measurement types and \\(n\\) is the number of samples\nSubtract off the mean for each measurement type\nCalculate the SVD or the eigenvectors of the covariance"
  },
  {
    "objectID": "nlp_lec4.html#pca-43",
    "href": "nlp_lec4.html#pca-43",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\n\nExample of when PCA fails (red lines). (a) Tracking a person on a ferris wheel (black dots). All dynamics can be described by the phase of the wheel θ, a non-linear combination of the naive basis. (b) Non-Gaussian distributed data and non-orthogonal axes causes PCA to fail. The axes with the largest variance do not correspond to the appropriate answer."
  },
  {
    "objectID": "nlp_lec4.html#pca-44",
    "href": "nlp_lec4.html#pca-44",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nLoss function\n\n\nIt can be proved that under a common loss function, mean squared error (\\(L_2\\) norm), PCA provides the optimal reduced representation of the data.\nThis means that selecting orthogonal directions for principal components is the best solution to predicting the original data.\n\n\n\n\n\n\nKernel PCA\n\n\nDecorrelation is removing second-order dependencies.\nHigher-order dependencies: if prior knowledge is known about the problem, then a nonlinearity (i.e. kernel) might be applied to the data to transform the data to a more appropriate naive basis. This parametric approach is often termed kernel PCA."
  },
  {
    "objectID": "nlp_lec4.html#lsa",
    "href": "nlp_lec4.html#lsa",
    "title": "Principal Component Analysis",
    "section": "LSA",
    "text": "LSA\n\n\n\nBasic concepts: LSA (latent semantic analysis)\n\n\nSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first e.g. 300 dimensions are used as the LSA embedding.\nAlternatively, this is PCA applied to NLP data."
  },
  {
    "objectID": "nlp_lec4.html#lsa-1",
    "href": "nlp_lec4.html#lsa-1",
    "title": "Principal Component Analysis",
    "section": "LSA",
    "text": "LSA\n\n\n\nIssues with TF-IDF\n\n\n\nfocus on spelling and word usage\nlemmatization might group some words, but synonyms will be handled separately\nTF-IDF assumes that frequency is the only thing that matters\n\n\n\n\n\n\n\nSolution\n\n\nUse topics - aggregated words."
  },
  {
    "objectID": "nlp_lec4.html#pca-45",
    "href": "nlp_lec4.html#pca-45",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nBasic concepts\n\n\n\nData preprocessing: Before applying PCA, it is important to preprocess the data by removing any missing values and scaling the features to have zero mean and unit variance.\nCovariance matrix: PCA starts by computing a covariance matrix, which is a matrix that contains the pairwise covariances between all the features in the dataset.\nEigenvectors and eigenvalues: The eigenvectors of the covariance matrix are the principal components (PCs), and the corresponding eigenvalues are the variance explained by each PC."
  },
  {
    "objectID": "nlp_lec4.html#pca-46",
    "href": "nlp_lec4.html#pca-46",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nBasic concepts\n\n\n\nDimensionality reduction: The data can then be projected onto the chosen PCs by multiplying the original data matrix with the matrix of PCs. This results in a lower-dimensional representation of the data.\nReconstruction: If needed, the reduced data can be reconstructed back to the original space by multiplying the reduced data matrix with the transpose of the matrix of PCs."
  },
  {
    "objectID": "nlp_lec4.html#pca-47",
    "href": "nlp_lec4.html#pca-47",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nWhat is a principal component: recap\n\n\nIn principal component analysis, a principal component is a new feature that is constructed from a linear combination of the original features in a dataset.\n\nThe principal components are ordered such that the first principal component has the highest possible variance (i.e., the greatest amount of spread or dispersion in the data)\neach subsequent component in turn has the highest variance possible under the constraint that it is orthogonal (i.e., uncorrelated) to the previous components.\n\n\n\n\n\n\n\nIdea behind PCA\n\n\nReduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space, while still preserving as much of the variance in the data as possible."
  },
  {
    "objectID": "nlp_lec4.html#pca-48",
    "href": "nlp_lec4.html#pca-48",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nMaking sense of principal components\n\n\nPCs are the directions in which the data varies the most. To make sense of the PCs, you can consider the following:\n\nVariance explained: The PCs are ranked in order of the variance they explain. The first PC explains the most variance, the second PC explains the second most variance, and so on. You can check the percentage of variance explained by each PC to understand how much information each PC captures.\nLoadings: Loadings are the coefficients that describe the relationship between the original features and the PCs. You can check the loadings of each feature to see which features contribute the most to each PC."
  },
  {
    "objectID": "nlp_lec4.html#pca-49",
    "href": "nlp_lec4.html#pca-49",
    "title": "Principal Component Analysis",
    "section": "PCA",
    "text": "PCA\n\n\n\nPCA items\n\n\n\nComponents: The PCs are linear combinations of the original features. The components of each PC tell you which features contribute the most to each PC.\nData visualization: You can also use data visualization like scatter plots or biplots to understand the PCs. Scatter plots can show you how the data is distributed along each PC, and biplots can show you the relationship between the original features and the PCs."
  },
  {
    "objectID": "nlp_lec4.html#pca-math-summary",
    "href": "nlp_lec4.html#pca-math-summary",
    "title": "Principal Component Analysis",
    "section": "PCA math summary",
    "text": "PCA math summary\n\n\n\nPreliminary\n\n\nBefore applying PCA, missing values need to be removed from the dataset and each variable should be scaled to have zero mean and unit variance.\n\n\n\n\n\n\nCompute\n\n\n\n\\(\\text{Covariance matrix} = (1/n) \\cdot X^T \\cdot X\\), where \\(X\\) is the data matrix with \\(n\\) samples and \\(p\\) features.\nThen eigenvectors and eigenvalues are obtained by solving the following equation: \\[\n\\text{Covariance matrix} \\cdot v = \\lambda \\cdot v\n\\] where \\(v\\) is the eigenvector and \\(\\lambda\\) is the corresponding eigenvalue."
  },
  {
    "objectID": "nlp_lec4.html#pca-math-summary-1",
    "href": "nlp_lec4.html#pca-math-summary-1",
    "title": "Principal Component Analysis",
    "section": "PCA math summary",
    "text": "PCA math summary\n\n\n\nCompute\n\n\n\nTo complete the dimensionality reduction, data can then be projected onto the chosen PCs by multiplying the original data matrix with the matrix of PCs: \\[\n   X_{reduced} = X \\cdot PCs\n   \\]\nTo reconstruct the reduced data back to the original space, we multiply the reduced data matrix with the transpose of the matrix of PCs: \\[\n   X_{reconstructed} = X_{reduced} * {PCs}^T\n   \\]"
  },
  {
    "objectID": "nlp_lec4.html#pca-in-python-1",
    "href": "nlp_lec4.html#pca-in-python-1",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n # Import libraries \nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# Load data\ndata = np.loadtxt('data.txt')\n\n# Normalize data\nmean = np.mean(data, axis=0)\nstd = np.std(data, axis=0)\ndata = (data - mean) / std"
  },
  {
    "objectID": "nlp_lec4.html#pca-in-python-2",
    "href": "nlp_lec4.html#pca-in-python-2",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n  # Compute covariance matrix\n  cov_matrix = np.cov(data, rowvar=False)\n\n  # Compute eigenvectors and eigenvalues\n  eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n  # Sort in descending order\n  idx = np.argsort(-eigenvalues)\n  eigenvalues = eigenvalues[idx]\n  eigenvectors = eigenvectors[:,idx]\n\n  # Select top principal components\n  k = 2\n  eigenvectors = eigenvectors[:,:k]"
  },
  {
    "objectID": "nlp_lec4.html#pca-in-python-3",
    "href": "nlp_lec4.html#pca-in-python-3",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n  # Project the data onto the principal components\n  projected_data = np.dot(data, eigenvectors.T)\n  # Visualize the results using scatter from matplotlib\n  plt.scatter(projected_data[:,0], projected)"
  },
  {
    "objectID": "nlp_lec4.html#neighbor-graph-algorithms",
    "href": "nlp_lec4.html#neighbor-graph-algorithms",
    "title": "Principal Component Analysis",
    "section": "Neighbor graph algorithms",
    "text": "Neighbor graph algorithms\n\n\n\nCommon steps\n\n\n\nCompute high dimensional probabilities \\(p\\).\nCompute low dimensional probabilities \\(q\\).\nCalculate the difference between the probabilities by a given cost function \\(C(p,q)\\).\nMinimize the cost function."
  },
  {
    "objectID": "nlp_lec4.html#sne",
    "href": "nlp_lec4.html#sne",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nDefinition\n\n\nStochastic Neighbor Embedding (SNE) tries to place the objects in a low-dimensional space so as to optimally preserve neighborhood identity, and can be naturally extended to allow multiple different low-d images of each object."
  },
  {
    "objectID": "nlp_lec4.html#sne-1",
    "href": "nlp_lec4.html#sne-1",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nStep 1.\n\n\nHigh-dimensional probabilities.\nCompute probability that that object \\(i\\) would pick \\(j\\) as neighbor: \\[\np_{ij} = \\dfrac{exp\\left(-d_{ij}^2\\right)}{\\sum\\limits_{k \\neq i} exp\\left(-d_{ik}^2\\right)}\n\\] Values \\(d_{ij}\\) represent between points \\(i\\) and \\(j\\): \\[\nd_{ij}^2 = \\dfrac{\\|\\boldsymbol{x}_i-\\boldsymbol{x}_j\\|^2}{2\\sigma_i^2}\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#sne-2",
    "href": "nlp_lec4.html#sne-2",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nStep 1.\n\n\nWhat is the meaning of \\(\\sigma_i\\)? It is found by a binary search that makes the Shannon entropy of the distribution over neighbors equal to \\(log_2 k\\), where \\(k\\) is the effective number of local neighbors or .\nWe compute entropy via: \\[\\begin{align*}\n&H = -\\sum\\limits_j p_{ij} log_2 p_{ij} = log_2 k,\\\\\n&k = 2^{-\\sum\\limits_j p_{ij} log_2 p_{ij}}\n\\end{align*}\\] By tuning \\(\\sigma_i\\) we try to match the value of \\(k\\) set by the user."
  },
  {
    "objectID": "nlp_lec4.html#sne-3",
    "href": "nlp_lec4.html#sne-3",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nStep 1.\n\n\nThe higher the effective number of local neighbors (perplexity), the higher \\(\\sigma_i\\) and the wider the Gaussian function used in the dissimilarities.\n\n\n\n\n\n\nIntuition\n\n\nMathematical intuition: The higher the perplexity, the more likely it is to consider points that are far away as neighbors.\n\n\n\n\n\nAdvice: The authors of SNE and t-SNE use perplexity values between 5 and 50."
  },
  {
    "objectID": "nlp_lec4.html#sne-4",
    "href": "nlp_lec4.html#sne-4",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nStep 2.\n\n\nLow-dimensional probabilities.\nNow that we have the high-dimensional probabilities, we move on to calculate the low dimensional ones, which depend on where the data points are mapped in the low dimensional space.\n\nIn the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance (which we set without loss of generality to be \\(\\dfrac{1}{2}\\)) so the induced probability \\(q_{ij}\\) that point \\(i\\) picks point \\(j\\) as its neighbor is a function of the low-dimensional images \\(y_i\\) of all the objects and is given by expression: \\[\nq_{ij} = \\dfrac{exp\\left(-\\|\\boldsymbol{y}_i-\\boldsymbol{y}_j\\|^2\\right)}{\\sum\\limits_{k \\neq i} exp\\left(-\\|\\boldsymbol{y}_i-\\boldsymbol{y}_k\\|^2\\right)}\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#sne-5",
    "href": "nlp_lec4.html#sne-5",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nStep 3.\n\n\nChoice of cost function.\nIf the points \\(Y_i\\) are placed correctly in the low-dimensional space, the conditional probabilities \\(p\\) and \\(q\\) will be very similar. To measure the mismatch between both probabilities, SNE uses the as a loss function for each point. Each point in both high and low dimensional space has a conditional probability to call another point its neighbor. Hence, we have as many loss functions as we have data points. We define the cost function as the sum of the KL divergences over all data points, \\[\nC = \\sum\\limits_i \\sum\\limits_j p_{ij} log\\dfrac{p_{ij}}{q_{ij}} = \\sum\\limits_i KL(P_i \\| Q_I).\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#sne-6",
    "href": "nlp_lec4.html#sne-6",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nKullback-Leibler Divergence\n\n\nThe KL divergence, which is closely related to relative entropy, information divergence, and information for discrimination, is a non-symmetric measure of the difference between two probability distributions \\(p(x)\\) and \\(q(x)\\).\nSpecifically, the KL divergence of \\(q(x)\\) from \\(p(x)\\), denoted DKL(p(x),q(x)), is a measure of the information lost when \\(q(x)\\) is used to approximate \\(p(x)\\)."
  },
  {
    "objectID": "nlp_lec4.html#sne-7",
    "href": "nlp_lec4.html#sne-7",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nKullback-Leibler Divergence\n\n\nLet \\(p(x)\\) and \\(q(x)\\) are two probability distributions of a discrete random variable \\(x\\). That is, both \\(p(x)\\) and \\(q(x)\\) sum up to \\(1\\), and \\(p(x) &gt; 0\\) and \\(q(x) &gt; 0\\) \\(\\forall x \\in X\\).\nKL divergence is defined as: \\[\nKL(p(x) \\| q(x)) = \\sum\\limits_{x \\in X} p(x) ln \\dfrac{p(x)}{q(x)}.\n\\] Continuous version: \\[\nKL(p(x) \\| q(x)) = \\int\\limits_{-\\infty}^{\\infty} p(x) ln \\dfrac{p(x)}{q(x)}dx.\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#sne-8",
    "href": "nlp_lec4.html#sne-8",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nDifferentiation of the cost function\n\n\n\\[\n\\dfrac{\\partial C}{\\partial \\boldsymbol{y}_i} = 2\\sum\\limits_j (\\boldsymbol{y}_i-\\boldsymbol{y}_j)(p_{ij}-q_{ij}+p_{ji}-q_{ij}).\n\\]\n\n\n\n\n\n\nInterpretation\n\n\nA sum of forces pulling toward or pushing it away depending on whether is observed to be a neighbor more or less often than desired."
  },
  {
    "objectID": "nlp_lec4.html#sne-9",
    "href": "nlp_lec4.html#sne-9",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\n\n\n\n\nThe algorithm starts by placing all the \\(y_i\\) in random locations very close to the origin, and then is trained minimizing the cost function \\(C\\) using gradient descent."
  },
  {
    "objectID": "nlp_lec4.html#sne-10",
    "href": "nlp_lec4.html#sne-10",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nThe result of running the SNE algorithm on 3000 256-dimensional grayscale images of handwritten digits."
  },
  {
    "objectID": "nlp_lec4.html#sne-11",
    "href": "nlp_lec4.html#sne-11",
    "title": "Principal Component Analysis",
    "section": "SNE",
    "text": "SNE\n\n\n\nEmbedding of NIPS authors in 2D (red dots are authors who published 6+ papers)."
  },
  {
    "objectID": "nlp_lec4.html#t-sne",
    "href": "nlp_lec4.html#t-sne",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\n\n\n\nProblems\n\n\n\ncost function is very difficult to optimize\na crowding problem\n\n\n\n\n\n\n\nSolutions\n\n\n\nSymmetrization\nuse of t-distributions for the low-dimensional probabilities"
  },
  {
    "objectID": "nlp_lec4.html#t-sne-1",
    "href": "nlp_lec4.html#t-sne-1",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\n\n\n\nSymmetric SNE.\n\n\nThe probability of point \\(x_i\\) to consider point \\(x_j\\) as its neighbor is not the same probability that point \\(x_j\\) would consider point \\(x_i\\) as a neighbor. We symmetrize pairwise probabilities in high dimensional space by defining \\[\n\\tilde{p}_{ij} = \\dfrac{p_{ij}+p_{ji}}{2n}\n\\]"
  },
  {
    "objectID": "nlp_lec4.html#t-sne-2",
    "href": "nlp_lec4.html#t-sne-2",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\n\n\n\nThe Crowding Problem\n\n\nIf we want to correctly project close distances between points, the moderate distances are distorted and appear as huge distances in the low dimensional space. \nTo solve this, use the Student t-Distribution (which is what gives the ‘t’ to t-SNE) with one degree of freedom for the low-dimensional probabilities: \\[\nq_{ij} = \\dfrac{\\left(1+\\|y_i-y_j\\|^2\\right)^{-1}}{\\sum\\limits_{k \\neq l} \\left(1+\\|y_k-y_l\\|^2\\right)^{-1}}\n\\]\nNow \\(p_{ij}=p_{ji}\\) and \\(q_{ij}=q_{ji}\\), and the gradient of the cost function \\[\nC = \\sum\\limits_i \\sum\\limits_j \\tilde{p}_{ij} log\\dfrac{\\tilde{p}_{ij}}{q_{ij}}\n\\] is easier to compute."
  },
  {
    "objectID": "nlp_lec4.html#t-sne-3",
    "href": "nlp_lec4.html#t-sne-3",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\n\n\n\nLocal structure\n\n\nSince t-SNE also uses KL divergence as its loss function, it also carries the problems discussed in the previous section. This is not to say that it is completely ignored, but the main takeaway is that t-SNE severely prioritizes the conservation of the local structure.\n\n\n\n\n\n\nGlobal structure\n\n\nSince the KL divergence function does not penalize the misplacement in low dimensional space of points that are far away in high dimensional space, we can conclude that the global structure is not well preserved. t-SNE will group similar data points together into clusters, but distances between clusters might not mean anything."
  },
  {
    "objectID": "nlp_lec4.html#t-sne-4",
    "href": "nlp_lec4.html#t-sne-4",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ndef read_glove(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            word = fields[0]\n            yield (word, vec)\nwords = []\nvectors = []\nfor word, vec in read_glove('data/glove/glove.42B.300d.txt'):\n    words.append(word)\n    vectors.append(vec)\nmodel = TSNE(n_components=2, init='pca', random_state=0) coordinates = model.fit_transform(vectors)\nplt.figure(figsize=(8, 8))\nfor word, xy in zip(words, coordinates):\n    plt.scatter(xy[0], xy[1])\n    plt.annotate(word,\nplt.xlim(25, 55)\nplt.ylim(-15, 15)\nplt.show()"
  },
  {
    "objectID": "nlp_lec4.html#t-sne-5",
    "href": "nlp_lec4.html#t-sne-5",
    "title": "Principal Component Analysis",
    "section": "t-SNE",
    "text": "t-SNE\n\nGloVe embeddings visualized by t-SNE"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization",
    "href": "nlp_lec4.html#python-pca-visualization",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\nUse MNIST data set.\n\n\n\nGet libraries\n\n\n  from __future__ import print_function\n  import time\n\n  import numpy as np\n  import pandas as pd\n\n  from sklearn.datasets import fetch_mldata\n  from sklearn.decomposition import PCA\n  from sklearn.manifold import TSNE\n\n  %matplotlib inline\n  import matplotlib.pyplot as plt\n  from mpl_toolkits.mplot3d import Axes3D\n\n  import seaborn as sns"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-1",
    "href": "nlp_lec4.html#python-pca-visualization-1",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nLoad data\n\n\n   mnist = fetch_mldata(\"MNIST original\")\n   X = mnist.data / 255.0\n   y = mnist.target\n\n   print(X.shape, y.shape)\n\n   [out] (70000, 784) (70000,)"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-2",
    "href": "nlp_lec4.html#python-pca-visualization-2",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nConvert to Pandas and randomize\n\n\n    feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n\n    df = pd.DataFrame(X,columns=feat_cols)\n    df['y'] = y\n    df['label'] = df['y'].apply(lambda i: str(i))\n\n    X, y = None, None\n\n    print('Size of the dataframe: {}'.format(df.shape))\n\n    [out] Size of the dataframe: (70000, 785)\n\n    # For reproducibility of the results\n    np.random.seed(42)\n\n    rndperm = np.random.permutation(df.shape[0])"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-3",
    "href": "nlp_lec4.html#python-pca-visualization-3",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nCheck some numbers\n\n\n    plt.gray()\n    fig = plt.figure( figsize=(16,7) )\n    for i in range(0,15):\n        ax = fig.add_subplot(3,5,i+1, title=\"Digit: {}\".format(str(df.loc[rndperm[i],'label'])) )\n    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n    plt.show()"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-4",
    "href": "nlp_lec4.html#python-pca-visualization-4",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nUse Scikit-learn for PCA\n\n\n    pca = PCA(n_components=3)\n    pca_result = pca.fit_transform(df[feat_cols].values)\n\n    df['pca-one'] = pca_result[:,0]\n    df['pca-two'] = pca_result[:,1] \n    df['pca-three'] = pca_result[:,2]\n\n    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n\n    Explained variation per principal component: [0.09746116 0.07155445 0.06149531]"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-5",
    "href": "nlp_lec4.html#python-pca-visualization-5",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nScatterplot of first 2 PCs\n\n\n    plt.figure(figsize=(16,10))\n    sns.scatterplot(\n        x=\"pca-one\", y=\"pca-two\",\n        hue=\"y\",\n        palette=sns.color_palette(\"hls\", 10),\n        data=df.loc[rndperm,:],\n        legend=\"full\",\n        alpha=0.3\n    )"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-6",
    "href": "nlp_lec4.html#python-pca-visualization-6",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-7",
    "href": "nlp_lec4.html#python-pca-visualization-7",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization\n\n\n\nScatterplot of first 2 PCs in 3D\n\n\n    ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n    ax.scatter(\n        xs=df.loc[rndperm,:][\"pca-one\"], \n        ys=df.loc[rndperm,:][\"pca-two\"], \n        zs=df.loc[rndperm,:][\"pca-three\"], \n        c=df.loc[rndperm,:][\"y\"], \n        cmap='tab10'\n    )\n    ax.set_xlabel('pca-one')\n    ax.set_ylabel('pca-two')\n    ax.set_zlabel('pca-three')\n    plt.show()"
  },
  {
    "objectID": "nlp_lec4.html#python-pca-visualization-8",
    "href": "nlp_lec4.html#python-pca-visualization-8",
    "title": "Principal Component Analysis",
    "section": "Python PCA Visualization",
    "text": "Python PCA Visualization"
  },
  {
    "objectID": "nlp_lec4.html#python-t-sne-visualization",
    "href": "nlp_lec4.html#python-t-sne-visualization",
    "title": "Principal Component Analysis",
    "section": "Python t-SNE Visualization",
    "text": "Python t-SNE Visualization\n\n\n\nRun PCA\n\n\n    N = 10000\n    df_subset = df.loc[rndperm[:N],:].copy()\n    data_subset = df_subset[feat_cols].values\n\n    pca = PCA(n_components=3)\n    pca_result = pca.fit_transform(data_subset)\n\n    df_subset['pca-one'] = pca_result[:,0]\n    df_subset['pca-two'] = pca_result[:,1] \n    df_subset['pca-three'] = pca_result[:,2]\n\n    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n\n    [out] Explained variation per principal component: [0.09730166 0.07135901 0.06183721]"
  },
  {
    "objectID": "nlp_lec4.html#python-t-sne-visualization-1",
    "href": "nlp_lec4.html#python-t-sne-visualization-1",
    "title": "Principal Component Analysis",
    "section": "Python t-SNE Visualization",
    "text": "Python t-SNE Visualization\n\n\n\nRun t-SNE\n\n\n    time_start = time.time()\n    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n    tsne_results = tsne.fit_transform(data_subset)\n    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n\n    [out] [t-SNE] Computing 121 nearest neighbors...\n    [t-SNE] Indexed 10000 samples in 0.564s...\n    [t-SNE] Computed neighbors for 10000 samples in 121.191s...\n    [t-SNE] Computed conditional probabilities for sample 1000 / 10000\n    ...\n    [t-SNE] Mean sigma: 2.129023\n    [t-SNE] KL divergence after 300 iterations: 2.823509\n    t-SNE done! Time elapsed: 157.3975932598114 seconds"
  },
  {
    "objectID": "nlp_lec4.html#python-t-sne-visualization-2",
    "href": "nlp_lec4.html#python-t-sne-visualization-2",
    "title": "Principal Component Analysis",
    "section": "Python t-SNE Visualization",
    "text": "Python t-SNE Visualization\n\n\n\nPlot t-SNE\n\n\n    df_subset['tsne-2d-one'] = tsne_results[:,0]\n    df_subset['tsne-2d-two'] = tsne_results[:,1]\n\n    plt.figure(figsize=(16,10))\n    sns.scatterplot(\n        x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n        hue=\"y\",\n        palette=sns.color_palette(\"hls\", 10),\n        data=df_subset,\n        legend=\"full\",\n        alpha=0.3\n    )"
  },
  {
    "objectID": "nlp_lec4.html#python-t-sne-visualization-3",
    "href": "nlp_lec4.html#python-t-sne-visualization-3",
    "title": "Principal Component Analysis",
    "section": "Python t-SNE Visualization",
    "text": "Python t-SNE Visualization\n\n\n\nPlot t-SNE"
  },
  {
    "objectID": "dl_lab1.html",
    "href": "dl_lab1.html",
    "title": "DL: Lab 1",
    "section": "",
    "text": "Lab overview\nWe’ll implement a binary classifier using logistic regression, but via a neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "nlp_lab14.html",
    "href": "nlp_lab14.html",
    "title": "NLP: Lab 13 (Generating Names with a Character-Level RNN)",
    "section": "",
    "text": "Description\nPlease complete notebook and exercises from https://docs.pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
  },
  {
    "objectID": "dl_lab3.html",
    "href": "dl_lab3.html",
    "title": "DL: Lab 3",
    "section": "",
    "text": "Lab overview\nWe’ll implement an \\(L\\)-layer neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns",
    "href": "dl_lec7.html#convolutional-nns",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nDefinition\n\n\nConvolutional networks (LeCun, 1989), also known as convolutional neural networks, or CNNs, are a specialized kind of neural network for processing data that has a known grid-like topology.\nCNNs are a family of models that were originally inspired by how the visual cortex of the human brain works when recognizing objects."
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-1",
    "href": "dl_lec7.html#convolutional-nns-1",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nWhat’s in a name?\n\n\nConvolutional networks have been tremendously successful in practical applications. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation.\n\n\n\n\n\n\nData Examples\n\n\nExamples include:\n\ntime-series data, which can be thought of as a 1-D grid taking samples at regular time intervals\nimage data, which can be thought of as a 2-D grid of pixels"
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-2",
    "href": "dl_lec7.html#convolutional-nns-2",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nImage processing challenges using feed-forward NNs\n\n\n\nImages generally have a high dimensionality, with typical cameras capturing images comprising tens of megapixels.\nMore significantly, such an approach fails to take account of the highly structured nature of image data, in which the relative positions of different pixels play a crucial role."
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-3",
    "href": "dl_lec7.html#convolutional-nns-3",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nUses\n\n\n\nClassification of images. This is sometimes called ‘image recognition’.\nDetection of objects in an image and determining their locations within the image.\nSegmentation of images.\nCaption generation.\nSynthesis of new images."
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-4",
    "href": "dl_lec7.html#convolutional-nns-4",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nUses\n\n\n\nInpainting.\nStyle transfer.\nSuper-resolution.\nDepth prediction.\nScene reconstruction."
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-5",
    "href": "dl_lec7.html#convolutional-nns-5",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nData properties\n\n\n\nstored as multi-dimensional arrays.\none or more axes for which ordering matters (e.g., width and height axes for an image, time axis for a sound clip).\none axis, called the channel axis, is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track)."
  },
  {
    "objectID": "dl_lec7.html#history-1",
    "href": "dl_lec7.html#history-1",
    "title": "Convolutional networks",
    "section": "History",
    "text": "History\n\n\n\nOrigins\n\n\nThe development of CNNs goes back to the 1990s, when Yann LeCun and his colleagues proposed a novel NN architecture for classifying handwritten digits from images\n\n\n\n\n\n\nTuring award\n\n\nSeveral years later, in 2019, Yann LeCun received the Turing award (the most prestigious award in computer science) for his contributions to the field of artificial intelligence (AI), along with two other researchers, Yoshua Bengio and Geoffrey Hinton.\n\n\n\n\n\nHandwritten Digit Recognition with a Back-Propagation Network by Y. LeCun, and colleagues, 1989, published at the Neural Information Processing Systems (NeurIPS) conference)."
  },
  {
    "objectID": "dl_lec7.html#history-2",
    "href": "dl_lec7.html#history-2",
    "title": "Convolutional networks",
    "section": "History",
    "text": "History\n\n\n\nCats\n\n\nThe original discovery of how the visual cortex of our brain functions was made by David H. Hubel and Torsten Wiesel in 1959, when they inserted a microelectrode into the primary visual cortex of an anesthetized cat.\nThey measured the electrical responses of individual neurons in the visual cortex of cats while presenting visual stimuli to the cats’ eyes."
  },
  {
    "objectID": "dl_lec7.html#cat-experiment",
    "href": "dl_lec7.html#cat-experiment",
    "title": "Convolutional networks",
    "section": "Cat experiment",
    "text": "Cat experiment"
  },
  {
    "objectID": "dl_lec7.html#visual-cortex",
    "href": "dl_lec7.html#visual-cortex",
    "title": "Convolutional networks",
    "section": "Visual cortex",
    "text": "Visual cortex"
  },
  {
    "objectID": "dl_lec7.html#visual-cortex-1",
    "href": "dl_lec7.html#visual-cortex-1",
    "title": "Convolutional networks",
    "section": "Visual cortex",
    "text": "Visual cortex\n\n\n\nGabor filters\n\n\n\nThese model responses of simple cells. \\[\n  \\begin{align*}\n    &G(x,y) = A \\exp \\left(-\\alpha \\tilde{x}^2 - \\beta \\tilde{y}^2\\right) \\sin \\left(\\omega \\tilde{x} + \\psi\\right), \\; \\text{where} \\\\\n    &\\tilde{x} = (x-x_0)\\cos \\theta + (y-y_0) \\sin \\theta, \\\\\n    &\\tilde{y} = -(x-x_0)\\sin \\theta + (y-y_0) \\cos \\theta.\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec7.html#gabor-filters-example",
    "href": "dl_lec7.html#gabor-filters-example",
    "title": "Convolutional networks",
    "section": "Gabor filters example",
    "text": "Gabor filters example"
  },
  {
    "objectID": "dl_lec7.html#alexnet",
    "href": "dl_lec7.html#alexnet",
    "title": "Convolutional networks",
    "section": "Alexnet",
    "text": "Alexnet"
  },
  {
    "objectID": "dl_lec7.html#convolutional-nns-7",
    "href": "dl_lec7.html#convolutional-nns-7",
    "title": "Convolutional networks",
    "section": "Convolutional NNs",
    "text": "Convolutional NNs\n\n\n\nDefinition\n\n\nConvolutional networks are neural networks that use convolution in place of general matrix multiplication in at least one of their layers.\n\n\n\n\n\n\n\n\nFeed-forward NN\n\n\n\\[\nz = Wx + b\n\\]\n\n\n\n\n\n\n\nConvolutional NN\n\n\n\\[\n\\textbf{Z} = \\textbf{W} \\ast \\textbf{X} + b\n\\]"
  },
  {
    "objectID": "dl_lec7.html#convolutions",
    "href": "dl_lec7.html#convolutions",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nPlan\n\n\n\nwhat’s a convolution?\nmotivation for convolutions in NNs\npooling\nvariants of convolution functions\nefficiency matters"
  },
  {
    "objectID": "dl_lec7.html#convolutions-1",
    "href": "dl_lec7.html#convolutions-1",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nGeneral definition\n\n\nAn operation on two functions of a real-valued argument.\n\n\n\n\n\n\nSpaceship example\n\n\n\nSuppose we are tracking the location of a spaceship with a laser sensor. Our laser sensor provides a single output \\(x(t)\\), the position of the spaceship at time \\(t\\). Both \\(x\\) and \\(t\\) are real valued.\nSuppose that measurements are noisy. Do averaging to compensate with a weighting function \\(w(a)\\), where \\(a\\) is the age of the measurement. Give more weight to recent measurements.\nResult: \\(s(t) = \\int x(a)w(t-a)da\\)."
  },
  {
    "objectID": "dl_lec7.html#convolutions-2",
    "href": "dl_lec7.html#convolutions-2",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nLimitations\n\n\n\n\\(w\\) needs to be a valid probability density function, or the output will not be a weighted average.\nAlso, \\(w\\) needs to be \\(0\\) for all negative arguments, or it will look into the future, which is presumably beyond our capabilities."
  },
  {
    "objectID": "dl_lec7.html#convolutions-3",
    "href": "dl_lec7.html#convolutions-3",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nNotation\n\n\n\\[\ns(t) = (x \\ast w)(t)\n\\]\n\n\n\n\n\n\nTerminology\n\n\n\nthe first argument \\(x\\) is the input or signal.\nthe second argument \\(w\\) is the filter orkernel.\noutput is the feature map."
  },
  {
    "objectID": "dl_lec7.html#convolutions-4",
    "href": "dl_lec7.html#convolutions-4",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nDiscrete convolution\n\n\nMeasurements cannot be continuous in practice, they will be discrete. Therefore: \\[\ns(t) = (x \\ast w)(t) = \\sum\\limits_{a=-\\infty}^{+\\infty} x(a)w(t-a).\n\\]"
  },
  {
    "objectID": "dl_lec7.html#convolutions-5",
    "href": "dl_lec7.html#convolutions-5",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\n2D\n\n\nMultiple axes: suppose we have a two-dimensional image \\(I\\) and thus two-dimensional kernel \\(K\\). \\[\nS(i, j) = (I \\ast K)(i,j) = \\sum\\limits_m \\sum\\limits_n I(m,n)K(i-m,j-n)\n\\] By commutativity: \\[\nS(i, j) = (K \\ast I)(i,j) = \\sum\\limits_m \\sum\\limits_n I(i-m,j-n)K(m,n)\n\\]"
  },
  {
    "objectID": "dl_lec7.html#cross-correlation",
    "href": "dl_lec7.html#cross-correlation",
    "title": "Convolutional networks",
    "section": "Cross-correlation",
    "text": "Cross-correlation\n\n\n\n\n\n\nCommutativity\n\n\nThe commutative property of convolution arises because we have flipped the kernel relative to the input, in the sense that as m increases, the index into the input increases, but the index into the kernel decreases. The only reason to flip the kernel is to obtain the commutative property.\n\n\n\n\n\n\nCross-correlation - Definition\n\n\n\\[\nS(i, j) = (I \\ast K)(i,j) = \\sum\\limits_m \\sum\\limits_n I(i+m,j+n)K(m,n)\n\\]"
  },
  {
    "objectID": "dl_lec7.html#convolutions-6",
    "href": "dl_lec7.html#convolutions-6",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nProperties\n\n\n\nDiscrete convolution preserves ordering!\nsparse (only a few input units contribute to a given output unit)\nreuses parameters (the same weights are applied to multiple locations in the input)."
  },
  {
    "objectID": "dl_lec7.html#convolutions-7",
    "href": "dl_lec7.html#convolutions-7",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nAnalogies with weight/bias\n\n\n\ninput is a multidimensional array of data (a tensor)\nkernel is a multidimensional array of parameters (also a tensor)\ninput/kernel are zero everywhere except for points where we have the data. Therefore, summation becomes finite."
  },
  {
    "objectID": "dl_lec7.html#example",
    "href": "dl_lec7.html#example",
    "title": "Convolutional networks",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "dl_lec7.html#analogy",
    "href": "dl_lec7.html#analogy",
    "title": "Convolutional networks",
    "section": "Analogy",
    "text": "Analogy\n\n\n\nAlgebraic analogy\n\n\nDiscrete convolution can be viewed as a matrix multiplication, but matrix has several entries constrained to be equal to other entries. Also, matrices are very sparse, because kernel is usually much smaller than the input.\nIn two dimensions, a doubly block circulant matrix corresponds to convolution."
  },
  {
    "objectID": "dl_lec7.html#toeplitz-matrix",
    "href": "dl_lec7.html#toeplitz-matrix",
    "title": "Convolutional networks",
    "section": "Toeplitz matrix",
    "text": "Toeplitz matrix\nFor univariate discrete convolution, each row of the matrix is constrained to be equal to the row above shifted by one element.\n\\[\n\\begin{pmatrix}\n2 & -1 & 0 & \\cdots & \\cdots & \\cdots & \\cdots & 0\\\\\n-1 & 2 & -1 & 0 & & & & \\vdots\\\\\n0 & -1 & 2 & -1 & \\ddots & & & \\vdots\\\\\n\\vdots & 0 & \\ddots & \\ddots & \\ddots & \\ddots & & \\vdots\\\\\n\\vdots & & \\ddots & \\ddots & \\ddots & \\ddots & 0 & \\vdots\\\\\n\\vdots & & & \\ddots & -1 & 2 & -1 & 0\\\\\n\\vdots & & & & 0 & -1 & 2 & -1\\\\\n0 & \\cdots & \\cdots  & \\cdots & \\cdots & 0 & -1 & 2\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec7.html#motivation",
    "href": "dl_lec7.html#motivation",
    "title": "Convolutional networks",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\nIdeas\n\n\n\nsparse interactions\nparameter sharing\nequivariant representations\nvariable-size inputs"
  },
  {
    "objectID": "dl_lec7.html#sparseness",
    "href": "dl_lec7.html#sparseness",
    "title": "Convolutional networks",
    "section": "Sparseness",
    "text": "Sparseness\n\n\n\n\n\n\nNote\n\n\n\nin traditional NN, every output unit interacts with every input unit.\nin CNN, we have sparse interactions (also referred to as sparse connectivity or sparse weights).\nthis is accomplished by making the kernel smaller than the input.\nresults in efficiency improvements"
  },
  {
    "objectID": "dl_lec7.html#sparseness-1",
    "href": "dl_lec7.html#sparseness-1",
    "title": "Convolutional networks",
    "section": "Sparseness",
    "text": "Sparseness\n\n\n\nComplexity notes\n\n\nFor matrix multiplication, in case of \\(m\\) inputs and \\(n\\) otputs: \\(O(m \\times n)\\).\nLimit number of output connections to \\(k\\), get \\(O(k \\times n)\\) complexity\n\n\n\n\n\n\nImprovement\n\n\nIt’s possible to obtain good performance while keeping \\(k \\ll m\\)."
  },
  {
    "objectID": "dl_lec7.html#sparseness-2",
    "href": "dl_lec7.html#sparseness-2",
    "title": "Convolutional networks",
    "section": "Sparseness",
    "text": "Sparseness\n\nSparse connectivity, viewed from below."
  },
  {
    "objectID": "dl_lec7.html#sparseness-3",
    "href": "dl_lec7.html#sparseness-3",
    "title": "Convolutional networks",
    "section": "Sparseness",
    "text": "Sparseness\n\nSparse connectivity, viewed from above."
  },
  {
    "objectID": "dl_lec7.html#sparseness-4",
    "href": "dl_lec7.html#sparseness-4",
    "title": "Convolutional networks",
    "section": "Sparseness",
    "text": "Sparseness\n\nLarge receptive field."
  },
  {
    "objectID": "dl_lec7.html#parameter-sharing",
    "href": "dl_lec7.html#parameter-sharing",
    "title": "Convolutional networks",
    "section": "Parameter sharing",
    "text": "Parameter sharing\n\n\n\nDefinition\n\n\nParameter sharing refers to using the same parameter for more than one function in a model. As a synonym for parameter sharing, one can say that a network has tied weights."
  },
  {
    "objectID": "dl_lec7.html#parameter-sharing-1",
    "href": "dl_lec7.html#parameter-sharing-1",
    "title": "Convolutional networks",
    "section": "Parameter sharing",
    "text": "Parameter sharing\n\nEfficiency of edge detection.The image on the right was formed by taking each pixel in the original image and subtracting the value of its neighboring pixel on the left."
  },
  {
    "objectID": "dl_lec7.html#equivariance",
    "href": "dl_lec7.html#equivariance",
    "title": "Convolutional networks",
    "section": "Equivariance",
    "text": "Equivariance\n\n\n\nDefinition\n\n\nTo say a function is equivariant means that if the input changes, the output changes in the same way.\nSpecifically, a function f(x) is equivariant to a function g if \\(f(g(x)) = g(f(x))\\).\nIn the case of convolution, if we let g be any function that translates the input, that is, shifts it, then the convolution function is equivariant to g."
  },
  {
    "objectID": "dl_lec7.html#convolutions-8",
    "href": "dl_lec7.html#convolutions-8",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\nKernel example."
  },
  {
    "objectID": "dl_lec7.html#convolutions-9",
    "href": "dl_lec7.html#convolutions-9",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions"
  },
  {
    "objectID": "dl_lec7.html#convolutions-10",
    "href": "dl_lec7.html#convolutions-10",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions"
  },
  {
    "objectID": "dl_lec7.html#convolutions-11",
    "href": "dl_lec7.html#convolutions-11",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nShape\n\n\nThe collection of kernels defining a discrete convolution has a shape corresponding to some permutation of \\((n, m, k_1, \\dots, k_N)\\), where\n\n$n $ number of output feature maps\n$m $ number of input feature maps\n$k_j $ kernel size along axis \\(j\\)"
  },
  {
    "objectID": "dl_lec7.html#convolutions-12",
    "href": "dl_lec7.html#convolutions-12",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\nShape\n\n\nThe following properties affect the output size \\(o_j\\) of a convolutional layer along axis \\(j\\):\n\n\\(i_j\\): input size along axis \\(j\\),\n\\(k_j\\): kernel size along axis \\(j\\),\n\\(s_j\\): stride (distance between two consecutive positions of the kernel) along axis \\(j\\),\n\\(p_j\\): zero padding (number of zeros concatenated at the beginning and at the end of an axis) along axis \\(j\\)."
  },
  {
    "objectID": "dl_lec7.html#convolutions-stride",
    "href": "dl_lec7.html#convolutions-stride",
    "title": "Convolutional networks",
    "section": "Convolutions: stride",
    "text": "Convolutions: stride\n\nStride \\(s\\)=2."
  },
  {
    "objectID": "dl_lec7.html#convolutions-13",
    "href": "dl_lec7.html#convolutions-13",
    "title": "Convolutional networks",
    "section": "Convolutions",
    "text": "Convolutions\n\n\n\n\n\nFor multiple feature maps, they are convolved with distinct kernels, and the results are summed up elementwise to produce the output feature map."
  },
  {
    "objectID": "dl_lec7.html#convolutions-padding",
    "href": "dl_lec7.html#convolutions-padding",
    "title": "Convolutional networks",
    "section": "Convolutions: padding",
    "text": "Convolutions: padding\n\n\n\n\n\n\nNote\n\n\n\\[\n\\begin{align*}\n  &y = x \\ast w \\\\\n  &y[i] = \\sum\\limits_{k=-\\infty}^{+\\infty} x[i-k]w[k]\n\\end{align*}\n\\]\n\n\n\n\nHow to deal with infinity?"
  },
  {
    "objectID": "dl_lec7.html#padding-1",
    "href": "dl_lec7.html#padding-1",
    "title": "Convolutional networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nPadding modes\n\n\nThere are three modes of padding that are commonly used in practice: full, same, and valid.\n\nIn full mode, the padding parameter, p, is set to p = m – 1. Full padding increases the dimensions of the output; thus, it is rarely used in CNN architectures.\nThe same padding mode is usually used to ensure that the output vector has the same size as the input vector, x. In this case, the padding parameter, p, is computed according to the filter size, along with the requirement that the input size and output size are the same.\nvalid mode refers to the case where p = 0 (no padding)."
  },
  {
    "objectID": "dl_lec7.html#padding-2",
    "href": "dl_lec7.html#padding-2",
    "title": "Convolutional networks",
    "section": "Padding",
    "text": "Padding"
  },
  {
    "objectID": "dl_lec7.html#padding-3",
    "href": "dl_lec7.html#padding-3",
    "title": "Convolutional networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nPros/cons\n\n\n\nThe most commonly used padding mode in CNNs is same padding. One of its advantages over the other padding modes is that same padding preserves the size of the vector\nOne big disadvantage of valid padding versus full and same padding is that the volume of the tensors will decrease substantially in NNs with many layers, which can be detrimental to the network’s performance.\nAs for full padding, its size results in an output larger than the input size. Full padding is usually used in signal processing applications where it is important to minimize boundary effects."
  },
  {
    "objectID": "dl_lec7.html#padding-4",
    "href": "dl_lec7.html#padding-4",
    "title": "Convolutional networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nExample\n\n\nPadding with size \\(p\\), input size \\(n\\) and filter size \\(m\\), \\(m \\leq n\\): \\[\n\\begin{gather}\n  y[i] = \\sum\\limits_{k=0}^{m-1} x^p [i+m-k]w[k]\n\\end{gather}\n\\]\nOutput size of a convolution is determined by: \\[\n\\begin{align*}\n   &o = \\lfloor \\frac{n+2p-m}{s} + 1\\rfloor\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec7.html#pooling-1",
    "href": "dl_lec7.html#pooling-1",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\nDescription\n\n\nPooling works very much like a discrete convolution, but replaces the linear combination described by the kernel with some other function."
  },
  {
    "objectID": "dl_lec7.html#pooling-2",
    "href": "dl_lec7.html#pooling-2",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nAverage pooling."
  },
  {
    "objectID": "dl_lec7.html#pooling-3",
    "href": "dl_lec7.html#pooling-3",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nMax pooling."
  },
  {
    "objectID": "dl_lec7.html#pooling-4",
    "href": "dl_lec7.html#pooling-4",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nMax pooling."
  },
  {
    "objectID": "dl_lec7.html#pooling-5",
    "href": "dl_lec7.html#pooling-5",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nMax/mean pooling"
  },
  {
    "objectID": "dl_lec7.html#pooling-6",
    "href": "dl_lec7.html#pooling-6",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nPooling invariance example"
  },
  {
    "objectID": "dl_lec7.html#pooling-7",
    "href": "dl_lec7.html#pooling-7",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\nProperties\n\n\nThe following properties affect the output size \\(o_j\\) of a pooling layer along axis \\(j\\):\n\n\\(i_j\\): input size along axis \\(j\\),\n\\(k_j\\): pooling window size along axis \\(j\\),\n\\(s_j\\): stride (distance between two consecutive positions of the pooling window) along axis \\(j\\)."
  },
  {
    "objectID": "dl_lec7.html#pooling-8",
    "href": "dl_lec7.html#pooling-8",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\nDefinition\n\n\nA pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs.\n\n\n\n\n\n\nExamples\n\n\n\nmax pooling (Zhou and Chellappa, 1988) operation reports the maximum output within a rectangular neighborhood.\nthe average of a rectangular neighborhood\n\\(L_2\\) norm of a rectangular neighborhood\na weighted average based on the distance from the central pixel."
  },
  {
    "objectID": "dl_lec7.html#pooling-9",
    "href": "dl_lec7.html#pooling-9",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\nWhy?\n\n\nIn order to make the representation approximately invariant to small translations of the input.\nInvariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.\n\n\n\n\n\n\nWhen?\n\n\n\nwe can assume that the layer must be invariant to small translations.\nwe care about whether feature is present at all, not exactly where. For example, eyes on the face."
  },
  {
    "objectID": "dl_lec7.html#pooling-10",
    "href": "dl_lec7.html#pooling-10",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nStride: 1 pixel, width: 3 pixels. Bottom row: shifted right."
  },
  {
    "objectID": "dl_lec7.html#pooling-11",
    "href": "dl_lec7.html#pooling-11",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\n\n\nIf we pool over the outputs of separately parameterized convolutions, the features can learn which transformations to become invariant to."
  },
  {
    "objectID": "dl_lec7.html#pooling-12",
    "href": "dl_lec7.html#pooling-12",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\nPooling with downsampling. Here we use max pooling with a pool width of three and a stride between pools of two. This reduces the representation size by a factor of two, which reduces the computational and statistical burden on the next layer. Note that the rightmost pooling region has a smaller size but must be included if we do not want to ignore some of the detector units."
  },
  {
    "objectID": "dl_lec7.html#pooling-13",
    "href": "dl_lec7.html#pooling-13",
    "title": "Convolutional networks",
    "section": "Pooling",
    "text": "Pooling\n\n\n\nAdvantages\n\n\n\nintroduces a local invariance. This means that small changes in a local neighborhood do not change the result of max-pooling. Therefore, it helps with generating features that are more robust to noise in the input data.\npooling decreases the size of features, which results in higher computational efficiency. Furthermore, reducing the number of features may reduce the degree of overfitting as well."
  },
  {
    "objectID": "dl_lec7.html#architecture",
    "href": "dl_lec7.html#architecture",
    "title": "Convolutional networks",
    "section": "Architecture",
    "text": "Architecture\n\n\n\nLayer stages\n\n\n\nperform several convolutions in parallel to produce a set of linear activations.\neach linear activation is run through a nonlinear activation function, such as the rectified linear activation function (aka the detector stage).\nuse a pooling function to modify the output of the layer further."
  },
  {
    "objectID": "dl_lec7.html#architecture-1",
    "href": "dl_lec7.html#architecture-1",
    "title": "Convolutional networks",
    "section": "Architecture",
    "text": "Architecture\n\nThe components of a typical convolutional neural network layer."
  },
  {
    "objectID": "dl_lec7.html#architecture-2",
    "href": "dl_lec7.html#architecture-2",
    "title": "Convolutional networks",
    "section": "Architecture",
    "text": "Architecture\n\nNumber of parameters for a CNN: \\(m_1 \\times m_2 \\times 3 \\times 5 + 5\\). Number of parameters for a fully-connected NN: \\((n_1 \\times n_2 \\times 3) \\times (n_1 \\times n_2 \\times 5)\\)."
  },
  {
    "objectID": "dl_lec7.html#deep-cnn-example",
    "href": "dl_lec7.html#deep-cnn-example",
    "title": "Convolutional networks",
    "section": "Deep CNN example",
    "text": "Deep CNN example"
  },
  {
    "objectID": "dl_lec7.html#deep-cnn-example-1",
    "href": "dl_lec7.html#deep-cnn-example-1",
    "title": "Convolutional networks",
    "section": "Deep CNN example",
    "text": "Deep CNN example\n\n\n\nLayer dimensions:\n\n\n\nInput: \\([batchsize \\times 28 \\times 28 \\times 1]\\)\nConv_1: \\([batchsize \\times 28 \\times 28 \\times 32]\\)\nPooling_1: \\([batchsize \\times 14 \\times 14 \\times 32]\\)\nConv_2: \\([batchsize \\times 14 \\times 14 \\times 64]\\)\nPooling_2: \\([batchsize \\times 7 \\times 7 \\times 64]\\)\nFC_1: \\([batchsize \\times 1024]\\)\nFC_2 and softmax layer: \\([batchsize \\times 10]\\)"
  },
  {
    "objectID": "dl_lec7.html#example-vgg-16",
    "href": "dl_lec7.html#example-vgg-16",
    "title": "Convolutional networks",
    "section": "Example: VGG-16",
    "text": "Example: VGG-16"
  },
  {
    "objectID": "dl_lec7.html#explainer",
    "href": "dl_lec7.html#explainer",
    "title": "Convolutional networks",
    "section": "Explainer",
    "text": "Explainer\nhttps://poloclub.github.io/cnn-explainer/"
  },
  {
    "objectID": "dl_lab7.html",
    "href": "dl_lab7.html",
    "title": "DL: Lab 7",
    "section": "",
    "text": "Review the Jupyter notebook here\nGrab the dataset from https://huggingface.co/datasets/food101\nImplement a convolutional neural network for multiclass image classification.\n\n\n\n\n\nhttps://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
  },
  {
    "objectID": "dl_lab7.html#task-description",
    "href": "dl_lab7.html#task-description",
    "title": "DL: Lab 7",
    "section": "",
    "text": "Review the Jupyter notebook here\nGrab the dataset from https://huggingface.co/datasets/food101\nImplement a convolutional neural network for multiclass image classification."
  },
  {
    "objectID": "dl_lab7.html#recommended-reading",
    "href": "dl_lab7.html#recommended-reading",
    "title": "DL: Lab 7",
    "section": "",
    "text": "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
  },
  {
    "objectID": "nlp_lab10.html",
    "href": "nlp_lab10.html",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "In this lab we will fetch documents from Reddit manually. After preprocessing we will use PCA for topic modeling.\n\n\nWe will use a Reddit Python library:\npip install praw\nWordcloud:\n!pip install wordcloud\nLibrary docs: https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\n\n\n\nNow we need to setup Reddit data fetching. For this we need to have a Reddit account.\nWe’ll use posts from https://www.reddit.com/r/Paranormal/comments/gc4ive/i_made_another_big_list_of_paranormal_and_mystery/ for inspiration.\nNow, create client_id and client_secret.\nHow?\n\nCreate a Reddit account if you don’t have one yet\nGo to https://www.reddit.com/prefs/apps and click create an app\nPick script app type. Name and description can be anything, but make sure to specify http://localhost:8080 as redirect uri\nPress create app. Your client_id is the string under personal use script and client_secret is the string next to secret\n\nBelow is the function for comment fetching:\nimport praw\nimport pandas as pd\n\n#reddit = praw.Reddit(\n #   client_id = '???',\n  #  client_secret = '???',\n   # user_agent = 'praw'\n\n# This function returns a Pandas dataframe\n# containing reddit post comments, given its id\ndef getComments(id):\n    submission = reddit.submission(id)\n    pandas_list = []\n    commentsList = submission.comments.list()\n    # 'limit' parameter can be increased \n    # in order to expand more comments\n    submission.comments.replace_more(limit=64)\n    print(len(commentsList))\n    i = 0\n    for c in commentsList:\n        if isinstance(c, praw.models.MoreComments):\n            continue\n        commentText = c.body\n        if commentText in {'[deleted]', '[removed]'}:\n            continue\n        pandas_list.append([i, commentText])\n        i+=1\n    df = pd.DataFrame(pandas_list)\n    df.columns = ['id', 'description']\n    return df\nRead reddit comments and put it in a DataFrame named corpus.\ncorpus = getComments('1detli')\n\nprint(corpus.shape)\ncorpus.head()\nprint('First spooky story : ',corpus.loc[0,'description'])\n\n\n\nUse str methods to clean the texts. Save cleaned-up text into a column named clean_description.\n# Remove HTML elements\n\n# CODE_START\n# corpus['clean_description'] = ...\n# CODE_END\n\n# Remove special characters and numbers\n\n# CODE_START\n#corpus['clean_description'] = ...\n# CODE_END\nprint('Description cleaned of the first product : ',corpus.loc[0,'clean_description'])\nTransform characters to lowercase:\n# Lowercase\n# CODE_START\n#corpus['clean_description'] = ...\n# CODE_END\nprint('First story lower-cased : ',corpus.loc[0,'clean_description'])\nUse NLTK to tokenize the documents and put the result in a new column named clean_tokens.\n## Tokenize the cleaned description\n# CODE_START\n#corpus['clean_tokens'] = ...\n# CODE_END\ncorpus.head()\nRemove the stop words and lemmatize clean_tokens. We’ll use the code we had in lab3.\n# Remove stop words\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tag import pos_tag\nstop_words = set(stopwords.words('english'))\n\n\n \nlemmatizer = WordNetLemmatizer()\ndef lemmatize(tokens):\n# CODE_START\n#...\n# CODE_END\n    \n# Lemmatize\n# CODE_START\n#corpus['clean_tokens'] = ...\n# CODE_END\ncorpus.head()\nWrite all the cleaned tokens into one single string and put it in a new column named clean_document.\n# Put back tokens into one single string\n# CODE_START\n#corpus[\"clean_document\"] = ...\n# CODE_END\ncorpus.head()\n\n\n\nWe can use clean_document to compute TF-IDF matrices:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF vector\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(corpus[\"clean_document\"])\n\n# X is a generator. We can transform that as an array\nX = X.toarray()\nprint(X.shape)\n\nEach line of X correspond to a story.\nEach column of X correspond to a word in the vocabulary.\nSo each cell of X correspond to the score TF-IDF for a word in a particular story.\n\n\n\n\nScikit-learn library has a TruncatedSVD class:\nfrom sklearn.decomposition import TruncatedSVD\n# Train SVD model\nsvd_model = TruncatedSVD(n_components=12) # We test on 12 topics\nlsa = svd_model.fit_transform(X)\ntopic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_\" + str(i) for i in range(lsa.shape[1])])\ntopic_encoded_df[\"documents\"] = corpus['clean_description']\ntopic_encoded_df.head()\nCreate a new column named main_topic in topic_encoded_df where we store the main topics related to each document:\nimport numpy as np\n\ndef extract_main_topics(x):\n    \"\"\"\n    Return the main topic for each document. The main topic is the one that has the maximum value for each line\n    \"\"\"\n\n    # CODE_START\n    #...\n    # CODE_END\n    return main_topic\n\n# Initialize column main_topics with 0\ntopic_encoded_df.loc[:, 'main_topic'] = 0\n\nfor i, row in topic_encoded_df.iloc[:,:-2].iterrows():\n    topic_encoded_df.loc[i, 'main_topic'] = extract_main_topics(row)\n\ntopic_encoded_df.head()\nCount each main topic in the corpus:\ntopic_encoded_df['main_topic'].value_counts()\nUse the attribute components_ of the SVD model to print the 5 most important words in each topic.\n# Create DataFrame containing the description of each topic in terms of the words in the vocabulary\ntopics_description = pd.DataFrame(svd_model.components_, columns = vectorizer.get_feature_names_out(), \n                                  index = ['topic_' + str(i) for i in range(svd_model.components_.shape[0])])\n\n# Compute absolute values of coefficients\ntopics_description = topics_description.apply(np.abs, axis = 1)\n\n# Each word is map with a score of relevance for each topic\ntopics_description.head()\n# Loop over each topic and print the 5 most important words\n\n# CODE_START\n#for i,row in ...\n\n# CODE_END\nCreate a wordcloud:\nimport wordcloud\nimport matplotlib.pyplot as plt\n\n# Loop over each topic and create wordcloud from documents that are related to this main topic\nwd = wordcloud.WordCloud()\n\ncols = [c for c in topic_encoded_df.columns if 'topic_' in c]\n\nfor t in cols:\n    print('-------------------------')\n    print()\n    print('TOPIC ', t)\n    \n    # Handle topics that are not main topics for any document in the corpus\n    if (topic_encoded_df['main_topic']==t).any() == False :\n        print('cannot create wordcloud for this topic')\n        continue\n    \n    texts = \" \".join(topic_encoded_df.loc[topic_encoded_df['main_topic']==t,'documents'])\n    cloud = wd.generate(texts)\n    plt.imshow(cloud)\n    plt.show()\n    \n    print()"
  },
  {
    "objectID": "nlp_lab10.html#libraries",
    "href": "nlp_lab10.html#libraries",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "We will use a Reddit Python library:\npip install praw\nWordcloud:\n!pip install wordcloud\nLibrary docs: https://praw.readthedocs.io/en/stable/getting_started/quick_start.html"
  },
  {
    "objectID": "nlp_lab10.html#data-loading",
    "href": "nlp_lab10.html#data-loading",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "Now we need to setup Reddit data fetching. For this we need to have a Reddit account.\nWe’ll use posts from https://www.reddit.com/r/Paranormal/comments/gc4ive/i_made_another_big_list_of_paranormal_and_mystery/ for inspiration.\nNow, create client_id and client_secret.\nHow?\n\nCreate a Reddit account if you don’t have one yet\nGo to https://www.reddit.com/prefs/apps and click create an app\nPick script app type. Name and description can be anything, but make sure to specify http://localhost:8080 as redirect uri\nPress create app. Your client_id is the string under personal use script and client_secret is the string next to secret\n\nBelow is the function for comment fetching:\nimport praw\nimport pandas as pd\n\n#reddit = praw.Reddit(\n #   client_id = '???',\n  #  client_secret = '???',\n   # user_agent = 'praw'\n\n# This function returns a Pandas dataframe\n# containing reddit post comments, given its id\ndef getComments(id):\n    submission = reddit.submission(id)\n    pandas_list = []\n    commentsList = submission.comments.list()\n    # 'limit' parameter can be increased \n    # in order to expand more comments\n    submission.comments.replace_more(limit=64)\n    print(len(commentsList))\n    i = 0\n    for c in commentsList:\n        if isinstance(c, praw.models.MoreComments):\n            continue\n        commentText = c.body\n        if commentText in {'[deleted]', '[removed]'}:\n            continue\n        pandas_list.append([i, commentText])\n        i+=1\n    df = pd.DataFrame(pandas_list)\n    df.columns = ['id', 'description']\n    return df\nRead reddit comments and put it in a DataFrame named corpus.\ncorpus = getComments('1detli')\n\nprint(corpus.shape)\ncorpus.head()\nprint('First spooky story : ',corpus.loc[0,'description'])"
  },
  {
    "objectID": "nlp_lab10.html#preprocessing",
    "href": "nlp_lab10.html#preprocessing",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "Use str methods to clean the texts. Save cleaned-up text into a column named clean_description.\n# Remove HTML elements\n\n# CODE_START\n# corpus['clean_description'] = ...\n# CODE_END\n\n# Remove special characters and numbers\n\n# CODE_START\n#corpus['clean_description'] = ...\n# CODE_END\nprint('Description cleaned of the first product : ',corpus.loc[0,'clean_description'])\nTransform characters to lowercase:\n# Lowercase\n# CODE_START\n#corpus['clean_description'] = ...\n# CODE_END\nprint('First story lower-cased : ',corpus.loc[0,'clean_description'])\nUse NLTK to tokenize the documents and put the result in a new column named clean_tokens.\n## Tokenize the cleaned description\n# CODE_START\n#corpus['clean_tokens'] = ...\n# CODE_END\ncorpus.head()\nRemove the stop words and lemmatize clean_tokens. We’ll use the code we had in lab3.\n# Remove stop words\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tag import pos_tag\nstop_words = set(stopwords.words('english'))\n\n\n \nlemmatizer = WordNetLemmatizer()\ndef lemmatize(tokens):\n# CODE_START\n#...\n# CODE_END\n    \n# Lemmatize\n# CODE_START\n#corpus['clean_tokens'] = ...\n# CODE_END\ncorpus.head()\nWrite all the cleaned tokens into one single string and put it in a new column named clean_document.\n# Put back tokens into one single string\n# CODE_START\n#corpus[\"clean_document\"] = ...\n# CODE_END\ncorpus.head()"
  },
  {
    "objectID": "nlp_lab10.html#tf-idf",
    "href": "nlp_lab10.html#tf-idf",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "We can use clean_document to compute TF-IDF matrices:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF vector\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(corpus[\"clean_document\"])\n\n# X is a generator. We can transform that as an array\nX = X.toarray()\nprint(X.shape)\n\nEach line of X correspond to a story.\nEach column of X correspond to a word in the vocabulary.\nSo each cell of X correspond to the score TF-IDF for a word in a particular story."
  },
  {
    "objectID": "nlp_lab10.html#lsa",
    "href": "nlp_lab10.html#lsa",
    "title": "NLP: Lab 10 (PCA)",
    "section": "",
    "text": "Scikit-learn library has a TruncatedSVD class:\nfrom sklearn.decomposition import TruncatedSVD\n# Train SVD model\nsvd_model = TruncatedSVD(n_components=12) # We test on 12 topics\nlsa = svd_model.fit_transform(X)\ntopic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_\" + str(i) for i in range(lsa.shape[1])])\ntopic_encoded_df[\"documents\"] = corpus['clean_description']\ntopic_encoded_df.head()\nCreate a new column named main_topic in topic_encoded_df where we store the main topics related to each document:\nimport numpy as np\n\ndef extract_main_topics(x):\n    \"\"\"\n    Return the main topic for each document. The main topic is the one that has the maximum value for each line\n    \"\"\"\n\n    # CODE_START\n    #...\n    # CODE_END\n    return main_topic\n\n# Initialize column main_topics with 0\ntopic_encoded_df.loc[:, 'main_topic'] = 0\n\nfor i, row in topic_encoded_df.iloc[:,:-2].iterrows():\n    topic_encoded_df.loc[i, 'main_topic'] = extract_main_topics(row)\n\ntopic_encoded_df.head()\nCount each main topic in the corpus:\ntopic_encoded_df['main_topic'].value_counts()\nUse the attribute components_ of the SVD model to print the 5 most important words in each topic.\n# Create DataFrame containing the description of each topic in terms of the words in the vocabulary\ntopics_description = pd.DataFrame(svd_model.components_, columns = vectorizer.get_feature_names_out(), \n                                  index = ['topic_' + str(i) for i in range(svd_model.components_.shape[0])])\n\n# Compute absolute values of coefficients\ntopics_description = topics_description.apply(np.abs, axis = 1)\n\n# Each word is map with a score of relevance for each topic\ntopics_description.head()\n# Loop over each topic and print the 5 most important words\n\n# CODE_START\n#for i,row in ...\n\n# CODE_END\nCreate a wordcloud:\nimport wordcloud\nimport matplotlib.pyplot as plt\n\n# Loop over each topic and create wordcloud from documents that are related to this main topic\nwd = wordcloud.WordCloud()\n\ncols = [c for c in topic_encoded_df.columns if 'topic_' in c]\n\nfor t in cols:\n    print('-------------------------')\n    print()\n    print('TOPIC ', t)\n    \n    # Handle topics that are not main topics for any document in the corpus\n    if (topic_encoded_df['main_topic']==t).any() == False :\n        print('cannot create wordcloud for this topic')\n        continue\n    \n    texts = \" \".join(topic_encoded_df.loc[topic_encoded_df['main_topic']==t,'documents'])\n    cloud = wd.generate(texts)\n    plt.imshow(cloud)\n    plt.show()\n    \n    print()"
  },
  {
    "objectID": "dl_lec4.html#parameters-to-tinker-with",
    "href": "dl_lec4.html#parameters-to-tinker-with",
    "title": "Regularization and Optimization",
    "section": "Parameters to tinker with",
    "text": "Parameters to tinker with\n\n\n\nHyperparameters\n\n\n\nnumber of layers\nnumber of hidden units in each layer\nlearning rates\nactivation functions for different layers"
  },
  {
    "objectID": "dl_lec4.html#train-dev-test-sets",
    "href": "dl_lec4.html#train-dev-test-sets",
    "title": "Regularization and Optimization",
    "section": "Train / Dev / Test sets",
    "text": "Train / Dev / Test sets\n\n\nused to be 60/20/20%\nnow it’s more like 98/1/1%.\n\n\n\n\n\n\n\nImportant\n\n\nTrain and dev sets should come from the same distribution."
  },
  {
    "objectID": "dl_lec4.html#biasvariance",
    "href": "dl_lec4.html#biasvariance",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance\n\n\nHigh Bias Simple hypothesis, not able to train properly on the training set and test set both. This is Underfitting.\nHigh Variance. Very complex hypothesis, not able to generalise. Will perform great on training data and poor on the test data. This is Overfitting.\nJust Right: The glorious balance"
  },
  {
    "objectID": "dl_lec4.html#biasvariance-1",
    "href": "dl_lec4.html#biasvariance-1",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance"
  },
  {
    "objectID": "dl_lec4.html#biasvariance-2",
    "href": "dl_lec4.html#biasvariance-2",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance\n\n\n\nExamples\n\n\n\nif training set error is 1%, and dev set error is 11%, then we say we have high variance.\nIf training set error is 15%, and dev set error is 16%, then we say we have high bias.\nIf training set error is 15%, and dev set error is 30%, then we say we have both high bias and high variance.\nIf training set error is 0.5%, and dev set error is 1%, then we say we have both low bias and low variance.\n\n\n\n\n\n\nThe above analysis is based on the assumption that optimal (Bayes) error is 0%."
  },
  {
    "objectID": "dl_lec4.html#high-bias",
    "href": "dl_lec4.html#high-bias",
    "title": "Regularization and Optimization",
    "section": "High bias",
    "text": "High bias\n\n\n\nSolutions for high bias\n\n\n\nbigger network\ntrain longer\nNN architecture search"
  },
  {
    "objectID": "dl_lec4.html#high-variance",
    "href": "dl_lec4.html#high-variance",
    "title": "Regularization and Optimization",
    "section": "High variance",
    "text": "High variance\n\n\n\nSolutions for high variance\n\n\n\nget more data\nregularization in order to reduce overfitting\nNN architecture search\n\n\n\n\n\n\n\nBias-variance tradeoff\n\n\nA machine learning concept. Bigger network and getting more data help to solve the tradeoff problem."
  },
  {
    "objectID": "dl_lec4.html#regularization-1",
    "href": "dl_lec4.html#regularization-1",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nDefinition\n\n\nRegularization is a strategy used in machine/deep learning designed to reduce the test error, possibly at the expense of increased training error.\n\n\n\n\n\n\nDefinition 2\n\n\nRegularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error."
  },
  {
    "objectID": "dl_lec4.html#regularization-2",
    "href": "dl_lec4.html#regularization-2",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization"
  },
  {
    "objectID": "dl_lec4.html#regularization-penalty-based",
    "href": "dl_lec4.html#regularization-penalty-based",
    "title": "Regularization and Optimization",
    "section": "Regularization: penalty-based",
    "text": "Regularization: penalty-based\n\\[\\begin{align*}\n  &\\hat{y} = \\sum\\limits_{i=0}^d w_i x_i,  \\\\\n  &L = \\sum (y-\\hat{y})^2\n\\end{align*}\\]\n\n\n\nSoft penalty\n\n\nLarger value of \\(d\\) increases overfitting. Decreasing \\(d\\) = economy of parameters.\nInstead of reducing a number of parameters, we can apply a soft penalty."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\n\\(L_2\\) regularization\n\n\nConsider logistic regression. We introduce an additional summand:\n\\(J(w, b) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m}\\|w\\|_2^2\\)\n\\(\\|w\\|_2^2 = \\sum\\limits_{j=1}^{n_x} w_j^2 = w^T w\\).\n\\(\\lambda\\) is called a regularization parameter."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties-1",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties-1",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\nWhy don’t we regularize \\(b\\)\n\n\n\nbecause it’s just a single parameter, compared to multiple in \\(w\\).\nthe biases typically require less data than the weights to fit accurately.\nfitting the weight well requires observing both variables in a variety of conditions.\neach bias controls only a single variable.\nthis means that we do not induce too much variance by leaving the biases unregularized.\nalso, regularizing the bias parameters can introduce a significant amount of underfitting."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties-2",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties-2",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\n\\(L_1\\) regularization\n\n\n\\(J(w, b) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) +\\dfrac{\\lambda}{2m}\\|w\\|_1\\).\nHere \\(\\|w\\|_1 =\\sum\\limits_{j=1}^{n_x} |w_j|\\).\nHere we end up having sparse vectors for \\(w\\) - meaning, they will contain lots of zeroes."
  },
  {
    "objectID": "dl_lec4.html#regularization-3",
    "href": "dl_lec4.html#regularization-3",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nA general case\n\n\nFor neural network, we add this to the cost function: \\[\nJ(\\vec{W}, \\vec{b}) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m}\\sum\\limits_{l=1}^L\\|\\vec{W}^{[l]}\\|_F^2\n\\] We define the matrix norm (Frobenius norm) as \\[\n\\|\\vec{W}^{[l]}\\|_F^2 = \\sum\\limits_{i=1}^{n^{[l]}}\\sum\\limits_{j=1}^{n^{[l-1]}}\\left(w_{ij}^{[l]}\\right)^2.\n\\]"
  },
  {
    "objectID": "dl_lec4.html#regularization-4",
    "href": "dl_lec4.html#regularization-4",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nA general case\n\n\nNew \\(dW^{[l]}\\) becomes \\[\ndW^{[l]} = (\\text{old one}) + \\dfrac{\\lambda}{m} W^{[l]}.\n\\]\n\n\n\n\n\n\nWeight decay\n\n\n\\(L_2\\) regularization is sometimes called weight decay. The gradient descent step: \\[\\begin{align*}\n&\\vec{W}^{[l]} = \\vec{W}^{[l]}-\\alpha\\left((\\text{old one}) + \\dfrac{\\lambda}{m} \\vec{W}^{[l]}\\right) = \\\\\n&= \\vec{W}^{[l]}\\left(1-\\dfrac{\\alpha \\lambda}{m}\\right) - \\alpha(\\text{old one}).\n\\end{align*}\\]\n\\(L_2\\) regularizer encourages weight values to decay towards \\(0\\)."
  },
  {
    "objectID": "dl_lec4.html#regularization-5",
    "href": "dl_lec4.html#regularization-5",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\n\\(L_1\\) vs \\(L_2\\)\n\n\n\naccuracy: \\(L_2\\) wins\n\\(L_1\\) creates sparse vectors (lots of \\(w_i\\)s are \\(0\\))\nthis means these components are dropped\ntherefore \\(L_1\\) regularizer acts as a feature selector"
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?"
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting-1",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting-1",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?\n\n\\(w^*\\) - minimum error for \\(\\lambda=0\\).\nwhen \\(\\lambda\\) &gt; 0, the minimum of the regularized error function \\(E(w) + \\lambda(w_1^2 + w_2^2)\\) is shifted towards the origin.\nThis shift is greater in the direction of \\(w_1\\) because the unregularized error is relatively insensitive to the parameter value, and less in direction \\(w_2\\) where the error is more strongly dependent on the parameter value.\nThe regularization term is effectively suppressing parameters that have only a small effect on the accuracy of the network predictions."
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting-2",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting-2",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?\n\nsetting large \\(\\lambda\\) helps to reduce \\(\\|w\\|\\) to zero.\ntherefore, s will also be close to zero.\n\\(L_2\\)-regularization relies on the assumption that a model with small weights is simpler than a model with large weights.\nthus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values.\nit becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes."
  },
  {
    "objectID": "dl_lec4.html#regularization-impact",
    "href": "dl_lec4.html#regularization-impact",
    "title": "Regularization and Optimization",
    "section": "Regularization Impact",
    "text": "Regularization Impact\n\n\n\n\\(L_2\\)-regularization impact\n\n\n\non the cost computation: A regularization term is added to the cost.\non the backpropagation function: There are extra terms in the gradients with respect to weight matrices.\non weights: they end up smaller (“weight decay”): weights are pushed to smaller values."
  },
  {
    "objectID": "dl_lec4.html#ensemble-methods",
    "href": "dl_lec4.html#ensemble-methods",
    "title": "Regularization and Optimization",
    "section": "Ensemble methods",
    "text": "Ensemble methods\n\n\n\nDefinition\n\n\nBagging (short for bootstrap aggregating) is a technique for reducing generalization error by combining several models. The idea is to train several different models separately, then have all the models vote on the output for test examples.\n\n\n\n\n\n\nEnsemble methods\n\n\nThis is an example of a general strategy in machine learning called model averaging.\nTechniques employing this strategy are known as ensemble methods.\n\n\n\n\n\n\nRationale\n\n\nDifferent models will not make same errors on the test set."
  },
  {
    "objectID": "dl_lec4.html#bagging-vs-sampling",
    "href": "dl_lec4.html#bagging-vs-sampling",
    "title": "Regularization and Optimization",
    "section": "Bagging vs sampling",
    "text": "Bagging vs sampling\n\n\n\nBagging\n\n\n\nSample size \\(s\\) = training data size \\(n\\) (classical bagging)\nResampled data will contain duplicates, and a fraction \\((1-1/n)^n \\approx 1/e\\) is not included at all\nBest results obtained with \\(s &lt;&lt; n\\).\n\n\n\n\n\n\n\nSampling\n\n\n\nSample size \\(s\\) &lt; training data size \\(n\\)\nSamples are created without replacement."
  },
  {
    "objectID": "dl_lec4.html#randomized-connection-dropping",
    "href": "dl_lec4.html#randomized-connection-dropping",
    "title": "Regularization and Optimization",
    "section": "Randomized connection dropping",
    "text": "Randomized connection dropping\nAka DropConnect."
  },
  {
    "objectID": "dl_lec4.html#dropout-regularization",
    "href": "dl_lec4.html#dropout-regularization",
    "title": "Regularization and Optimization",
    "section": "Dropout Regularization",
    "text": "Dropout Regularization\n\n\n\nOverview\n\n\n\nfor each training example, drop a different set of NN nodes.\nthere are several techniques:\n\nactivation scaling\ninverted dropout."
  },
  {
    "objectID": "dl_lec4.html#dropout",
    "href": "dl_lec4.html#dropout",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout"
  },
  {
    "objectID": "dl_lec4.html#dropout-1",
    "href": "dl_lec4.html#dropout-1",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout"
  },
  {
    "objectID": "dl_lec4.html#dropout-regularization-1",
    "href": "dl_lec4.html#dropout-regularization-1",
    "title": "Regularization and Optimization",
    "section": "Dropout Regularization",
    "text": "Dropout Regularization\n\n\n\nInverted dropout\n\n\nCreate a random matrix e.g. for layer 3:\n\n\\[\\begin{align*}\n&d3 = np.random.randn(a3.shape[0], a3.shape[1]) &lt; keep\\_prob \\\\\n&a3 = np.multiply(a3, d3) \\\\\n&a3 /= keep\\_prob\n\\end{align*}\\]\nThis ensures that the expected value of keep_prob remains the same. At test time we’re not using dropout."
  },
  {
    "objectID": "dl_lec4.html#dropout-2",
    "href": "dl_lec4.html#dropout-2",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout\n\n\n\nFeatures\n\n\n\nNodes cannot rely on any single feature, as they might go away randomly, so it has to spread out weights.\nSpreading out weights will shrink the squared norm of the weights.\nIt’s possible to vary keep_prob by layer.\nDropout is often used in computer vision, as we often don’t have enough data.\n\n\n\n\n\n\n\nDownside\n\n\nWe don’t have a well-defined cost function."
  },
  {
    "objectID": "dl_lec4.html#dropout-3",
    "href": "dl_lec4.html#dropout-3",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout\n\n\n\nNotes\n\n\n\nDropout is a regularization technique.\nA common mistake when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training.\nYou only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.\nApply dropout both during forward and backward propagation.\nDuring training time, divide each dropout layer by keep_prob to keep the same expected value for the activations."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods",
    "href": "dl_lec4.html#other-regularization-methods",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nData augmentation\n\n\nFor example, flip images to generate extra training samples. Or do random distortions."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-1",
    "href": "dl_lec4.html#other-regularization-methods-1",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nEarly stopping\n\n\n\nPlot gradient descent. On \\(x\\) axis we’ll have number of iterations, on \\(y\\) axis - cost.\nPlot both train set error and dev set error\nAnd stop before they start diverging."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-2",
    "href": "dl_lec4.html#other-regularization-methods-2",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nOrthogonalization\n\n\nOrthogonalization: think about minimizing cost and not overfitting separately.\n\n\n\n\n\n\nDownside\n\n\nDownside of early stopping is that it merges these two tasks."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-3",
    "href": "dl_lec4.html#other-regularization-methods-3",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nEarly stopping\n\n\n\nEvery time the error on the validation set improves, we store a copy of the model parameters.\nWhen the training algorithm terminates, we return these parameters, rather than the latest parameters.\nThe algorithm terminates when no parameters have improved over the best recorded\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIt is probably the most commonly used form of regularization in deep learning. Its popularity is due to both its effectiveness and its simplicity."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-4",
    "href": "dl_lec4.html#other-regularization-methods-4",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\nNoise injection: injecting a matrix of random values from a Gaussian distribution."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs",
    "href": "dl_lec4.html#normalizing-inputs",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\nFirst step - subtract mean. \\[\\begin{align*}\n  & \\mu = \\dfrac{1}{m} \\sum\\limits_{i=1}^m x^{(i)}, \\\\\n  & x := x - \\mu\n\\end{align*}\\]\nThen - normalize variance. \\[\\begin{align*}\n  & \\sigma^2 = \\dfrac{1}{m} \\sum\\limits_{i=1}^m (x^{(i)})^2, \\text{ (element-wise) }\\\\\n  & x := x / \\sigma\n\\end{align*}\\]\nNormalize train and dev sets similarly, using same \\(\\mu\\) and \\(\\sigma\\)."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs-1",
    "href": "dl_lec4.html#normalizing-inputs-1",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\n\nImpact\n\n\n\nNormalizing the features allows the cost function to look more symmetric, as opposed to elongated.\nElongated shape forces a smaller learning_rate."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs-2",
    "href": "dl_lec4.html#normalizing-inputs-2",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\n\nVanishing / Exploding Gradients\n\n\n\nIf we use linear activation function \\(g(z)=z\\), then we can show that \\(y = w^{[L]}*w^{[L-1]}*\\dots*w^{[1]}\\)."
  },
  {
    "objectID": "dl_lec4.html#vanishing-exploding-gradients-1",
    "href": "dl_lec4.html#vanishing-exploding-gradients-1",
    "title": "Regularization and Optimization",
    "section": "Vanishing / Exploding Gradients",
    "text": "Vanishing / Exploding Gradients\nSuppose that weight matrices look like this: \\[\\begin{align*}\n  & w^{[l]} = \\begin{bmatrix}\n    1.5 & 0 \\\\\n    0 & 1.5\n  \\end{bmatrix}\n\\end{align*}\\] Then we’ll have that \\(\\hat{y} = 1.5^{L-1}x\\).\nSo the value of \\(\\hat{y}\\) will explode. Conversely, if we have 0.5s in the weight matrix, then activation values will vanish.\nSame thing will happen to derivatives.\nThis problem can be solved by careful initialization of the weights."
  },
  {
    "objectID": "dl_lec4.html#weight-initialization-for-deep-networks",
    "href": "dl_lec4.html#weight-initialization-for-deep-networks",
    "title": "Regularization and Optimization",
    "section": "Weight Initialization for Deep Networks",
    "text": "Weight Initialization for Deep Networks\nSuppose we have a single neuron.\n\nSo we’ll have \\(z=w_1 x_1 + \\dots + w_n x_n\\).\nThe larger \\(n\\) becomes, the less should the weights \\(w_i\\) be.\nWe can set the variance of w to be \\(\\dfrac{1}{n}\\) for \\(tanh\\) (Xavier initialization) (or \\(\\dfrac{2}{n}\\) for ReLU).\nSometimes also this is used: \\(\\sqrt{\\dfrac{2}{n^{[l-1]}n^{[l]}}}\\). \\[\nw^{[l]} = np.random.randn(w.shape)* np.sqrt(1/n^{[l-1]})\n\\]"
  },
  {
    "objectID": "dl_lec4.html#gradient-computation",
    "href": "dl_lec4.html#gradient-computation",
    "title": "Regularization and Optimization",
    "section": "Gradient computation",
    "text": "Gradient computation\n\n\n\nKinds\n\n\n\nanalytical manual derivation. Time-consuming, error-prone.\nnumeric calculation using finite differences. Scales poorly. Useful for debugging\nsymbolic differentiation. Causes expression swell.\nautodiff"
  },
  {
    "objectID": "dl_lec4.html#expression-swell",
    "href": "dl_lec4.html#expression-swell",
    "title": "Regularization and Optimization",
    "section": "Expression swell",
    "text": "Expression swell\n\n\n\nExample\n\n\n\\[\\begin{align*}\n  & z = h(w_1 x + b_1) \\\\\n  & y = h(w_2 x + b_2) \\\\\n  & h(a) = ln  (1+ exp(a)) \\text{ (soft ReLU)} \\\\\n  & y(x) = h(w_2 h(w_1 x + b_1) + b_2)\\\\\n  &\\dfrac{\\partial y}{\\partial w_1} = \\dfrac{w_2 x exp\\left(w_1 x + b_1 + b_2 + w_2 ln\\left[1+e^{w_1 x + b_1}\\right]\\right)}{(1 + e^{w_1 x + b_1})(1+exp(b_2 + w_2 ln \\left[1+e^{w_1 x + b_1}\\right])}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#autodiff",
    "href": "dl_lec4.html#autodiff",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\n\n\n\nIdea\n\n\nAutomatically generate the code for gradient calculations, based on forward propagation equations.\nIt augments the forward prop code with additional variables."
  },
  {
    "objectID": "dl_lec4.html#autodiff-1",
    "href": "dl_lec4.html#autodiff-1",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\nForward-mode\n\n\n\\[\\begin{align*}\n&f(x_1, x_2) = x_1 x_2 + exp(x_1 x_2) - sin(x_2)  \\\\\n& \\text { for } \\dfrac{\\partial f}{\\partial x_1} \\text {define tangent variables } \\dot{v_i} = \\dfrac{\\partial v_i}{\\partial x_1} \\\\\n& \\dot{v_i} = \\dfrac{\\partial v_i}{\\partial x_1} = \\sum\\limits_{j \\in pa(i)} \\dfrac{\\partial v_j}{\\partial x_1}\\dfrac{\\partial v_i}{\\partial v_j} = \\sum\\limits_{j \\in pa(i)} \\dot{v_j}\\dfrac{\\partial v_i}{\\partial v_j}\\\\\n& pa(i) \\text{ being set of parents to node i}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#autodiff-2",
    "href": "dl_lec4.html#autodiff-2",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\nReverse-mode\n\n\nAugment each intermediate variable \\(v_i\\) with additional varibles \\(\\overline{v_i}\\).\n\\(f\\) - output function. \\[\\begin{align*}\n& \\overline{v_i} = \\dfrac{\\partial f}{\\partial v_i} = \\sum\\limits_{j \\in pa(i)} \\dfrac{\\partial f}{\\partial v_j}\\dfrac{\\partial v_j}{\\partial v_i} = \\sum\\limits_{j \\in pa(i)} \\overline{v_j}\\dfrac{\\partial v_j}{\\partial v_i}\\\\\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nBackpropagation is a special-case of reverse-mode autodiff."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients",
    "href": "dl_lec4.html#numerical-approximation-of-gradients",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nGradient checking\n\n\nWe check analytical gradients numerically.\nGives much better approximation of derivatives (two-sided) \\(f(\\theta+\\epsilon), f(\\theta-\\epsilon)\\) Approximation error becomes \\(O(\\epsilon^2)\\), for one-sided approximation it’s \\(O(\\epsilon)\\).\n\n\n\n\n\n\nProcedure\n\n\nFirst, we take all our parameters \\(W^{[l]}, b^{[l]}\\) and reshape them into one data vector \\(\\theta\\).\nSo the cost function will be transformed in a following way: \\[\\begin{align*}\n  &J(W^{[1]}, b^{[1]},\\dots, W^{[L]}, b^{[L]}) = J(\\theta)\n\\end{align*}\\] Differentials can also be reshaped into a vector \\(d\\theta\\)."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients-1",
    "href": "dl_lec4.html#numerical-approximation-of-gradients-1",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nProcedure\n\n\nThen we compute a differential for each \\(i\\): \\[\\begin{align*}\n  &d\\theta_{approx}[i] = \\dfrac{J(\\theta_1,\\dots, \\theta_i+\\epsilon,\\dots) - J(\\theta_1,\\dots, \\theta_i-\\epsilon,\\dots)}{2\\epsilon} \\approx d\\theta[i].\n\\end{align*}\\] How do we check? \\[\nval = \\dfrac{\\|d\\theta_{approx}-d\\theta\\|_2}{\\|d\\theta_{approx}\\|^2 +\\|d\\theta\\|^2}\n\\]\n\n\n\n\n\n\nOK\n\n\nIf \\(\\epsilon = 10^{-7}\\) and \\(val=10^{-7}\\), then everything’s great.\n\n\n\n\n\n\nNot OK\n\n\nIf val is big, gradients should be rechecked."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients-2",
    "href": "dl_lec4.html#numerical-approximation-of-gradients-2",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nNotes\n\n\n\nOnly compute \\(d\\theta_{approx}\\) in debug mode.\nDon’t forget about regularization.\nGrad check doesn’t work with dropout\nTry running at random initialization"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent",
    "href": "dl_lec4.html#mini-batch-gradient-descent",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\nIf we split training sets in smaller sets, we call them mini-batches."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-1",
    "href": "dl_lec4.html#mini-batch-gradient-descent-1",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-2",
    "href": "dl_lec4.html#mini-batch-gradient-descent-2",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\nWe’ll denote first minibatch \\(X^{\\{1\\}}\\). Same for \\(Y\\).\nfor t in 1...5000:\n\\[\\begin{align*}\n  &\\text{forward prop on } X^{\\{t\\}} \\\\\n  &Z^{[1]} = W^{[1]}X^{\\{t\\}} + b^{[1]}\\\\\n  &A^{[1]} = g^{[1]}(Z^{[1]})\\\\\n  &\\vdots \\\\\n  &A^{[l]} = g^{[l]}(Z^{[l]})\\\\\n  &J^{\\{t\\}} = \\dfrac{1}{1000} \\sum L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2 * 1000} \\sum \\|w^{[l]}\\|^2_F \\\\\n  &\\text{backward prop to compute gradients  of } J^{\\{t\\}}\\\\\n  & w^{[l]} = w^{[l]} - \\alpha dw^{[l]},\\, b^{[l]} = b^{[l]} - \\alpha db^{[l]}\n\\end{align*}\\] So we take 5000 gradient descent steps in one epoch."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-3",
    "href": "dl_lec4.html#mini-batch-gradient-descent-3",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\n\nNote\n\n\nMini-batch cost will have oscillations, depending on characteristics of mini-batches."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-4",
    "href": "dl_lec4.html#mini-batch-gradient-descent-4",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\nIf mini-batch size = \\(m\\), we end up with batch gradient descent.\nIf mini-batch size = \\(1\\), we end up with stochastic gradient descent. Every example is its own mini-batch.\n\n\n\n\n\n\n\nNote\n\n\nStochastic GD will oscillate a lot. Disadvantage is that we lose the speedup from vectorization."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-5",
    "href": "dl_lec4.html#mini-batch-gradient-descent-5",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\nGuidelines for mini-batch size\n\n\n\nfor small training sets (&lt; 2000), just use batch GD\ntypical sizes would be 64, 128, 256, 512, etc.\ntraining set \\(X^{\\{t\\}}\\) should fit in CPU/GPU memory\nmini-batch size is another hyperparameter"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-6",
    "href": "dl_lec4.html#mini-batch-gradient-descent-6",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\nMomentum: intuition\n\n\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence.\nUsing momentum can reduce these oscillations.\nMomentum takes into account the past gradients to smooth out the update. The ‘direction’ of the previous gradients is stored in the variable.\nFormally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of as the “velocity” of a ball rolling downhill."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages",
    "href": "dl_lec4.html#exponentially-weighted-averages",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\n\n\n\nSpeed\n\n\nFaster than GD.\n\n\n\n\nExponentially weighted (moving) averages for yearly temperatures: \\[\nV_t = \\beta*V_{t-1} + (1-\\beta)*\\theta_t\n\\] where \\(v_0=0\\), \\(\\theta_i\\) is temperature on day \\(i\\)."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-1",
    "href": "dl_lec4.html#exponentially-weighted-averages-1",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\\(V_t\\) averages over last \\(\\dfrac{1}{1-\\beta}\\) temperature.\nE.g., if \\(\\beta=0.9\\), then \\(V_t\\) averages over last 10 days.\n\n\n\nHigh values\n\n\nWith high \\(\\beta\\) values, we get a much smoother plot, but shifted to the right, because large \\(\\beta\\) values cause slower adaptation of the graph.\n\n\n\n\n\n\nLow values\n\n\nWith smaller \\(\\beta\\) values, the graph is noisier, but it adapts faster."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-2",
    "href": "dl_lec4.html#exponentially-weighted-averages-2",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\nProcedure\n\n\nRecursively expand \\(V_{100}\\): \\[\nV_{100} = \\sum\\limits_{i=1}^100 (1-\\beta)\\beta^{100-i} \\theta_i\n\\] All these coefficients above add up to a number close to \\(1\\).\nWe multiply daily temperature with an exponentially decaying function. \\[\n(1-\\epsilon)^{\\dfrac{1}{\\epsilon}} = \\dfrac{1}{e}.\n\\]"
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-3",
    "href": "dl_lec4.html#exponentially-weighted-averages-3",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\n\n\n\nImplementation\n\n\n\\[\nv_{\\theta} := \\beta v_{\\theta} + (1-\\beta)\\theta_i\n\\] Very efficient from computation and memory efficiency points of view."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-4",
    "href": "dl_lec4.html#exponentially-weighted-averages-4",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\nBias Correction in Exponentially Weighted Averages\n\n\nThe problem is that the curves starts really low.\n\n\n\n\n\n\n\n\n\n\nSuggestion\n\n\nDivide \\(v_t\\) by \\(1-\\beta^t\\)."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum",
    "href": "dl_lec4.html#gradient-descent-with-momentum",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\n\n\n\nBasic idea\n\n\nTo compute exponentially weighted average of gradients and use that in GD update step. Helps the Gradient Descent process in navigating flat regions and local optima."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-1",
    "href": "dl_lec4.html#gradient-descent-with-momentum-1",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\nProcedure\n\n\nOn iteration \\(t\\) we compute \\(v_{dw} = \\beta v_{dw} + (1-\\beta)dw\\).\nIn this formula, we can think of first summand as velocity, second - as acceleration, \\(\\beta\\) - as friction.\nSimilarly, we compute \\(v_{db}\\).\nThen we update the weights as follows: \\[\nW := W - \\alpha v_{dW}, \\, b := b - \\alpha v_{db}\n\\] This smoothes out the GD steps. Results in much smaller oscillations in vertical direction, while moving horizontally."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-2",
    "href": "dl_lec4.html#gradient-descent-with-momentum-2",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum"
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-3",
    "href": "dl_lec4.html#gradient-descent-with-momentum-3",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\nImplementation details\n\n\nOn iteration \\(t\\):\n\ncompute \\(dW\\), \\(db\\) on the current mini-batch\n\\(v_{dW} = \\beta v_{dW} + (1-\\beta)dW\\),\n\\(v_{db} = \\beta v_{db} + (1-\\beta)db\\),\n\\(v_{db} = \\beta v_{db} + (1-\\beta)db\\),\n\\(W := W - \\alpha v_{dW}, \\, b := b - \\alpha v_{db}\\)\n\n\n\n\n\n\n\nNote\n\n\nWe have \\(\\alpha\\) and \\(\\beta\\) as hyperparameters.\nSometimes \\(1-\\beta\\) is omitted."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-4",
    "href": "dl_lec4.html#gradient-descent-with-momentum-4",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\n\n\n\nImportant notes\n\n\n\nMomentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with:\n\nbatch gradient descent\nmini-batch gradient descent\nor stochastic gradient descent.\n\nYou have to tune a momentum hyperparameter \\(\\beta\\) and a learning rate \\(\\alpha\\)."
  },
  {
    "objectID": "dl_lec4.html#rmsprop",
    "href": "dl_lec4.html#rmsprop",
    "title": "Regularization and Optimization",
    "section": "RMSprop",
    "text": "RMSprop\n\n\n\nDefinition: RMSprop: acronym for “Root Mean Square prop”.\n\n\nVertical oscillations - this is parameter \\(b\\). \\(w\\) is horizontal direction. \\[\\begin{align*}\n  &s_{dW} = \\beta s_{dW} + (1-\\beta)dW^2, \\; s_{db} = \\beta s_{db} + (1-\\beta)db^2, \\\\\n  &W := W - \\alpha \\dfrac{dW}{\\sqrt{s_{dw}+\\epsilon}}, \\, b := b - \\alpha \\dfrac{db}{\\sqrt{s_{db}+\\epsilon}}\n\\end{align*}\\]\n\n\n\n\n\n\nIntuition\n\n\n\n\\(db^2\\) is large, \\(dW^2\\) is small\nSo updates in vertical direction are divided by larger number.\nThus we can use larger \\(\\alpha\\).\n\nWe will denote the parameter \\(\\beta_2\\)."
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm",
    "href": "dl_lec4.html#adam-optimization-algorithm",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nDescription\n\n\nWorks across a wide range of DL architectures. It’s a combination of GD with momentum and RMSprop.\nAdam stands for ADAptive Moment estimation.\n\ncalculate an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction).\ncalculate an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction).\nupdate parameters in a direction based on combining information from previous steps."
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-1",
    "href": "dl_lec4.html#adam-optimization-algorithm-1",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\n\\[\\begin{align*}\n   & v_{dW}=0,\\, s_{dW}=0 \\\\\n   & v_{db}=0,\\, s_{db}=0 \\\\\n   & \\text{on iteration } t \\text{ compute } dW, db \\\\\n   & v_{dW} = \\beta_1 v_{dW} + (1-\\beta_1)dW,\\\\\n   & v_{db} = \\beta_1 v_{db} + (1-\\beta_1)db,\\\\\n   & s_{dW} = \\beta_2 s_{dW} + (1-\\beta_2)dW^2,\\\\\n   & v_{db} = \\beta_2 v_{db} + (1-\\beta_2)db^2\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-2",
    "href": "dl_lec4.html#adam-optimization-algorithm-2",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\nWe also implement bias correction: \\[\\begin{align*}\n   &V_{dW}^{corrected} = \\dfrac{V_{dW}}{1-\\beta_1^t},\\\\\n   &V_{db}^{corrected} = \\dfrac{V_{db}}{1-\\beta_1^t},\\\\\n   &S_{dW}^{corrected} = \\dfrac{S_{dW}}{1-\\beta_2^t},\\\\\n   &S_{db}^{corrected} = \\dfrac{S_{db}}{1-\\beta_2^t},\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-3",
    "href": "dl_lec4.html#adam-optimization-algorithm-3",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\nAnd the update steps: \\[\\begin{align*}\n&W := W - \\alpha \\dfrac{V_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected}} + \\epsilon},\\\\   \n&b := b - \\alpha \\dfrac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}} + \\epsilon}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-4",
    "href": "dl_lec4.html#adam-optimization-algorithm-4",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nChoice of hyperparameters\n\n\n\n\\(\\alpha\\) needs to be tuned\n\\(\\beta_1\\) usually is \\(0.1\\)\n\\(\\beta_2\\) recommended to be \\(0.999\\) by authors of the Adam paper\nchoice of \\(\\epsilon\\) doesn’t matter much, authors of the Adam paper recommend \\(10^{-8}\\)."
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay",
    "href": "dl_lec4.html#learning-rate-decay",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\n\n\n\nClarification\n\n\n\\[\n\\alpha = \\dfrac{1}{1+decayRate\\times epochNumber}\\alpha_0\n\\]\n\n\n\nWe can speed up the learning algorithm by slowly reducing the learning rate over time.\nSuppose we have small mini-batches.\ndecay_rate becomes another hyperparameter.\nAnother option is exponential decay: \\(\\alpha = 0.95^{epochNumber} \\alpha_0\\)."
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-1",
    "href": "dl_lec4.html#learning-rate-decay-1",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nSome other options:\n\n\\(\\alpha = \\dfrac{k}{\\sqrt{epochNumber}} \\alpha_0\\)\n\\(\\alpha = \\dfrac{k}{\\sqrt{t}} \\alpha_0\\)"
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-2",
    "href": "dl_lec4.html#learning-rate-decay-2",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay"
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-3",
    "href": "dl_lec4.html#learning-rate-decay-3",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nManual decay: works if we train on a small number of models.\n\n\n\nFixed interval scheduling"
  },
  {
    "objectID": "dl_lec4.html#the-problem-of-local-optima",
    "href": "dl_lec4.html#the-problem-of-local-optima",
    "title": "Regularization and Optimization",
    "section": "The Problem of Local Optima",
    "text": "The Problem of Local Optima\nIn multidimensional spaces we are much less likely to encounter local optima, but rather saddle points.\nOur understanding of these spaces is still evolving.\nPlateau - a region where a derivative is close to zero for a long time."
  },
  {
    "objectID": "dl_lab4.html",
    "href": "dl_lab4.html",
    "title": "DL: Lab 4",
    "section": "",
    "text": "Lab overview\nImplement various initialization/regularization approaches for a deep neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "nlp_lab13.html",
    "href": "nlp_lab13.html",
    "title": "NLP: Lab 13 (Generating Names with a Character-Level RNN)",
    "section": "",
    "text": "Description\nPlease complete notebook and exercises from https://docs.pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
  }
]