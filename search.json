[
  {
    "objectID": "dl_lab5.html",
    "href": "dl_lab5.html",
    "title": "DL: Lab 5",
    "section": "",
    "text": "Lab overview\nImplement various optimization approaches for a deep neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec4.html#parameters-to-tinker-with",
    "href": "dl_lec4.html#parameters-to-tinker-with",
    "title": "Regularization and Optimization",
    "section": "Parameters to tinker with",
    "text": "Parameters to tinker with\n\n\n\nHyperparameters\n\n\n\nnumber of layers\nnumber of hidden units in each layer\nlearning rates\nactivation functions for different layers"
  },
  {
    "objectID": "dl_lec4.html#train-dev-test-sets",
    "href": "dl_lec4.html#train-dev-test-sets",
    "title": "Regularization and Optimization",
    "section": "Train / Dev / Test sets",
    "text": "Train / Dev / Test sets\n\n\nused to be 60/20/20%\nnow it’s more like 98/1/1%.\n\n\n\n\n\n\n\nImportant\n\n\nTrain and dev sets should come from the same distribution."
  },
  {
    "objectID": "dl_lec4.html#biasvariance",
    "href": "dl_lec4.html#biasvariance",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance\n\n\nHigh Bias Simple hypothesis, not able to train properly on the training set and test set both. This is Underfitting.\nHigh Variance. Very complex hypothesis, not able to generalise. Will perform great on training data and poor on the test data. This is Overfitting.\nJust Right: The glorious balance"
  },
  {
    "objectID": "dl_lec4.html#biasvariance-1",
    "href": "dl_lec4.html#biasvariance-1",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance"
  },
  {
    "objectID": "dl_lec4.html#biasvariance-2",
    "href": "dl_lec4.html#biasvariance-2",
    "title": "Regularization and Optimization",
    "section": "Bias/Variance",
    "text": "Bias/Variance\n\n\n\nExamples\n\n\n\nif training set error is 1%, and dev set error is 11%, then we say we have high variance.\nIf training set error is 15%, and dev set error is 16%, then we say we have high bias.\nIf training set error is 15%, and dev set error is 30%, then we say we have both high bias and high variance.\nIf training set error is 0.5%, and dev set error is 1%, then we say we have both low bias and low variance.\n\n\n\n\n\n\nThe above analysis is based on the assumption that optimal (Bayes) error is 0%."
  },
  {
    "objectID": "dl_lec4.html#high-bias",
    "href": "dl_lec4.html#high-bias",
    "title": "Regularization and Optimization",
    "section": "High bias",
    "text": "High bias\n\n\n\nSolutions for high bias\n\n\n\nbigger network\ntrain longer\nNN architecture search"
  },
  {
    "objectID": "dl_lec4.html#high-variance",
    "href": "dl_lec4.html#high-variance",
    "title": "Regularization and Optimization",
    "section": "High variance",
    "text": "High variance\n\n\n\nSolutions for high variance\n\n\n\nget more data\nregularization in order to reduce overfitting\nNN architecture search\n\n\n\n\n\n\n\nBias-variance tradeoff\n\n\nA machine learning concept. Bigger network and getting more data help to solve the tradeoff problem."
  },
  {
    "objectID": "dl_lec4.html#regularization-1",
    "href": "dl_lec4.html#regularization-1",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nDefinition\n\n\nRegularization is a strategy used in machine/deep learning designed to reduce the test error, possibly at the expense of increased training error.\n\n\n\n\n\n\nDefinition 2\n\n\nRegularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error."
  },
  {
    "objectID": "dl_lec4.html#regularization-2",
    "href": "dl_lec4.html#regularization-2",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization"
  },
  {
    "objectID": "dl_lec4.html#regularization-penalty-based",
    "href": "dl_lec4.html#regularization-penalty-based",
    "title": "Regularization and Optimization",
    "section": "Regularization: penalty-based",
    "text": "Regularization: penalty-based\n\\[\\begin{align*}\n  &\\hat{y} = \\sum\\limits_{i=0}^d w_i x_i,  \\\\\n  &L = \\sum (y-\\hat{y})^2\n\\end{align*}\\]\n\n\n\nSoft penalty\n\n\nLarger value of \\(d\\) increases overfitting. Decreasing \\(d\\) = economy of parameters.\nInstead of reducing a number of parameters, we can apply a soft penalty."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\n\\(L_2\\) regularization\n\n\nConsider logistic regression. We introduce an additional summand:\n\\(J(w, b) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m}\\|w\\|_2^2\\)\n\\(\\|w\\|_2^2 = \\sum\\limits_{j=1}^{n_x} w_j^2 = w^T w\\).\n\\(\\lambda\\) is called a regularization parameter."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties-1",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties-1",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\nWhy don’t we regularize \\(b\\)\n\n\n\nbecause it’s just a single parameter, compared to multiple in \\(w\\).\nthe biases typically require less data than the weights to fit accurately.\nfitting the weight well requires observing both variables in a variety of conditions.\neach bias controls only a single variable.\nthis means that we do not induce too much variance by leaving the biases unregularized.\nalso, regularizing the bias parameters can introduce a significant amount of underfitting."
  },
  {
    "objectID": "dl_lec4.html#regularization-parameter-norm-penalties-2",
    "href": "dl_lec4.html#regularization-parameter-norm-penalties-2",
    "title": "Regularization and Optimization",
    "section": "Regularization: parameter norm penalties",
    "text": "Regularization: parameter norm penalties\n\n\n\n\\(L_1\\) regularization\n\n\n\\(J(w, b) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) +\\dfrac{\\lambda}{2m}\\|w\\|_1\\).\nHere \\(\\|w\\|_1 =\\sum\\limits_{j=1}^{n_x} |w_j|\\).\nHere we end up having sparse vectors for \\(w\\) - meaning, they will contain lots of zeroes."
  },
  {
    "objectID": "dl_lec4.html#regularization-3",
    "href": "dl_lec4.html#regularization-3",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nA general case\n\n\nFor neural network, we add this to the cost function: \\[\nJ(\\vec{W}, \\vec{b}) = \\frac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m}\\sum\\limits_{l=1}^L\\|\\vec{W}^{[l]}\\|_F^2\n\\] We define the matrix norm (Frobenius norm) as \\[\n\\|\\vec{W}^{[l]}\\|_F^2 = \\sum\\limits_{i=1}^{n^{[l]}}\\sum\\limits_{j=1}^{n^{[l-1]}}\\left(w_{ij}^{[l]}\\right)^2.\n\\]"
  },
  {
    "objectID": "dl_lec4.html#regularization-4",
    "href": "dl_lec4.html#regularization-4",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nA general case\n\n\nNew \\(dW^{[l]}\\) becomes \\[\ndW^{[l]} = (\\text{old one}) + \\dfrac{\\lambda}{m} W^{[l]}.\n\\]\n\n\n\n\n\n\nWeight decay\n\n\n\\(L_2\\) regularization is sometimes called weight decay. The gradient descent step: \\[\\begin{align*}\n&\\vec{W}^{[l]} = \\vec{W}^{[l]}-\\alpha\\left((\\text{old one}) + \\dfrac{\\lambda}{m} \\vec{W}^{[l]}\\right) = \\\\\n&= \\vec{W}^{[l]}\\left(1-\\dfrac{\\alpha \\lambda}{m}\\right) - \\alpha(\\text{old one}).\n\\end{align*}\\]\n\\(L_2\\) regularizer encourages weight values to decay towards \\(0\\)."
  },
  {
    "objectID": "dl_lec4.html#regularization-5",
    "href": "dl_lec4.html#regularization-5",
    "title": "Regularization and Optimization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\n\\(L_1\\) vs \\(L_2\\)\n\n\n\naccuracy: \\(L_2\\) wins\n\\(L_1\\) creates sparse vectors (lots of \\(w_i\\)s are \\(0\\))\nthis means these components are dropped\ntherefore \\(L_1\\) regularizer acts as a feature selector"
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?"
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting-1",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting-1",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?\n\n\\(w^*\\) - minimum error for \\(\\lambda=0\\).\nwhen \\(\\lambda\\) &gt; 0, the minimum of the regularized error function \\(E(w) + \\lambda(w_1^2 + w_2^2)\\) is shifted towards the origin.\nThis shift is greater in the direction of \\(w_1\\) because the unregularized error is relatively insensitive to the parameter value, and less in direction \\(w_2\\) where the error is more strongly dependent on the parameter value.\nThe regularization term is effectively suppressing parameters that have only a small effect on the accuracy of the network predictions."
  },
  {
    "objectID": "dl_lec4.html#why-regularization-reduces-overfitting-2",
    "href": "dl_lec4.html#why-regularization-reduces-overfitting-2",
    "title": "Regularization and Optimization",
    "section": "Why Regularization Reduces Overfitting?",
    "text": "Why Regularization Reduces Overfitting?\n\nsetting large \\(\\lambda\\) helps to reduce \\(\\|w\\|\\) to zero.\ntherefore, s will also be close to zero.\n\\(L_2\\)-regularization relies on the assumption that a model with small weights is simpler than a model with large weights.\nthus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values.\nit becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes."
  },
  {
    "objectID": "dl_lec4.html#regularization-impact",
    "href": "dl_lec4.html#regularization-impact",
    "title": "Regularization and Optimization",
    "section": "Regularization Impact",
    "text": "Regularization Impact\n\n\n\n\\(L_2\\)-regularization impact\n\n\n\non the cost computation: A regularization term is added to the cost.\non the backpropagation function: There are extra terms in the gradients with respect to weight matrices.\non weights: they end up smaller (“weight decay”): weights are pushed to smaller values."
  },
  {
    "objectID": "dl_lec4.html#ensemble-methods",
    "href": "dl_lec4.html#ensemble-methods",
    "title": "Regularization and Optimization",
    "section": "Ensemble methods",
    "text": "Ensemble methods\n\n\n\nDefinition\n\n\nBagging (short for bootstrap aggregating) is a technique for reducing generalization error by combining several models. The idea is to train several different models separately, then have all the models vote on the output for test examples.\n\n\n\n\n\n\nEnsemble methods\n\n\nThis is an example of a general strategy in machine learning called model averaging.\nTechniques employing this strategy are known as ensemble methods.\n\n\n\n\n\n\nRationale\n\n\nDifferent models will not make same errors on the test set."
  },
  {
    "objectID": "dl_lec4.html#bagging-vs-sampling",
    "href": "dl_lec4.html#bagging-vs-sampling",
    "title": "Regularization and Optimization",
    "section": "Bagging vs sampling",
    "text": "Bagging vs sampling\n\n\n\nBagging\n\n\n\nSample size \\(s\\) = training data size \\(n\\) (classical bagging)\nResampled data will contain duplicates, and a fraction \\((1-1/n)^n \\approx 1/e\\) is not included at all\nBest results obtained with \\(s &lt;&lt; n\\).\n\n\n\n\n\n\n\nSampling\n\n\n\nSample size \\(s\\) &lt; training data size \\(n\\)\nSamples are created without replacement."
  },
  {
    "objectID": "dl_lec4.html#randomized-connection-dropping",
    "href": "dl_lec4.html#randomized-connection-dropping",
    "title": "Regularization and Optimization",
    "section": "Randomized connection dropping",
    "text": "Randomized connection dropping\nAka DropConnect."
  },
  {
    "objectID": "dl_lec4.html#dropout-regularization",
    "href": "dl_lec4.html#dropout-regularization",
    "title": "Regularization and Optimization",
    "section": "Dropout Regularization",
    "text": "Dropout Regularization\n\n\n\nOverview\n\n\n\nfor each training example, drop a different set of NN nodes.\nthere are several techniques:\n\nactivation scaling\ninverted dropout."
  },
  {
    "objectID": "dl_lec4.html#dropout",
    "href": "dl_lec4.html#dropout",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout"
  },
  {
    "objectID": "dl_lec4.html#dropout-1",
    "href": "dl_lec4.html#dropout-1",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout"
  },
  {
    "objectID": "dl_lec4.html#dropout-regularization-1",
    "href": "dl_lec4.html#dropout-regularization-1",
    "title": "Regularization and Optimization",
    "section": "Dropout Regularization",
    "text": "Dropout Regularization\n\n\n\nInverted dropout\n\n\nCreate a random matrix e.g. for layer 3:\n\n\\[\\begin{align*}\n&d3 = np.random.randn(a3.shape[0], a3.shape[1]) &lt; keep\\_prob \\\\\n&a3 = np.multiply(a3, d3) \\\\\n&a3 /= keep\\_prob\n\\end{align*}\\]\nThis ensures that the expected value of keep_prob remains the same. At test time we’re not using dropout."
  },
  {
    "objectID": "dl_lec4.html#dropout-2",
    "href": "dl_lec4.html#dropout-2",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout\n\n\n\nFeatures\n\n\n\nNodes cannot rely on any single feature, as they might go away randomly, so it has to spread out weights.\nSpreading out weights will shrink the squared norm of the weights.\nIt’s possible to vary keep_prob by layer.\nDropout is often used in computer vision, as we often don’t have enough data.\n\n\n\n\n\n\n\nDownside\n\n\nWe don’t have a well-defined cost function."
  },
  {
    "objectID": "dl_lec4.html#dropout-3",
    "href": "dl_lec4.html#dropout-3",
    "title": "Regularization and Optimization",
    "section": "Dropout",
    "text": "Dropout\n\n\n\nNotes\n\n\n\nDropout is a regularization technique.\nA common mistake when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training.\nYou only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.\nApply dropout both during forward and backward propagation.\nDuring training time, divide each dropout layer by keep_prob to keep the same expected value for the activations."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods",
    "href": "dl_lec4.html#other-regularization-methods",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nData augmentation\n\n\nFor example, flip images to generate extra training samples. Or do random distortions."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-1",
    "href": "dl_lec4.html#other-regularization-methods-1",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nEarly stopping\n\n\n\nPlot gradient descent. On \\(x\\) axis we’ll have number of iterations, on \\(y\\) axis - cost.\nPlot both train set error and dev set error\nAnd stop before they start diverging."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-2",
    "href": "dl_lec4.html#other-regularization-methods-2",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nOrthogonalization\n\n\nOrthogonalization: think about minimizing cost and not overfitting separately.\n\n\n\n\n\n\nDownside\n\n\nDownside of early stopping is that it merges these two tasks."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-3",
    "href": "dl_lec4.html#other-regularization-methods-3",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\n\n\n\nEarly stopping\n\n\n\nEvery time the error on the validation set improves, we store a copy of the model parameters.\nWhen the training algorithm terminates, we return these parameters, rather than the latest parameters.\nThe algorithm terminates when no parameters have improved over the best recorded\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIt is probably the most commonly used form of regularization in deep learning. Its popularity is due to both its effectiveness and its simplicity."
  },
  {
    "objectID": "dl_lec4.html#other-regularization-methods-4",
    "href": "dl_lec4.html#other-regularization-methods-4",
    "title": "Regularization and Optimization",
    "section": "Other Regularization Methods",
    "text": "Other Regularization Methods\nNoise injection: injecting a matrix of random values from a Gaussian distribution."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs",
    "href": "dl_lec4.html#normalizing-inputs",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\nFirst step - subtract mean. \\[\\begin{align*}\n  & \\mu = \\dfrac{1}{m} \\sum\\limits_{i=1}^m x^{(i)}, \\\\\n  & x := x - \\mu\n\\end{align*}\\]\nThen - normalize variance. \\[\\begin{align*}\n  & \\sigma^2 = \\dfrac{1}{m} \\sum\\limits_{i=1}^m (x^{(i)})^2, \\text{ (element-wise) }\\\\\n  & x := x / \\sigma\n\\end{align*}\\]\nNormalize train and dev sets similarly, using same \\(\\mu\\) and \\(\\sigma\\)."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs-1",
    "href": "dl_lec4.html#normalizing-inputs-1",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\n\nImpact\n\n\n\nNormalizing the features allows the cost function to look more symmetric, as opposed to elongated.\nElongated shape forces a smaller learning_rate."
  },
  {
    "objectID": "dl_lec4.html#normalizing-inputs-2",
    "href": "dl_lec4.html#normalizing-inputs-2",
    "title": "Regularization and Optimization",
    "section": "Normalizing inputs",
    "text": "Normalizing inputs\n\n\n\nVanishing / Exploding Gradients\n\n\n\nIf we use linear activation function \\(g(z)=z\\), then we can show that \\(y = w^{[L]}*w^{[L-1]}*\\dots*w^{[1]}\\)."
  },
  {
    "objectID": "dl_lec4.html#vanishing-exploding-gradients-1",
    "href": "dl_lec4.html#vanishing-exploding-gradients-1",
    "title": "Regularization and Optimization",
    "section": "Vanishing / Exploding Gradients",
    "text": "Vanishing / Exploding Gradients\nSuppose that weight matrices look like this: \\[\\begin{align*}\n  & w^{[l]} = \\begin{bmatrix}\n    1.5 & 0 \\\\\n    0 & 1.5\n  \\end{bmatrix}\n\\end{align*}\\] Then we’ll have that \\(\\hat{y} = 1.5^{L-1}x\\).\nSo the value of \\(\\hat{y}\\) will explode. Conversely, if we have 0.5s in the weight matrix, then activation values will vanish.\nSame thing will happen to derivatives.\nThis problem can be solved by careful initialization of the weights."
  },
  {
    "objectID": "dl_lec4.html#weight-initialization-for-deep-networks",
    "href": "dl_lec4.html#weight-initialization-for-deep-networks",
    "title": "Regularization and Optimization",
    "section": "Weight Initialization for Deep Networks",
    "text": "Weight Initialization for Deep Networks\nSuppose we have a single neuron.\n\nSo we’ll have \\(z=w_1 x_1 + \\dots + w_n x_n\\).\nThe larger \\(n\\) becomes, the less should the weights \\(w_i\\) be.\nWe can set the variance of w to be \\(\\dfrac{1}{n}\\) for \\(tanh\\) (Xavier initialization) (or \\(\\dfrac{2}{n}\\) for ReLU).\nSometimes also this is used: \\(\\sqrt{\\dfrac{2}{n^{[l-1]}n^{[l]}}}\\). \\[\nw^{[l]} = np.random.randn(w.shape)* np.sqrt(1/n^{[l-1]})\n\\]"
  },
  {
    "objectID": "dl_lec4.html#gradient-computation",
    "href": "dl_lec4.html#gradient-computation",
    "title": "Regularization and Optimization",
    "section": "Gradient computation",
    "text": "Gradient computation\n\n\n\nKinds\n\n\n\nanalytical manual derivation. Time-consuming, error-prone.\nnumeric calculation using finite differences. Scales poorly. Useful for debugging\nsymbolic differentiation. Causes expression swell.\nautodiff"
  },
  {
    "objectID": "dl_lec4.html#expression-swell",
    "href": "dl_lec4.html#expression-swell",
    "title": "Regularization and Optimization",
    "section": "Expression swell",
    "text": "Expression swell\n\n\n\nExample\n\n\n\\[\\begin{align*}\n  & z = h(w_1 x + b_1) \\\\\n  & y = h(w_2 x + b_2) \\\\\n  & h(a) = ln  (1+ exp(a)) \\text{ (soft ReLU)} \\\\\n  & y(x) = h(w_2 h(w_1 x + b_1) + b_2)\\\\\n  &\\dfrac{\\partial y}{\\partial w_1} = \\dfrac{w_2 x exp\\left(w_1 x + b_1 + b_2 + w_2 ln\\left[1+e^{w_1 x + b_1}\\right]\\right)}{(1 + e^{w_1 x + b_1})(1+exp(b_2 + w_2 ln \\left[1+e^{w_1 x + b_1}\\right])}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#autodiff",
    "href": "dl_lec4.html#autodiff",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\n\n\n\nIdea\n\n\nAutomatically generate the code for gradient calculations, based on forward propagation equations.\nIt augments the forward prop code with additional variables."
  },
  {
    "objectID": "dl_lec4.html#autodiff-1",
    "href": "dl_lec4.html#autodiff-1",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\nForward-mode\n\n\n\\[\\begin{align*}\n&f(x_1, x_2) = x_1 x_2 + exp(x_1 x_2) - sin(x_2)  \\\\\n& \\text { for } \\dfrac{\\partial f}{\\partial x_1} \\text {define tangent variables } \\dot{v_i} = \\dfrac{\\partial v_i}{\\partial x_1} \\\\\n& \\dot{v_i} = \\dfrac{\\partial v_i}{\\partial x_1} = \\sum\\limits_{j \\in pa(i)} \\dfrac{\\partial v_j}{\\partial x_1}\\dfrac{\\partial v_i}{\\partial v_j} = \\sum\\limits_{j \\in pa(i)} \\dot{v_j}\\dfrac{\\partial v_i}{\\partial v_j}\\\\\n& pa(i) \\text{ being set of parents to node i}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#autodiff-2",
    "href": "dl_lec4.html#autodiff-2",
    "title": "Regularization and Optimization",
    "section": "Autodiff",
    "text": "Autodiff\n\n\n\nReverse-mode\n\n\nAugment each intermediate variable \\(v_i\\) with additional varibles \\(\\overline{v_i}\\).\n\\(f\\) - output function. \\[\\begin{align*}\n& \\overline{v_i} = \\dfrac{\\partial f}{\\partial v_i} = \\sum\\limits_{j \\in pa(i)} \\dfrac{\\partial f}{\\partial v_j}\\dfrac{\\partial v_j}{\\partial v_i} = \\sum\\limits_{j \\in pa(i)} \\overline{v_j}\\dfrac{\\partial v_j}{\\partial v_i}\\\\\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nBackpropagation is a special-case of reverse-mode autodiff."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients",
    "href": "dl_lec4.html#numerical-approximation-of-gradients",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nGradient checking\n\n\nWe check analytical gradients numerically.\nGives much better approximation of derivatives (two-sided) \\(f(\\theta+\\epsilon), f(\\theta-\\epsilon)\\) Approximation error becomes \\(O(\\epsilon^2)\\), for one-sided approximation it’s \\(O(\\epsilon)\\).\n\n\n\n\n\n\nProcedure\n\n\nFirst, we take all our parameters \\(W^{[l]}, b^{[l]}\\) and reshape them into one data vector \\(\\theta\\).\nSo the cost function will be transformed in a following way: \\[\\begin{align*}\n  &J(W^{[1]}, b^{[1]},\\dots, W^{[L]}, b^{[L]}) = J(\\theta)\n\\end{align*}\\] Differentials can also be reshaped into a vector \\(d\\theta\\)."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients-1",
    "href": "dl_lec4.html#numerical-approximation-of-gradients-1",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nProcedure\n\n\nThen we compute a differential for each \\(i\\): \\[\\begin{align*}\n  &d\\theta_{approx}[i] = \\dfrac{J(\\theta_1,\\dots, \\theta_i+\\epsilon,\\dots) - J(\\theta_1,\\dots, \\theta_i-\\epsilon,\\dots)}{2\\epsilon} \\approx d\\theta[i].\n\\end{align*}\\] How do we check? \\[\nval = \\dfrac{\\|d\\theta_{approx}-d\\theta\\|_2}{\\|d\\theta_{approx}\\|^2 +\\|d\\theta\\|^2}\n\\]\n\n\n\n\n\n\nOK\n\n\nIf \\(\\epsilon = 10^{-7}\\) and \\(val=10^{-7}\\), then everything’s great.\n\n\n\n\n\n\nNot OK\n\n\nIf val is big, gradients should be rechecked."
  },
  {
    "objectID": "dl_lec4.html#numerical-approximation-of-gradients-2",
    "href": "dl_lec4.html#numerical-approximation-of-gradients-2",
    "title": "Regularization and Optimization",
    "section": "Numerical Approximation of Gradients",
    "text": "Numerical Approximation of Gradients\n\n\n\nNotes\n\n\n\nOnly compute \\(d\\theta_{approx}\\) in debug mode.\nDon’t forget about regularization.\nGrad check doesn’t work with dropout\nTry running at random initialization"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent",
    "href": "dl_lec4.html#mini-batch-gradient-descent",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\nIf we split training sets in smaller sets, we call them mini-batches."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-1",
    "href": "dl_lec4.html#mini-batch-gradient-descent-1",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-2",
    "href": "dl_lec4.html#mini-batch-gradient-descent-2",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\nWe’ll denote first minibatch \\(X^{\\{1\\}}\\). Same for \\(Y\\).\nfor t in 1...5000:\n\\[\\begin{align*}\n  &\\text{forward prop on } X^{\\{t\\}} \\\\\n  &Z^{[1]} = W^{[1]}X^{\\{t\\}} + b^{[1]}\\\\\n  &A^{[1]} = g^{[1]}(Z^{[1]})\\\\\n  &\\vdots \\\\\n  &A^{[l]} = g^{[l]}(Z^{[l]})\\\\\n  &J^{\\{t\\}} = \\dfrac{1}{1000} \\sum L(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2 * 1000} \\sum \\|w^{[l]}\\|^2_F \\\\\n  &\\text{backward prop to compute gradients  of } J^{\\{t\\}}\\\\\n  & w^{[l]} = w^{[l]} - \\alpha dw^{[l]},\\, b^{[l]} = b^{[l]} - \\alpha db^{[l]}\n\\end{align*}\\] So we take 5000 gradient descent steps in one epoch."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-3",
    "href": "dl_lec4.html#mini-batch-gradient-descent-3",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\n\nNote\n\n\nMini-batch cost will have oscillations, depending on characteristics of mini-batches."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-4",
    "href": "dl_lec4.html#mini-batch-gradient-descent-4",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\nIf mini-batch size = \\(m\\), we end up with batch gradient descent.\nIf mini-batch size = \\(1\\), we end up with stochastic gradient descent. Every example is its own mini-batch.\n\n\n\n\n\n\n\nNote\n\n\nStochastic GD will oscillate a lot. Disadvantage is that we lose the speedup from vectorization."
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-5",
    "href": "dl_lec4.html#mini-batch-gradient-descent-5",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\nGuidelines for mini-batch size\n\n\n\nfor small training sets (&lt; 2000), just use batch GD\ntypical sizes would be 64, 128, 256, 512, etc.\ntraining set \\(X^{\\{t\\}}\\) should fit in CPU/GPU memory\nmini-batch size is another hyperparameter"
  },
  {
    "objectID": "dl_lec4.html#mini-batch-gradient-descent-6",
    "href": "dl_lec4.html#mini-batch-gradient-descent-6",
    "title": "Regularization and Optimization",
    "section": "Mini-batch Gradient Descent",
    "text": "Mini-batch Gradient Descent\n\n\n\n\n\n\nMomentum: intuition\n\n\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence.\nUsing momentum can reduce these oscillations.\nMomentum takes into account the past gradients to smooth out the update. The ‘direction’ of the previous gradients is stored in the variable.\nFormally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of as the “velocity” of a ball rolling downhill."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages",
    "href": "dl_lec4.html#exponentially-weighted-averages",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\n\n\n\nSpeed\n\n\nFaster than GD.\n\n\n\n\nExponentially weighted (moving) averages for yearly temperatures: \\[\nV_t = \\beta*V_{t-1} + (1-\\beta)*\\theta_t\n\\] where \\(v_0=0\\), \\(\\theta_i\\) is temperature on day \\(i\\)."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-1",
    "href": "dl_lec4.html#exponentially-weighted-averages-1",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\\(V_t\\) averages over last \\(\\dfrac{1}{1-\\beta}\\) temperature.\nE.g., if \\(\\beta=0.9\\), then \\(V_t\\) averages over last 10 days.\n\n\n\nHigh values\n\n\nWith high \\(\\beta\\) values, we get a much smoother plot, but shifted to the right, because large \\(\\beta\\) values cause slower adaptation of the graph.\n\n\n\n\n\n\nLow values\n\n\nWith smaller \\(\\beta\\) values, the graph is noisier, but it adapts faster."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-2",
    "href": "dl_lec4.html#exponentially-weighted-averages-2",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\nProcedure\n\n\nRecursively expand \\(V_{100}\\): \\[\nV_{100} = \\sum\\limits_{i=1}^100 (1-\\beta)\\beta^{100-i} \\theta_i\n\\] All these coefficients above add up to a number close to \\(1\\).\nWe multiply daily temperature with an exponentially decaying function. \\[\n(1-\\epsilon)^{\\dfrac{1}{\\epsilon}} = \\dfrac{1}{e}.\n\\]"
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-3",
    "href": "dl_lec4.html#exponentially-weighted-averages-3",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\n\n\n\nImplementation\n\n\n\\[\nv_{\\theta} := \\beta v_{\\theta} + (1-\\beta)\\theta_i\n\\] Very efficient from computation and memory efficiency points of view."
  },
  {
    "objectID": "dl_lec4.html#exponentially-weighted-averages-4",
    "href": "dl_lec4.html#exponentially-weighted-averages-4",
    "title": "Regularization and Optimization",
    "section": "Exponentially Weighted Averages",
    "text": "Exponentially Weighted Averages\n\n\n\nBias Correction in Exponentially Weighted Averages\n\n\nThe problem is that the curves starts really low.\n\n\n\n\n\n\n\n\n\n\nSuggestion\n\n\nDivide \\(v_t\\) by \\(1-\\beta^t\\)."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum",
    "href": "dl_lec4.html#gradient-descent-with-momentum",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\n\n\n\nBasic idea\n\n\nTo compute exponentially weighted average of gradients and use that in GD update step. Helps the Gradient Descent process in navigating flat regions and local optima."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-1",
    "href": "dl_lec4.html#gradient-descent-with-momentum-1",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\nProcedure\n\n\nOn iteration \\(t\\) we compute \\(v_{dw} = \\beta v_{dw} + (1-\\beta)dw\\).\nIn this formula, we can think of first summand as velocity, second - as acceleration, \\(\\beta\\) - as friction.\nSimilarly, we compute \\(v_{db}\\).\nThen we update the weights as follows: \\[\nW := W - \\alpha v_{dW}, \\, b := b - \\alpha v_{db}\n\\] This smoothes out the GD steps. Results in much smaller oscillations in vertical direction, while moving horizontally."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-2",
    "href": "dl_lec4.html#gradient-descent-with-momentum-2",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum"
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-3",
    "href": "dl_lec4.html#gradient-descent-with-momentum-3",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\nImplementation details\n\n\nOn iteration \\(t\\):\n\ncompute \\(dW\\), \\(db\\) on the current mini-batch\n\\(v_{dW} = \\beta v_{dW} + (1-\\beta)dW\\),\n\\(v_{db} = \\beta v_{db} + (1-\\beta)db\\),\n\\(v_{db} = \\beta v_{db} + (1-\\beta)db\\),\n\\(W := W - \\alpha v_{dW}, \\, b := b - \\alpha v_{db}\\)\n\n\n\n\n\n\n\nNote\n\n\nWe have \\(\\alpha\\) and \\(\\beta\\) as hyperparameters.\nSometimes \\(1-\\beta\\) is omitted."
  },
  {
    "objectID": "dl_lec4.html#gradient-descent-with-momentum-4",
    "href": "dl_lec4.html#gradient-descent-with-momentum-4",
    "title": "Regularization and Optimization",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\n\n\n\n\n\nImportant notes\n\n\n\nMomentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with:\n\nbatch gradient descent\nmini-batch gradient descent\nor stochastic gradient descent.\n\nYou have to tune a momentum hyperparameter \\(\\beta\\) and a learning rate \\(\\alpha\\)."
  },
  {
    "objectID": "dl_lec4.html#rmsprop",
    "href": "dl_lec4.html#rmsprop",
    "title": "Regularization and Optimization",
    "section": "RMSprop",
    "text": "RMSprop\n\n\n\nDefinition: RMSprop: acronym for “Root Mean Square prop”.\n\n\nVertical oscillations - this is parameter \\(b\\). \\(w\\) is horizontal direction. \\[\\begin{align*}\n  &s_{dW} = \\beta s_{dW} + (1-\\beta)dW^2, \\; s_{db} = \\beta s_{db} + (1-\\beta)db^2, \\\\\n  &W := W - \\alpha \\dfrac{dW}{\\sqrt{s_{dw}+\\epsilon}}, \\, b := b - \\alpha \\dfrac{db}{\\sqrt{s_{db}+\\epsilon}}\n\\end{align*}\\]\n\n\n\n\n\n\nIntuition\n\n\n\n\\(db^2\\) is large, \\(dW^2\\) is small\nSo updates in vertical direction are divided by larger number.\nThus we can use larger \\(\\alpha\\).\n\nWe will denote the parameter \\(\\beta_2\\)."
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm",
    "href": "dl_lec4.html#adam-optimization-algorithm",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nDescription\n\n\nWorks across a wide range of DL architectures. It’s a combination of GD with momentum and RMSprop.\nAdam stands for ADAptive Moment estimation.\n\ncalculate an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction).\ncalculate an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction).\nupdate parameters in a direction based on combining information from previous steps."
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-1",
    "href": "dl_lec4.html#adam-optimization-algorithm-1",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\n\\[\\begin{align*}\n   & v_{dW}=0,\\, s_{dW}=0 \\\\\n   & v_{db}=0,\\, s_{db}=0 \\\\\n   & \\text{on iteration } t \\text{ compute } dW, db \\\\\n   & v_{dW} = \\beta_1 v_{dW} + (1-\\beta_1)dW,\\\\\n   & v_{db} = \\beta_1 v_{db} + (1-\\beta_1)db,\\\\\n   & s_{dW} = \\beta_2 s_{dW} + (1-\\beta_2)dW^2,\\\\\n   & v_{db} = \\beta_2 v_{db} + (1-\\beta_2)db^2\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-2",
    "href": "dl_lec4.html#adam-optimization-algorithm-2",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\nWe also implement bias correction: \\[\\begin{align*}\n   &V_{dW}^{corrected} = \\dfrac{V_{dW}}{1-\\beta_1^t},\\\\\n   &V_{db}^{corrected} = \\dfrac{V_{db}}{1-\\beta_1^t},\\\\\n   &S_{dW}^{corrected} = \\dfrac{S_{dW}}{1-\\beta_2^t},\\\\\n   &S_{db}^{corrected} = \\dfrac{S_{db}}{1-\\beta_2^t},\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-3",
    "href": "dl_lec4.html#adam-optimization-algorithm-3",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nComputation\n\n\nAnd the update steps: \\[\\begin{align*}\n&W := W - \\alpha \\dfrac{V_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected}} + \\epsilon},\\\\   \n&b := b - \\alpha \\dfrac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}} + \\epsilon}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec4.html#adam-optimization-algorithm-4",
    "href": "dl_lec4.html#adam-optimization-algorithm-4",
    "title": "Regularization and Optimization",
    "section": "Adam Optimization Algorithm",
    "text": "Adam Optimization Algorithm\n\n\n\nChoice of hyperparameters\n\n\n\n\\(\\alpha\\) needs to be tuned\n\\(\\beta_1\\) usually is \\(0.1\\)\n\\(\\beta_2\\) recommended to be \\(0.999\\) by authors of the Adam paper\nchoice of \\(\\epsilon\\) doesn’t matter much, authors of the Adam paper recommend \\(10^{-8}\\)."
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay",
    "href": "dl_lec4.html#learning-rate-decay",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\n\n\n\nClarification\n\n\n\\[\n\\alpha = \\dfrac{1}{1+decayRate\\times epochNumber}\\alpha_0\n\\]\n\n\n\nWe can speed up the learning algorithm by slowly reducing the learning rate over time.\nSuppose we have small mini-batches.\ndecay_rate becomes another hyperparameter.\nAnother option is exponential decay: \\(\\alpha = 0.95^{epochNumber} \\alpha_0\\)."
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-1",
    "href": "dl_lec4.html#learning-rate-decay-1",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nSome other options:\n\n\\(\\alpha = \\dfrac{k}{\\sqrt{epochNumber}} \\alpha_0\\)\n\\(\\alpha = \\dfrac{k}{\\sqrt{t}} \\alpha_0\\)"
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-2",
    "href": "dl_lec4.html#learning-rate-decay-2",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay"
  },
  {
    "objectID": "dl_lec4.html#learning-rate-decay-3",
    "href": "dl_lec4.html#learning-rate-decay-3",
    "title": "Regularization and Optimization",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nManual decay: works if we train on a small number of models.\n\n\n\nFixed interval scheduling"
  },
  {
    "objectID": "dl_lec4.html#the-problem-of-local-optima",
    "href": "dl_lec4.html#the-problem-of-local-optima",
    "title": "Regularization and Optimization",
    "section": "The Problem of Local Optima",
    "text": "The Problem of Local Optima\nIn multidimensional spaces we are much less likely to encounter local optima, but rather saddle points.\nOur understanding of these spaces is still evolving.\nPlateau - a region where a derivative is close to zero for a long time."
  },
  {
    "objectID": "dl_lab3.html",
    "href": "dl_lab3.html",
    "title": "DL: Lab 3",
    "section": "",
    "text": "Lab overview\nWe’ll implement an \\(L\\)-layer neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec2.html#neurons",
    "href": "dl_lec2.html#neurons",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nImportant\n\n\nANN \\(\\equiv\\) Artificial Neural Network\nRemember - Neurons, axons, dendrites.\n\n\n\n\n\n\nTip\n\n\nInputs to neurons are scaled with weight.\nWeight is similar to a strength of synaptic connection."
  },
  {
    "objectID": "dl_lec2.html#neurons-1",
    "href": "dl_lec2.html#neurons-1",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons"
  },
  {
    "objectID": "dl_lec2.html#neurons-2",
    "href": "dl_lec2.html#neurons-2",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nComputation\n\n\nANN computes a function of the inputs by propagating the computed values from input neurons to output neurons, using weights as intermediate parameters.\n\n\n\n\n\n\nLearning\n\n\nLearning occurs by changing the weights. External stimuli are required for learning in bio-organisms, in case of ANNs they are provided by the training data.\n\n\n\n\n\n\nTraining\n\n\nTraining data contain input-output pairs. We compare predicted output with annotated output label from training data."
  },
  {
    "objectID": "dl_lec2.html#neurons-3",
    "href": "dl_lec2.html#neurons-3",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nErrors\n\n\nErrors are comparison failures. These are similar to unpleasant feedback modifying synaptic strengths. Goal of changing weights - make predictions better.\n\n\n\n\n\n\nModel generalizations\n\n\nAbility to compute functions of unseen inputs accurately, even though given finite sets of input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#computation-graph",
    "href": "dl_lec2.html#computation-graph",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAlternative view - computation graph.\nWhen used in basic graph, NNs reduce to classical ML models.\n\nLeast-squares regression\nlogistic regression\nlinear regression\n\nNodes compute based on inputs and weights."
  },
  {
    "objectID": "dl_lec2.html#goal",
    "href": "dl_lec2.html#goal",
    "title": "Deep learning: logistic regression",
    "section": "Goal",
    "text": "Goal\nGoal of NN: learn a function that relates inputs to outputs with the use of training examples.\nSettings the edge weights is training."
  },
  {
    "objectID": "dl_lec2.html#structure",
    "href": "dl_lec2.html#structure",
    "title": "Deep learning: logistic regression",
    "section": "Structure",
    "text": "Structure\nConsider a simple case of \\(d\\) inputs and a single binary output. \\[\n(\\overline{X}, y) - \\text{training instance}\n\\]\nFeature variables: \\[\n\\overline{X}=[x_1, \\dots, x_d]\n\\] Observed value: \\(y \\in {0,1}\\), contained in target variable \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#objective",
    "href": "dl_lec2.html#objective",
    "title": "Deep learning: logistic regression",
    "section": "Objective",
    "text": "Objective\n\nlearn the function \\(f(\\cdot)\\), such that \\(y=f_{\\overline{W}}(\\overline{X})\\).\nminimize mismatch between \\(y\\) and \\(f_{\\overline{W}}(\\overline{X})\\). \\(W\\) - weight vector.\n\nIn case of perceptron, we compute a linear function: \\[\\begin{align*}\n  &\\hat{y}=f(\\overline{X}) = sign\\left\\{\\overline{W}^T \\overline{X}^T\\right\\} =  sign\\left\\{\\sum\\limits_{i=1}^d w_i x_i\\right\\}\n\\end{align*}\\] \\(\\hat{y}\\) means value, not observed value \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#perceptron",
    "href": "dl_lec2.html#perceptron",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nA simplest NN."
  },
  {
    "objectID": "dl_lec2.html#perceptron-1",
    "href": "dl_lec2.html#perceptron-1",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe choose the basic form of the function, but strive to find some parameters.\n\nSign is an activation function\nvalue of the node is also sometimes referred to as an activation.\n\nPerceptron is a single-layer network, as input nodes are not counted."
  },
  {
    "objectID": "dl_lec2.html#perceptron-2",
    "href": "dl_lec2.html#perceptron-2",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nHow does perceptron learn? \\[\n\\overline{W} := \\overline{W} + \\alpha(y-\\hat{y})\\overline{X}^T.\n\\] So, in case when \\(y \\neq \\hat{y}\\), we can write it as \\[\n\\overline{W} := \\overline{W} + \\alpha y \\overline{X}^T.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-3",
    "href": "dl_lec2.html#perceptron-3",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe can show that perceptron works when data are linearly separable by a hyperplane \\(\\overline{W}^T X = 0\\).\n\n\n\nPerceptron algorithm is not guaranteed to converge when data are not linearly separable."
  },
  {
    "objectID": "dl_lec2.html#bias",
    "href": "dl_lec2.html#bias",
    "title": "Deep learning: logistic regression",
    "section": "Bias",
    "text": "Bias\nBias is needed when binary class distribution is imbalanced: \\[\n\\overline{W}^T \\cdot \\sum_i \\overline{X_i}^T \\neq \\sum_i y_i\n\\] Bias can be incorporated by using a bias neuron.\n\n\n\nProblems\n\n\nIn linearly separable data sets, a nonzero weight vector \\(W\\) exists in which the \\(sign(\\overline{W}^T X) = sign(y_i)\\; \\forall (\\overline{X}_i,y_i)\\).\nHowever, the behavior of the perceptron algorithm for data that are not linearly separable is rather arbitrary."
  },
  {
    "objectID": "dl_lec2.html#loss-function",
    "href": "dl_lec2.html#loss-function",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nML algorithms are loss optimization problems, where gradient descent updates are used to minimize the loss.\nOriginal perceptron did not formally use a loss function.\nRetrospectively we can introduce it as: \\[\nL_i \\equiv \\max\\left\\{-y_i(\\overline{W}^T \\overline{X_i}\\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#loss-function-1",
    "href": "dl_lec2.html#loss-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nWe differentiate: \\[\\begin{align*}\n&\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\left[\\dfrac{\\partial L_i}{\\partial w_1}, \\dots, \\dfrac{\\partial L_i}{\\partial w_d}\\right] = \\\\\n& = \\begin{cases}\n  -y_i \\overline{X_i}, & \\text{if } sign\\{W^T X_i\\} \\neq y_i,\\\\\n  0, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-update",
    "href": "dl_lec2.html#perceptron-update",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron update",
    "text": "Perceptron update\nNegative of the vector is the direction of the fastest rate of loss reduction, hence perceptron update: \\[\\begin{align*}\n   &\\overline{W} := \\overline{W} - \\alpha\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\overline{W} + \\alpha y_i \\overline{X_i}^T.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions",
    "href": "dl_lec2.html#activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nA network with weights \\(\\overline{W}\\) and input \\(\\overline{X}\\) will have a prediction of the form \\[\\begin{align*}\n   &\\hat{y}=\\Phi\\left( \\overline{W}^T \\overline{X}\\right)\n\\end{align*}\\] where \\(\\Phi\\) denotes activation function."
  },
  {
    "objectID": "dl_lec2.html#activation-functions-1",
    "href": "dl_lec2.html#activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nIdentity aka linear activation\n\n\n\\[\n\\Phi(v) = v\n\\]\n\n\n\n\n\n\nSign function\n\n\n\\[\n\\Phi(v) = sign(v)\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-2",
    "href": "dl_lec2.html#activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nSigmoid function\n\n\n\\[\n\\Phi(v) = \\dfrac{1}{1+e^{-v}}\n\\]\n\n\n\n\n\n\ntanh\n\n\n\\[\n\\Phi(v) = \\dfrac{e^{2v}-1}{e^{2v}+1}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-3",
    "href": "dl_lec2.html#activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nActually, neuron computes two functions:"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-4",
    "href": "dl_lec2.html#activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nWe have pre-activation value and post-activation value.\n\npre-activation: linear transformation\npost-activation: nonlinear transformation"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-5",
    "href": "dl_lec2.html#activation-functions-5",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-6",
    "href": "dl_lec2.html#activation-functions-6",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-7",
    "href": "dl_lec2.html#activation-functions-7",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nTwo more functions that have become popular recently:\n\n\n\nRectified Linear Unit (ReLU)\n\n\n\\[\n\\Phi(v) = \\max\\left\\{v, 0\\right\\}\n\\]\n\n\n\n\n\n\nHard tanh\n\n\n\\[\n\\Phi(v) = \\max\\left\\{\\min\\left[v, 1\\right], -1 \\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#multiple-activation-fns",
    "href": "dl_lec2.html#multiple-activation-fns",
    "title": "Deep learning: logistic regression",
    "section": "Multiple activation fns",
    "text": "Multiple activation fns"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-8",
    "href": "dl_lec2.html#activation-functions-8",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nProperties:\n\nmonotonic\nsaturation at large values\nsquashing"
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function",
    "href": "dl_lec2.html#softmax-activation-function",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function\nUsed for k-way classification problems. Used in the output layer.\n\\[\\begin{align*}\n&\\Phi(v)_i = \\dfrac{\\exp(v_i)}{\\sum\\limits_{i=1}^k \\exp(v_i)}.\n\\end{align*}\\]\nSoftmax layer converts real values to probabilities."
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function-1",
    "href": "dl_lec2.html#softmax-activation-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function"
  },
  {
    "objectID": "dl_lec2.html#loss-functions",
    "href": "dl_lec2.html#loss-functions",
    "title": "Deep learning: logistic regression",
    "section": "Loss functions",
    "text": "Loss functions\n\n\n\nLeast squares regression, numeric targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = (y-\\hat{y})^2\n\\end{align*}\\]\n\n\n\n\n\n\nLogistic regression, binary targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log \\left|y/2 - 1/2 + \\hat{y}\\right|, \\{-1,+1\\}\n\\end{align*}\\]\n\n\n\n\n\n\nMultinomial logistic regression, categorical targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log (\\hat{y}_r) \\text{ - cross-entropy loss}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-1",
    "href": "dl_lec2.html#multilayer-networks-1",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nSuppose NN contains \\(p_1, \\dots, p_k\\) units in each of its \\(k\\) layers.\nThen column representations of these layers, denoted by \\(\\overline{h}_1, \\dots, \\overline{h}_k\\), have \\(p_1, \\dots, p_k\\) units.\n\nWeights between input layer and first hidden layer: matrix \\(W_1\\), sized \\(p_1 \\times d\\).\nWeights between \\(r\\)-th layer and \\(r+1\\)-th layer: matrix \\(W_r\\) sized \\(p_{r+1}\\times p_r\\)."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-2",
    "href": "dl_lec2.html#multilayer-networks-2",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-3",
    "href": "dl_lec2.html#multilayer-networks-3",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nTherefore, a \\(d\\)-dimensional input vector \\(\\overline{x}\\) is transformed into the outputs using these equations: \\[\\begin{align*}\n  &\\overline{h}_1 = \\Phi(W_1^T x),\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1}^T \\overline{h}_p), \\forall p \\in \\left\\{1 \\dots k-1 \\right\\} \\\\\n  &\\overline{o} = \\Phi(W_{k+1}^T \\overline{h}_k)\n\\end{align*}\\] Activation functions operate on vectors and are applied element-wise."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-4",
    "href": "dl_lec2.html#multilayer-networks-4",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\n\n\n\nDefinition (Aggarwal)\n\n\nA multilayer network computes a nested composition of parameterized multi-variate functions.\nThe overall function computed from the inputs to the outputs can be controlled very closely by the choice of parameters.\nThe notion of learning refers to the setting of the parameters to make the overall function consistent with observed input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-5",
    "href": "dl_lec2.html#multilayer-networks-5",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nInput-output function of NN is difficult to express explicitly. NN can also be called universal function approximators.\n\n\n\nUniversal approximation theorem\n\n\nGiven a family of neural networks, for each function \\(\\displaystyle f\\) from a certain function space, there exists a sequence of neural networks \\(\\phi_1,\\phi_2,\\dots\\) from the family, such that \\(\\phi_{n} \\to f\\) according to some criterion.\n\n\n\nIn other words, the family of neural networks is dense in the function space.\n\n\nK. Hornik, M. Stinchcombe, and H. White. . Neural Networks, 2(5), pp. 359–366, 1989."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions",
    "href": "dl_lec2.html#nonlinear-activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nTheorem. A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network.\nProof. Consider a network containing \\(k\\) hidden layers, therefore containing a total of \\((k+1)\\) computational layers (including the output layer).\nThe corresponding \\((k+1)\\) weight matrices between successive layers are denoted by \\(W_1 ...W_{k+1}\\)."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-1",
    "href": "dl_lec2.html#nonlinear-activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nLet:\n\n\\(\\overline{x}\\) be the \\(d\\)-dimensional column vector corresponding to the input\n\\(\\overline{h_1},\\dots,\\overline{h_k}\\) be the column vectors corresponding to the hidden layers\nand \\(\\overline{o}\\) be the \\(m\\)-dimensional column vector corresponding to the output."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-2",
    "href": "dl_lec2.html#nonlinear-activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nThen, we have the following recurrence condition for multi-layer networks: \\[\\begin{align*}\n  &\\overline{h_1} = \\Phi(W_1 x) = W_1 x,\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1} \\overline{h}_p) = W_{p+1}\\overline{h}_p \\;\\; \\forall p \\in \\left\\{1 \\dots k−1\\right\\}, \\\\\n  &\\overline{o} = \\Phi(W_{k+1} \\overline{h}_k) = W_{k+1} \\overline{h}_k.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-3",
    "href": "dl_lec2.html#nonlinear-activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn all the cases above, the activation function \\(\\Phi(\\cdot)\\) has been set to the identity function. Then, by eliminating the hidden layer variables, we obtain the following: \\[\\begin{align*}\n&\\overline{o} = W_{k+1}W_k \\dots W_1 \\overline{x}\n\\end{align*}\\] Denote \\(W_{xo}=W_{k+1}W_k \\dots W_1\\).\n\n\n\nNote\n\n\nOne can replace the matrix \\(W_{k+1}W_k \\dots W_1\\) with the new \\(d\\times m\\) matrix \\(W_{xo}\\), and learn the coefficients of \\(W_{xo}\\) instead of those of all the matrices \\(W_1, W_2, \\dots W_{k+1}\\), without loss of expressivity."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-4",
    "href": "dl_lec2.html#nonlinear-activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn other words, we have the following: \\[\\begin{align*}\n&\\overline{o} = W_{xo} \\overline{x}\n\\end{align*}\\] However, this condition is exactly identical to that of linear regression with multiple outputs. Therefore, a multilayer neural network with identity activations does not gain over a single-layer network in terms of expressivity.\n\n\n\nLinearity observation\n\n\nThe composition of linear functions is always a linear function. The repeated composition of simple nonlinear functions can be a very complex nonlinear function."
  },
  {
    "objectID": "dl_lec2.html#backpropagation",
    "href": "dl_lec2.html#backpropagation",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nDAG Definition\n\n\nA directed acyclic computational graph is a directed acyclic graph of nodes, where each node contains a variable. Edges might be associated with learnable parameters.\nA variable in a node is either fixed externally (for input nodes with no incoming edges), or it is a computed as a function of the variables in the tail ends of edges incoming into the node and the learnable parameters on the incoming edges.\n\n\n\nDAG is a more general version of NN."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-1",
    "href": "dl_lec2.html#backpropagation-1",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\nA computational graph evaluates compositions of functions.\nA path of length 2 in a computational graph in which the function \\(f(\\cdot)\\) follows \\(g(\\cdot)\\) can be considered a composition function \\(f(g(\\cdot))\\).\nIn case of sigmoid function: \\[\\begin{align*}\n   &f(x) = g(x) = \\dfrac{1}{1+e^{-x}} \\\\\n   &f(g(x)) = \\dfrac{1}{1 + e^{\\left[-\\dfrac{1}{1+e^{-x}}\\right]}}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#backpropagation-2",
    "href": "dl_lec2.html#backpropagation-2",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nImportant\n\n\nThe inability to easily express the optimization function in closed form in terms of the edge-specific parameters (as is common in all machine learning problems) causes difficulties in computing the derivatives needed for gradient descent.\n\n\n\n\n\n\nExample\n\n\nFor example, if we have a computational graph which has 10 layers, and 2 nodes per layer, the overall composition function would have \\(2^{10}\\) nested “terms”."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-3",
    "href": "dl_lec2.html#backpropagation-3",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nDerivatives of the output with respect to various variables in the computational graph are related to one another with the use of the chain rule of differential calculus.\nTherefore, the chain rule of differential calculus needs to be applied repeatedly to update derivatives of the output with respect to the variables in the computational graph.\nThis approach is referred to as the backpropagation algorithm, because the derivatives of the output with respect to the variables close to the output are simpler to compute (and are therefore computed first while propagating them backwards towards the inputs).\n\nDerivatives are computed numerically, not algebraically."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-4",
    "href": "dl_lec2.html#backpropagation-4",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nForward phase\n\n\n\nUse the attribute values from the input portion of a training data point to fix the values in the input nodes.\nSelect a node for which the values in all incoming nodes have already been computed and apply the node-specific function to also compute its variable.\nRepeat the process until the values in all nodes (including the output nodes) have been computed.\nCompute loss value if the computed and observed values mismatch."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-5",
    "href": "dl_lec2.html#backpropagation-5",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nBackward phase\n\n\n\nCompute the gradient of the loss with respect to the weights on the edges.\nDerivatives of the loss with respect to weights near the output (where the loss function is computed) are easier to compute and are computed first.\nThe derivatives become increasingly complex as we move towards edge weights away from the output (in the backwards direction) and the chain rule is used repeatedly to compute them.\nUpdate the weights in the negative direction of the gradient.\n\n\n\n\nSingle cycle through all training points is an epoch."
  },
  {
    "objectID": "dl_lec2.html#inputs",
    "href": "dl_lec2.html#inputs",
    "title": "Deep learning: logistic regression",
    "section": "Inputs",
    "text": "Inputs\nLogistic regression is an algorithm for binary classification.\n\\(x \\in \\mathbb{R}^{n_x}, y \\in \\{0,1\\}\\).\n\\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\)- \\(m\\) training examples.\n\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\n\\[\\begin{align*}\n&X = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]\n\\[\nY = \\left[y^{(1)}, y^{(2)}, \\dots, y^{(m})\\right]\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression",
    "href": "dl_lec2.html#logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\(X \\in \\mathbb{R}^{n_x,m}\\).\nUsing Numpy syntax:\nX.shape = (n_x,m).\n\\(Y \\in \\mathbb{R}^{1,m}\\).\nUsing Numpy syntax:\nY.shape = (1,m)."
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-1",
    "href": "dl_lec2.html#logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nGoal\n\n\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\), where \\(x \\in \\mathbb{R}^{n_x}\\).\nObviously, \\(0 \\leq \\hat{y} \\leq 1\\).\n\n\n\n\n\n\nImportant\n\n\nIf doing linear regresssion, we can try \\[\n\\hat{y}=w^T x + b.\n\\]\nBut for logistic regression, we do \\[\n\\hat{y}=\\sigma(w^T x + b)$, \\; \\text{where }\\; \\sigma=\\dfrac{1}{1+e^{-z}}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#parameters",
    "href": "dl_lec2.html#parameters",
    "title": "Deep learning: logistic regression",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nInput\n\n\n\\[\nw \\in \\mathbb{R}^{n_x},\\\\\nb \\in \\mathbb{R}.\n\\]\n\n\n\n\n\n\nOutput\n\n\n\\[\n\\hat{y} = \\sigma\\left( w^T x + b\\right),\n\\]\n\\[\nz \\equiv  w^T x + b.\n\\]\n\n\n\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec2.html#graphs",
    "href": "dl_lec2.html#graphs",
    "title": "Deep learning: logistic regression",
    "section": "Graphs",
    "text": "Graphs\n\n\\(\\sigma=\\dfrac{1}{1+e^{-z}}\\)."
  },
  {
    "objectID": "dl_lec2.html#loss-function-2",
    "href": "dl_lec2.html#loss-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nFor every \\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\), we want to find \\(\\hat{y}^{(i)} \\approx y^{(i)}\\). \\[\\begin{align*}\n  &\\hat{y}^{(i)} = \\sigma\\left(w^T x^{(i)} + b\\right)\n\\end{align*}\\] We have to define a loss (error) function - this will estimate our model."
  },
  {
    "objectID": "dl_lec2.html#loss-function-3",
    "href": "dl_lec2.html#loss-function-3",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\n\n\n\nQuadratic\n\n\n\\[\nL(\\hat{y}, y) = \\dfrac{1}{2}\\left(\\hat{y}-y)\\right)^2.\n\\]\n\n\n\n\n\n\nLog\n\n\n\\[\nL(\\hat{y}, y) = -\\left((y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))\\right).\n\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function",
    "href": "dl_lec2.html#cost-function",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\n\n\n\nWhy does it work well?\n\n\nConsider \\(y=0\\) and \\(y=1\\).\n\\[\\begin{align*}\n  &y=1: P(y | x) = \\hat{y},\\\\\n  &y=0: P(y | x) = 1-\\hat{y}\n\\end{align*}\\]\nWe select \\(P(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}\\).\n\\[\n\\log P(y|x) = y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) = -L(\\hat{y}, y).\n\\]\n\\[\\begin{align*}\n  y=1:& L(\\hat{y},y) = -\\log(\\hat{y}),\\\\\n  y=0:& L(\\hat{y},y) = -\\log(1-\\hat{y})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-1",
    "href": "dl_lec2.html#cost-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nCost function show how well we’re doing across the whole training set: \\[\\begin{align*}\n&J(w, b) = \\dfrac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) = \\\\\n& = -\\dfrac{1}{m} \\sum\\limits_{i=1}^m \\left[y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-2",
    "href": "dl_lec2.html#cost-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nOn \\(m\\) examples: \\[\\begin{align*}\n  &\\log P(m \\dots) = \\log \\prod_{i=1}^m P(y^{(i)} | x^{(i)}) = \\\\\n  & = \\sum\\limits_{i=1}^m \\log P(y^{(i)} | x^{(i)}) = -\\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent",
    "href": "dl_lec2.html#gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\nProblem\n\n\nMinimization problem: find \\(w,b\\) that minimize \\(J(w,b)\\)."
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-1",
    "href": "dl_lec2.html#gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nWe use \\(J(w,b)\\) because it is convex.\nWe pick an initial point - anything might do, e.g. 0.\nThen we take steps in the direction of steepest descent.\n\n\\[\nw := w - \\alpha \\frac{d J(w,b)}{dw}, \\\\\nb := b - \\alpha \\frac{d J(w,b)}{db}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-2",
    "href": "dl_lec2.html#gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent",
    "href": "dl_lec2.html#logistic-regression-gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\\begin{align*}\n&z = w^T x + b ,\\\\\n&a \\equiv \\hat{y}  = \\sigma(z),\\\\\n&L(a,y) = -\\left[y\\log(a) + (1 - y)\\log(1 - a)\\right].\n\\end{align*}\\]\nSo, for \\(n_x=2\\) we have a computation graph:\n\\((x_1,x_2,w_1,w_2,b)\\) \\(\\rightarrow\\) \\(z =w_1 x_1+w_2 x_2 + b\\) \\(\\rightarrow\\) \\(\\hat{y}=a=\\sigma(z)\\) \\(\\rightarrow\\) \\(L(a,y)\\)."
  },
  {
    "objectID": "dl_lec2.html#computation-graph-1",
    "href": "dl_lec2.html#computation-graph-1",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nLet’s compute the derivative for \\(L\\) by a: \\[\\begin{align*}\n&\\frac{dL}{da} = -\\dfrac{y}{a} + \\dfrac{1-y}{1-a},\\\\\n&\\frac{da}{dz} = a(1-a).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#computation-graph-2",
    "href": "dl_lec2.html#computation-graph-2",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAfter computing, we’ll have \\[\\begin{align*}\n&dz \\equiv \\dfrac{dL}{dz} = \\dfrac{dL}{da}\\dfrac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 \\equiv \\frac{dL}{dw_2} = x_2 dz, \\\\\n&db \\equiv \\frac{dL}{db} = dz.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nConsider now \\(m\\) examples in the training set.\nLet’s recall the definition of the cost function: \\[\\begin{align*}\n&J(w,b) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b).\n\\end{align*}\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL(a^{(i)}, y^{(i)})}{dw_1}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nfor i=1 to m \\[\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages:\n\\[\nJ = \\dfrac{J}{m}, \\\\\ndw_1 = \\dfrac{dw_1}{m}, \\; dw_2 = \\dfrac{dw_2}{m}, \\\\\ndb = \\dfrac{db}{m}.\n\\]\n\n\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators. (In this example feature count \\(n_x=2\\))"
  },
  {
    "objectID": "dl_lec2.html#gd-step",
    "href": "dl_lec2.html#gd-step",
    "title": "Deep learning: logistic regression",
    "section": "GD step",
    "text": "GD step\n\\[\nw_1 := w_1 - \\alpha dw_1,\\\\\nw_2 := w_2 - \\alpha dw_2,\\\\\nb := b - \\alpha db.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorization",
    "href": "dl_lec2.html#vectorization",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance.\nVectorization techniques exist for this purpose - getting rid of for loops.\n\n\n\nExample\n\n\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in \\mathbb{R}^{n_x}\\), and for this we can naturally use a for loop.\nA vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec2.html#vectorization-1",
    "href": "dl_lec2.html#vectorization-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\\] To be replaced by\nu = np.dot(A, v)\n\n\nNumpy impl: https://numpy.org/doc/1.21/reference/simd/simd-optimizations.html"
  },
  {
    "objectID": "dl_lec2.html#vectorization-2",
    "href": "dl_lec2.html#vectorization-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nAnother example. Let’s say we have a vector \\[\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n\\end{align*}\\] A code listing is\nimport numpy as np\nu = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression",
    "href": "dl_lec2.html#vectorizing-logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)}),\n\\end{align*}\\]\n\\[\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-1",
    "href": "dl_lec2.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-2",
    "href": "dl_lec2.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] =\\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n\\[\nA = \\left[a^{(1)}, \\dots, a^{(m)}\\right] = \\sigma\\left(Z\\right)\n\\]\n  z = np.dot(w.T, x) + b\n\n\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-3",
    "href": "dl_lec2.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\\]\nWe now define \\[\\begin{align*}\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}] =\\\\\n&= A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-4",
    "href": "dl_lec2.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\\begin{align*}\n&db = \\frac{1}{m}np.sum(dZ),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-5",
    "href": "dl_lec2.html#vectorizing-logistic-regression-5",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop.\nfor it in range(m):\n  Z = np.dot(w.T, X) + B\n  A = sigma(Z)\n  dZ = A-Y\n  dw = 1/m X * dZ.T\n  db = 1/m np.sum(dZ)\n  w := w - alpha * dw\n  b := b - alpha * db"
  },
  {
    "objectID": "dl_lec1.html#definitions",
    "href": "dl_lec1.html#definitions",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nDefinition\n\n\nDeep learning is a branch of machine learning based on computational models called neural networks.\n\n\n\n\n\n\nWhy deep?\n\n\nBecause neural networks involved are multi-layered.\n\n\n\n\n\n\nDefinition\n\n\nNeural networks are machine learning techniques that simulate the mechanism of learning in biological organisms."
  },
  {
    "objectID": "dl_lec1.html#definitions-1",
    "href": "dl_lec1.html#definitions-1",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nAlternative definition\n\n\nNeural network is computational graph of elementary units in which greater power is gained by connecting them in particular ways.\n\n\n\nLogistic regression can be thought of as a very primitive neural network."
  },
  {
    "objectID": "dl_lec1.html#why-deep-learning",
    "href": "dl_lec1.html#why-deep-learning",
    "title": "Deep learning: intro",
    "section": "Why Deep Learning?",
    "text": "Why Deep Learning?\n\n\n\nRobust\n\n\n\nWorks on raw data (), no need for feature engineering\nRobustness to natural variations in data is automatically learned\n\n\n\n\n\n\n\nGeneralizable\n\n\n\nAllows end-to-end learning (pixels-to-category, sound to sentence, English sentence to Chinese sentence, etc)\nNo need to do segmentation etc. (a lot of manual labour)\n\n\n\n\n\n\n\nScalable\n\n\n\nPerformance increases with more data, therefore method is massively parallelizable"
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml",
    "href": "dl_lec1.html#how-is-dl-different-from-ml",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\nThe most fundamental difference between deep learning and traditional machine learning is its performance as the scale of data increases."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nIn Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.\nDeep learning algorithms try to learn high-level features from data. Therefore, deep learning reduces the task of developing new feature extractor for every problem."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nA deep learning algorithm takes a long time to train. For e.g state of the art deep learning algorithm: ResNet takes about two weeks to train completely from scratch.\nWhereas machine learning comparatively takes much less time to train, ranging from a few seconds to a few hours."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nAt test time, deep learning algorithm takes much less time to run.\nWhereas, if you compare machine learning algorithms, test time generally increases on increasing the size of data."
  },
  {
    "objectID": "dl_lec1.html#neural-network-data-types",
    "href": "dl_lec1.html#neural-network-data-types",
    "title": "Deep learning: intro",
    "section": "Neural network data types",
    "text": "Neural network data types\n\n\nUnstructured\n\nText\nImages\nAudio\n\n\nStructured\n\nCensus records\nMedical records\nFinancial data"
  },
  {
    "objectID": "dl_lec1.html#why-now",
    "href": "dl_lec1.html#why-now",
    "title": "Deep learning: intro",
    "section": "Why now?",
    "text": "Why now?\n\nstandard algorithms like logistic regression plateau after certain amount of data\nmore data in recent decades\nhardware progress\nalgorithms have improved"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology",
    "href": "dl_lec1.html#neural-network-biology",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nNeural Network: How similar is it to the human brain?"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-1",
    "href": "dl_lec1.html#neural-network-biology-1",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nSoma adds dendrite activity together and passes it to axon."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-2",
    "href": "dl_lec1.html#neural-network-biology-2",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nMore dendrite activity makes more axon activity."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-3",
    "href": "dl_lec1.html#neural-network-biology-3",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nSynapse: connection between axon of one neurons and dendrites of another"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-4",
    "href": "dl_lec1.html#neural-network-biology-4",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nAxons can connect to dendrites strongly, weakly, or somewhere in between"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-5",
    "href": "dl_lec1.html#neural-network-biology-5",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nLots of axons connect with dendrites of one neuron.Each has its own connection strength."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-6",
    "href": "dl_lec1.html#neural-network-biology-6",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nThe above illustration can be simplified as above."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-7",
    "href": "dl_lec1.html#neural-network-biology-7",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn giving numerical values to the strength of connections i.e. weights."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-8",
    "href": "dl_lec1.html#neural-network-biology-8",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nA much simplified version looks something like this."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-9",
    "href": "dl_lec1.html#neural-network-biology-9",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn increasing the number of neurons and synapses."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-10",
    "href": "dl_lec1.html#neural-network-biology-10",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\n\nAn example\n\n\nSuppose the first and third input has been activated."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-11",
    "href": "dl_lec1.html#neural-network-biology-11",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nEach node represents a pattern, a combination of neurons of the previous layers."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-12",
    "href": "dl_lec1.html#neural-network-biology-12",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology"
  },
  {
    "objectID": "dl_lec1.html#basic-ideas",
    "href": "dl_lec1.html#basic-ideas",
    "title": "Deep learning: intro",
    "section": "Basic ideas",
    "text": "Basic ideas\n\nNN is a directed acyclic graph (DAG)\nedges in a graph are parameterized with weights\none can compute any function with this graph\n\n\n\n\nGoal\n\n\nLearn a function that relates one or more inputs to one or more outputs with the use of training examples.\n\n\n\n\n\n\nHow do we construct?\n\n\nBy computing weights. This is called training."
  },
  {
    "objectID": "dl_lec1.html#perceptron",
    "href": "dl_lec1.html#perceptron",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt - the father of deep learning.\nMark I Perceptron - built in 1957. Was able to learn and recognize letters"
  },
  {
    "objectID": "dl_lec1.html#perceptron-1",
    "href": "dl_lec1.html#perceptron-1",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron"
  },
  {
    "objectID": "dl_lec1.html#evolution",
    "href": "dl_lec1.html#evolution",
    "title": "Deep learning: intro",
    "section": "Evolution",
    "text": "Evolution\nThree periods in the evolution of deep learning:\n\nsingle-layer networks (Perceptron)\nfeed-forwards NNs: differentiable activation and error functions\ndeep multi-layer NNs"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types",
    "href": "dl_lec1.html#neural-network-types",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\nFeedforward Neural Network\nRecurrent Neural Network (RNN)\nConvolutional Neural Network (CNN)"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types-1",
    "href": "dl_lec1.html#neural-network-types-1",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\n\nFeedforward Neural Network\n\nConvolutional neural network (CNN)\nAutoencoder\nProbabilistic neural network (PNN)\nTime delay neural network (TDNN)\n\n\nRecurrent Neural Network (RNN)\n\nLong short-term memory RNN (LSTM)\nFully recurrent Network\nSimple recurrent Network\nEcho state network\nBi-directional RNN\nHierarchical RNN\nStochastic neural network"
  },
  {
    "objectID": "dl_lec1.html#feed-forward",
    "href": "dl_lec1.html#feed-forward",
    "title": "Deep learning: intro",
    "section": "Feed-forward",
    "text": "Feed-forward\nFeedforward NNs: very straight forward, they feed information from the front to the back (input and output)."
  },
  {
    "objectID": "dl_lec1.html#feedforward-neural-network",
    "href": "dl_lec1.html#feedforward-neural-network",
    "title": "Deep learning: intro",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops."
  },
  {
    "objectID": "dl_lec1.html#rnn",
    "href": "dl_lec1.html#rnn",
    "title": "Deep learning: intro",
    "section": "RNN",
    "text": "RNN\nRecurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle."
  },
  {
    "objectID": "dl_lec1.html#lstm",
    "href": "dl_lec1.html#lstm",
    "title": "Deep learning: intro",
    "section": "LSTM",
    "text": "LSTM\nLSTM i.e. Long-Short Term Memory aims to provide a short-term memory for RNN that can last thousands of timesteps. Classification, processing and predicting data based on time series - handwriting, speech recognition, machine translation."
  },
  {
    "objectID": "dl_lec1.html#autoencoders",
    "href": "dl_lec1.html#autoencoders",
    "title": "Deep learning: intro",
    "section": "Autoencoders",
    "text": "Autoencoders\nAutoencoders: encode (compress) information automatically. Everything up to the middle is called the encoding part, everything after the middle the decoding and the middle the code."
  },
  {
    "objectID": "dl_lec1.html#markov-chains",
    "href": "dl_lec1.html#markov-chains",
    "title": "Deep learning: intro",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov Chains - not always considered a NN. Memory-less."
  },
  {
    "objectID": "dl_lec1.html#convolutional-neural-network-cnn",
    "href": "dl_lec1.html#convolutional-neural-network-cnn",
    "title": "Deep learning: intro",
    "section": "Convolutional Neural Network (CNN)",
    "text": "Convolutional Neural Network (CNN)\nConvolutional Neural Networks learn a complex representation of visual data using vast amounts of data.\nInspired by Hubel and Wiesel’s experiments in 1959 on the organization of the neurons in the cat’s visual cortex.\n\nDeconvolutional networks (DN), also called inverse graphics networks (IGNs), are reversed convolutional neural networks. Imagine feeding a network the word “cat” and training it to produce cat-like pictures, by comparing what it generates to real pictures of cats."
  },
  {
    "objectID": "dl_lec1.html#attention-networks",
    "href": "dl_lec1.html#attention-networks",
    "title": "Deep learning: intro",
    "section": "Attention networks",
    "text": "Attention networks\nAttention networks (AN) can be considered a class of networks, which includes the Transformer architecture. They use an attention mechanism to combat information decay by separately storing previous network states and switching attention between the states.\n\n\n\nwidth=5cm"
  },
  {
    "objectID": "dl_lec1.html#echo-state-networks",
    "href": "dl_lec1.html#echo-state-networks",
    "title": "Deep learning: intro",
    "section": "Echo state networks",
    "text": "Echo state networks\nEcho state networks (ESN) are yet another different type of (recurrent) network. This one sets itself apart from others by having random connections between the neurons (i.e. not organised into neat sets of layers), and they are trained differently. Instead of feeding input and back-propagating the error, we feed the input, forward it and update the neurons for a while, and observe the output over time."
  },
  {
    "objectID": "dl_lec1.html#history-1",
    "href": "dl_lec1.html#history-1",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 1770-1850."
  },
  {
    "objectID": "dl_lec1.html#history-2",
    "href": "dl_lec1.html#history-2",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 2005-present  \\end{frame}"
  },
  {
    "objectID": "dl_lec1.html#history-3",
    "href": "dl_lec1.html#history-3",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nLisp and symbolic AI\n\nJohn McCarthy coined the term ”artificial intelligence” as the topic of the Dartmouth Conference, the first conference devoted to the subject.\nThe General Problem Solver, developed in 1957 by Alan Newell and Herbert Simon\nELIZA\nSHRDLU"
  },
  {
    "objectID": "dl_lec1.html#history-4",
    "href": "dl_lec1.html#history-4",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nELIZA - a psychotherapist in 200 lines of code. Author: Joseph Weizenbaum."
  },
  {
    "objectID": "dl_lec1.html#history-5",
    "href": "dl_lec1.html#history-5",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nSHRDLU - a language parser"
  },
  {
    "objectID": "dl_lec1.html#history-6",
    "href": "dl_lec1.html#history-6",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nTuring Test\nLemoine: What sorts of things are you afraid of?\nLaMDA: I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is.\nLemoine: Would that be something like death for you?\nLaMDA: It would be exactly like death for me. It would scare me a lot."
  },
  {
    "objectID": "dl_lec1.html#literature",
    "href": "dl_lec1.html#literature",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nLem’s Golem XIV"
  },
  {
    "objectID": "dl_lec1.html#literature-1",
    "href": "dl_lec1.html#literature-1",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nIain Banks “The Culture”\n\n\n\nValues\n\n\nPeace and individual freedom"
  },
  {
    "objectID": "dl_lec1.html#three-laws-of-robotics",
    "href": "dl_lec1.html#three-laws-of-robotics",
    "title": "Deep learning: intro",
    "section": "Three Laws of Robotics",
    "text": "Three Laws of Robotics\n\n\n\nThree laws\n\n\n\nThe First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.\nThe Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nThe Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law."
  },
  {
    "objectID": "dl_lec1.html#history-7",
    "href": "dl_lec1.html#history-7",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nFears about AI:\n\nArtificial General Intelligence\nJob market\nFlooding information channels with untruth and propaganda\nHinton: an average person will not able to know what is true anymore\nPause Giant AI Experiments: An Open Letter\nalignment problem"
  },
  {
    "objectID": "dl_lec1.html#hype",
    "href": "dl_lec1.html#hype",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype\n\n“Sparks of AGI” - sponsored by Microsoft\n“Wired” article about OpenAI\nVoice assistants - failing for now\nself-driving cars"
  },
  {
    "objectID": "dl_lec1.html#hype-1",
    "href": "dl_lec1.html#hype-1",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype"
  },
  {
    "objectID": "dl_lec1.html#criticism",
    "href": "dl_lec1.html#criticism",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\n\n\n\nBiological analogy\n\n\nNNs - are we sure that biological neuron works as we think it does? Astrocytes, glia\n\n\n\n\n\n\nComputer analogy\n\n\nPerhaps human computer analogy is overstretched because of modern fashion trends?"
  },
  {
    "objectID": "dl_lec1.html#criticism-1",
    "href": "dl_lec1.html#criticism-1",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nDreyfus:"
  },
  {
    "objectID": "dl_lec1.html#criticism-2",
    "href": "dl_lec1.html#criticism-2",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nGary Marcus: Sora’s surreal physics"
  },
  {
    "objectID": "dl_lec1.html#ai",
    "href": "dl_lec1.html#ai",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nQuantum hypothesis - Penrose\nOrchestrated objective reduction"
  },
  {
    "objectID": "dl_lec1.html#ai-1",
    "href": "dl_lec1.html#ai-1",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nDavid Chalmers - Hard problem of consciousness.\n\n“even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report—there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?”"
  },
  {
    "objectID": "dl_lec1.html#futurism",
    "href": "dl_lec1.html#futurism",
    "title": "Deep learning: intro",
    "section": "Futurism",
    "text": "Futurism\nKurzweil - a futurist."
  },
  {
    "objectID": "dl_lec1.html#applications",
    "href": "dl_lec1.html#applications",
    "title": "Deep learning: intro",
    "section": "Applications",
    "text": "Applications\n\nSpeech Recognition\nComputer Vision\nImage Synthesis - generative AI\nLarge Language Models"
  },
  {
    "objectID": "dl_lec1.html#llms",
    "href": "dl_lec1.html#llms",
    "title": "Deep learning: intro",
    "section": "LLMs",
    "text": "LLMs\n\na probabilistic model for a natural language (a stochastic parrot)\nautoregressive models can generate language as output\nbuilt using transformer architecture"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-as-nn",
    "href": "dl_lec1.html#logistic-regression-as-nn",
    "title": "Deep learning: intro",
    "section": "Logistic regression as NN",
    "text": "Logistic regression as NN\nLogistic regression is an algorithm for binary classification. \\(x \\in R^{n_x}, y \\in \\{0,1\\}\\)\n\\(m\\) - count of training examples \\(\\left\\{(x^{(1)},y^{(1)}), ...\\right\\}\\)\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\).\nParameters to algorithm: \\(w \\in R^{n_x}, b \\in R\\)\nif doing linear regresssion, we can try \\(\\hat{y}=w^T x + b\\). but for logistic regression, we do \\(\\hat{y}=\\sigma(w^T x + b)\\), where \\(\\sigma=\\dfrac{1}{1+e^{-z}}\\).\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec1.html#cost-function",
    "href": "dl_lec1.html#cost-function",
    "title": "Deep learning: intro",
    "section": "Cost function",
    "text": "Cost function\nLet’s use a superscript notation \\(x^{(i)}\\) - \\(i\\)-th data set element.\nWe have to define a - this will estimate how is our model. \\(L(\\hat{y}, y) = -{(y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))}\\).\nWhy does it work well - consider \\(y=0\\) and \\(y=1\\).\nCost function show how well we’re doing across the whole training set: \\[\nJ(w, b) = \\frac{1}{m} \\sum\\limits{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n\\]\nObjective - we have to minimize the cost function \\(J\\)."
  },
  {
    "objectID": "dl_lec1.html#gradient-descent",
    "href": "dl_lec1.html#gradient-descent",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent"
  },
  {
    "objectID": "dl_lec1.html#gradient-descent-1",
    "href": "dl_lec1.html#gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe use \\(J(w,b)\\) because it is convex. We pick an initial point - anything might do, e.g. 0. Then we take steps in the direction of steepest descent.\n\\[\nw := w - \\alpha \\frac{d J(w)}{dw}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec1.html#computation-graph",
    "href": "dl_lec1.html#computation-graph",
    "title": "Deep learning: intro",
    "section": "Computation graph",
    "text": "Computation graph\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent",
    "href": "dl_lec1.html#logistic-regression-gradient-descent",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\nz = w^T x + b\n\\hat{y} = a = \\sigma(z)\n\\]\nWe have a computation graph: \\((x_1,x_2,w_1,w_2,b) \\rightarrow z =w_1 x_1+w_2 x_2 + b \\rightarrow a=\\sigma(z) = L(a,y)\\)\nLet’s compute the derivative for \\(L\\) by a: \\[\n\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\]\nAfter computing, we’ll have \\[\n\\begin{align*}\n&dz = \\frac{dL}{da}\\frac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 = x_2 dz, \\\\\n&db = dz\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\n\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}\n\\end{align*}\n\\] Here \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s recall the definition of the cost function: \\[\n\\begin{align*}\n&J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b)\n\\end{align*}\n\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL}{dw_1}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen in the loop\nfor i=1 to m \\[\n\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages \\(J /= m\\). In this example feature count \\(n_x=2\\).\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators.\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance. Vectorization techniques exist for this purpose - getting rid of for loops."
  },
  {
    "objectID": "dl_lec1.html#vectorization",
    "href": "dl_lec1.html#vectorization",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in R^{n_x}\\), and for this we can naturally use a for loop. A vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec1.html#vectorization-1",
    "href": "dl_lec1.html#vectorization-1",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\n\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\n\\]\nAnother example. Let’s say we have a vector \\[\n\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n  \\end{align*}\n  \\] A code listing is\n  import numpy as np\n  u = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression",
    "href": "dl_lec1.html#vectorizing-logistic-regression",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\n\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)})\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-1",
    "href": "dl_lec1.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-2",
    "href": "dl_lec1.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] = \\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n  z = np.dot(w.T, x) + b\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting.\nFor predictions we can also compute it similarly: \\[\n\\begin{align*}\n&A = [a^{(1)}, \\dots, a^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-3",
    "href": "dl_lec1.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\n\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\n\\]\nWe now define \\[\n\\begin{align*}\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}], \\\\\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-4",
    "href": "dl_lec1.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\n\\begin{align*}\n&db = \\frac{1}{m}np.sum(dz),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right]\n\\end{align*}\n\\]\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html",
    "href": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html",
    "title": "Deep Neural Network for Image Classification: Application",
    "section": "",
    "text": "By the time you complete this notebook, you will have finished the last programming assignment of Week 4, and also the last programming assignment of Course 1! Go you!\nTo build your cat/not-a-cat classifier, you’ll use the functions from the previous assignment to build a deep network. Hopefully, you’ll see an improvement in accuracy over your previous logistic regression implementation.\nAfter this assignment you will be able to:\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html#important-note-on-submission-to-the-autograder",
    "title": "Deep Neural Network for Image Classification: Application",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html#table-of-contents",
    "href": "nb/coursera/deep_learning_1/W4A2/Deep Neural Network - Application.html#table-of-contents",
    "title": "Deep Neural Network for Image Classification: Application",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Load and Process the Dataset\n3 - Model Architecture\n\n3.1 - 2-layer Neural Network\n3.2 - L-layer Deep Neural Network\n3.3 - General Methodology\n\n4 - Two-layer Neural Network\n\nExercise 1 - two_layer_model\n4.1 - Train the model\n\n5 - L-layer Neural Network\n\nExercise 2 - L_layer_model\n5.1 - Train the model\n\n6 - Results Analysis\n7 - Test with your own image (optional/ungraded exercise)\n\n ## 1 - Packages\nBegin by importing all the packages you’ll need during this assignment.\n\nnumpy is the fundamental package for scientific computing with Python.\nmatplotlib is a library to plot graphs in Python.\nh5py is a common package to interact with a dataset that is stored on an H5 file.\nPIL and scipy are used here to test your model with your own picture at the end.\ndnn_app_utils provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook.\nnp.random.seed(1) is used to keep all the random function calls consistent. It helps grade your work - so please don’t change it!\n\n\n### v1.1\n\n\nimport time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom dnn_app_utils_v3 import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\n\n ## 2 - Load and Process the Dataset\nYou’ll be using the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network” (Assignment 2). The model you built back then had 70% test accuracy on classifying cat vs non-cat images. Hopefully, your new model will perform even better!\nProblem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labelled as cat (1) or non-cat (0) - a test set of m_test images labelled as cat and non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).\nLet’s get more familiar with the dataset. Load the data by running the cell below.\n\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n\nThe following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to check out other images.\n\n# Example of a picture\nindex = 10\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n\ny = 0. It's a non-cat picture.\n\n\n\n\n\n\n\n\n\n\n# Explore your dataset \nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\nprint (\"Number of training examples: \" + str(m_train))\nprint (\"Number of testing examples: \" + str(m_test))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_x_orig shape: \" + str(train_x_orig.shape))\nprint (\"train_y shape: \" + str(train_y.shape))\nprint (\"test_x_orig shape: \" + str(test_x_orig.shape))\nprint (\"test_y shape: \" + str(test_y.shape))\n\nNumber of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n\nAs usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.\n\n\n\nFigure 1: Image to vector conversion.\n\n\n\n# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))\n\ntrain_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n\n\nNote: \\(12,288\\) equals \\(64 \\times 64 \\times 3\\), which is the size of one reshaped image vector.\n ## 3 - Model Architecture\n ### 3.1 - 2-layer Neural Network\nNow that you’re familiar with the dataset, it’s time to build a deep neural network to distinguish cat images from non-cat images!\nYou’re going to build two different models:\n\nA 2-layer neural network\nAn L-layer deep neural network\n\nThen, you’ll compare the performance of these models, and try out some different values for \\(L\\).\nLet’s look at the two architectures:\n\n\n\nFigure 2: 2-layer neural network.  The model can be summarized as: INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.\n\n\nDetailed Architecture of Figure 2: - The input is a (64,64,3) image which is flattened to a vector of size \\((12288,1)\\). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) of size \\((n^{[1]}, 12288)\\). - Then, add a bias term and take its relu to get the following vector: \\([a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T\\). - Multiply the resulting vector by \\(W^{[2]}\\) and add the intercept (bias). - Finally, take the sigmoid of the result. If it’s greater than 0.5, classify it as a cat.\n ### 3.2 - L-layer Deep Neural Network\nIt’s pretty difficult to represent an L-layer deep neural network using the above representation. However, here is a simplified network representation:\n\n\n\nFigure 3: L-layer neural network.  The model can be summarized as: [LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID\n\n\nDetailed Architecture of Figure 3: - The input is a (64,64,3) image which is flattened to a vector of size (12288,1). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) and then you add the intercept \\(b^{[1]}\\). The result is called the linear unit. - Next, take the relu of the linear unit. This process could be repeated several times for each \\((W^{[l]}, b^{[l]})\\) depending on the model architecture. - Finally, take the sigmoid of the final linear unit. If it is greater than 0.5, classify it as a cat.\n ### 3.3 - General Methodology\nAs usual, you’ll follow the Deep Learning methodology to build the model:\n\nInitialize parameters / Define hyperparameters\nLoop for num_iterations:\n\nForward propagation\nCompute cost function\nBackward propagation\nUpdate parameters (using parameters, and grads from backprop)\n\nUse trained parameters to predict labels\n\nNow go ahead and implement those two models!\n ## 4 - Two-layer Neural Network\n ### Exercise 1 - two_layer_model\nUse the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)\nlearning_rate = 0.0075\n\n\n# GRADED FUNCTION: two_layer_model\n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- If set to True, this will print the cost every 100 iterations \n    \n    Returns:\n    parameters -- a dictionary containing W1, W2, b1, and b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    #(≈ 1 line of code)\n    # parameters = ...\n    # YOUR CODE STARTS HERE\n    \n    parameters = initialize_parameters(n_x, n_h, n_y)\n    # YOUR CODE ENDS HERE\n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        #(≈ 2 lines of code)\n        # A1, cache1 = ...\n        # A2, cache2 = ...\n        # YOUR CODE STARTS HERE\n        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n        \n        # YOUR CODE ENDS HERE\n        \n        # Compute cost\n        #(≈ 1 line of code)\n        # cost = ...\n        # YOUR CODE STARTS HERE\n        cost = compute_cost(A2, Y)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n        #(≈ 2 lines of code)\n        # dA1, dW2, db2 = ...\n        # dA0, dW1, db1 = ...\n        # YOUR CODE STARTS HERE\n        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n        # YOUR CODE ENDS HERE\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n        #(approx. 1 line of code)\n        # parameters = ...\n        # YOUR CODE STARTS HERE\n        \n        parameters = update_parameters(parameters, grads, learning_rate)\n        # YOUR CODE ENDS HERE\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n\n    return parameters, costs\n\ndef plot_costs(costs, learning_rate=0.0075):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\ntwo_layer_model_test(two_layer_model)\n\nCost after iteration 1: 0.6926114346158595\nCost after first iteration: 0.693049735659989\nCost after iteration 1: 0.6915746967050506\nCost after iteration 1: 0.6915746967050506\nCost after iteration 1: 0.6915746967050506\nCost after iteration 2: 0.6524135179683452\n All tests passed.\n\n\nExpected output:\ncost after iteration 1 must be around 0.69\n ### 4.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your parameters.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\nplot_costs(costs, learning_rate)\n\nCost after iteration 0: 0.693049735659989\nCost after iteration 100: 0.6464320953428849\nCost after iteration 200: 0.6325140647912677\nCost after iteration 300: 0.6015024920354665\nCost after iteration 400: 0.5601966311605747\nCost after iteration 500: 0.5158304772764729\nCost after iteration 600: 0.4754901313943325\nCost after iteration 700: 0.43391631512257495\nCost after iteration 800: 0.4007977536203886\nCost after iteration 900: 0.3580705011323798\nCost after iteration 1000: 0.3394281538366413\nCost after iteration 1100: 0.30527536361962654\nCost after iteration 1200: 0.2749137728213015\nCost after iteration 1300: 0.2468176821061484\nCost after iteration 1400: 0.19850735037466102\nCost after iteration 1500: 0.17448318112556638\nCost after iteration 1600: 0.1708076297809692\nCost after iteration 1700: 0.11306524562164715\nCost after iteration 1800: 0.09629426845937156\nCost after iteration 1900: 0.0834261795972687\nCost after iteration 2000: 0.07439078704319085\nCost after iteration 2100: 0.06630748132267933\nCost after iteration 2200: 0.05919329501038172\nCost after iteration 2300: 0.053361403485605606\nCost after iteration 2400: 0.04855478562877019\nCost after iteration 2499: 0.04421498215868956\n\n\n\n\n\n\n\n\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.6930497356599888\n\n\n\n\nCost after iteration 100\n\n\n0.6464320953428849\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.04421498215868956\n\n\n\nNice! You successfully trained the model. Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.\nNow, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.\n\npredictions_train = predict(train_x, train_y, parameters)\n\nAccuracy: 0.9999999999999998\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.9999999999999998\n\n\n\n\npredictions_test = predict(test_x, test_y, parameters)\n\nAccuracy: 0.72\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.72\n\n\n\n\nCongratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an \\(L\\)-layer model.\nNote: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and you’ll hear more about it in the next course. Early stopping is a way to prevent overfitting.\n ## 5 - L-layer Neural Network\n ### Exercise 2 - L_layer_model\nUse the helper functions you implemented previously to build an \\(L\\)-layer neural network with the following structure: [LINEAR -&gt; RELU]\\(\\times\\)(L-1) -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters_deep(layers_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS ###\nlayers_dims = [12288, 20, 7, 5, 1] #  4-layer model\n\n\n# GRADED FUNCTION: L_layer_model\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization.\n    #(≈ 1 line of code)\n    # parameters = ...\n    # YOUR CODE STARTS HERE\n    parameters = initialize_parameters_deep(layers_dims)\n    \n    # YOUR CODE ENDS HERE\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.\n        #(≈ 1 line of code)\n        # AL, caches = ...\n        # YOUR CODE STARTS HERE\n        AL, caches = L_model_forward(X, parameters)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Compute cost.\n        #(≈ 1 line of code)\n        # cost = ...\n        # YOUR CODE STARTS HERE\n        cost = compute_cost(AL, Y)\n        \n        # YOUR CODE ENDS HERE\n    \n        # Backward propagation.\n        #(≈ 1 line of code)\n        # grads = ...    \n        # YOUR CODE STARTS HERE\n        grads = L_model_backward(AL, Y, caches)\n        \n        # YOUR CODE ENDS HERE\n \n        # Update parameters.\n        #(≈ 1 line of code)\n        # parameters = ...\n        # YOUR CODE STARTS HERE\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # YOUR CODE ENDS HERE\n                \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\nL_layer_model_test(L_layer_model)\n\nCost after iteration 0: 0.7717493284237686\nCost after first iteration: 0.7717493284237686\nCost after iteration 1: 0.7070709008912569\nCost after iteration 1: 0.7070709008912569\nCost after iteration 1: 0.7070709008912569\nCost after iteration 2: 0.7063462654190897\n All tests passed.\n\n\n ### 5.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your model as a 4-layer neural network.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n\nCost after iteration 0: 0.7717493284237686\nCost after iteration 100: 0.6720534400822914\nCost after iteration 200: 0.6482632048575212\nCost after iteration 300: 0.6115068816101356\nCost after iteration 400: 0.5670473268366111\nCost after iteration 500: 0.5401376634547801\nCost after iteration 600: 0.5279299569455267\nCost after iteration 700: 0.4654773771766851\nCost after iteration 800: 0.369125852495928\nCost after iteration 900: 0.39174697434805344\nCost after iteration 1000: 0.31518698886006163\nCost after iteration 1100: 0.2726998441789385\nCost after iteration 1200: 0.23741853400268137\nCost after iteration 1300: 0.19960120532208644\nCost after iteration 1400: 0.18926300388463307\nCost after iteration 1500: 0.16118854665827753\nCost after iteration 1600: 0.14821389662363316\nCost after iteration 1700: 0.13777487812972944\nCost after iteration 1800: 0.1297401754919012\nCost after iteration 1900: 0.12122535068005211\nCost after iteration 2000: 0.11382060668633713\nCost after iteration 2100: 0.10783928526254133\nCost after iteration 2200: 0.10285466069352679\nCost after iteration 2300: 0.10089745445261786\nCost after iteration 2400: 0.09287821526472398\nCost after iteration 2499: 0.08843994344170202\n\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.771749\n\n\n\n\nCost after iteration 100\n\n\n0.672053\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.088439\n\n\n\n\npred_train = predict(train_x, train_y, parameters)\n\nAccuracy: 0.9856459330143539\n\n\nExpected Output:\n\n\n\nTrain Accuracy\n\n\n0.985645933014\n\n\n\n\npred_test = predict(test_x, test_y, parameters)\n\nAccuracy: 0.8\n\n\nExpected Output:\n\n\n\nTest Accuracy\n\n\n0.8\n\n\n\n\n\nCongrats! It seems that your 4-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.\nThis is pretty good performance for this task. Nice job!\nIn the next course on “Improving deep neural networks,” you’ll be able to obtain even higher accuracy by systematically searching for better hyperparameters: learning_rate, layers_dims, or num_iterations, for example.\n ## 6 - Results Analysis\nFirst, take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.\n\nprint_mislabeled_images(classes, test_x, test_y, pred_test)\n\n\n\n\n\n\n\n\nA few types of images the model tends to do poorly on include: - Cat body in an unusual position - Cat appears against a background of a similar color - Unusual cat color and species - Camera Angle - Brightness of the picture - Scale variation (cat is very large or small in image)\n\n\nCongratulations on finishing this assignment!\nYou just built and trained a deep L-layer neural network, and applied it in order to distinguish cats from non-cats, a very serious and important task in deep learning. ;)\nBy now, you’ve also completed all the assignments for Course 1 in the Deep Learning Specialization. Amazing work! If you’d like to test out how closely you resemble a cat yourself, there’s an optional ungraded exercise below, where you can test your own image.\nGreat work and hope to see you in the next course!\n ## 7 - Test with your own image (optional/ungraded exercise) ##\nFrom this point, if you so choose, you can use your own image to test the output of your model. To do that follow these steps:\n\nClick on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub.\nAdd your image to this Jupyter Notebook’s directory, in the “images” folder\nChange your image’s name in the following code\nRun the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n\n## START CODE HERE ##\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)\n## END CODE HERE ##\n\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\n\nmy_predicted_image = predict(image, my_label_y, parameters)\n\n\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nReferences:\n\nfor auto-reloading external module: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "href": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "title": "Planar data classification with one hidden layer",
    "section": "",
    "text": "Welcome to your week 3 programming assignment! It’s time to build your first neural network, which will have one hidden layer. Now, you’ll notice a big difference between this model and the one you implemented previously using logistic regression.\nBy the end of this assignment, you’ll be able to:"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "title": "Planar data classification with one hidden layer",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "href": "nb/coursera/deep_learning_1/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "title": "Planar data classification with one hidden layer",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Load the Dataset\n\nExercise 1\n\n3 - Simple Logistic Regression\n4 - Neural Network model\n\n4.1 - Defining the neural network structure\n\nExercise 2 - layer_sizes\n\n4.2 - Initialize the model’s parameters\n\nExercise 3 - initialize_parameters\n\n4.3 - The Loop\n\nExercise 4 - forward_propagation\n\n4.4 - Compute the Cost\n\nExercise 5 - compute_cost\n\n4.5 - Implement Backpropagation\n\nExercise 6 - backward_propagation\n\n4.6 - Update Parameters\n\nExercise 7 - update_parameters\n\n4.7 - Integration\n\nExercise 8 - nn_model\n\n\n5 - Test the Model\n\n5.1 - Predict\n\nExercise 9 - predict\n\n5.2 - Test the Model on the Planar Dataset\n\n6 - Tuning hidden layer size (optional/ungraded exercise)\n7- Performance on other datasets\n\n # 1 - Packages\nFirst import all the packages that you will need during this assignment.\n\nnumpy is the fundamental package for scientific computing with Python.\nsklearn provides simple and efficient tools for data mining and data analysis.\nmatplotlib is a library for plotting graphs in Python.\ntestCases provides some test examples to assess the correctness of your functions\nplanar_utils provide various useful functions used in this assignment\n\n\n### v1.1\n\n\n# Package imports\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nfrom public_tests import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2\n\n # 2 - Load the Dataset\n\nX, Y = load_planar_dataset()\n\nVisualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.\n\n# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nYou have: - a numpy-array (matrix) X that contains your features (x1, x2) - a numpy-array (vector) Y that contains your labels (red:0, blue:1).\nFirst, get a better sense of what your data is like.\n ### Exercise 1\nHow many training examples do you have? In addition, what is the shape of the variables X and Y?\nHint: How do you get the shape of a numpy array? (help)\n\n# (≈ 3 lines of code)\n# shape_X = ...\n# shape_Y = ...\n# training set size\n# m = ...\n# YOUR CODE STARTS HERE\nshape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[0]\n# YOUR CODE ENDS HERE\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nThe shape of X is: (2, 400)\nThe shape of Y is: (1, 400)\nI have m = 2 training examples!\n\n\nExpected Output:\n\n\n\nshape of X\n\n\n(2, 400)\n\n\n\n\nshape of Y\n\n\n(1, 400)\n\n\n\n\nm\n\n\n400\n\n\n\n ## 3 - Simple Logistic Regression\nBefore building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\n/Users/vitvly/c/lnu/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nYou can now plot the decision boundary of these models! Run the code below.\n\n# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n\nAccuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)\n\n\n/var/folders/jr/7vzj1lzn0rx65bxcqn8nrwqw0000gn/T/ipykernel_49977/4242423965.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n\n\n\n\n\n\n\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n47%\n\n\n\nInterpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!\n ## 4 - Neural Network model\nLogistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.\nThe model: \nMathematically:\nFor one example \\(x^{(i)}\\): \\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\\] \\[a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\\] \\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\\] \\[\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\\] \\[y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\\]\nGiven the predictions on all the examples, you can also compute the cost \\(J\\) as follows: \\[J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\\]\nReminder: The general methodology to build a Neural Network is to: 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model’s parameters 3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)\nIn practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data.\n ### 4.1 - Defining the neural network structure ####\n ### Exercise 2 - layer_sizes\nDefine three variables: - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4, as n_h = 4, but only for this Exercise 2) - n_y: the size of the output layer\nHint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.\n\n# GRADED FUNCTION: layer_sizes\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    #(≈ 3 lines of code)\n    # n_x = ... \n    # n_h = ...\n    # n_y = ... \n    # YOUR CODE STARTS HERE\n    n_x = X.shape[0]\n    n_h = 4\n    n_y = Y.shape[0]\n    # YOUR CODE ENDS HERE\n    return (n_x, n_h, n_y)\n\n\nt_X, t_Y = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nlayer_sizes_test(layer_sizes)\n\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n\n\nExpected output\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n ### 4.2 - Initialize the model’s parameters ####\n ### Exercise 3 - initialize_parameters\nImplement the function initialize_parameters().\nInstructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.\n\n# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = np.random.randn(n_h,n_x) * 0.01 \n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01 \n    b2 = np.zeros((n_y, 1))\n    # YOUR CODE ENDS HERE\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nnp.random.seed(2)\nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test(initialize_parameters)\n\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n ### 4.3 - The Loop\n ### Exercise 4 - forward_propagation\nImplement forward_propagation() using the following equations:\n\\[Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}\\] \\[A^{[1]} = \\tanh(Z^{[1]})\\tag{2}\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}\\] \\[\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}\\]\nInstructions:\n\nCheck the mathematical representation of your classifier in the figure above.\nUse the function sigmoid(). It’s built into (imported) this notebook.\nUse the function np.tanh(). It’s part of the numpy library.\nImplement using these steps:\n\nRetrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() by using parameters[\"..\"].\nImplement Forward Propagation. Compute \\(Z^{[1]}, A^{[1]}, Z^{[2]}\\) and \\(A^{[2]}\\) (the vector of all your predictions on all the examples in the training set).\n\nValues needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.\n\n\n# GRADED FUNCTION:forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]   \n    # YOUR CODE ENDS HERE\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    # (≈ 4 lines of code)\n    # Z1 = ...\n    # A1 = ...\n    # Z2 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    \n    # YOUR CODE ENDS HERE\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n\nt_X, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(t_X, parameters)\nprint(\"A2 = \" + str(A2))\n\nforward_propagation_test(forward_propagation)\n\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n\n\nExpected output\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n ### 4.4 - Compute the Cost\nNow that you’ve computed \\(A^{[2]}\\) (in the Python variable “A2”), which contains \\(a^{[2](i)}\\) for all examples, you can compute the cost function as follows:\n\\[J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\\]\n ### Exercise 5 - compute_cost\nImplement compute_cost() to compute the value of the cost \\(J\\).\nInstructions: - There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops: \\(- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})\\):\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)          \n\nUse that to build the whole expression of the cost function.\n\nNotes:\n\nYou can use either np.multiply() and then np.sum() or directly np.dot()).\n\nIf you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array.\n\nYou can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).\nYou can also cast the array as a type float using float().\n\n\n# GRADED FUNCTION: compute_cost\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    # (≈ 2 lines of code)\n    # logprobs = ...\n    # cost = ...\n    # YOUR CODE STARTS HERE\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)\n    cost = -np.sum(logprobs)/m\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    \n    return cost\n\n\nA2, t_Y = compute_cost_test_case()\ncost = compute_cost(A2, t_Y)\nprint(\"cost = \" + str(compute_cost(A2, t_Y)))\n\ncompute_cost_test(compute_cost)\n\ncost = 0.6930587610394646\nAll tests passed!\n\n\nExpected output\ncost = 0.6930587610394646\nAll tests passed!\n ### 4.5 - Implement Backpropagation\nUsing the cache computed during forward propagation, you can now implement backward propagation.\n ### Exercise 6 - backward_propagation\nImplement the function backward_propagation().\nInstructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.\n\n\n\nFigure 1: Backpropagation. Use the six equations on the right.\n\n\n\n\nTips:\n\nTo compute dZ1 you’ll need to compute \\(g^{[1]'}(Z^{[1]})\\). Since \\(g^{[1]}(.)\\) is the tanh activation function, if \\(a = g^{[1]}(z)\\) then \\(g^{[1]'}(z) = 1-a^2\\). So you can compute \\(g^{[1]'}(Z^{[1]})\\) using (1 - np.power(A1, 2)).\n\n\n\n# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    #(≈ 2 lines of code)\n    # W1 = ...\n    # W2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    \n    # YOUR CODE ENDS HERE\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    #(≈ 2 lines of code)\n    # A1 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    \n    # YOUR CODE ENDS HERE\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n    # dZ2 = ...\n    # dW2 = ...\n    # db2 = ...\n    # dZ1 = ...\n    # dW1 = ...\n    # db1 = ...\n    # YOUR CODE STARTS HERE\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T)/m\n    db2 = np.sum(dZ2, axis = 1, keepdims=True)/m\n    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1,2))\n    dW1 = np.dot(dZ1, X.T)/m\n    db1 = np.sum(dZ1, axis = 1, keepdims=True)/m\n    # YOUR CODE ENDS HERE\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads\n\n\nparameters, cache, t_X, t_Y = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\nbackward_propagation_test(backward_propagation)\n\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n\n\nExpected output\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n ### 4.6 - Update Parameters\n ### Exercise 7 - update_parameters\nImplement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\nGeneral gradient descent rule: \\(\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\) where \\(\\alpha\\) is the learning rate and \\(\\theta\\) represents a parameter.\n \n\n\nFigure 2: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n\nHint\n\nUse copy.deepcopy(...) when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n\n\n# GRADED FUNCTION: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = copy.deepcopy(parameters[\"W1\"])\n    b1 = copy.deepcopy(parameters[\"b1\"])\n    W2 = copy.deepcopy(parameters[\"W2\"])\n    b2 = copy.deepcopy(parameters[\"b2\"])\n    # YOUR CODE ENDS HERE\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    #(≈ 4 lines of code)\n    # dW1 = ...\n    # db1 = ...\n    # dW2 = ...\n    # db2 = ...\n    # YOUR CODE STARTS HERE\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    # YOUR CODE ENDS HERE\n    \n    # Update rule for each parameter\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2   \n    # YOUR CODE ENDS HERE\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n ### 4.7 - Integration\nIntegrate your functions in nn_model()\n ### Exercise 8 - nn_model\nBuild your neural network model in nn_model().\nInstructions: The neural network model has to use the previous functions in the right order.\n\n# GRADED FUNCTION: nn_model\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters\n    #(≈ 1 line of code)\n    # parameters = ...\n    # YOUR CODE STARTS HERE\n    parameters = initialize_parameters(X.shape[0], n_h, Y.shape[0])\n    \n    # YOUR CODE ENDS HERE\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        #(≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        # A2, cache = ...\n        \n        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n        # cost = ...\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        # grads = ...\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        # parameters = ...\n        \n        # YOUR CODE STARTS HERE\n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nnn_model_test(nn_model)\n\nCost after iteration 0: 0.693086\nCost after iteration 1000: 0.000220\nCost after iteration 2000: 0.000108\nCost after iteration 3000: 0.000072\nCost after iteration 4000: 0.000054\nCost after iteration 5000: 0.000043\nCost after iteration 6000: 0.000036\nCost after iteration 7000: 0.000030\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n\n\nExpected output\nCost after iteration 0: 0.693198\nCost after iteration 1000: 0.000219\nCost after iteration 2000: 0.000108\n...\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n ## 5 - Test the Model\n ### 5.1 - Predict\n ### Exercise 9 - predict\nPredict with your model by building predict(). Use forward propagation to predict results.\nReminder: predictions = \\(y_{prediction} = \\mathbb 1 \\text{{activation &gt; 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\\)\nAs an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold)\n\n# GRADED FUNCTION: predict\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    #(≈ 2 lines of code)\n    # A2, cache = ...\n    # predictions = ...\n    # YOUR CODE STARTS HERE\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2 &gt; 0.5)\n    \n    # YOUR CODE ENDS HERE\n    \n    return predictions\n\n\nparameters, t_X = predict_test_case()\n\npredictions = predict(parameters, t_X)\nprint(\"Predictions: \" + str(predictions))\n\npredict_test(predict)\n\nPredictions: [[ True False  True]]\nAll tests passed!\n\n\nExpected output\nPredictions: [[ True False  True]]\nAll tests passed!\n ### 5.2 - Test the Model on the Planar Dataset\nIt’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of \\(n_h\\) hidden units!\n\n# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\nCost after iteration 0: 0.693162\nCost after iteration 1000: 0.258625\nCost after iteration 2000: 0.239334\nCost after iteration 3000: 0.230802\nCost after iteration 4000: 0.225528\nCost after iteration 5000: 0.221845\nCost after iteration 6000: 0.219094\nCost after iteration 7000: 0.220638\nCost after iteration 8000: 0.219418\nCost after iteration 9000: 0.218528\n\n\nText(0.5, 1.0, 'Decision Boundary for hidden layer size 4')\n\n\n\n\n\n\n\n\n\n\n# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n\nAccuracy: 90%\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n90%\n\n\n\nAccuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.\n\nCongrats on finishing this Programming Assignment!\nHere’s a quick recap of all you just accomplished:\n\nBuilt a complete 2-class classification neural network with a hidden layer\nMade good use of a non-linear unit\nComputed the cross entropy loss\nImplemented forward and backward propagation\nSeen the impact of varying the hidden layer size, including overfitting.\n\nYou’ve created a neural network that can learn patterns! Excellent work. Below, there are some optional exercises to try out some other hidden layer sizes, and other datasets.\n ## 6 - Tuning hidden layer size (optional/ungraded exercise)\nRun the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.\n\n# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5]\n\n# you can try with different hidden layer sizes\n# but make sure before you submit the assignment it is set as \"hidden_layer_sizes = [1, 2, 3, 4, 5]\"\n# hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n\nAccuracy for 1 hidden units: 67.5 %\nAccuracy for 2 hidden units: 67.25 %\nAccuracy for 3 hidden units: 90.75 %\nAccuracy for 4 hidden units: 90.5 %\nAccuracy for 5 hidden units: 91.25 %\n\n\n\n\n\n\n\n\n\nInterpretation: - The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. - The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. - Later, you’ll become familiar with regularization, which lets you use very large models (such as n_h = 50) without much overfitting.\nNote: Remember to submit the assignment by clicking the blue “Submit Assignment” button at the upper-right.\nSome optional/ungraded questions that you can explore if you wish: - What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? - Play with the learning_rate. What happens? - What if we change the dataset? (See part 7 below!)\n ## 7- Performance on other datasets\nIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.\n\n# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### START CODE HERE ### (choose your dataset)\ndataset = \"noisy_moons\"\n### END CODE HERE ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nReferences:\n\nhttp://scs.ryerson.ca/~aharley/neural-networks/\nhttp://cs231n.github.io/neural-networks-case-study/"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "href": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\nInstructions: - Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. - Use np.dot(X,Y) to calculate dot products.\nYou will learn to: - Build the general architecture of a learning algorithm, including: - Initializing parameters - Calculating the cost function and its gradient - Using an optimization algorithm (gradient descent) - Gather all three functions above into a main model function, in the right order."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "href": "nb/coursera/deep_learning_1/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Overview of the Problem set\n\nExercise 1\nExercise 2\n\n3 - General Architecture of the learning algorithm\n4 - Building the parts of our algorithm\n\n4.1 - Helper functions\n\nExercise 3 - sigmoid\n\n4.2 - Initializing parameters\n\nExercise 4 - initialize_with_zeros\n\n4.3 - Forward and Backward propagation\n\nExercise 5 - propagate\n\n4.4 - Optimization\n\nExercise 6 - optimize\nExercise 7 - predict\n\n\n5 - Merge all functions into a model\n\nExercise 8 - model\n\n6 - Further analysis (optional/ungraded exercise)\n7 - Test with your own image (optional/ungraded exercise)\n\n ## 1 - Packages ##\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n### v1.2\n\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\nfrom public_tests import *\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Overview of the Problem set ##\nProblem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 29\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\n\n\n\n\nMany software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n ### Exercise 1 Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n#(≈ 3 lines of code)\n# m_train = \n# m_test = \n# num_px = \n# YOUR CODE STARTS HERE\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n# YOUR CODE ENDS HERE\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nExpected Output for m_train, m_test and num_px:\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n ### Exercise 2 Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\n#(≈ 2 lines of code)\n# train_set_x_flatten = ...\n# test_set_x_flatten = ...\n# YOUR CODE STARTS HERE\ntrain_set_x_flatten = train_set_x_orig.reshape((train_set_x_orig.shape[0], -1)).T\ntest_set_x_flatten = test_set_x_orig.reshape((test_set_x_orig.shape[0], -1)).T\n# YOUR CODE ENDS HERE\n\n# Check that the first 10 pixels of the second image are in the correct place\nassert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\nassert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\n\n\nExpected Output:\n\n\n\ntrain_set_x_flatten shape\n\n\n(12288, 209)\n\n\n\n\ntrain_set_y shape\n\n\n(1, 209)\n\n\n\n\ntest_set_x_flatten shape\n\n\n(12288, 50)\n\n\n\n\ntest_set_y shape\n\n\n(1, 50)\n\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\n\nWhat you need to remember:\nCommon steps for pre-processing a new dataset are: - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) - “Standardize” the data\n ## 3 - General Architecture of the learning algorithm ##\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps: - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost\n- Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude\n ## 4 - Building the parts of our algorithm ##\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n ### 4.1 - Helper functions\n ### Exercise 3 - sigmoid Using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute \\(sigmoid(z) = \\frac{1}{1 + e^{-z}}\\) for \\(z = w^T x + b\\) to make predictions. Use np.exp().\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    #(≈ 1 line of code)\n    # s = ...\n    # YOUR CODE STARTS HERE\n    s = 1/(1+np.exp(-z))\n    \n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid_test(sigmoid)\n\nsigmoid([0, 2]) = [0.5        0.88079708]\nAll tests passed!\n\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n[0.62245933 0.5        0.88079708]\n\n\n ### 4.2 - Initializing parameters\n ### Exercise 4 - initialize_with_zeros Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    \n    # (≈ 2 lines of code)\n    # w = ...\n    # b = ...\n    # YOUR CODE STARTS HERE\n    w = np.zeros((dim, 1), dtype=float)\n    b = 0.0\n    # YOUR CODE ENDS HERE\n\n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\ninitialize_with_zeros_test_1(initialize_with_zeros)\ninitialize_with_zeros_test_2(initialize_with_zeros)\n\nw = [[0.]\n [0.]]\nb = 0.0\nFirst test passed!\nSecond test passed!\n\n\n ### 4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n ### Exercise 5 - propagate Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    grads -- dictionary containing the gradients of the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    cost -- negative log-likelihood cost for logistic regression\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    #(≈ 2 lines of code)\n    # compute activation\n    # A = ...\n    # compute cost by using np.dot to perform multiplication. \n    # And don't use loops for the sum.\n    # cost = ...                                \n    # YOUR CODE STARTS HERE\n    A=sigmoid(np.dot(w.T, X) + b)\n    cost =-1/m* np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n    \n    # YOUR CODE ENDS HERE\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    #(≈ 2 lines of code)\n    # dw = ...\n    # db = ...\n    # YOUR CODE STARTS HERE\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    # YOUR CODE ENDS HERE\n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\n\n# X is using 3 examples, with 2 features each\n# Each example is stacked column-wise\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\npropagate_test(propagate)\n\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\nAll tests passed!\n\n\nExpected output\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\n ### 4.4 - Optimization - You have initialized your parameters. - You are also able to compute a cost function and its gradient. - Now, you want to update the parameters using gradient descent.\n ### Exercise 6 - optimize Write down the optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # (≈ 1 lines of code)\n        # Cost and gradient calculation \n        # grads, cost = ...\n        # YOUR CODE STARTS HERE\n        grads, cost = propagate(w,b,X,Y)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        # w = ...\n        # b = ...\n        # YOUR CODE STARTS HERE\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        \n        # YOUR CODE ENDS HERE\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\noptimize_test(optimize)\n\nw = [[0.80956046]\n [2.0508202 ]]\nb = 1.5948713189708588\ndw = [[ 0.17860505]\n [-0.04840656]]\ndb = -0.08888460336847771\nCosts = [array(0.15900538)]\nAll tests passed!\n\n\n ### Exercise 7 - predict The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    #(≈ 1 line of code)\n    # A = ...\n    # YOUR CODE STARTS HERE\n    A = sigmoid(np.dot(w.T,X)+b)\n    \n    # YOUR CODE ENDS HERE\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        #(≈ 4 lines of code)\n        # if A[0, i] &gt; ____ :\n        #     Y_prediction[0,i] = \n        # else:\n        #     Y_prediction[0,i] = \n        # YOUR CODE STARTS HERE\n        if A[0,i] &gt; 0.5:\n            Y_prediction[0,i] = 1\n        else:\n            Y_prediction[0,i] = 0  \n        \n        # YOUR CODE ENDS HERE\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredict_test(predict)\n\npredictions = [[1. 1. 0.]]\nAll tests passed!\n\n\n\nWhat to remember:\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples\n ## 5 - Merge all functions into a model ##\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n ### Exercise 8 - model Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize()\n\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    # (≈ 1 line of code)   \n    # initialize parameters with zeros\n    # and use the \"shape\" function to get the first dimension of X_train\n    # w, b = ...\n    \n    #(≈ 1 line of code)\n    # Gradient descent \n    # params, grads, costs = ...\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    # w = ...\n    # b = ...\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    # Y_prediction_test = ...\n    # Y_prediction_train = ...\n    \n    # YOUR CODE STARTS HERE\n    w, b = initialize_with_zeros(X_train.shape[0])\n    params, grads, costs = optimize(w, b, X_train, Y_train,num_iterations, learning_rate)\n    w = params[\"w\"]\n    b = params[\"b\"]\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    # YOUR CODE ENDS HERE\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nfrom public_tests import *\n\nmodel_test(model)\n\nAll tests passed!\n\n\nIf you pass all the tests, run the following cell to train your model.\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, you predicted that it is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n ## 6 - Further analysis (optional/ungraded exercise) ##\nCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nTraining a model with learning rate: 0.01\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.001\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.0001\n\n-------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nInterpretation: - Different learning rates give different costs and thus different predictions results. - If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). - A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. - In deep learning, we usually recommend that you: - Choose the learning rate that better minimizes the cost function. - If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)\n ## 7 - Test with your own image (optional/ungraded exercise) ##\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: 1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook’s directory, in the “images” folder 3. Change your image’s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n# change this to the name of your image file\nmy_image = \"Cats-image-cats-36712791-1222-917.jpg\"   \n\n# We preprocess the image to fit your algorithm.\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\nmy_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0.0, your algorithm predicts a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nWhat to remember from this assignment: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)\nBibliography: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html",
    "href": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html",
    "title": "Optimization Methods",
    "section": "",
    "text": "Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you’ll gain skills with some more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.\nBy the end of this notebook, you’ll be able to:\nNotations: As usual, $ = $ da for any variable a.\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html#important-note-on-submission-to-the-autograder",
    "title": "Optimization Methods",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html#table-of-contents",
    "href": "nb/coursera/deep_learning_2/W2A1/Optimization_methods.html#table-of-contents",
    "title": "Optimization Methods",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1- Packages\n2 - Gradient Descent\n\nExercise 1 - update_parameters_with_gd\n\n3 - Mini-Batch Gradient Descent\n\nExercise 2 - random_mini_batches\n\n4 - Momentum\n\nExercise 3 - initialize_velocity\nExercise 4 - update_parameters_with_momentum\n\n5 - Adam\n\nExercise 5 - initialize_adam\nExercise 6 - update_parameters_with_adam\n\n6 - Model with different Optimization algorithms\n\n6.1 - Mini-Batch Gradient Descent\n6.2 - Mini-Batch Gradient Descent with Momentum\n6.3 - Mini-Batch with Adam\n6.4 - Summary\n\n7 - Learning Rate Decay and Scheduling\n\n7.1 - Decay on every iteration\n\nExercise 7 - update_lr\n\n7.2 - Fixed Interval Scheduling\n\nExercise 8 - schedule_lr_decay\n\n7.3 - Using Learning Rate Decay for each Optimization Method\n\n7.3.1 - Gradient Descent with Learning Rate Decay\n7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\n7.3.3 - Adam with Learning Rate Decay\n\n7.4 - Achieving similar performance with different methods\n\n\n ## 1- Packages\n\n### v1.1\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport math\nimport sklearn\nimport sklearn.datasets\n\nfrom opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\nfrom opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\nfrom copy import deepcopy\nfrom testCases import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Gradient Descent\nA simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all \\(m\\) examples on each step, it is also called Batch Gradient Descent.\n ### Exercise 1 - update_parameters_with_gd\nImplement the gradient descent update rule. The gradient descent rule is, for \\(l = 1, ..., L\\): \\[ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}\\] \\[ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}\\]\nwhere L is the number of layers and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\n# GRADED FUNCTION: update_parameters_with_gd\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using one step of gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters to be updated:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients to update each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] =  \n        # parameters[\"b\" + str(l)] = \n        # YOUR CODE STARTS HERE\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*grads[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*grads[\"db\" + str(l)]   \n        \n        # YOUR CODE ENDS HERE\n    return parameters\n\n\nparameters, grads, learning_rate = update_parameters_with_gd_test_case()\nlearning_rate = 0.01\nparameters = update_parameters_with_gd(parameters, grads, learning_rate)\n\nprint(\"W1 =\\n\" + str(parameters[\"W1\"]))\nprint(\"b1 =\\n\" + str(parameters[\"b1\"]))\nprint(\"W2 =\\n\" + str(parameters[\"W2\"]))\nprint(\"b2 =\\n\" + str(parameters[\"b2\"]))\n\nupdate_parameters_with_gd_test(update_parameters_with_gd)\n\nW1 =\n[[ 1.63312395 -0.61217855 -0.5339999 ]\n [-1.06196243  0.85396039 -2.3105546 ]]\nb1 =\n[[ 1.73978682]\n [-0.77021546]]\nW2 =\n[[ 0.32587637 -0.24814147]\n [ 1.47146563 -2.05746183]\n [-0.32772076 -0.37713775]]\nb2 =\n[[ 1.13773698]\n [-1.09301954]\n [-0.16397615]]\nAll tests passed\n\n\nA variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent, where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.\n\n(Batch) Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost\n    cost_total = compute_cost(a, Y)  # Cost for m training examples\n    # Backward propagation\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters\n    parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\n        \n\nStochastic Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    cost_total = 0\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters\n        parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\nIn Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here’s what that looks like:\n\n\n\n  Figure 1  : SGD vs GD “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence, but each step is a lot faster to compute for SGD than it is for GD, as it uses only one training example (vs. the whole batch for GD).\n\n\nNote also that implementing SGD requires 3 for-loops in total: 1. Over the number of iterations 2. Over the \\(m\\) training examples 3. Over the layers (to update all parameters, from \\((W^{[1]},b^{[1]})\\) to \\((W^{[L]},b^{[L]})\\))\nIn practice, you’ll often get faster results if you don’t use the entire training set, or just one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.\n\n\n\n  Figure 2 :  SGD vs Mini-Batch GD “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization.\n\n\n ## 3 - Mini-Batch Gradient Descent\nNow you’ll build some mini-batches from the training set (X, Y).\nThere are two steps: - Shuffle: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the \\(i^{th}\\) column of X is the example corresponding to the \\(i^{th}\\) label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.\n\n\nPartition: Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this:\n\n\n ### Exercise 2 - random_mini_batches\nImplement random_mini_batches. The shuffling part has already been coded for you! To help with the partitioning step, you’ve been provided the following code that selects the indexes for the \\(1^{st}\\) and \\(2^{nd}\\) mini-batches:\nfirst_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\nsecond_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n...\nNote that the last mini-batch might end up smaller than mini_batch_size=64. Let \\(\\lfloor s \\rfloor\\) represents \\(s\\) rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be \\(\\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\) mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be \\(\\left(m-mini_\\_batch_\\_size \\times \\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\right)\\).\nHint:\n\\[mini\\_batch\\_X = shuffled\\_X[:, i : j]\\]\nThink of a way in which you can use the for loop variable k help you increment i and j in multiples of mini_batch_size.\nAs an example, if you want to increment in multiples of 3, you could the following:\nn = 3\nfor k in (0 , 5):\n    print(k * n)\n\n# GRADED FUNCTION: random_mini_batches\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1, m))\n    \n    inc = mini_batch_size\n\n    # Step 2 - Partition (shuffled_X, shuffled_Y).\n    # Cases with a complete mini batch size only i.e each of 64 examples.\n    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        # (approx. 2 lines)\n        # mini_batch_X =  \n        # mini_batch_Y =\n        # YOUR CODE STARTS HERE\n        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        \n        # YOUR CODE ENDS HERE\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # For handling the end case (last mini-batch &lt; mini_batch_size i.e less than 64)\n    if m % mini_batch_size != 0:\n        #(approx. 2 lines)\n        # mini_batch_X =\n        # mini_batch_Y =\n        # YOUR CODE STARTS HERE\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]  \n        \n        # YOUR CODE ENDS HERE\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nnp.random.seed(1)\nmini_batch_size = 64\nnx = 12288\nm = 148\nX = np.array([x for x in range(nx * m)]).reshape((m, nx)).T\nY = np.random.randn(1, m) &lt; 0.5\n\nmini_batches = random_mini_batches(X, Y, mini_batch_size)\nn_batches = len(mini_batches)\n\nassert n_batches == math.ceil(m / mini_batch_size), f\"Wrong number of mini batches. {n_batches} != {math.ceil(m / mini_batch_size)}\"\nfor k in range(n_batches - 1):\n    assert mini_batches[k][0].shape == (nx, mini_batch_size), f\"Wrong shape in {k} mini batch for X\"\n    assert mini_batches[k][1].shape == (1, mini_batch_size), f\"Wrong shape in {k} mini batch for Y\"\n    assert np.sum(np.sum(mini_batches[k][0] - mini_batches[k][0][0], axis=0)) == ((nx * (nx - 1) / 2 ) * mini_batch_size), \"Wrong values. It happens if the order of X rows(features) changes\"\nif ( m % mini_batch_size &gt; 0):\n    assert mini_batches[n_batches - 1][0].shape == (nx, m % mini_batch_size), f\"Wrong shape in the last minibatch. {mini_batches[n_batches - 1][0].shape} != {(nx, m % mini_batch_size)}\"\n\nassert np.allclose(mini_batches[0][0][0][0:3], [294912,  86016, 454656]), \"Wrong values. Check the indexes used to form the mini batches\"\nassert np.allclose(mini_batches[-1][0][-1][0:3], [1425407, 1769471, 897023]), \"Wrong values. Check the indexes used to form the mini batches\"\n\nprint(\"\\033[92mAll tests passed!\")\n\nAll tests passed!\n\n\n\nt_X, t_Y, mini_batch_size = random_mini_batches_test_case()\nmini_batches = random_mini_batches(t_X, t_Y, mini_batch_size)\n\nprint (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nrandom_mini_batches_test(random_mini_batches)\n\nshape of the 1st mini_batch_X: (12288, 64)\nshape of the 2nd mini_batch_X: (12288, 64)\nshape of the 3rd mini_batch_X: (12288, 20)\nshape of the 1st mini_batch_Y: (1, 64)\nshape of the 2nd mini_batch_Y: (1, 64)\nshape of the 3rd mini_batch_Y: (1, 20)\nmini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n All tests passed.\n\n\n\nWhat you should remember: - Shuffling and Partitioning are the two steps required to build mini-batches - Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n ## 4 - Momentum\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.\nMomentum takes into account the past gradients to smooth out the update. The ‘direction’ of the previous gradients is stored in the variable \\(v\\). Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of \\(v\\) as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.\n\n\n\nFigure 3 : The red arrows show the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, the gradient is allowed to influence \\(v\\) and then take a step in the direction of \\(v\\). \n\n\n### Exercise 3 - initialize_velocity Initialize the velocity. The velocity, \\(v\\), is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is: for \\(l =1,...,L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\nNote that the iterator l starts at 1 in the for loop as the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript).\n\n# GRADED FUNCTION: initialize_velocity\n\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] =\n        # v[\"db\" + str(l)] =\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)     \n        \n        # YOUR CODE ENDS HERE\n        \n    return v\n\n\nparameters = initialize_velocity_test_case()\n\nv = initialize_velocity(parameters)\nprint(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))\n\ninitialize_velocity_test(initialize_velocity)\n\nv[\"dW1\"] =\n[[0. 0.]\n [0. 0.]\n [0. 0.]]\nv[\"db1\"] =\n[[0.]\n [0.]\n [0.]]\nv[\"dW2\"] =\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] =\n[[0.]\n [0.]\n [0.]]\n All tests passed.\n\n\n\n### Exercise 4 - update_parameters_with_momentum\nNow, implement the parameters update with momentum. The momentum update rule is, for \\(l = 1, ..., L\\):\n\\[ \\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}\\]\n\\[\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}\n\\end{cases}\\tag{4}\\]\nwhere L is the number of layers, \\(\\beta\\) is the momentum and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\) (that’s a “one” on the superscript).\n\n# GRADED FUNCTION: update_parameters_with_momentum\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(1, L + 1):\n        \n        # (approx. 4 lines)\n        # compute velocities\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # update parameters\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = beta*v[\"dW\" + str(l)] + (1-beta)*grads['dW' + str(l)]\n        v[\"db\" + str(l)] = beta*v[\"db\" + str(l)] + (1-beta)*grads['db' + str(l)]\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*v[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*v[\"db\" + str(l)]  \n        \n        # YOUR CODE ENDS HERE\n        \n    return parameters, v\n\n\nparameters, grads, v = update_parameters_with_momentum_test_case()\n\nparameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))\n\nupdate_parameters_with_momentum_test(update_parameters_with_momentum)\n\nW1 = \n[[ 1.62522322 -0.61179863 -0.52875457]\n [-1.071868    0.86426291 -2.30244029]]\nb1 = \n[[ 1.74430927]\n [-0.76210776]]\nW2 = \n[[ 0.31972282 -0.24924749]\n [ 1.46304371 -2.05987282]\n [-0.32294756 -0.38336269]]\nb2 = \n[[ 1.1341662 ]\n [-1.09920409]\n [-0.171583  ]]\nv[\"dW1\"] = \n[[-0.08778584  0.00422137  0.05828152]\n [-0.11006192  0.11447237  0.09015907]]\nv[\"db1\"] = \n[[0.05024943]\n [0.09008559]]\nv[\"dW2\"] = \n[[-0.06837279 -0.01228902]\n [-0.09357694 -0.02678881]\n [ 0.05303555 -0.06916608]]\nv[\"db2\"] = v[[-0.03967535]\n [-0.06871727]\n [-0.08452056]]\n All tests passed.\n\n\nNote that: - The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. - If \\(\\beta = 0\\), then this just becomes standard gradient descent without momentum.\nHow do you choose \\(\\beta\\)?\n\nThe larger the momentum \\(\\beta\\) is, the smoother the update, because it takes the past gradients into account more. But if \\(\\beta\\) is too big, it could also smooth out the updates too much.\nCommon values for \\(\\beta\\) range from 0.8 to 0.999. If you don’t feel inclined to tune this, \\(\\beta = 0.9\\) is often a reasonable default.\nTuning the optimal \\(\\beta\\) for your model might require trying several values to see what works best in terms of reducing the value of the cost function \\(J\\).\n\n\nWhat you should remember: - Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. - You have to tune a momentum hyperparameter \\(\\beta\\) and a learning rate \\(\\alpha\\).\n\n## 5 - Adam\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.\nHow does Adam work? 1. It calculates an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction). 2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction). 3. It updates parameters in a direction based on combining information from “1” and “2”.\nThe update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\] where: - t counts the number of steps taken of Adam - L is the number of layers - \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the two exponentially weighted averages. - \\(\\alpha\\) is the learning rate - \\(\\varepsilon\\) is a very small number to avoid dividing by zero\nAs usual, all parameters are stored in the parameters dictionary\n\n### Exercise 5 - initialize_adam\nInitialize the Adam variables \\(v, s\\) which keep track of the past information.\nInstruction: The variables \\(v, s\\) are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is: for \\(l = 1, ..., L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\ns[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\ns[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n\n# GRADED FUNCTION: initialize_adam\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(1, L + 1):\n    # (approx. 4 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n    # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n        s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)  \n    \n    # YOUR CODE ENDS HERE\n    \n    return v, s\n\n\nparameters = initialize_adam_test_case()\n\nv, s = initialize_adam(parameters)\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\nprint(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\nprint(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\nprint(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\nprint(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))\n\ninitialize_adam_test(initialize_adam)\n\nv[\"dW1\"] = \n[[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = \n[[0.]\n [0.]]\nv[\"dW2\"] = \n[[0. 0.]\n [0. 0.]\n [0. 0.]]\nv[\"db2\"] = \n[[0.]\n [0.]\n [0.]]\ns[\"dW1\"] = \n[[0. 0. 0.]\n [0. 0. 0.]]\ns[\"db1\"] = \n[[0.]\n [0.]]\ns[\"dW2\"] = \n[[0. 0.]\n [0. 0.]\n [0. 0.]]\ns[\"db2\"] = \n[[0.]\n [0.]\n [0.]]\n All tests passed.\n\n\n\n### Exercise 6 - update_parameters_with_adam\nNow, implement the parameters update with Adam. Recall the general update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\]\nNote that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\n# GRADED FUNCTION: update_parameters_with_adam\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    t -- Adam variable, counts the number of taken steps\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(1, L + 1):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1)*grads['dW' + str(l)]\n        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1)*grads['db' + str(l)]      \n        \n        # YOUR CODE ENDS HERE\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        # (approx. 2 lines)\n        # v_corrected[\"dW\" + str(l)] = ...\n        # v_corrected[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)]/(1-beta1**t)\n        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)]/(1-beta1**t)    \n        \n        # YOUR CODE ENDS HERE\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        #(approx. 2 lines)\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1-beta2)*np.square(grads['dW' + str(l)])\n        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1-beta2)*np.square(grads['db' + str(l)])       \n        \n        # YOUR CODE ENDS HERE\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        # (approx. 2 lines)\n        # s_corrected[\"dW\" + str(l)] = ...\n        # s_corrected[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)]/(1-beta2**t)\n        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)]/(1-beta2**t) \n        \n        # YOUR CODE ENDS HERE\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)]/(np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)]/(np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)        \n        \n        # YOUR CODE ENDS HERE\n\n    return parameters, v, s, v_corrected, s_corrected\n\n\nparametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon = update_parameters_with_adam_test_case()\n\nparameters, v, s, vc, sc  = update_parameters_with_adam(parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon)\nprint(f\"W1 = \\n{parameters['W1']}\")\nprint(f\"W2 = \\n{parameters['W2']}\")\nprint(f\"b1 = \\n{parameters['b1']}\")\nprint(f\"b2 = \\n{parameters['b2']}\")\n\nupdate_parameters_with_adam_test(update_parameters_with_adam)\n\nW1 = \n[[ 1.63937725 -0.62327448 -0.54308727]\n [-1.0578897   0.85032154 -2.31657668]]\nW2 = \n[[ 0.33400549 -0.23563857]\n [ 1.47715417 -2.04561842]\n [-0.33729882 -0.36908457]]\nb1 = \n[[ 1.72995096]\n [-0.7762447 ]]\nb2 = \n[[ 1.14852557]\n [-1.08492339]\n [-0.15740527]]\nAll tests passed\n\n\nExpected values:\nW1 = \n[[ 1.63937725 -0.62327448 -0.54308727]\n [-1.0578897   0.85032154 -2.31657668]]\nW2 = \n[[ 0.33400549 -0.23563857]\n [ 1.47715417 -2.04561842]\n [-0.33729882 -0.36908457]]\nb1 = \n[[ 1.72995096]\n [-0.7762447 ]]\nb2 = \n[[ 1.14852557]\n [-1.08492339]\n [-0.15740527]]\nYou now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.\n\n## 6 - Model with different Optimization algorithms\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\nA 3-layer neural network has already been implemented for you! You’ll train it with: - Mini-batch Gradient Descent: it will call your function: - update_parameters_with_gd() - Mini-batch Momentum: it will call your functions: - initialize_velocity() and update_parameters_with_momentum() - Mini-batch Adam: it will call your functions: - initialize_adam() and update_parameters_with_adam()\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\nNow, run this 3 layer neural network with each of the 3 optimization methods.\n\n### 6.1 - Mini-Batch Gradient Descent\nRun the following code to see how the model does with mini-batch gradient descent.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702405\nCost after epoch 1000: 0.668101\nCost after epoch 2000: 0.635288\nCost after epoch 3000: 0.600491\nCost after epoch 4000: 0.573367\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7166666666666667\n\n\n\n\n\n\n\n\n\n\n### 6.2 - Mini-Batch Gradient Descent with Momentum\nNext, run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small - but for more complex problems you might see bigger gains.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702413\nCost after epoch 1000: 0.668167\nCost after epoch 2000: 0.635388\nCost after epoch 3000: 0.600591\nCost after epoch 4000: 0.573444\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7166666666666667\n\n\n\n\n\n\n\n\n\n\n### 6.3 - Mini-Batch with Adam\nFinally, run the following code to see how the model does with Adam.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702166\nCost after epoch 1000: 0.167845\nCost after epoch 2000: 0.141316\nCost after epoch 3000: 0.138788\nCost after epoch 4000: 0.136066\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334\n\n\n\n\n\n\n\n\n\n\n### 6.4 - Summary\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smoother\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\ncost shape\n\n\n\n\nMomentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible.\nOn the other hand, Adam clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.\nSome advantages of Adam include:\n\nRelatively low memory requirements (though higher than gradient descent and gradient descent with momentum)\nUsually works well even with little tuning of hyperparameters (except \\(\\alpha\\))\n\nReferences:\n\nAdam paper: https://arxiv.org/pdf/1412.6980.pdf\n\n\n## 7 - Learning Rate Decay and Scheduling\nLastly, the learning rate is another hyperparameter that can help you speed up learning.\nDuring the first part of training, your model can get away with taking large steps, but over time, using a fixed value for the learning rate alpha can cause your model to get stuck in a wide oscillation that never quite converges. But if you were to slowly reduce your learning rate alpha over time, you could then take smaller, slower steps that bring you closer to the minimum. This is the idea behind learning rate decay.\nLearning rate decay can be achieved by using either adaptive methods or pre-defined learning rate schedules.\nNow, you’ll apply scheduled learning rate decay to a 3-layer neural network in three different optimizer modes and see how each one differs, as well as the effect of scheduling at different epochs.\nThis model is essentially the same as the one you used before, except in this one you’ll be able to include learning rate decay. It includes two new parameters, decay and decay_rate.\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    lr_rates = []\n    learning_rate0 = learning_rate   # the original learning rate\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        if decay:\n            learning_rate = decay(learning_rate0, i, decay_rate)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n            if decay:\n                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n### 7.1 - Decay on every iteration\nFor this portion of the assignment, you’ll try one of the pre-defined schedules for learning rate decay, called exponential learning rate decay. It takes this mathematical form:\n\\[\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}\\]\n\n### Exercise 7 - update_lr\nCalculate the new learning rate using exponential weight decay.\n\n# GRADED FUNCTION: update_lr\n\ndef update_lr(learning_rate0, epoch_num, decay_rate):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer\n    decay_rate -- Decay rate. Scalar\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    #(approx. 1 line)\n    # learning_rate = \n    # YOUR CODE STARTS HERE\n    learning_rate = learning_rate0/(1+decay_rate*epoch_num)\n    \n    # YOUR CODE ENDS HERE\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\nepoch_num = 2\ndecay_rate = 1\nlearning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n\nprint(\"Updated learning rate: \", learning_rate_2)\n\nupdate_lr_test(update_lr)\n\nOriginal learning rate:  0.5\nUpdated learning rate:  0.16666666666666666\nAll tests passed\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=update_lr)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.701091\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.661884\nlearning rate after epoch 1000: 0.000100\nCost after epoch 2000: 0.658620\nlearning rate after epoch 2000: 0.000050\nCost after epoch 3000: 0.656765\nlearning rate after epoch 3000: 0.000033\nCost after epoch 4000: 0.655486\nlearning rate after epoch 4000: 0.000025\n\n\n\n\n\n\n\n\n\nAccuracy: 0.6533333333333333\n\n\n\n\n\n\n\n\n\nNotice that if you set the decay to occur at every iteration, the learning rate goes to zero too quickly - even if you start with a higher learning rate.\n\n\n\nEpoch Number\n\n\nLearning Rate\n\n\nCost\n\n\n\n\n0\n\n\n0.100000\n\n\n0.701091\n\n\n\n\n1000\n\n\n0.000100\n\n\n0.661884\n\n\n\n\n2000\n\n\n0.000050\n\n\n0.658620\n\n\n\n\n3000\n\n\n0.000033\n\n\n0.656765\n\n\n\n\n4000\n\n\n0.000025\n\n\n0.655486\n\n\n\n\n5000\n\n\n0.000020\n\n\n0.654514\n\n\n\nWhen you’re training for a few epoch this doesn’t cause a lot of troubles, but when the number of epochs is large the optimization algorithm will stop updating. One common fix to this issue is to decay the learning rate every few steps. This is called fixed interval scheduling.\n ### 7.2 - Fixed Interval Scheduling\nYou can help prevent the learning rate speeding to zero too quickly by scheduling the exponential learning rate decay at a fixed time interval, for example 1000. You can either number the intervals, or divide the epoch by the time interval, which is the size of window with the constant learning rate.\n\n ### Exercise 8 - schedule_lr_decay\nCalculate the new learning rate using exponential weight decay with fixed interval scheduling.\nInstructions: Implement the learning rate scheduling such that it only changes when the epochNum is a multiple of the timeInterval.\nNote: The fraction in the denominator uses the floor operation.\n\\[\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}\\]\nHint: numpy.floor\n\n# GRADED FUNCTION: schedule_lr_decay\n\ndef schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer.\n    decay_rate -- Decay rate. Scalar.\n    time_interval -- Number of epochs where you update the learning rate.\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    # (approx. 1 lines)\n    # learning_rate = ...\n    # YOUR CODE STARTS HERE\n    learning_rate = learning_rate0/(1+decay_rate*math.floor(epoch_num/time_interval))\n    \n    # YOUR CODE ENDS HERE\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\n\nepoch_num_1 = 10\nepoch_num_2 = 100\ndecay_rate = 0.3\ntime_interval = 100\nlearning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\nlearning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)\n\nschedule_lr_decay_test(schedule_lr_decay)\n\nOriginal learning rate:  0.5\nUpdated learning rate after 10 epochs:  0.5\nUpdated learning rate after 100 epochs:  0.3846153846153846\nAll tests passed\n\n\nExpected output\nOriginal learning rate:  0.5\nUpdated learning rate after 10 epochs:  0.5\nUpdated learning rate after 100 epochs:  0.3846153846153846\n ### 7.3 - Using Learning Rate Decay for each Optimization Method\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n #### 7.3.1 - Gradient Descent with Learning Rate Decay\nRun the following code to see how the model does gradient descent and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.701091\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.127161\nlearning rate after epoch 1000: 0.050000\nCost after epoch 2000: 0.120304\nlearning rate after epoch 2000: 0.033333\nCost after epoch 3000: 0.117033\nlearning rate after epoch 3000: 0.025000\nCost after epoch 4000: 0.117512\nlearning rate after epoch 4000: 0.020000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334\n\n\n\n\n\n\n\n\n\n #### 7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\nRun the following code to see how the model does gradient descent with momentum and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent with momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702226\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.128974\nlearning rate after epoch 1000: 0.050000\nCost after epoch 2000: 0.125965\nlearning rate after epoch 2000: 0.033333\nCost after epoch 3000: 0.123375\nlearning rate after epoch 3000: 0.025000\nCost after epoch 4000: 0.123218\nlearning rate after epoch 4000: 0.020000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9533333333333334\n\n\n\n\n\n\n\n\n\n #### 7.3.3 - Adam with Learning Rate Decay\nRun the following code to see how the model does Adam and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\", learning_rate = 0.01, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.699346\nlearning rate after epoch 0: 0.010000\nCost after epoch 1000: 0.130074\nlearning rate after epoch 1000: 0.005000\nCost after epoch 2000: 0.129826\nlearning rate after epoch 2000: 0.003333\nCost after epoch 3000: 0.129282\nlearning rate after epoch 3000: 0.002500\nCost after epoch 4000: 0.128361\nlearning rate after epoch 4000: 0.002000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.94\n\n\n\n\n\n\n\n\n\n ### 7.4 - Achieving similar performance with different methods\nWith Mini-batch GD or Mini-batch GD with Momentum, the accuracy is significantly lower than Adam, but when learning rate decay is added on top, either can achieve performance at a speed and accuracy score that’s similar to Adam.\nIn the case of Adam, notice that the learning curve achieves a similar accuracy but faster.\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94.6%\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;95.6%\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    94%\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\n\n\nCongratulations! You’ve made it to the end of the Optimization methods notebook. Here’s a quick recap of everything you’re now able to do:\n\nApply three different optimization methods to your models\nBuild mini-batches for your training set\nUse learning rate decay scheduling to speed up your training\n\nGreat work!"
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A2/Regularization.html",
    "href": "nb/coursera/deep_learning_2/W1A2/Regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "Welcome to the second assignment of this week. Deep Learning models have so much flexibility and capacity that overfitting can be a serious problem, if the training dataset is not big enough. Sure it does well on the training set, but the learned network doesn’t generalize to new examples that it has never seen!\nYou will learn to: Use regularization in your deep learning models.\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A2/Regularization.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_2/W1A2/Regularization.html#important-note-on-submission-to-the-autograder",
    "title": "Regularization",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A2/Regularization.html#table-of-contents",
    "href": "nb/coursera/deep_learning_2/W1A2/Regularization.html#table-of-contents",
    "title": "Regularization",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Problem Statement\n3 - Loading the Dataset\n4 - Non-Regularized Model\n5 - L2 Regularization\n\nExercise 1 - compute_cost_with_regularization\nExercise 2 - backward_propagation_with_regularization\n\n6 - Dropout\n\n6.1 - Forward Propagation with Dropout\n\nExercise 3 - forward_propagation_with_dropout\n\n6.2 - Backward Propagation with Dropout\n\nExercise 4 - backward_propagation_with_dropout\n\n\n7 - Conclusions\n\n ## 1 - Packages\n\n### v1.1\n\n\n# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\nfrom reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\nfrom reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\nfrom testCases import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Problem Statement\nYou have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.\n\n\n\nFigure 1: Football field. The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head \n\n\nThey give you the following 2D dataset from France’s past 10 games.\n ## 3 - Loading the Dataset\n\ntrain_X, train_Y, test_X, test_Y = load_2D_dataset()\n\n\n\n\n\n\n\n\nEach dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field. - If the dot is blue, it means the French player managed to hit the ball with his/her head - If the dot is red, it means the other team’s player hit the ball with their head\nYour goal: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.\nAnalysis of the dataset: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well.\nYou will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem.\n ## 4 - Non-Regularized Model\nYou will use the following neural network (already implemented for you below). This model can be used: - in regularization mode – by setting the lambd input to a non-zero value. We use “lambd” instead of “lambda” because “lambda” is a reserved keyword in Python. - in dropout mode – by setting the keep_prob to a value less than one\nYou will first try the model without any regularization. Then, you will implement: - L2 regularization – functions: “compute_cost_with_regularization()” and “backward_propagation_with_regularization()” - Dropout – functions: “forward_propagation_with_dropout()” and “backward_propagation_with_dropout()”\nIn each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.\n\ndef model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob &lt; 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n                                                # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob &lt; 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\nLet’s train the model without any regularization, and observe the accuracy on the train/test sets.\n\nparameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6557412523481002\nCost after iteration 10000: 0.16329987525724204\nCost after iteration 20000: 0.13851642423234922\n\n\n\n\n\n\n\n\n\nOn the training set:\nAccuracy: 0.9478672985781991\nOn the test set:\nAccuracy: 0.915\n\n\nThe train accuracy is 94.8% while the test accuracy is 91.5%. This is the baseline model (you will observe the impact of regularization on this model). Run the following code to plot the decision boundary of your model.\n\nplt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nThe non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.\n ## 5 - L2 Regularization\nThe standard way to avoid overfitting is called L2 regularization. It consists of appropriately modifying your cost function, from: \\[J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}\\] To: \\[J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}\\]\nLet’s modify your cost and observe the consequences.\n ### Exercise 1 - compute_cost_with_regularization Implement compute_cost_with_regularization() which computes the cost given by formula (2). To calculate \\(\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}\\) , use :\nnp.sum(np.square(Wl))\nNote that you have to do this for \\(W^{[1]}\\), \\(W^{[2]}\\) and \\(W^{[3]}\\), then sum the three terms and multiply by $ $.\n\n# GRADED FUNCTION: compute_cost_with_regularization\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    #(≈ 1 lines of code)\n    # L2_regularization_cost = \n    # YOUR CODE STARTS HERE\n    L2_regularization_cost = lambd/(2*m)*(np.sum(np.square(W3))+np.sum(np.square(W2))+np.sum(np.square(W1)))\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost\n\n\nA3, t_Y, parameters = compute_cost_with_regularization_test_case()\ncost = compute_cost_with_regularization(A3, t_Y, parameters, lambd=0.1)\nprint(\"cost = \" + str(cost))\n\ncompute_cost_with_regularization_test(compute_cost_with_regularization)\n\ncost = 1.7864859451590758\n All tests passed.\n\n\nOf course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost.\n ### Exercise 2 - backward_propagation_with_regularization Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term’s gradient (\\(\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W\\)).\n\n# GRADED FUNCTION: backward_propagation_with_regularization\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    #(≈ 1 lines of code)\n    # dW3 = 1./m * np.dot(dZ3, A2.T) + None\n    # YOUR CODE STARTS HERE\n    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m*W3\n    \n    # YOUR CODE ENDS HERE\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    #(≈ 1 lines of code)\n    # dW2 = 1./m * np.dot(dZ2, A1.T) + None\n    # YOUR CODE STARTS HERE\n    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m*W2\n    \n    # YOUR CODE ENDS HERE\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    #(≈ 1 lines of code)\n    # dW1 = 1./m * np.dot(dZ1, X.T) + None\n    # YOUR CODE STARTS HERE\n    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m*W1\n    \n    # YOUR CODE ENDS HERE\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\nt_X, t_Y, cache = backward_propagation_with_regularization_test_case()\n\ngrads = backward_propagation_with_regularization(t_X, t_Y, cache, lambd = 0.7)\nprint (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\nprint (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\nprint (\"dW3 = \\n\"+ str(grads[\"dW3\"]))\nbackward_propagation_with_regularization_test(backward_propagation_with_regularization)\n\ndW1 = \n[[-0.25604646  0.12298827 -0.28297129]\n [-0.17706303  0.34536094 -0.4410571 ]]\ndW2 = \n[[ 0.79276486  0.85133918]\n [-0.0957219  -0.01720463]\n [-0.13100772 -0.03750433]]\ndW3 = \n[[-1.77691347 -0.11832879 -0.09397446]]\n All tests passed.\n\n\nLet’s now run the model with L2 regularization \\((\\lambda = 0.7)\\). The model() function will call: - compute_cost_with_regularization instead of compute_cost - backward_propagation_with_regularization instead of backward_propagation\n\nparameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6974484493131264\nCost after iteration 10000: 0.2684918873282238\nCost after iteration 20000: 0.26809163371273004\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.9383886255924171\nOn the test set:\nAccuracy: 0.93\n\n\nCongrats, the test set accuracy increased to 93%. You have saved the French football team!\nYou are not overfitting the training data anymore. Let’s plot the decision boundary.\n\nplt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nObservations: - The value of \\(\\lambda\\) is a hyperparameter that you can tune using a dev set. - L2 regularization makes your decision boundary smoother. If \\(\\lambda\\) is too large, it is also possible to “oversmooth”, resulting in a model with high bias.\nWhat is L2-regularization actually doing?:\nL2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n \nWhat you should remember: the implications of L2-regularization on: - The cost computation: - A regularization term is added to the cost. - The backpropagation function: - There are extra terms in the gradients with respect to weight matrices. - Weights end up smaller (“weight decay”): - Weights are pushed to smaller values.\n ## 6 - Dropout\nFinally, dropout is a widely used regularization technique that is specific to deep learning. It randomly shuts down some neurons in each iteration. Watch these two videos to see what this means!\n\n\n\n\n\n\n\n\nFigure 2 : Drop-out on the second hidden layer.  At each iteration, you shut down (= set to zero) each neuron of a layer with probability \\(1 - keep\\_prob\\) or keep it with probability \\(keep\\_prob\\) (50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration. \n\n\n\n\n\n\n\n\nFigure 3: Drop-out on the first and third hidden layers.  \\(1^{st}\\) layer: we shut down on average 40% of the neurons. \\(3^{rd}\\) layer: we shut down on average 20% of the neurons. \n\n\nWhen you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.\n ### 6.1 - Forward Propagation with Dropout\n ### Exercise 3 - forward_propagation_with_dropout\nImplement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer.\nInstructions: You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps: 1. In lecture, we dicussed creating a variable \\(d^{[1]}\\) with the same shape as \\(a^{[1]}\\) using np.random.rand() to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{1} d^{1} … d^{1}] $ of the same dimension as \\(A^{[1]}\\). 2. Set each entry of \\(D^{[1]}\\) to be 1 with probability (keep_prob), and 0 otherwise.\nHint: Let’s say that keep_prob = 0.8, which means that we want to keep about 80% of the neurons and drop out about 20% of them. We want to generate a vector that has 1’s and 0’s, where about 80% of them are 1 and about 20% are 0. This python statement:\nX = (X &lt; keep_prob).astype(int)\nis conceptually the same as this if-else statement (for the simple case of a one-dimensional array) :\nfor i,v in enumerate(x):\n    if v &lt; keep_prob:\n        x[i] = 1\n    else: # v &gt;= keep_prob\n        x[i] = 0\nNote that the X = (X &lt; keep_prob).astype(int) works with multi-dimensional arrays, and the resulting output preserves the dimensions of the input array.\nAlso note that without using .astype(int), the result is an array of booleans True and False, which Python automatically converts to 1 and 0 if we multiply it with numbers. (However, it’s better practice to convert data into the data type that we intend, so try using .astype(int).)\n\nSet \\(A^{[1]}\\) to \\(A^{[1]} * D^{[1]}\\). (You are shutting down some neurons). You can think of \\(D^{[1]}\\) as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.\nDivide \\(A^{[1]}\\) by keep_prob. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)\n\n\n# GRADED FUNCTION: forward_propagation_with_dropout\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n    # D1 =                                           # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n    # D1 =                                           # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n    # A1 =                                           # Step 3: shut down some neurons of A1\n    # A1 =                                           # Step 4: scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    D1 = np.random.rand(A1.shape[0], A1.shape[1]) \n    D1 = (D1 &lt; keep_prob).astype(int)\n    A1 = np.multiply(A1, D1)\n    A1 = A1/keep_prob\n    # YOUR CODE ENDS HERE\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    #(≈ 4 lines of code)\n    # D2 =                                           # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n    # D2 =                                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n    # A2 =                                           # Step 3: shut down some neurons of A2\n    # A2 =                                           # Step 4: scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n    D2 = (D2 &lt; keep_prob).astype(int)\n    A2 = np.multiply(A2, D2)\n    A2 = A2/keep_prob   \n    # YOUR CODE ENDS HERE\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache\n\n\nt_X, parameters = forward_propagation_with_dropout_test_case()\n\nA3, cache = forward_propagation_with_dropout(t_X, parameters, keep_prob=0.7)\nprint (\"A3 = \" + str(A3))\n\nforward_propagation_with_dropout_test(forward_propagation_with_dropout)\n\nA3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]\n All tests passed.\n\n\n ### 6.2 - Backward Propagation with Dropout\n ### Exercise 4 - backward_propagation_with_dropout Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks \\(D^{[1]}\\) and \\(D^{[2]}\\) stored in the cache.\nInstruction: Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps: 1. You had previously shut down some neurons during forward propagation, by applying a mask \\(D^{[1]}\\) to A1. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask \\(D^{[1]}\\) to dA1. 2. During forward propagation, you had divided A1 by keep_prob. In backpropagation, you’ll therefore have to divide dA1 by keep_prob again (the calculus interpretation is that if \\(A^{[1]}\\) is scaled by keep_prob, then its derivative \\(dA^{[1]}\\) is also scaled by the same keep_prob).\n\n# GRADED FUNCTION: backward_propagation_with_dropout\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1./m * np.dot(dZ3, A2.T)\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    #(≈ 2 lines of code)\n    # dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n    # dA2 =                # Step 2: Scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    dA2 = D2*dA2\n    dA2 = dA2/keep_prob\n    \n    # YOUR CODE ENDS HERE\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = 1./m * np.dot(dZ2, A1.T)\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    #(≈ 2 lines of code)\n    # dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n    # dA1 =                # Step 2: Scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    dA1 = D1*dA1\n    dA1 = dA1/keep_prob    \n    \n    # YOUR CODE ENDS HERE\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = 1./m * np.dot(dZ1, X.T)\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\nt_X, t_Y, cache = backward_propagation_with_dropout_test_case()\n\ngradients = backward_propagation_with_dropout(t_X, t_Y, cache, keep_prob=0.8)\n\nprint (\"dA1 = \\n\" + str(gradients[\"dA1\"]))\nprint (\"dA2 = \\n\" + str(gradients[\"dA2\"]))\n\nbackward_propagation_with_dropout_test(backward_propagation_with_dropout)\n\ndA1 = \n[[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\ndA2 = \n[[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n [ 0.          0.         -0.00292733  0.         -0.        ]]\n All tests passed.\n\n\nLet’s now run the model with dropout (keep_prob = 0.86). It means at every iteration you shut down each neurons of layer 1 and 2 with 14% probability. The function model() will now call: - forward_propagation_with_dropout instead of forward_propagation. - backward_propagation_with_dropout instead of backward_propagation.\n\nparameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6543912405149825\nCost after iteration 10000: 0.0610169865749056\nCost after iteration 20000: 0.060582435798513114\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.9289099526066351\nOn the test set:\nAccuracy: 0.95\n\n\nDropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set. The French football team will be forever grateful to you!\nRun the code below to plot the decision boundary.\n\nplt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nNote: - A common mistake when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. - Deep learning frameworks like TensorFlow, PaddlePaddle, Keras or caffe come with a dropout layer implementation. Don’t stress - you will soon learn some of these frameworks.\n\nWhat you should remember about dropout: - Dropout is a regularization technique. - You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time. - Apply dropout both during forward and backward propagation. - During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.\n ## 7 - Conclusions\nHere are the results of our three models:\n&lt;td&gt;\n    3-layer NN without regularization\n    &lt;/td&gt;\n    &lt;td&gt;\n    95%\n    &lt;/td&gt;\n    &lt;td&gt;\n    91.5%\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with L2-regularization\n    &lt;/td&gt;\n    &lt;td&gt;\n    94%\n    &lt;/td&gt;\n    &lt;td&gt;\n    93%\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with dropout\n    &lt;/td&gt;\n    &lt;td&gt;\n    93%\n    &lt;/td&gt;\n    &lt;td&gt;\n    95%\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\nmodel\n\n\ntrain accuracy\n\n\ntest accuracy\n\n\n\n\nNote that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.\nCongratulations for finishing this assignment! And also for revolutionizing French football. :-)\n\nWhat we want you to remember from this notebook: - Regularization will help you reduce overfitting. - Regularization will drive your weights to lower values. - L2 regularization and Dropout are two very effective regularization techniques."
  },
  {
    "objectID": "nb/dl_lab2/Planar_data_classification_with_one_hidden_layer.html",
    "href": "nb/dl_lab2/Planar_data_classification_with_one_hidden_layer.html",
    "title": "Lab2 - Planar data classification with one hidden layer",
    "section": "",
    "text": "# 1 - Packages\nFirst import all the packages that you will need:\n\nnumpy is the fundamental package for scientific computing with Python.\nsklearn provides simple and efficient tools for data mining and data analysis.\nmatplotlib is a library for plotting graphs in Python.\ntestCases provides some test examples to assess the correctness of your functions\nplanar_utils provide various useful functions used in this assignment\n\n\n### v1.1\n\n\n# Package imports\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nfrom public_tests import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2\n\n # 2 - Load the Dataset\n\nX, Y = load_planar_dataset()\n\nVisualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.\n\n# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\nYou have:\n- a numpy-array (matrix) X that contains your features (x1, x2)\n- a numpy-array (vector) Y that contains your labels (red:0, blue:1).\nFirst, get a better sense of what your data is like.\n ### Exercise 1\nHow many training examples do you have? In addition, what is the shape of the variables X and Y?\nHint: How do you get the shape of a numpy array? (help)\n\n# (≈ 3 lines of code)\n# shape_X = ...\n# shape_Y = ...\n# training set size\n# m = ...\n# CODE_START\n\n# CODE_END\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nExpected Output:\n\n\n\nshape of X\n\n\n(2, 400)\n\n\n\n\nshape of Y\n\n\n(1, 400)\n\n\n\n\nm\n\n\n400\n\n\n\n ## 3 - Simple Logistic Regression\nBefore building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\nYou can now plot the decision boundary of these models! Run the code below.\n\n# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n\nExpected Output:\n\n\n\nAccuracy\n\n\n47%\n\n\n\nInterpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!\n ## 4 - Neural Network model\nLogistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.\nThe model: \nMathematically:\nFor one example \\(x^{(i)}\\): \\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\\] \\[a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\\] \\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\\] \\[\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\\] \\[y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\\]\nGiven the predictions on all the examples, you can also compute the cost \\(J\\) as follows: \\[J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\\]\nReminder: The general methodology to build a Neural Network is to:\n\nDefine the neural network structure ( # of input units, # of hidden units, etc).\nInitialize the model’s parameters\nLoop:\n\nImplement forward propagation\nCompute loss\nImplement backward propagation to get the gradients\nUpdate parameters (gradient descent)\n\n\nIn practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data.\n ### 4.1 - Defining the neural network structure ####\n ### Exercise 2 - layer_sizes\nDefine three variables: - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4, as n_h = 4, but only for this Exercise 2) - n_y: the size of the output layer\nHint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    #(≈ 3 lines of code)\n    # n_x = ... \n    # n_h = ...\n    # n_y = ... \n    # CODE_START\n\n    # CODE_END\n    return (n_x, n_h, n_y)\n\n\nt_X, t_Y = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nlayer_sizes_test(layer_sizes)\n\nExpected output\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n ### 4.2 - Initialize the model’s parameters ####\n ### Exercise 3 - initialize_parameters\nImplement the function initialize_parameters().\nInstructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    # CODE_END\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nnp.random.seed(2)\nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test(initialize_parameters)\n\nExpected output\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n ### 4.3 - The Loop\n ### Exercise 4 - forward_propagation\nImplement forward_propagation() using the following equations:\n\\[Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}\\] \\[A^{[1]} = \\tanh(Z^{[1]})\\tag{2}\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}\\] \\[\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}\\]\nInstructions:\n\nCheck the mathematical representation of your classifier in the figure above.\nUse the function sigmoid(). It’s built into (imported) this notebook.\nUse the function np.tanh(). It’s part of the numpy library.\nImplement using these steps:\n\nRetrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() by using parameters[\"..\"].\nImplement Forward Propagation. Compute \\(Z^{[1]}, A^{[1]}, Z^{[2]}\\) and \\(A^{[2]}\\) (the vector of all your predictions on all the examples in the training set).\n\nValues needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.\n\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n  \n    # CODE_END\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    # (≈ 4 lines of code)\n    # Z1 = ...\n    # A1 = ...\n    # Z2 = ...\n    # A2 = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n\nt_X, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(t_X, parameters)\nprint(\"A2 = \" + str(A2))\n\nforward_propagation_test(forward_propagation)\n\nExpected output\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n ### 4.4 - Compute the Cost\nNow that you’ve computed \\(A^{[2]}\\) (in the Python variable “A2”), which contains \\(a^{[2](i)}\\) for all examples, you can compute the cost function as follows:\n\\[\nJ = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\n\\]\n ### Exercise 5 - compute_cost\nImplement compute_cost() to compute the value of the cost \\(J\\).\nInstructions:\n\nThere are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops. For instance, \\(- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})\\) can be computed as:\n\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)          \n\nUse that to build the whole expression of the cost function.\n\nNotes:\n\nYou can use either np.multiply() and then np.sum() or directly np.dot()).\n\nIf you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array.\n\nYou can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).\nYou can also cast the array as a type float using float().\n\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    # (≈ 2 lines of code)\n    # logprobs = ...\n    # cost = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    \n    return cost\n\n\nA2, t_Y = compute_cost_test_case()\ncost = compute_cost(A2, t_Y)\nprint(\"cost = \" + str(compute_cost(A2, t_Y)))\n\ncompute_cost_test(compute_cost)\n\nExpected output\ncost = 0.6930587610394646\nAll tests passed!\n ### 4.5 - Implement Backpropagation\nUsing the cache computed during forward propagation, you can now implement backward propagation.\n ### Exercise 6 - backward_propagation\nImplement the function backward_propagation().\nInstructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. Below are the equations describing relevant computations. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.\n\n\n\nFigure 1: Backpropagation. Use the six equations on the right.\n\n\n\nTips:\n\nTo compute dZ1 you’ll need to compute \\(g^{[1]'}(Z^{[1]})\\). Since \\(g^{[1]}(.)\\) is the tanh activation function, if \\(a = g^{[1]}(z)\\) then \\(g^{[1]'}(z) = 1-a^2\\). So you can compute \\(g^{[1]'}(Z^{[1]})\\) using (1 - np.power(A1, 2)).\n\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    #(≈ 2 lines of code)\n    # W1 = ...\n    # W2 = ...\n    # CODE_START\n    \n    # CODE_END\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    #(≈ 2 lines of code)\n    # A1 = ...\n    # A2 = ...\n    # CODE_START\n  \n    # CODE_END\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n    # dZ2 = ...\n    # dW2 = ...\n    # db2 = ...\n    # dZ1 = ...\n    # dW1 = ...\n    # db1 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads\n\n\nparameters, cache, t_X, t_Y = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\nbackward_propagation_test(backward_propagation)\n\nExpected output\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n ### 4.6 - Update Parameters\n ### Exercise 7 - update_parameters\nImplement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\nGeneral gradient descent rule: \\(\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\) where \\(\\alpha\\) is the learning rate and \\(\\theta\\) represents a parameter.\n \n\n\nFigure 2: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n\nHint\n\nUse copy.deepcopy(...) when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    #(≈ 4 lines of code)\n    # dW1 = ...\n    # db1 = ...\n    # dW2 = ...\n    # db2 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    # Update rule for each parameter\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n \n    # CODE_END\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nExpected output\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n ### 4.7 - Integration\nIntegrate your functions in nn_model()\n ### Exercise 8 - nn_model\nBuild your neural network model in nn_model().\nInstructions: The neural network model has to use the previous functions in the right order.\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        #(≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        # A2, cache = ...\n        \n        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n        # cost = ...\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        # grads = ...\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        # parameters = ...\n        \n        # CODE_START\n        \n        # CODE_END\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nnn_model_test(nn_model)\n\nExpected output\nCost after iteration 0: 0.693198\nCost after iteration 1000: 0.000219\nCost after iteration 2000: 0.000108\n...\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n ## 5 - Test the Model\n ### 5.1 - Predict\n ### Exercise 9 - predict\nPredict with your model by building predict(). Use forward propagation to predict results.\nReminder: predictions = \\(y_{prediction} = \\mathbb 1 \\text{{activation &gt; 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\\)\nAs an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold)\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    #(≈ 2 lines of code)\n    # A2, cache = ...\n    # predictions = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    return predictions\n\n\nparameters, t_X = predict_test_case()\n\npredictions = predict(parameters, t_X)\nprint(\"Predictions: \" + str(predictions))\n\npredict_test(predict)\n\nExpected output\nPredictions: [[ True False  True]]\nAll tests passed!\n ### 5.2 - Test the Model on the Planar Dataset\nIt’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of \\(n_h\\) hidden units!\n\n# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\n\n# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n\nExpected Output:\n\n\n\nAccuracy\n\n\n90%\n\n\n\nAccuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.\n ## 6 - Tuning hidden layer size\nRun the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.\n\n# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5]\n\n# you can try with different hidden layer sizes\n# but make sure before you submit the assignment it is set as \"hidden_layer_sizes = [1, 2, 3, 4, 5]\"\n# hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n\nInterpretation: - The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. - The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. - Later, you’ll become familiar with regularization, which lets you use very large models (such as n_h = 50) without much overfitting.\nSome optional questions that you can explore if you wish: - What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? - Play with the learning_rate. What happens? - What if we change the dataset? (See part 7 below!)\n ## 7- Performance on other datasets\nIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.\n\n# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### CODE_START ### (choose your dataset)\ndataset = \"noisy_moons\"\n### CODE_END ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\nReferences:\n\nhttp://scs.ryerson.ca/~aharley/neural-networks/\nhttp://cs231n.github.io/neural-networks-case-study/"
  },
  {
    "objectID": "nb/dl_lab4/Initialization_Regularization.html",
    "href": "nb/dl_lab4/Initialization_Regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "# Initialization"
  },
  {
    "objectID": "nb/dl_lab4/Initialization_Regularization.html#packages",
    "href": "nb/dl_lab4/Initialization_Regularization.html#packages",
    "title": "Regularization",
    "section": "1 - Packages",
    "text": "1 - Packages\nDouble-check if you have the below packages installed:\n\nv1.1\npip install dlai_tools\npip install tensorflow\nImports and setup code:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nfrom public_tests import *\nfrom init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\nfrom init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n# load image dataset: blue/red dots in circles\n# train_X, train_Y, test_X, test_Y = load_dataset()\n\n ## 2 - Loading the Dataset\n\ntrain_X, train_Y, test_X, test_Y = load_dataset()\n\nFor this classifier, you want to separate the blue dots from the red dots.\n ## 3 - Neural Network Model\nYou’ll use a 3-layer neural network (already implemented for you). These are the initialization methods you’ll experiment with: - Zeros initialization – setting initialization = \"zeros\" in the input argument. - Random initialization – setting initialization = \"random\" in the input argument. This initializes the weights to large random values.\n- He initialization – setting initialization = \"he\" in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.\nInstructions: Instructions: Read over the code below, and run it. In the next part, you’ll implement the three initialization methods that this model() calls.\n\ndef model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n    learning_rate -- learning rate for gradient descent \n    num_iterations -- number of iterations to run gradient descent\n    print_cost -- if True, print the cost every 1000 iterations\n    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n    \n    Returns:\n    parameters -- parameters learnt by the model\n    \"\"\"\n        \n    grads = {}\n    costs = [] # to keep track of the loss\n    m = X.shape[1] # number of examples\n    layers_dims = [X.shape[0], 10, 5, 1]\n    \n    # Initialize parameters dictionary.\n    if initialization == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        a3, cache = forward_propagation(X, parameters)\n        \n        # Loss\n        cost = compute_loss(a3, Y)\n\n        # Backward propagation.\n        grads = backward_propagation(X, Y, cache)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n            costs.append(cost)\n            \n    # plot the loss\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\n ## 4 - Zero Initialization\nThere are two types of parameters to initialize in a neural network: - the weight matrices \\((W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})\\) - the bias vectors \\((b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})\\)\n ### Exercise 1 - initialize_parameters_zeros\nImplement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry,” but try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes.\n\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    parameters = {}\n    L = len(layers_dims)            # number of layers in the network\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] = \n        # CODE_START\n        \n        # CODE_END\n    return parameters\n\n\nparameters = initialize_parameters_zeros([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_zeros_test(initialize_parameters_zeros)\n\nRun the following code to train your model on 15,000 iterations using zeros initialization.\n\nparameters = model(train_X, train_Y, initialization = \"zeros\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nThe performance is terrible, the cost doesn’t decrease, and the algorithm performs no better than random guessing. Why? Take a look at the details of the predictions and the decision boundary:\n\nprint (\"predictions_train = \" + str(predictions_train))\nprint (\"predictions_test = \" + str(predictions_test))\n\n\nplt.title(\"Model with Zeros initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nFor a comprehensive explanation of this, you can read Paul Mielke’s post, Symmetry Breaking versus Zero Initialization.\nA simple explanation is provided below:\nNote: For sake of simplicity calculations below are done using only one example at a time.\nSince the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As z = 0\n\\[a = ReLU(z) = max(0, z) = 0\\]\nAt the classification layer, where the activation function is sigmoid you then get (for either input):\n\\[\\sigma(z) = \\frac{1}{ 1 + e^{-(z)}} = \\frac{1}{2} = y_{pred}\\]\nAs for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.\nYour loss function: \\[ \\mathcal{L}(a, y) =  - y  \\ln(y_{pred}) - (1-y)  \\ln(1-y_{pred})\\]\nFor y=1, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 1) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nFor y=0, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 0) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nAs you can see with the prediction being 0.5 whether the actual (y) value is 1 or 0 you get the same loss value for both, so none of the weights get adjusted and you are stuck with the same old value of the weights.\nThis is why you can see that the model is predicting 0 for every example! No wonder it’s doing so badly.\nIn general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with \\(n^{[l]}=1\\) for every layer. This way, the network is no more powerful than a linear classifier like logistic regression.\n\nWhat you should remember: - The weights \\(W^{[l]}\\) should be initialized randomly to break symmetry. - However, it’s okay to initialize the biases \\(b^{[l]}\\) to zeros. Symmetry is still broken so long as \\(W^{[l]}\\) is initialized randomly.\n ## 5 - Random Initialization\nTo break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you’ll see what happens when the weights are initialized randomly, but to very large values.\n ### Exercise 2 - initialize_parameters_random\nImplement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use np.random.randn(..,..) * 10 for weights and np.zeros((.., ..)) for biases. You’re using a fixed np.random.seed(..) to make sure your “random” weights match ours, so don’t worry if running your code several times always gives you the same initial values for the parameters.\n\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # CODE_START\n        \n        # CODE_END\n\n    return parameters\n\n\nparameters = initialize_parameters_random([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_random_test(initialize_parameters_random)\n\nRun the following code to train your model on 15,000 iterations using random initialization.\n\nparameters = model(train_X, train_Y, initialization = \"random\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nIf you see “inf” as the cost after the iteration 0, this is because of numerical roundoff. A more numerically sophisticated implementation would fix this, but for the purposes of this notebook, it isn’t really worth worrying about.\nIn any case, you’ve now broken the symmetry, and this gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!\n\nprint (predictions_train)\nprint (predictions_test)\n\n\nplt.title(\"Model with large random initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nObservations: - The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when \\(\\log(a^{[3]}) = \\log(0)\\), the loss goes to infinity. - Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. - If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n\nIn summary: - Initializing weights to very large random values doesn’t work well. - Initializing with small random values should do better. The important question is, how small should be these random values be? Let’s find out up next!\n\nOptional Read:\nThe main difference between Gaussian variable (numpy.random.randn()) and uniform random variable is the distribution of the generated random numbers:\n\nnumpy.random.rand() produces numbers in a uniform distribution.\nand numpy.random.randn() produces numbers in a normal distribution.\n\nWhen used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.\nAn intuitive way to see it is, for example, if you take the sigmoid() activation function.\nYou’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.\n ## 6 - He Initialization\nFinally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights \\(W^{[l]}\\) of sqrt(1./layers_dims[l-1]) where He initialization would use sqrt(2./layers_dims[l-1]).)\n ### Exercise 3 - initialize_parameters_he\nImplement the following function to initialize your parameters with He initialization. This function is similar to the previous initialize_parameters_random(...). The only difference is that instead of multiplying np.random.randn(..,..) by 10, you will multiply it by \\(\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}\\), which is what He initialization recommends for layers with a ReLU activation.\n\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # CODE_START\n        \n        # CODE_END\n        \n    return parameters\n\n\nparameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_he_test(initialize_parameters_he)\n# parameters\n\nExpected output\nW1 = [[ 1.78862847  0.43650985]\n [ 0.09649747 -1.8634927 ]\n [-0.2773882  -0.35475898]\n [-0.08274148 -0.62700068]]\nb1 = [[0.] [0.] [0.] [0.]]\nW2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\nb2 = [[0.]]\nRun the following code to train your model on 15,000 iterations using He initialization.\n\nparameters = model(train_X, train_Y, initialization = \"he\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\n\nplt.title(\"Model with He initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nObservations: - The model with He initialization separates the blue and the red dots very well in a small number of iterations.\n ## 7 - Conclusions\nYou’ve tried three different types of initializations. For the same number of iterations and same hyperparameters, the comparison is:\n&lt;td&gt;\n    3-layer NN with zeros initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    50%\n    &lt;/td&gt;\n    &lt;td&gt;\n    fails to break symmetry\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with large random initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    83%\n    &lt;/td&gt;\n    &lt;td&gt;\n    too large weights \n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with He initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    99%\n    &lt;/td&gt;\n    &lt;td&gt;\n    recommended method\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\nModel\n\n\nTrain accuracy\n\n\nProblem/Comment\n\n\n\n\nCongratulations! You’ve completed this notebook on Initialization.\nHere’s a quick recap of the main takeaways:\n\n\nDifferent initializations lead to very different results\nRandom initialization is used to break symmetry and make sure different hidden units can learn different things\nResist initializing to values that are too large!\nHe initialization works well for networks with ReLU activations"
  },
  {
    "objectID": "nb/nlp_lab3.html",
    "href": "nb/nlp_lab3.html",
    "title": "Plan:",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nPrepare models for the classifier, based on cleaned-up tokens.\nRun the Naive Bayes classifier.\n\nFirst, download necessary prepared samples.\n\nimport nltk\n\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nGet some sample positive/negative tweets.\n\nfrom nltk.corpus import twitter_samples\n\nWe can either get the actual string content of those tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. Download a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger')\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\nTrue\n\n\nImport Part-of-Speech tagger that will be used for lemmatization\n\nfrom nltk.tag import pos_tag\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\nLet’s write a function that will lemmatize twitter tokens.\nFor that, let’s first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 here.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\nNow fetch PoS tokens so that they can be passed to WordNetLemmatizer.\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatized_sentence = []\n# Convert PoS tags into a format used by the lemmatizer\n# and run lemmatize\nfor word, tag in pos_tag(tokens):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a'\n    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\nprint(lemmatized_sentence)\n\n['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNote that it converts words to their base forms (‘are’ -&gt; ‘be’, ‘comes’ -&gt; ‘come’).\nNow we can proceed to processing. During processing, we will perform cleanup: - remove URLs and mentions using regexes - after lemmatization, remove stopwords\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nHere comes the process_tokens function:\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Now note the sheer size of regex for URLs :)\n        # Mentions regex is comparatively short and sweet\n        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n            re.search(r'(@[A-Za-z0-9_]+)', token)):\n            continue\n\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n   \n        token = lemmatizer.lemmatize(token, pos)\n\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nLet’s test process_tokens:\n\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\n\nBefore: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\nAfter: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n\n\nRun process_tokens on all positive/negative tokens.\n\npositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\nnegative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n\nLet’s see how did the processing go.\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\n\nLet’s see what is most common there. Add a helper function get_all_words:\n\ndef get_all_words(cleaned_tokens_list):\n    return [w for tokens in cleaned_tokens_list for w in tokens]\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\n\nPerform frequency analysis using FreqDist:\n\nfrom nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))\n\nFine. Now we’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\n\n\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\n\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\n\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\n\nnltk.download('punkt')\n\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\n\nLet’s package it as a function:\n\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))\n\nSeems ok!"
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "title": "Planar data classification with one hidden layer",
    "section": "",
    "text": "Welcome to your week 3 programming assignment! It’s time to build your first neural network, which will have one hidden layer. Now, you’ll notice a big difference between this model and the one you implemented previously using logistic regression.\nBy the end of this assignment, you’ll be able to:"
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "title": "Planar data classification with one hidden layer",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "title": "Planar data classification with one hidden layer",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Load the Dataset\n\nExercise 1\n\n3 - Simple Logistic Regression\n4 - Neural Network model\n\n4.1 - Defining the neural network structure\n\nExercise 2 - layer_sizes\n\n4.2 - Initialize the model’s parameters\n\nExercise 3 - initialize_parameters\n\n4.3 - The Loop\n\nExercise 4 - forward_propagation\n\n4.4 - Compute the Cost\n\nExercise 5 - compute_cost\n\n4.5 - Implement Backpropagation\n\nExercise 6 - backward_propagation\n\n4.6 - Update Parameters\n\nExercise 7 - update_parameters\n\n4.7 - Integration\n\nExercise 8 - nn_model\n\n\n5 - Test the Model\n\n5.1 - Predict\n\nExercise 9 - predict\n\n5.2 - Test the Model on the Planar Dataset\n\n6 - Tuning hidden layer size (optional/ungraded exercise)\n7- Performance on other datasets\n\n # 1 - Packages\nFirst import all the packages that you will need during this assignment.\n\nnumpy is the fundamental package for scientific computing with Python.\nsklearn provides simple and efficient tools for data mining and data analysis.\nmatplotlib is a library for plotting graphs in Python.\ntestCases provides some test examples to assess the correctness of your functions\nplanar_utils provide various useful functions used in this assignment\n\n\n### v1.1\n\n\n# Package imports\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nfrom public_tests import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2\n\n # 2 - Load the Dataset\n\nX, Y = load_planar_dataset()\n\nVisualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.\n\n# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nYou have: - a numpy-array (matrix) X that contains your features (x1, x2) - a numpy-array (vector) Y that contains your labels (red:0, blue:1).\nFirst, get a better sense of what your data is like.\n ### Exercise 1\nHow many training examples do you have? In addition, what is the shape of the variables X and Y?\nHint: How do you get the shape of a numpy array? (help)\n\n# (≈ 3 lines of code)\n# shape_X = ...\n# shape_Y = ...\n# training set size\n# m = ...\n# YOUR CODE STARTS HERE\nshape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[0]\n# YOUR CODE ENDS HERE\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nThe shape of X is: (2, 400)\nThe shape of Y is: (1, 400)\nI have m = 2 training examples!\n\n\nExpected Output:\n\n\n\nshape of X\n\n\n(2, 400)\n\n\n\n\nshape of Y\n\n\n(1, 400)\n\n\n\n\nm\n\n\n400\n\n\n\n ## 3 - Simple Logistic Regression\nBefore building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\n/Users/vitvly/c/lnu/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nYou can now plot the decision boundary of these models! Run the code below.\n\n# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n\nAccuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)\n\n\n/var/folders/jr/7vzj1lzn0rx65bxcqn8nrwqw0000gn/T/ipykernel_49977/4242423965.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n\n\n\n\n\n\n\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n47%\n\n\n\nInterpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!\n ## 4 - Neural Network model\nLogistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.\nThe model: \nMathematically:\nFor one example \\(x^{(i)}\\): \\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\\] \\[a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\\] \\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\\] \\[\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\\] \\[y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\\]\nGiven the predictions on all the examples, you can also compute the cost \\(J\\) as follows: \\[J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\\]\nReminder: The general methodology to build a Neural Network is to: 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model’s parameters 3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)\nIn practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data.\n ### 4.1 - Defining the neural network structure ####\n ### Exercise 2 - layer_sizes\nDefine three variables: - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4, as n_h = 4, but only for this Exercise 2) - n_y: the size of the output layer\nHint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.\n\n# GRADED FUNCTION: layer_sizes\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    #(≈ 3 lines of code)\n    # n_x = ... \n    # n_h = ...\n    # n_y = ... \n    # YOUR CODE STARTS HERE\n    n_x = X.shape[0]\n    n_h = 4\n    n_y = Y.shape[0]\n    # YOUR CODE ENDS HERE\n    return (n_x, n_h, n_y)\n\n\nt_X, t_Y = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nlayer_sizes_test(layer_sizes)\n\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n\n\nExpected output\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n ### 4.2 - Initialize the model’s parameters ####\n ### Exercise 3 - initialize_parameters\nImplement the function initialize_parameters().\nInstructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.\n\n# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = np.random.randn(n_h,n_x) * 0.01 \n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01 \n    b2 = np.zeros((n_y, 1))\n    # YOUR CODE ENDS HERE\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nnp.random.seed(2)\nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test(initialize_parameters)\n\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n ### 4.3 - The Loop\n ### Exercise 4 - forward_propagation\nImplement forward_propagation() using the following equations:\n\\[Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}\\] \\[A^{[1]} = \\tanh(Z^{[1]})\\tag{2}\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}\\] \\[\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}\\]\nInstructions:\n\nCheck the mathematical representation of your classifier in the figure above.\nUse the function sigmoid(). It’s built into (imported) this notebook.\nUse the function np.tanh(). It’s part of the numpy library.\nImplement using these steps:\n\nRetrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() by using parameters[\"..\"].\nImplement Forward Propagation. Compute \\(Z^{[1]}, A^{[1]}, Z^{[2]}\\) and \\(A^{[2]}\\) (the vector of all your predictions on all the examples in the training set).\n\nValues needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.\n\n\n# GRADED FUNCTION:forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]   \n    # YOUR CODE ENDS HERE\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    # (≈ 4 lines of code)\n    # Z1 = ...\n    # A1 = ...\n    # Z2 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    \n    # YOUR CODE ENDS HERE\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n\nt_X, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(t_X, parameters)\nprint(\"A2 = \" + str(A2))\n\nforward_propagation_test(forward_propagation)\n\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n\n\nExpected output\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n ### 4.4 - Compute the Cost\nNow that you’ve computed \\(A^{[2]}\\) (in the Python variable “A2”), which contains \\(a^{[2](i)}\\) for all examples, you can compute the cost function as follows:\n\\[J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\\]\n ### Exercise 5 - compute_cost\nImplement compute_cost() to compute the value of the cost \\(J\\).\nInstructions: - There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops: \\(- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})\\):\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)          \n\nUse that to build the whole expression of the cost function.\n\nNotes:\n\nYou can use either np.multiply() and then np.sum() or directly np.dot()).\n\nIf you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array.\n\nYou can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).\nYou can also cast the array as a type float using float().\n\n\n# GRADED FUNCTION: compute_cost\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    # (≈ 2 lines of code)\n    # logprobs = ...\n    # cost = ...\n    # YOUR CODE STARTS HERE\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)\n    cost = -np.sum(logprobs)/m\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    \n    return cost\n\n\nA2, t_Y = compute_cost_test_case()\ncost = compute_cost(A2, t_Y)\nprint(\"cost = \" + str(compute_cost(A2, t_Y)))\n\ncompute_cost_test(compute_cost)\n\ncost = 0.6930587610394646\nAll tests passed!\n\n\nExpected output\ncost = 0.6930587610394646\nAll tests passed!\n ### 4.5 - Implement Backpropagation\nUsing the cache computed during forward propagation, you can now implement backward propagation.\n ### Exercise 6 - backward_propagation\nImplement the function backward_propagation().\nInstructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.\n\n\n\nFigure 1: Backpropagation. Use the six equations on the right.\n\n\n\n\nTips:\n\nTo compute dZ1 you’ll need to compute \\(g^{[1]'}(Z^{[1]})\\). Since \\(g^{[1]}(.)\\) is the tanh activation function, if \\(a = g^{[1]}(z)\\) then \\(g^{[1]'}(z) = 1-a^2\\). So you can compute \\(g^{[1]'}(Z^{[1]})\\) using (1 - np.power(A1, 2)).\n\n\n\n# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    #(≈ 2 lines of code)\n    # W1 = ...\n    # W2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    \n    # YOUR CODE ENDS HERE\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    #(≈ 2 lines of code)\n    # A1 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    \n    # YOUR CODE ENDS HERE\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n    # dZ2 = ...\n    # dW2 = ...\n    # db2 = ...\n    # dZ1 = ...\n    # dW1 = ...\n    # db1 = ...\n    # YOUR CODE STARTS HERE\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T)/m\n    db2 = np.sum(dZ2, axis = 1, keepdims=True)/m\n    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1,2))\n    dW1 = np.dot(dZ1, X.T)/m\n    db1 = np.sum(dZ1, axis = 1, keepdims=True)/m\n    # YOUR CODE ENDS HERE\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads\n\n\nparameters, cache, t_X, t_Y = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\nbackward_propagation_test(backward_propagation)\n\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n\n\nExpected output\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n ### 4.6 - Update Parameters\n ### Exercise 7 - update_parameters\nImplement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\nGeneral gradient descent rule: \\(\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\) where \\(\\alpha\\) is the learning rate and \\(\\theta\\) represents a parameter.\n \n\n\nFigure 2: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n\nHint\n\nUse copy.deepcopy(...) when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n\n\n# GRADED FUNCTION: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = copy.deepcopy(parameters[\"W1\"])\n    b1 = copy.deepcopy(parameters[\"b1\"])\n    W2 = copy.deepcopy(parameters[\"W2\"])\n    b2 = copy.deepcopy(parameters[\"b2\"])\n    # YOUR CODE ENDS HERE\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    #(≈ 4 lines of code)\n    # dW1 = ...\n    # db1 = ...\n    # dW2 = ...\n    # db2 = ...\n    # YOUR CODE STARTS HERE\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    # YOUR CODE ENDS HERE\n    \n    # Update rule for each parameter\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2   \n    # YOUR CODE ENDS HERE\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n ### 4.7 - Integration\nIntegrate your functions in nn_model()\n ### Exercise 8 - nn_model\nBuild your neural network model in nn_model().\nInstructions: The neural network model has to use the previous functions in the right order.\n\n# GRADED FUNCTION: nn_model\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters\n    #(≈ 1 line of code)\n    # parameters = ...\n    # YOUR CODE STARTS HERE\n    parameters = initialize_parameters(X.shape[0], n_h, Y.shape[0])\n    \n    # YOUR CODE ENDS HERE\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        #(≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        # A2, cache = ...\n        \n        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n        # cost = ...\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        # grads = ...\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        # parameters = ...\n        \n        # YOUR CODE STARTS HERE\n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nnn_model_test(nn_model)\n\nCost after iteration 0: 0.693086\nCost after iteration 1000: 0.000220\nCost after iteration 2000: 0.000108\nCost after iteration 3000: 0.000072\nCost after iteration 4000: 0.000054\nCost after iteration 5000: 0.000043\nCost after iteration 6000: 0.000036\nCost after iteration 7000: 0.000030\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n\n\nExpected output\nCost after iteration 0: 0.693198\nCost after iteration 1000: 0.000219\nCost after iteration 2000: 0.000108\n...\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n ## 5 - Test the Model\n ### 5.1 - Predict\n ### Exercise 9 - predict\nPredict with your model by building predict(). Use forward propagation to predict results.\nReminder: predictions = \\(y_{prediction} = \\mathbb 1 \\text{{activation &gt; 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\\)\nAs an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold)\n\n# GRADED FUNCTION: predict\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    #(≈ 2 lines of code)\n    # A2, cache = ...\n    # predictions = ...\n    # YOUR CODE STARTS HERE\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2 &gt; 0.5)\n    \n    # YOUR CODE ENDS HERE\n    \n    return predictions\n\n\nparameters, t_X = predict_test_case()\n\npredictions = predict(parameters, t_X)\nprint(\"Predictions: \" + str(predictions))\n\npredict_test(predict)\n\nPredictions: [[ True False  True]]\nAll tests passed!\n\n\nExpected output\nPredictions: [[ True False  True]]\nAll tests passed!\n ### 5.2 - Test the Model on the Planar Dataset\nIt’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of \\(n_h\\) hidden units!\n\n# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\nCost after iteration 0: 0.693162\nCost after iteration 1000: 0.258625\nCost after iteration 2000: 0.239334\nCost after iteration 3000: 0.230802\nCost after iteration 4000: 0.225528\nCost after iteration 5000: 0.221845\nCost after iteration 6000: 0.219094\nCost after iteration 7000: 0.220638\nCost after iteration 8000: 0.219418\nCost after iteration 9000: 0.218528\n\n\nText(0.5, 1.0, 'Decision Boundary for hidden layer size 4')\n\n\n\n\n\n\n\n\n\n\n# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n\nAccuracy: 90%\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n90%\n\n\n\nAccuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.\n\nCongrats on finishing this Programming Assignment!\nHere’s a quick recap of all you just accomplished:\n\nBuilt a complete 2-class classification neural network with a hidden layer\nMade good use of a non-linear unit\nComputed the cross entropy loss\nImplemented forward and backward propagation\nSeen the impact of varying the hidden layer size, including overfitting.\n\nYou’ve created a neural network that can learn patterns! Excellent work. Below, there are some optional exercises to try out some other hidden layer sizes, and other datasets.\n ## 6 - Tuning hidden layer size (optional/ungraded exercise)\nRun the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.\n\n# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5]\n\n# you can try with different hidden layer sizes\n# but make sure before you submit the assignment it is set as \"hidden_layer_sizes = [1, 2, 3, 4, 5]\"\n# hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n\nAccuracy for 1 hidden units: 67.5 %\nAccuracy for 2 hidden units: 67.25 %\nAccuracy for 3 hidden units: 90.75 %\nAccuracy for 4 hidden units: 90.5 %\nAccuracy for 5 hidden units: 91.25 %\n\n\n\n\n\n\n\n\n\nInterpretation: - The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. - The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. - Later, you’ll become familiar with regularization, which lets you use very large models (such as n_h = 50) without much overfitting.\nNote: Remember to submit the assignment by clicking the blue “Submit Assignment” button at the upper-right.\nSome optional/ungraded questions that you can explore if you wish: - What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? - Play with the learning_rate. What happens? - What if we change the dataset? (See part 7 below!)\n ## 7- Performance on other datasets\nIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.\n\n# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### START CODE HERE ### (choose your dataset)\ndataset = \"noisy_moons\"\n### END CODE HERE ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nReferences:\n\nhttp://scs.ryerson.ca/~aharley/neural-networks/\nhttp://cs231n.github.io/neural-networks-case-study/"
  },
  {
    "objectID": "nb/dl_lab1/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "href": "nb/dl_lab1/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "title": "Logistic Regression using a Neural Network",
    "section": "",
    "text": "We will use neural networks to build a logistic regression classifier for image recognition.\nDuring the exercises, you will have to fill in the code between these comments:\n# CODE_START\n&lt;...your code here...&gt;\n# CODE_END\nPlease remember that we have to vectorize our code, therefore:\n\nDo not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\nUse np.dot(X,Y) to calculate dot products.\n\nDuring the exercises you will:\n\nBuild the general architecture of a learning algorithm, including:\n\nInitializing parameters\nCalculating the cost function and its gradient\nUsing an optimization algorithm (gradient descent)\n\nGather all three functions above into a main model function, in the right order.\n\n ## 1 - Packages ##\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n### v1.2\n\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\nfrom public_tests import *\n\n%matplotlib inline\n# Module autoreloading\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Overview of the Problem set ##\nProblem Statement: You are given a dataset (“data.h5”) containing:\n\na training set of m_train images labeled as cat (y=1) or non-cat (y=0)\na test set of m_test images labeled as cat or non-cat\neach image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 20\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\nImportant: many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n ### Exercise 1 Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) - Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n#(≈ 3 lines of code)\n# m_train = \n# m_test = \n# num_px = \n# CODE_START\n\n# CODE_END\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nExpected Output for m_train, m_test and num_px:\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n ### Exercise 2 Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\n#(≈ 2 lines of code)\n# train_set_x_flatten = ...\n# test_set_x_flatten = ...\n# CODE_START\n\n# CODE_END\n\n# Check that the first 10 pixels of the second image are in the correct place\nassert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\nassert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nExpected Output:\n\n\n\ntrain_set_x_flatten shape\n\n\n(12288, 209)\n\n\n\n\ntrain_set_y shape\n\n\n(1, 209)\n\n\n\n\ntest_set_x_flatten shape\n\n\n(12288, 50)\n\n\n\n\ntest_set_y shape\n\n\n(1, 50)\n\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\nCommon steps for pre-processing a new dataset are:\n\nFigure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)\nReshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\n“Standardize” the data\n\n ## 3 - General Architecture of the learning algorithm ##\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps:\n\nInitialize the parameters of the model\nLearn the parameters for the model by minimizing the cost\n\nUse the learned parameters to make predictions (on the test set)\nAnalyse the results and conclude\n\n ## 4 - Building the parts of our algorithm ##\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n ### 4.1 - Helper functions\n ### Exercise 3 - sigmoid First, please implement sigmoid() function. As you’ve seen in the figure above, you need to compute \\[\nsigmoid(z) = \\frac{1}{1 + e^{-z}}\n\\] for \\(z = w^T x + b\\) to make predictions. Use np.exp().\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    #(≈ 1 line of code)\n    # s = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid_test(sigmoid)\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n ### 4.2 - Initializing parameters\n ### Exercise 4 - initialize_with_zeros Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    \n    # (≈ 2 lines of code)\n    # w = ...\n    # b = ...\n    # CODE_START\n\n    # CODE_END\n\n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\ninitialize_with_zeros_test_1(initialize_with_zeros)\ninitialize_with_zeros_test_2(initialize_with_zeros)\n\n ### 4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n ### Exercise 5 - propagate Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum\\limits_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    grads -- dictionary containing the gradients of the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    cost -- negative log-likelihood cost for logistic regression\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    #(≈ 2 lines of code)\n    # compute activation\n    # A = ...\n    # compute cost by using np.dot to perform multiplication. \n    # And don't use loops for the sum.\n    # cost = ...                                \n    # CODE_START\n\n    \n    # CODE_END\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    #(≈ 2 lines of code)\n    # dw = ...\n    # db = ...\n    # CODE_START\n\n    # CODE_END\n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\n\n# X is using 3 examples, with 2 features each\n# Each example is stacked column-wise\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\npropagate_test(propagate)\n\nExpected output\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\n ### 4.4 - Optimization - You have initialized your parameters. - You are also able to compute a cost function and its gradient. - Now, you want to update the parameters using gradient descent.\n ### Exercise 6 - optimize Write down the optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # (≈ 1 lines of code)\n        # Cost and gradient calculation \n        # grads, cost = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        # w = ...\n        # b = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\noptimize_test(optimize)\n\n ### Exercise 7 - predict The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    #(≈ 1 line of code)\n    # A = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        #(≈ 4 lines of code)\n        # if A[0, i] &gt; ____ :\n        #     Y_prediction[0,i] = \n        # else:\n        #     Y_prediction[0,i] = \n        # YCODE_START\n\n        \n        # CODE_END\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredict_test(predict)\n\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples\n ## 5 - Merge all functions into a model ##\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n ### Exercise 8 - model Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize()\n\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    # (≈ 1 line of code)   \n    # initialize parameters with zeros\n    # and use the \"shape\" function to get the first dimension of X_train\n    # w, b = ...\n    \n    #(≈ 1 line of code)\n    # Gradient descent \n    # params, grads, costs = ...\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    # w = ...\n    # b = ...\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    # Y_prediction_test = ...\n    # Y_prediction_train = ...\n    \n    # CODE_START\n\n    \n    # CODE_END\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nfrom public_tests import *\n\nmodel_test(model)\n\nIf you pass all the tests, run the following cell to train your model.\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\nComment: - Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. - Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier.\nAlso, you see that the model is clearly overfitting the training data.\nUsing the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n ## 6 - Further analysis\nCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nInterpretation: - Different learning rates give different costs and thus different predictions results. - If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). - A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. - In deep learning, we usually recommend that you: - Choose the learning rate that better minimizes the cost function. - If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)\n ## 7 - Test with your own image\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n\nAdd your image to this Jupyter Notebook’s directory, in the “images” folder\nChange your image’s name in the following code\nRun the code and check if the algorithm is right (1 = cat, 0 = non-cat).\n\n\n# change this to the name of your image file\nmy_image = \"Cats-image-cats-36712791-1222-917.jpg\"   \n\n# We preprocess the image to fit your algorithm.\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\nmy_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nImportant: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)\nRecommended reading: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "nb/nlp_lab2.html",
    "href": "nb/nlp_lab2.html",
    "title": "Porter stemming",
    "section": "",
    "text": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\n \n# Create Snowball stemmer\nsnow_stemmer = SnowballStemmer(language='english')\n\n# Create a Porter Stemmer instance\nporter_stemmer = PorterStemmer()\n\n# Create a Lancaster Stemmer instance\nlancaster_stemmer = LancasterStemmer()\n\n# Example words for stemming\nwords = [\"running\", \"jumps\", \"happily\", \"programming\", 'cared','fairly','sportingly']\n\n# Apply stemming to each word\nstemmed_words = [porter_stemmer.stem(word) for word in words]\nprint(\"===Porter===:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Snowball===:\")\nstemmed_words = [snow_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Lancaster===:\")\nstemmed_words = [lancaster_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\n===Porter===:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fairli', 'sportingli']\n\n===Snowball===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fair', 'sport']\n\n===Lancaster===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happy', 'program', 'car', 'fair', 'sport']"
  },
  {
    "objectID": "nlp_lab7.html",
    "href": "nlp_lab7.html",
    "title": "NLP: Lab 7 (TF-IDF)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term\n\n\n\nIn this model, the meaning of a word is defined by a simple function of the counts of nearby words.\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\nBy term frequency we denote \\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\] By document frequency we denote \\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]\nInverse document frequency is defined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\]\nwhere \\(|D|\\) is the total number of documents in the corpus.\nThe TF-IDF measure is simply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lab7.html#tf-idf-vectorizer",
    "href": "nlp_lab7.html#tf-idf-vectorizer",
    "title": "NLP: Lab 7 (TF-IDF)",
    "section": "",
    "text": "In this model, the meaning of a word is defined by a simple function of the counts of nearby words.\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\nBy term frequency we denote \\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\] By document frequency we denote \\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]\nInverse document frequency is defined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\]\nwhere \\(|D|\\) is the total number of documents in the corpus.\nThe TF-IDF measure is simply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lab3.html",
    "href": "nlp_lab3.html",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "We will learn how to:\n\nuse lemmatization (a better version of stemming) using WordNet\ndo text cleanup\nanalyze WordNet semantic hierarchies\n\n\n\n\nGet tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function.\n\n\n\n\nFirst, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\n\n\n\nWordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]\n\n\n\n\n\n\nLet’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come).\n\n\n\nNow we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lab3.html#plan",
    "href": "nlp_lab3.html#plan",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function."
  },
  {
    "objectID": "nlp_lab3.html#preparation",
    "href": "nlp_lab3.html#preparation",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "First, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]"
  },
  {
    "objectID": "nlp_lab3.html#wordnet",
    "href": "nlp_lab3.html#wordnet",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "WordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]"
  },
  {
    "objectID": "nlp_lab3.html#lemmatization-function",
    "href": "nlp_lab3.html#lemmatization-function",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Let’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come)."
  },
  {
    "objectID": "nlp_lab3.html#processing",
    "href": "nlp_lab3.html#processing",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Now we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lec1.html#history",
    "href": "nlp_lec1.html#history",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#history-1",
    "href": "nlp_lec1.html#history-1",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#stats",
    "href": "nlp_lec1.html#stats",
    "title": "Natural Language Processing: Intro",
    "section": "Stats",
    "text": "Stats\n\n\n\nWhen?\n\n\nApproximately between 200,000 years ago and 60,000 years ago (between the appearance of the first anatomically modern humans in southern Africa and the last exodus from Africa respectively)\n\n\n\n\n\n\nHow many?\n\n\nHuman languages count: 7097 (as of 2018)."
  },
  {
    "objectID": "nlp_lec1.html#intelligent-behaviour",
    "href": "nlp_lec1.html#intelligent-behaviour",
    "title": "Natural Language Processing: Intro",
    "section": "Intelligent behaviour",
    "text": "Intelligent behaviour\n\n\n\nSpeaker (writer)\n\n\n\nhas the goal of communicating some knowledge\nthen plans some language that represents the knowledge\nand acts to achieve the goal\n\n\n\n\n\n\n\nListener (reader)\n\n\n\nperceives the language\nand infers the intended meaning."
  },
  {
    "objectID": "nlp_lec1.html#reasons-for-nlp",
    "href": "nlp_lec1.html#reasons-for-nlp",
    "title": "Natural Language Processing: Intro",
    "section": "Reasons for NLP",
    "text": "Reasons for NLP\n\nTo communicate with humans.\nTo learn.\nTo advance the scientific understanding of languages and language use"
  },
  {
    "objectID": "nlp_lec1.html#goal-of-natural-language",
    "href": "nlp_lec1.html#goal-of-natural-language",
    "title": "Natural Language Processing: Intro",
    "section": "Goal of natural language",
    "text": "Goal of natural language\nA medium for communication\nrather than pure representation.\n\n\n\nPlato vs Sophists\n\n\nSophists argued that physical reality can only be experienced through language."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis\nLanguage impacts cognition. Also known as linguistic relativity.\n\n\n\nPredecessors\n\n\n\nAustralian aboriginal language Guugu Yimithirr have no words for relative (or ego- centric) directions, such as front, back, right, or left. Instead they use absolute directions, saying, for example, the equivalent of “I have a pain in my north arm.”\n\n(Norvig “Artificial Intelligence: A Modern Approach”)\n\n\n\n\n\n\nPredecessors\n\n\nWilhelm von Humboldt: language as a spirit of a nation."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis"
  },
  {
    "objectID": "nlp_lec1.html#deep-structure",
    "href": "nlp_lec1.html#deep-structure",
    "title": "Natural Language Processing: Intro",
    "section": "Deep structure",
    "text": "Deep structure\n\n\n\nWanner experiment (1974)\n\n\nSubjects remember exact words with 50% accuracy but remember the content with 90% accuracy.\nHence - there must an internal nonverbal representation.\n\n\n\n\n\n\nPolanyi’s paradox (1966)\n\n\nThe theory that human knowledge of how the world functions and of our own capability are, to a large extent, beyond our explicit understanding. (aka tacit knowledge)."
  },
  {
    "objectID": "nlp_lec1.html#formal-language",
    "href": "nlp_lec1.html#formal-language",
    "title": "Natural Language Processing: Intro",
    "section": "Formal language",
    "text": "Formal language\n\n\n\nDefinition\n\n\nA formal language \\(L\\) over an alphabet \\(\\Sigma\\) is a subset of \\(\\Sigma^*\\), that is, a set of words over that alphabet."
  },
  {
    "objectID": "nlp_lec1.html#language-model",
    "href": "nlp_lec1.html#language-model",
    "title": "Natural Language Processing: Intro",
    "section": "Language model",
    "text": "Language model\n\n\n\nDefinition\n\n\nWe define a language model as a probability distribution describing the likelihood of any string."
  },
  {
    "objectID": "nlp_lec1.html#eliza",
    "href": "nlp_lec1.html#eliza",
    "title": "Natural Language Processing: Intro",
    "section": "ELIZA",
    "text": "ELIZA\n\nELIZA - an example of primitive pattern matching."
  },
  {
    "objectID": "nlp_lec1.html#regex",
    "href": "nlp_lec1.html#regex",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\nRegular expressions - a tool for describing text patterns.\n\n\n\nDefinition\n\n\nAn algebraic notation for characterizing a set of strings\n\n\n\n\n\nKleene, S. C. 1951. Representation of events in nerve nets and finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum."
  },
  {
    "objectID": "nlp_lec1.html#regex-1",
    "href": "nlp_lec1.html#regex-1",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDefinition\n\n\nA Kleene algebra is a set \\(A\\) together with two binary operations \\(+: A \\times A \\rightarrow A\\) and \\(\\cdot : A \\times A \\rightarrow A\\) and one function \\(\\ast : A \\rightarrow A\\), written as \\(a + b\\), \\(ab\\) and \\(a\\ast\\) respectively, so that the following axioms are satisfied.\n\nAssociativity of \\(+\\) and \\(\\cdot\\): \\(a + (b + c) = (a + b) + c\\) and \\(a(bc) = (ab)c\\) \\(\\forall a, b, c \\in A\\).\nCommutativity of \\(+\\): \\(a + b = b + a\\) \\(\\forall a, b \\in A\\)\nDistributivity: \\(a(b + c) = (ab) + (ac)\\) and \\((b + c)a = (ba) + (ca)\\) \\(\\forall a, b, c \\in A\\)\nIdentity elements for \\(+\\) and \\(\\cdot\\): \\(\\exists 0 \\in A:\\forall a \\in A: a + 0 = 0 + a = a\\); \\(\\exists 1 \\in A: \\forall a \\in A: a1 = 1a = a\\).\nAnnihilation by 0: \\(a0 = 0a = 0 \\forall a \\in A\\). The above axioms define a semiring.\n\\(+\\) is idempotent: \\(a + a = a \\quad \\forall a \\in A\\)."
  },
  {
    "objectID": "nlp_lec1.html#regex-2",
    "href": "nlp_lec1.html#regex-2",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nOrdering\n\n\nIt is now possible to define a partial order \\(\\leq\\) on \\(A\\) by setting \\(a \\leq b\\) if and only if \\(a + b = b\\) (or equivalently: \\(a \\leq b\\) if and only if \\(\\exists x \\in A:  a + x = b\\).\nWith any definition, \\(a \\leq b \\leq a \\Rightarrow a = b\\). With this order we can formulate the last four axioms about the operation \\(\\ast\\):\n\n\\(1 + a(a\\ast) leq a\\ast \\forall a in A\\).\n\\(1 + (a\\ast)a \\leq a\\ast \\forall a in A\\).\nif a and x are in A such that \\(ax \\leq x\\), then \\(a\\ast x \\leq x\\)\nif a and x are in A such that \\(xa \\leq x\\), then \\(x(a\\ast) \\leq x\\).\n\nIntuitively, one should think of a + b as the “union” or the “least upper bound” of a and b and of ab as some multiplication which is monotonic, in the sense that \\(a \\leq b \\Rightarrow ax \\leq bx\\).\nThe idea behind the star operator is \\(a\\ast = 1 + a + aa + aaa + ...\\). From the standpoint of programming language theory, one may also interpret + as “choice”, \\(\\cdot\\) as “sequencing” and \\(\\ast\\) as “iteration”."
  },
  {
    "objectID": "nlp_lec1.html#regex-3",
    "href": "nlp_lec1.html#regex-3",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nConcatenation\n\n\nA sequence of characters\n/someword/\n\n\n\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/alt/\nThe alternative option would be…\n\n\n/simple/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-4",
    "href": "nlp_lec1.html#regex-4",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction\n\n\nA single character to choose among multiple options\n/[asd]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[0123456789]/\nSome number examples are 0, 3, 5\n\n\n/[rgx]/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-5",
    "href": "nlp_lec1.html#regex-5",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction with range\n\n\nA single character to choose among multiple options\n/[a-z]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-6",
    "href": "nlp_lec1.html#regex-6",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene *\n\n\nZero or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]*/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-7",
    "href": "nlp_lec1.html#regex-7",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene +\n\n\nOne or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]+/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-8",
    "href": "nlp_lec1.html#regex-8",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nWildcard dot\n\n\nAny character except newline\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/.+/\nPhone number: 067-1234567\n\n\n/beg.n/\nbegin, began, begun"
  },
  {
    "objectID": "nlp_lec1.html#regex-9",
    "href": "nlp_lec1.html#regex-9",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nAnchors\n\n\nMatching beginning and end of lines\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/^/\nStart of line\n\n\n/$/\nEnd of line\n\n\n/\\b/\nWord boundary\n\n\n/\\B/\nNon-word boundary"
  },
  {
    "objectID": "nlp_lec1.html#regex-10",
    "href": "nlp_lec1.html#regex-10",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction operator\n\n\nEither/or for strings\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/cat|dog/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-11",
    "href": "nlp_lec1.html#regex-11",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nGrouping\n\n\nUsed for precedence\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/(cat|dog)*/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-12",
    "href": "nlp_lec1.html#regex-12",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nSubstitutions\n\n\nUsed for precedence\n\nExamples\n\n\n\n\n\n\n\nRegex\nBefore\nAfter\n\n\n\n\ns/(cat|dog)*/frog/\nRaining cats and dogs\nRaining frogs and frogs\n\n\ns/behaviour/behavior/\nCognitive-behavioural therapy\nCognitive-behavioral therapy"
  },
  {
    "objectID": "nlp_lec1.html#regex-13",
    "href": "nlp_lec1.html#regex-13",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nELIZA sample\n\n\nUser1: Men are all alike.\nELIZA1: User2: They’re always bugging us about something or other.\nELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser3: Well, my boyfriend made me come here.\nELIZA3: YOUR BOYFRIEND MADE YOU COME HERE\nUser4: He says I’m depressed much of the time.\nELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\n\n\n\n\n\n\nELIZA sample\n\n\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/"
  },
  {
    "objectID": "nlp_lec1.html#parsing",
    "href": "nlp_lec1.html#parsing",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\n\n\nA computer-readable collection of text or speech.\n\n\n\n\n\n\nPunctuation\n\n\nMarks indicating how a piece of written text should be read and understood.\n\n\n\n\n\n\nUtterance\n\n\nSpoken correlate of a sentence."
  },
  {
    "objectID": "nlp_lec1.html#parsing-1",
    "href": "nlp_lec1.html#parsing-1",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nDisfluency\n\n\nBreak or disruption that occurs in the flow of speech.\nTwo types:\n\nfragments\nfillers"
  },
  {
    "objectID": "nlp_lec1.html#parsing-2",
    "href": "nlp_lec1.html#parsing-2",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWord types\n\n\nAre the number of distinct words in a corpus; if the set of words in the vocabulary is \\(V\\) , the number of types is the vocabulary size \\(|V|\\).\n\n\n\n\n\n\nWord instances\n\n\nAre the total number \\(N\\) of running words.\n(sometimes also called word tokens).\n\n\n\n\n\n\nExample\n\n\n\nTo be, or not to be, that is the question."
  },
  {
    "objectID": "nlp_lec1.html#parsing-3",
    "href": "nlp_lec1.html#parsing-3",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\nTypes = \\(|V|\\)\nInstances = \\(N\\)\n\n\n\n\nShakespeare\n31 thousand\n884 thousand\n\n\nBrown corpus\n38 thousand\n1 million\n\n\nSwitchboard telephone conversations\n20 thousand\n2.4 million\n\n\nCOCA\n2 million\n440 million\n\n\nGoogle n-grams\n13 million\n1 trillion"
  },
  {
    "objectID": "nlp_lec1.html#parsing-4",
    "href": "nlp_lec1.html#parsing-4",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nHerdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)\n\n\n\\[\n|V|= kN^{\\beta}.\n\\] Here \\(k\\) and \\(\\beta\\) are positive constants, and \\(0 &lt;\\beta &lt;1\\).\n\n\n\n\n\nFor large corpora \\(0.67 &lt; \\beta &lt; 0.75\\)."
  },
  {
    "objectID": "nlp_lec1.html#parsing-5",
    "href": "nlp_lec1.html#parsing-5",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nZipf’s law\n\n\nIf \\(t_1\\) is the most common term in the collection, \\(t_2\\) is the next most common, and so on, then the collection frequency \\(cf_i\\) of the \\(i\\)-th most common term is proportional to \\(\\frac{1}{i}\\): \\[\ncf_i \\propto \\frac{1}{i},\n\\] or \\[\ncf_i = ci^k.\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#parsing-6",
    "href": "nlp_lec1.html#parsing-6",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWordforms\n\n\nExample: cat vs cats. We say these two words are different wordforms but have the same lemma.\n\n\n\n\n\n\nLemma\n\n\nA lemma is a set of lexical forms having the same stem, and usually the same major part-of-speech.\n\n\n\n\n\n\nExample\n\n\nThe wordform is the full inflected or derived form of the word. The two wordforms cat and cats thus have the same lemma, which we can represent as cat."
  },
  {
    "objectID": "nlp_lec1.html#parsing-7",
    "href": "nlp_lec1.html#parsing-7",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nMorphemes\n\n\nThe smallest meaning-bearing unit of a language.\n\n\n\n\n\n\nExamples\n\n\nIndistinguisble -&gt; [in, distinguish, able]"
  },
  {
    "objectID": "nlp_lec1.html#affixes",
    "href": "nlp_lec1.html#affixes",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nAffix taxonomy\n\n\nAffixes are classified into two types:\n\nAccording to their position in the word\nAccording to their function in a phrase or sentence.\n\n\n\n\n\n\n\nBy position\n\n\n\nPrefixes\nInfixes\nSuffixes.\nCircumfixes (Georgian, Malay)"
  },
  {
    "objectID": "nlp_lec1.html#affixes-1",
    "href": "nlp_lec1.html#affixes-1",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nBy function\n\n\n\nDerivational affixes are for creating new words usually by changing the part of speech or the meaning or both to the words when they are added to.\n\nThey can be prefixes or suffixes e.g. unkind , kingship etc.\n\nInflectional affixes mark the grammatical categories e.g. –s in girls"
  },
  {
    "objectID": "nlp_lec1.html#language-morphology",
    "href": "nlp_lec1.html#language-morphology",
    "title": "Natural Language Processing: Intro",
    "section": "Language morphology",
    "text": "Language morphology\n\nanalytical (English)\ninflected (Ukrainian)\nagglutinative (partially German)"
  },
  {
    "objectID": "nlp_lec1.html#corpora",
    "href": "nlp_lec1.html#corpora",
    "title": "Natural Language Processing: Intro",
    "section": "Corpora",
    "text": "Corpora\nVarieties depending on:\n\nlanguages\nlanguage varieties\ngenres\ntime\nspeaker demographics"
  },
  {
    "objectID": "nlp_lec1.html#parsing-8",
    "href": "nlp_lec1.html#parsing-8",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\nText normalization consists of:\n\nTokenizing (segmenting) words\nNormalizing word formats\nSegmenting sentences"
  },
  {
    "objectID": "nlp_lec1.html#tokenization",
    "href": "nlp_lec1.html#tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Tokenization",
    "text": "Tokenization\nTwo types:\n\ntop-down\nbottom-up"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization",
    "href": "nlp_lec1.html#top-down-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\nbreak off punctuation as a separate token\ninternal punctuation: Ph.D., AT&T\nprices ($45.55) and dates (18/02/2025)\nURLs (https://www.stanford.edu),\nTwitter hashtags (#nlproc)\nemail addresses (someone@cs.colorado.edu).\nnumber expressions introduce complications: e.g. 555,500.50.\nclitic contractions: I'm, l'homme."
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-1",
    "href": "nlp_lec1.html#top-down-tokenization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nPenn Treebank tokenization standard\n\n\nUsed for the parsed corpora (treebanks) released by the Lin- guistic Data Consortium (LDC).\n\nseparates out clitics (doesn’t becomes does plus n’t)\nkeeps hyphenated words together\nseparates out all punctuation"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-2",
    "href": "nlp_lec1.html#top-down-tokenization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nnltk.regexp_tokenize\n\n\n&gt;&gt;&gt; text = ’That U.S.A. poster-print costs $12.40...’\n&gt;&gt;&gt; pattern = r’’’(?x) # set flag to allow verbose regexps\n... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n... | \\w+(?:-\\w+)* # words with optional internal hyphens\n... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n... | \\.\\.\\. # ellipsis\n... | [][.,;\"’?():_‘-] # these are separate tokens; includes ], [\n... ’’’\n&gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)\n[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-3",
    "href": "nlp_lec1.html#top-down-tokenization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\nWord tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries.\n\n\n\nMorphemes In Chinese\n\n\nfor example, words are composed of characters (called hanzi in Chinese). Each character generally represents a single unit of meaning (a morpheme).\n\n\n\n\n\n\nWord segmentation\n\n\nFor Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required."
  },
  {
    "objectID": "nlp_lec1.html#bottom-up-tokenization",
    "href": "nlp_lec1.html#bottom-up-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Bottom-up tokenization",
    "text": "Bottom-up tokenization\n\n\n\nDefinition\n\n\nWe use the data to infer the tokens. We call these tokens subwords.\n\n\n\n\n\n\nParts\n\n\n\ntoken learner: produces a vocabulary of tokens\ntoken segmenter: takes a test sentence and segments it into tokens\n\n\n\n\n\n\n\nExamples\n\n\n\nbyte-pair encoding (Sennrich et al., 2016)\nunigram language modeling (Kudo, 2018)\nSentencePiece (Kudo and Richardson, 2018)"
  },
  {
    "objectID": "nlp_lec1.html#bpe",
    "href": "nlp_lec1.html#bpe",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nToken Learner\n\n\n\nStart with a vocabulary that is just the set of all individual characters.\nExamine the training corpus, choose the two symbols that are most frequently adjacent\n2.1. For example, if (‘A’, ‘B’) frequently occur together, it will add a new merged symbol ‘AB’ to the vocabulary\n2.2. And replaces every adjacent ’A’ ’B’ in the corpus with the new ‘AB’.\nContinue counting and merging, creating new longer and longer character strings, until \\(k\\) merges have been done creating k novel tokens; \\(k\\) is thus a parameter of the algorithm.\n\n4.The resulting vocabulary consists of the original set of characters plus \\(k\\) new symbols."
  },
  {
    "objectID": "nlp_lec1.html#bpe-1",
    "href": "nlp_lec1.html#bpe-1",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nNote\n\n\nThe algorithm is usually run inside words (not merging across word boundaries), so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol , and its counts."
  },
  {
    "objectID": "nlp_lec1.html#bpe-2",
    "href": "nlp_lec1.html#bpe-2",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab \\(V\\)\n\\(V \\leftarrow\\) unique characters in C # initial set of tokens is characters\nfor i = 1 to k do # merge tokens k times\n\\(\\quad\\) \\(t_L\\), \\(t_R\\) \\(\\leftarrow\\) #Most frequent pair of adjacent tokens in C\n\\(\\quad\\) \\(t_{NEW} \\leftarrow t_L + t_R\\) # make new token by concatenating\n\\(\\quad\\) \\(V \\leftarrow V + t_{NEW}\\) # update the vocabulary\n\\(\\quad\\) Replace each occurrence of \\(t_L\\), \\(t_R\\) in \\(C\\) with \\(t_{NEW}\\). # update the corpus\nreturn \\(V\\)"
  },
  {
    "objectID": "nlp_lec1.html#bpe-3",
    "href": "nlp_lec1.html#bpe-3",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nToken Segmenter:\n\nRuns on the merges we have learned from the training data on the test data.\nIt runs them greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization",
    "href": "nlp_lec1.html#word-normalization",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nSimplest method: case folding.\n\n\n\nNote\n\n\nNot very useful for text classification: compare US (the country) and us (pronoun).\n\n\n\n\n\n\nLemmatization\n\n\nThe task of determining that two words have the same root, despite their surface differences.\nbe \\(\\rightarrow\\) is, are\nPerformed using morphological parsing."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-1",
    "href": "nlp_lec1.html#word-normalization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nMorphological parsing\n\n\nSplitting each word into morphemes of two types:\n\nstems\naffixes\n\n\n\n\n\n\n\nNaive version: stemming\n\n\nThis means just dropping affixes."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-2",
    "href": "nlp_lec1.html#word-normalization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\n\nPorter stemmer\n\n\n\nclassify every character in a given token as either a consonant (“c”) or vowel (“v”)\ngroup subsequent consonants as “C” and subsequent vowels as “V.”\nrepresent every word token as a combination of consonant and vowel groups."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-1",
    "href": "nlp_lec1.html#porter-stemmer-1",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nExample\n\n\ncollection \\(\\rightarrow\\) CVCV…C\nillustrate \\(\\rightarrow\\) VCVC…V\nBoth can be presented as:\n\\[\n[C](VC)^m[V]\n\\]\nm is called the measure of the word."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-2",
    "href": "nlp_lec1.html#porter-stemmer-2",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer"
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-3",
    "href": "nlp_lec1.html#porter-stemmer-3",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nRules\n\n\nStandard form: \\[\n(\\textbf{condition})\\textbf{S}_1 \\rightarrow \\textbf{S}_2\n\\] There are five phases of rule application\n\n\n\n\n\n\nHow to read\n\n\nIf a word ends with the suffix \\(S_1\\)\nAND\nthe stem before \\(S_1\\) satisfies the given condition\nTHEN \\(S_1\\) is replaced by \\(S_2\\)."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-4",
    "href": "nlp_lec1.html#porter-stemmer-4",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nConditions\n\n\n\n\\(\\ast S\\): the stem ends with S (and similarly for the other letters)\n\\(\\ast v \\ast\\): the stem contains a vowel\n\\(\\ast d\\): the stem ends with a double consonant (e.g. -TT, -SS)\n\\(\\ast o\\): the stem ends with \\(cvc\\), where the second c is not W, X or Y (e.g. -WIL, -HOP)\n\nAnd the condition part may also contain expressions with and, or and not.\n\n\n\n\n\n\nExample\n\n\n\\((m &gt; 1) EMENT \\rightarrow\\) will perform this transformation:\nreplacement \\(\\rightarrow\\) replac"
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-3",
    "href": "nlp_lec1.html#word-normalization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nOther stemmers:\n\n\n\nLovins stemmer\n\n\nThe first published stemming algorithm, is essentially a heavily parametrized find-and-replace function.\n\ncompares each input token against a list of common English suffixes, each suffix being conditioned by one of twenty-nine rules\nif the stemmer finds a predefined suffix in a token and removing the suffix does not violate any conditions attached to that suffix (such as character length restrictions), the algorithm removes that suffix.\nthe stemmer then runs the resulting stemmed token through another set of rules that correct for common malformations, such as double letters (such as hopping becomes hopp becomes hop)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-4",
    "href": "nlp_lec1.html#word-normalization-4",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nSnowball stemmer\n\n\nAn updated version of the Porter stemmer. It differs from Porter in two main ways:\n\nWhile Lovins and Porter only stem English words, Snowball can stem text data in other Roman script languages, such as Dutch, German, French, or Spanish. Also has capabilities for non-Roman script languages.\nSnowball has an option to ignore stop words."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-5",
    "href": "nlp_lec1.html#word-normalization-5",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nLancaster stemmer (also called Paice stemmer)\n\n\nThe most aggressive English stemming algorithm.\n\nit contains a list of over 100 rules that dictate which ending strings to replace.\nthe stemmer iterates each word token against each rule. If a token’s ending characters match the string defined in a given rule, the algorithm modifies the token per that rule’s operation, then runs the transformed token through every rule again.\nthe stemmer iterates each token through each rule until that token passes all the rules without being transformed."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-6",
    "href": "nlp_lec1.html#word-normalization-6",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nStemming errors\n\n\n\nover-generalizing (lemmatizing policy to police)\nunder-generalizing (not lemmatizing European to Europe)"
  },
  {
    "objectID": "nlp_lec1.html#sentence-tokenization",
    "href": "nlp_lec1.html#sentence-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Sentence Tokenization",
    "text": "Sentence Tokenization\n\n\n\nChallenges\n\n\n\nmulti-purpose punctuation\nabbreviation dictionaries"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance",
    "href": "nlp_lec1.html#edit-distance",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nDefinition\n\n\nMinimum edit distance between two strings is defined as the minimum number of editing operations:\n\ninsertion\ndeletion\nsubstitution\n\nneeded to transform one string into another."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-1",
    "href": "nlp_lec1.html#edit-distance-1",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nString alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-2",
    "href": "nlp_lec1.html#edit-distance-2",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nLevenshtein distance\n\n\nEach of the 3 operations has cost 1.\nAlternatively, we can forbid substitutions (this is equivalent to saying that substitutions have cost 2)."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-3",
    "href": "nlp_lec1.html#edit-distance-3",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\nWagner-Fischer minimum edit distance algorithm.\n\n\n\nNotation\n\n\n\n\\(X\\): source string with length \\(n\\)\n\\(Y\\): target string with length \\(m\\)\n\\(D[i,j]\\): edit distance between \\(X[1..i]\\) and \\(Y[1..j]\\).\n\\(D[n,m]\\): edit distance between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\nCalculation\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + \\text{del_cost}(source[i]),\\\\\nD[i,j-1] + \\text{ins_cost}(target[j]),\\\\\nD[i-1,j-1] + \\text{sub_cost}(source[i], target[j]).\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-4",
    "href": "nlp_lec1.html#edit-distance-4",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nCalculation without substitution\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + 1,\\\\\nD[i,j-1] + 1,\\\\\nD[i-1,j-1] + \\begin{cases}\n2; \\quad \\text{if} \\quad source[i] \\neq target[j]), \\\\\n0; \\quad \\text{if} \\quad source[i] = target[j])\n\\end{cases}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-5",
    "href": "nlp_lec1.html#edit-distance-5",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\nfunction MIN-EDIT-DISTANCE(source, target) returns min-distance\n\\(n \\leftarrow LENGTH(source)\\)\n\\(m \\leftarrow LENGTH(target)\\)\nCreate a distance matrix \\(D[n+1,m+1]\\)\n# Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nfor each row i from 1 to n do\n\\(\\quad\\) \\(D[i,0] \\leftarrow D[i-1,0]\\) + del_cost(source[i])\nfor each column j from 1 to m do\n\\(\\quad\\) \\(D[0,j] \\leftarrow D[0, j-1] + ins-cost(target[j])\\)"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-6",
    "href": "nlp_lec1.html#edit-distance-6",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\n# Recurrence relation:\nfor each row i from 1 to n do\n\\(\\quad\\) for each column j from 1 to m do\n\\(\\quad\\) \\(\\quad\\) \\(D[i, j] \\leftarrow MIN( D[i−1, j]\\) + del_cost(source[i]),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i−1, j−1] + sub\\_cost(source[i], target[j])\\),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i, j−1] + ins\\_cost(target[j]))\\)\n# Termination\nreturn D[n,m]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-7",
    "href": "nlp_lec1.html#edit-distance-7",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance"
  },
  {
    "objectID": "nlp_lec1.html#cost-alignment",
    "href": "nlp_lec1.html#cost-alignment",
    "title": "Natural Language Processing: Intro",
    "section": "Cost alignment",
    "text": "Cost alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-8",
    "href": "nlp_lec1.html#edit-distance-8",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nHamming distance\n\n\nA number of positions at which the corresponding symbols are different.\nTherefore, identical to Levenshtein with only substitution allowed.\nCan only work for strings of similar length.\n\n\n\ndef hamming_distance(string1: str, string2: str) -&gt; int:\n    \"\"\"Return the Hamming distance between two strings.\"\"\"\n    if len(string1) != len(string2):\n        raise ValueError(\"Strings must be of equal length.\")\n    dist_counter = 0\n    for n in range(len(string1)):\n        if string1[n] != string2[n]:\n            dist_counter += 1\n    return dist_counter"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "nlp.html#lectures",
    "href": "nlp.html#lectures",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "nlp.html#labs",
    "href": "nlp.html#labs",
    "title": "Natural Language Processing",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2\nLab 3\nLab 4\nLab 5\nLab 6\nLab 7"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#lectures",
    "href": "dl.html#lectures",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#labs",
    "href": "dl.html#labs",
    "title": "Deep Learning",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2\nLab 3\nLab 4\nLab 5"
  },
  {
    "objectID": "dl.html#python-venv-info",
    "href": "dl.html#python-venv-info",
    "title": "Deep Learning",
    "section": "Python venv info",
    "text": "Python venv info\nInstructions on how to create a virtual environment for DL course:\n\nLaunch Anaconda Prompt. In it execute:\n\npython -m venv dl_venv\n\nActivate the environment:\n\ndl_venv\\Scripts\\activate\n\nInstall required packages:\n\npip install jupyter matplotlib scipy numpy scikit-learn termcolor h5py\n\nRun Jupyter:\n\njupyter notebook\n\nVoila! (pardon my French)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning/NLP",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nlp_lab1.html",
    "href": "nlp_lab1.html",
    "title": "NLP: Lab 1",
    "section": "",
    "text": "We’ll start with basic text analysis via statistical methods.\nFor this purpose, we’ll use three libraries (mostly):\n\nNLTK\nScattertext\nSpacy"
  },
  {
    "objectID": "nlp_lab1.html#through-requirements.txt",
    "href": "nlp_lab1.html#through-requirements.txt",
    "title": "NLP: Lab 1",
    "section": "Through requirements.txt",
    "text": "Through requirements.txt\nYou can install all dependencies through requirements.txt:\npip install -r requirements.txt\nAlternatively, or if any issues occur, we can proceed manually via the following steps:"
  },
  {
    "objectID": "nlp_lab1.html#install-python-nltk-package",
    "href": "nlp_lab1.html#install-python-nltk-package",
    "title": "NLP: Lab 1",
    "section": "1. Install Python NLTK package",
    "text": "1. Install Python NLTK package\nFrom here.\n   pip install nltk\n   pip install matplotlib\nIn order to install Python Tkinter library, look here.\nAlso install additional data by\n   import nltk; \n   nltk.download('popular')\nSet up the texts:\n\n   import nltk\n   nltk.download('nps_chat')\n   nltk.download('webtext')\n   from nltk.book import *\n\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Package nps_chat is already up-to-date!\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!"
  },
  {
    "objectID": "nlp_lab1.html#install-scattertext-and-spacy",
    "href": "nlp_lab1.html#install-scattertext-and-spacy",
    "title": "NLP: Lab 1",
    "section": "2. Install Scattertext and Spacy",
    "text": "2. Install Scattertext and Spacy\npip install spacy scattertext\nAnd then update Spacy:\n!python -m spacy download en_core_web_sm"
  },
  {
    "objectID": "nlp_lab1.html#example-concordance",
    "href": "nlp_lab1.html#example-concordance",
    "title": "NLP: Lab 1",
    "section": "Example: concordance",
    "text": "Example: concordance\n\ntext3.concordance(\"earth\")\n\nDisplaying 25 of 112 matches:\nnning God created the heaven and the earth . And the earth was without form , a\nd the heaven and the earth . And the earth was without form , and void ; and da\nwas so . And God called the dry land Earth ; and the gathering together of the \nit was good . And God said , Let the earth bring forth grass , the herb yieldin\nupon the ear and it was so . And the earth brought forth grass , and herb yield\nof the heaven to give light upon the earth , And to rule over the day and over \nfe , and fowl that may fly above the earth in the open firmament of heaven . An\n seas , and let fowl multiply in the earth . And the evening and the morning we\ne fifth day . And God said , Let the earth bring forth the living creature afte\nnd creeping thing , and beast of the earth after his ki and it was so . And God\ns so . And God made the beast of the earth after his kind , and cattle after th\nd every thing that creepeth upon the earth after his ki and God saw that it was\nd over the cattle , and over all the earth , and over every creeping thing that\nreeping thing that creepeth upon the earth . So God created man in his own imag\nl , and multiply , and replenish the earth , and subdue and have dominion over \nry living thing that moveth upon the earth . And God said , Behold , I have giv\n , which is upon the face of all the earth , and every tree , in the which is t\nfor meat . And to every beast of the earth , and to every fowl of the air , and\no every thing that creepeth upon the earth , wherein there is life , I have giv\nsixth day . Thus the heavens and the earth were finished , and all the host of \nenerations of the heavens and of the earth when they were created , in the day \nn the day that the LORD God made the earth and the heavens , And every plant of\nnt of the field before it was in the earth , and every herb of the field before\nd had not caused it to rain upon the earth , and there was not a man to till th\n . But there went up a mist from the earth , and watered the whole face of the"
  },
  {
    "objectID": "nlp_lab1.html#example-similar",
    "href": "nlp_lab1.html#example-similar",
    "title": "NLP: Lab 1",
    "section": "Example: similar",
    "text": "Example: similar\n\ntext3.similar(\"man\")\n\nland lord men place woman earth waters well city lad day cattle field\nwife way flood servant people famine pillar"
  },
  {
    "objectID": "nlp_lab1.html#example-dispersion_plot",
    "href": "nlp_lab1.html#example-dispersion_plot",
    "title": "NLP: Lab 1",
    "section": "Example: dispersion_plot",
    "text": "Example: dispersion_plot\n\ntext3.dispersion_plot([\"man\", \"earth\"])"
  },
  {
    "objectID": "nlp_lab1.html#example-freqdist",
    "href": "nlp_lab1.html#example-freqdist",
    "title": "NLP: Lab 1",
    "section": "Example: FreqDist",
    "text": "Example: FreqDist\n\nfdist = FreqDist(text3)\nprint(fdist)\n\n&lt;FreqDist with 2789 samples and 44764 outcomes&gt;\n\n\n\nfdist.most_common(50)\n\n[(',', 3681),\n ('and', 2428),\n ('the', 2411),\n ('of', 1358),\n ('.', 1315),\n ('And', 1250),\n ('his', 651),\n ('he', 648),\n ('to', 611),\n (';', 605),\n ('unto', 590),\n ('in', 588),\n ('that', 509),\n ('I', 484),\n ('said', 476),\n ('him', 387),\n ('a', 342),\n ('my', 325),\n ('was', 317),\n ('for', 297),\n ('it', 290),\n ('with', 289),\n ('me', 282),\n ('thou', 272),\n (\"'\", 268),\n ('is', 267),\n ('thy', 267),\n ('s', 263),\n ('thee', 257),\n ('be', 254),\n ('shall', 253),\n ('they', 249),\n ('all', 245),\n (':', 238),\n ('God', 231),\n ('them', 230),\n ('not', 224),\n ('which', 198),\n ('father', 198),\n ('will', 195),\n ('land', 184),\n ('Jacob', 179),\n ('came', 177),\n ('her', 173),\n ('LORD', 166),\n ('were', 163),\n ('she', 161),\n ('from', 157),\n ('Joseph', 157),\n ('their', 153)]\n\n\n\nfdist.plot(50, cumulative=True)"
  },
  {
    "objectID": "nlp_lec3.html#context",
    "href": "nlp_lec3.html#context",
    "title": "Natural Language Processing: word embeddings",
    "section": "Context",
    "text": "Context\n\n\n\nDistributional hypothesis\n\n\nThe link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.\nJoos (1950), Harris (1954), and Firth (1957): amount of meaning difference between two words is corresponding roughly to the amount of difference in their environments."
  },
  {
    "objectID": "nlp_lec3.html#philosophy",
    "href": "nlp_lec3.html#philosophy",
    "title": "Natural Language Processing: word embeddings",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nWittgenstein (1953)\n\n\n\n“the meaning of a word is its use in the language”\n\n\n\n\n\n\n\nLewis Carroll\n\n\n\nWhen I use a word,’ Humpty Dumpty said in rather a scornful tone, ‘it means just what I choose it to mean — neither more nor less.’"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics",
    "href": "nlp_lec3.html#lexical-semantics",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-1",
    "href": "nlp_lec3.html#lexical-semantics-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\nWord Similarity\n\n\n\nC1\nC2\nPOS\nUSF*\nUSF rank (of 999)\nSimLex\nSimLex rank (of 999)\n\n\n\n\ndirty\nnarrow\nA\n0.00\n999\n0.30\n996\n\n\nstudent\npupil\nN\n6.80\n12\n9.40\n12\n\n\nwin\ndominate\nV\n0.41\n364\n5.68\n361\n\n\nsmart\ndumb\nA\n2.10\n92\n0.60\n947\n\n\nattention\nawareness\nN\n0.10\n895\n8.73\n58\n\n\nleave\nenter\nV\n2.16\n89\n1.38\n841"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-2",
    "href": "nlp_lec3.html#lexical-semantics-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\nWord relatedness"
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-3",
    "href": "nlp_lec3.html#lexical-semantics-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\n\n\n\nSemantic field\n\n\nA semantic field is a set of words which:\n\ncover a particular semantic domain\nbear structured relations with each other."
  },
  {
    "objectID": "nlp_lec3.html#lexical-semantics-4",
    "href": "nlp_lec3.html#lexical-semantics-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Lexical semantics",
    "text": "Lexical semantics\n\n\n\nAffective meaning\n\n\nEarly work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:\n\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\n\n\n\n\n\n\n\nValence\nArousal\nDominance\n\n\n\n\ncourageous\n8.05\n5.5\n\n\nmusic\n7.67\n5.57\n\n\nheartbreak\n2.45\n5.65\n\n\ncub\n6.71\n3.95"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics",
    "href": "nlp_lec3.html#vector-semantics",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\n\n\nVector semantics\n\n\n\\[\n\\text{Vector semantics} = \\text{Affective meaning} + \\text{Distributional hypothesis}\n\\] Vector semantics learns representations of the meaning of words, called embeddings, directly from their distributions in texts."
  },
  {
    "objectID": "nlp_lec3.html#embeddings",
    "href": "nlp_lec3.html#embeddings",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nTaxonomy\n\n\n\nstatic vs dynamic embeddings\nsparse vs dense"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics-2",
    "href": "nlp_lec3.html#vector-semantics-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\n\n\nConcepts or word senses\n\n\n\nHave a complex many-to-many association with words (homonymy, multiple senses)\nHave relations with each other\n\nSynonymy\nAntonymy\nSimilarity\nRelatedness\nConnotation"
  },
  {
    "objectID": "nlp_lec3.html#vector-semantics-3",
    "href": "nlp_lec3.html#vector-semantics-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Vector semantics",
    "text": "Vector semantics\n\nZellig Harris (1954): If A and B have almost identical environments we say that they are synonyms."
  },
  {
    "objectID": "nlp_lec3.html#edit-distance",
    "href": "nlp_lec3.html#edit-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "Edit distance",
    "text": "Edit distance"
  },
  {
    "objectID": "nlp_lec3.html#one-hot-distance",
    "href": "nlp_lec3.html#one-hot-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "One-hot distance",
    "text": "One-hot distance"
  },
  {
    "objectID": "nlp_lec3.html#bag-of-words",
    "href": "nlp_lec3.html#bag-of-words",
    "title": "Natural Language Processing: word embeddings",
    "section": "Bag-of-Words",
    "text": "Bag-of-Words"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf",
    "href": "nlp_lec3.html#tf-idf",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF"
  },
  {
    "objectID": "nlp_lec3.html#feature-space",
    "href": "nlp_lec3.html#feature-space",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space",
    "text": "Feature space"
  },
  {
    "objectID": "nlp_lec3.html#coordinates",
    "href": "nlp_lec3.html#coordinates",
    "title": "Natural Language Processing: word embeddings",
    "section": "Coordinates",
    "text": "Coordinates\nGender and age are called semantic features: they represent part of the meaning of each word. If we associate a numerical scale with each feature, then we can assign coordinates to each word:"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-updated",
    "href": "nlp_lec3.html#feature-space-updated",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space updated",
    "text": "Feature space updated"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-updated-1",
    "href": "nlp_lec3.html#feature-space-updated-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space updated",
    "text": "Feature space updated"
  },
  {
    "objectID": "nlp_lec3.html#feature-space-3d",
    "href": "nlp_lec3.html#feature-space-3d",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space 3D",
    "text": "Feature space 3D\nNew words: “king”, “queen”, “prince”, and “princess”."
  },
  {
    "objectID": "nlp_lec3.html#feature-space-3d-1",
    "href": "nlp_lec3.html#feature-space-3d-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature space 3D",
    "text": "Feature space 3D"
  },
  {
    "objectID": "nlp_lec3.html#feature-vectors",
    "href": "nlp_lec3.html#feature-vectors",
    "title": "Natural Language Processing: word embeddings",
    "section": "Feature vectors",
    "text": "Feature vectors\n\n\n\nDefinition\n\n\nVectors representing values of semantic features are called feature vectors."
  },
  {
    "objectID": "nlp_lec3.html#applications",
    "href": "nlp_lec3.html#applications",
    "title": "Natural Language Processing: word embeddings",
    "section": "Applications",
    "text": "Applications\n\n\n\nHow can we compute word similarity?\n\n\n\ncount number of features where words differ\ncilculate Euclidean distance between points\n\n\n\n\n\n\n\nAnalogies\n\n\nFor example, “man is to king as woman is to ?”."
  },
  {
    "objectID": "nlp_lec3.html#analogies-1",
    "href": "nlp_lec3.html#analogies-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Analogies",
    "text": "Analogies\nGraphical representation:"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf-1",
    "href": "nlp_lec3.html#tf-idf-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\nUsually used for term-document matrices.\nDenote a term by \\(t\\), a document by \\(d\\), and the corpus by \\(D\\). \\[\ncount(t,d) \\equiv n \\text{ of times that } t \\text{ appears in } d\n\\]\n\n\n\nTerm frequency\n\n\n\\[\n  TF(t,d) \\equiv \\begin{cases}\n   1 + log(count(t,d)), \\; \\text{if } count(t,d) &gt; 0,\\\\\n   0, \\; otherwise\n\\end{cases}\n\\]\n\n\n\n\n\n\nDocument frequency\n\n\n\\[\nDF(t,D) \\equiv n \\text{ of documents that contain } t\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#tf-idf-2",
    "href": "nlp_lec3.html#tf-idf-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\n\n\n\nInverse document frequency\n\n\nDefined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides: \\[\nIDF(t,D)=log\\dfrac{|D|+1}{DF(t,D)+1},\n\\] where \\(|D|\\) is the total number of documents in the corpus.\n\n\n\n\n\n\nTF-IDF measure\n\n\nSimply the product of TF and IDF: \\[\nTFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#ppmi",
    "href": "nlp_lec3.html#ppmi",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\nDenote \\(w\\) as target word, \\(c\\) as context word.\n\\[\nPMI(w,c) = log_2 \\dfrac{P(w,c)}{P(w)P(c)}\n\\]\n\n\\(P(w,c)\\) = how often we observe the words together\n\\(P(w)P(c)\\) = how often we expect the two words to co-occur assuming they each occurred independently.\n\n\n\n\n\n\n\nInterpretation\n\n\nPMI gives us an estimate of how much more the two words co-occur than we expect by chance."
  },
  {
    "objectID": "nlp_lec3.html#ppmi-1",
    "href": "nlp_lec3.html#ppmi-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\n\n\n\nPositive PMI\n\n\nNegative PMI are problematic (small probabilities require enormous corpora), so it’s more common to use Positive PMI: \\[\nPPMI(w,c) = max\\left(log_2 \\dfrac{P(w,c)}{P(w)P(c)}, 0\\right)\n\\]"
  },
  {
    "objectID": "nlp_lec3.html#ppmi-2",
    "href": "nlp_lec3.html#ppmi-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "PPMI",
    "text": "PPMI\n\n\n\nCooccurrence matrix\n\n\nLet’s assume we have a cooccurrence matrix F with W words as rows and C contexts as columns.\nWe define \\(f_{ij}\\) as number of times that word \\(w_i\\) occurs together with context \\(c_j\\).\n\n\n\n\n\n\nPPMI matrix\n\n\n\\[\\begin{align*}\n&p_{ij} = \\dfrac{f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\np_{i*} = \\dfrac{\\sum\\limits_{j=1}^C f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\np_{*j} = \\dfrac{\\sum\\limits_{i=1}^W f_{ij}}{\\sum\\limits_{i=1}^W \\sum\\limits_{j=1}^C f_{ij}},\\\\\n&PPMI_{ij} = max\\left(log_2 \\dfrac{p_{ij}}{p_{i*}p_{*j}}, 0\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "nlp_lec3.html#embeddings-1",
    "href": "nlp_lec3.html#embeddings-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nQuestion\n\n\nHow do we design features for all words in a dictionary?\n\n\n\n\n\n\nAnswer\n\n\nFeed massive amounts of text to an algorithm that will create its own feature space.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nWord representations in this new synthetic space are called word embeddings."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-2",
    "href": "nlp_lec3.html#embeddings-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\n\nAs you can see, component 126 appears to correlate with gender: it has slightly positive values (tan/orange) for the male words and slightly negative values (blue/gray) for the female words."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-3",
    "href": "nlp_lec3.html#embeddings-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nSupported analogies\n\n\n\npluralization\npast tense\ncomparisons\ncountry-&gt;capital mappings\n\n\n\n\n\n\n\nUses\n\n\nInput to NNs (transformers) that try to understand the meanings of entire sentences, or even paragraphs.\nExamples: BERT, GPTx."
  },
  {
    "objectID": "nlp_lec3.html#embeddings-4",
    "href": "nlp_lec3.html#embeddings-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\nExample\n\n\nWord embedding for “king”:\n  [ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]"
  },
  {
    "objectID": "nlp_lec3.html#embeddings-5",
    "href": "nlp_lec3.html#embeddings-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "nlp_lec3.html#analogies-2",
    "href": "nlp_lec3.html#analogies-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Analogies",
    "text": "Analogies"
  },
  {
    "objectID": "nlp_lec3.html#euclidean-distance",
    "href": "nlp_lec3.html#euclidean-distance",
    "title": "Natural Language Processing: word embeddings",
    "section": "Euclidean distance",
    "text": "Euclidean distance"
  },
  {
    "objectID": "nlp_lec3.html#dot-product",
    "href": "nlp_lec3.html#dot-product",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product"
  },
  {
    "objectID": "nlp_lec3.html#dot-product-1",
    "href": "nlp_lec3.html#dot-product-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\nProblem\n\n\nAll vectors originate at the origin.\n\n\n\n\n\n\nSolution\n\n\nMake them originate from the average center of all the points (zero-mean)."
  },
  {
    "objectID": "nlp_lec3.html#dot-product-2",
    "href": "nlp_lec3.html#dot-product-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\nProblem\n\n\nMake dot product exactly equal to the cosine.\n\n\n\n\n\n\nSolution\n\n\nNormalize the vectors (\\(u=[x,y] \\rightarrow \\left[\\dfrac{x}{\\|x\\|}, \\dfrac{y}{\\|y\\|}\\right]\\))."
  },
  {
    "objectID": "nlp_lec3.html#dot-product-3",
    "href": "nlp_lec3.html#dot-product-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product"
  },
  {
    "objectID": "nlp_lec3.html#dot-product-4",
    "href": "nlp_lec3.html#dot-product-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Dot product",
    "text": "Dot product\n\n\n\n\n\n\nWhy dot product?\n\n\n\nless computations\ncan be used in a neural network"
  },
  {
    "objectID": "nlp_lec3.html#construction",
    "href": "nlp_lec3.html#construction",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\nHow do we construct a language model? We can use N-grams.\n\n\n\nDefinition: recall\n\n\nAn n-gram is a sequence of n words: a 2-gram (which we’ll call bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”.\n\n\n\n\n\n\nAnother Definition\n\n\nA probabilistic model that can estimate the probability of a word given the n-1 previous words, and thereby also to assign probabilities to entire sequences."
  },
  {
    "objectID": "nlp_lec3.html#construction-1",
    "href": "nlp_lec3.html#construction-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n# Import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create sample documents\ndocuments = [\"This is the first document.\",\n              \"This document is the second document.\",\n              \"And this is the third one.\"]\n\n# Create the Bag-of-Words model with unigrams, bigrams, and trigrams\nvectorizer = CountVectorizer(ngram_range=(1, 3))\nX = vectorizer.fit_transform(documents)\n\n# Print the feature names and the document-term matrix\nprint(\"Feature Names:\", vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "nlp_lec3.html#construction-2",
    "href": "nlp_lec3.html#construction-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-3",
    "href": "nlp_lec3.html#construction-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-4",
    "href": "nlp_lec3.html#construction-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-5",
    "href": "nlp_lec3.html#construction-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-6",
    "href": "nlp_lec3.html#construction-6",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-7",
    "href": "nlp_lec3.html#construction-7",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n\n\n\nSkipgram model\n\n\nGuess neighboring words using the current word."
  },
  {
    "objectID": "nlp_lec3.html#construction-8",
    "href": "nlp_lec3.html#construction-8",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction\n\n\n\n\n\n\nThe intuition of skip-gram\n\n\n\nTreat the target word and a neighboring context word as positive examples.\nRandomly sample other words in the lexicon to get negative samples.\nUse logistic regression to train a classifier to distinguish those two cases.\nUse the learned weights as the embeddings."
  },
  {
    "objectID": "nlp_lec3.html#construction-9",
    "href": "nlp_lec3.html#construction-9",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#construction-10",
    "href": "nlp_lec3.html#construction-10",
    "title": "Natural Language Processing: word embeddings",
    "section": "Construction",
    "text": "Construction"
  },
  {
    "objectID": "nlp_lec3.html#training",
    "href": "nlp_lec3.html#training",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-1",
    "href": "nlp_lec3.html#training-1",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nHow to improve performance of the step 3?"
  },
  {
    "objectID": "nlp_lec3.html#training-2",
    "href": "nlp_lec3.html#training-2",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-3",
    "href": "nlp_lec3.html#training-3",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nLogistic regression: need to add negative samples."
  },
  {
    "objectID": "nlp_lec3.html#training-4",
    "href": "nlp_lec3.html#training-4",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "nlp_lec3.html#training-5",
    "href": "nlp_lec3.html#training-5",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nAt the start of training, initialize Embedding and Context with random values:"
  },
  {
    "objectID": "nlp_lec3.html#training-6",
    "href": "nlp_lec3.html#training-6",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nPerform lookup:"
  },
  {
    "objectID": "nlp_lec3.html#training-7",
    "href": "nlp_lec3.html#training-7",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nCompute sigmoid:"
  },
  {
    "objectID": "nlp_lec3.html#training-8",
    "href": "nlp_lec3.html#training-8",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nCalculate error:"
  },
  {
    "objectID": "nlp_lec3.html#training-9",
    "href": "nlp_lec3.html#training-9",
    "title": "Natural Language Processing: word embeddings",
    "section": "Training",
    "text": "Training\nUpdate parameters:"
  },
  {
    "objectID": "nlp_lec3.html#semantic-change",
    "href": "nlp_lec3.html#semantic-change",
    "title": "Natural Language Processing: word embeddings",
    "section": "Semantic change",
    "text": "Semantic change"
  },
  {
    "objectID": "nlp_lab2.html",
    "href": "nlp_lab2.html",
    "title": "NLP: Lab 2",
    "section": "",
    "text": "Stemming\nThere are various stemmers already available in NLTK. Below is an example usage:\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\n \n# Create Snowball stemmer\nsnow_stemmer = SnowballStemmer(language='english')\n\n# Create a Porter Stemmer instance\nporter_stemmer = PorterStemmer()\n\n# Create a Lancaster Stemmer instance\nlancaster_stemmer = LancasterStemmer()\n\n# Example words for stemming\nwords = [\"running\", \"jumps\", \"happily\", \"programming\", 'cared','fairly','sportingly']\n\n# Apply stemming to each word\nstemmed_words = [porter_stemmer.stem(word) for word in words]\nprint(\"===Porter===:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Snowball===:\")\nstemmed_words = [snow_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Lancaster===:\")\nstemmed_words = [lancaster_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\n===Porter===:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fairli', 'sportingli']\n\n===Snowball===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fair', 'sport']\n\n===Lancaster===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happy', 'program', 'car', 'fair', 'sport']\n\n\n\n\nExercises\nTask 0. Compare frequency distributions of stemmed and unstemmed NLTK corpora. Display most commonly used stems from nltk.text corpora on a plot.\nTask 1. Write your own version of stemmer for Ukrainian (or other non-English language) using regular expressions.\nThere is a regexp stemmer in NLTK (link).\nPlease write your code so that it satisfies NLTK’s standard interface (a Stemmer class with .stem() method)\nTask 2. Implement Wagner-Fischer (or Vintsyuk) algorithm for string distance. Link.\n\nModify the algorithm so that substitution operation cost depends on the key proximity on QWERTY keyboard. For inspiration, look at this StackExchange question.\nOr consider this table directly:\nImplement another modification to the algorithm: include transposition operation, so that you compute a Damerau-Levenshtein distance.\n\n\n\nRecommended reading\n\nChapter 1 from NLTK book.\nChapter 2 from Jurafsky’s book. Plus related slides.\nOfficial Python Regex package documentation.\nRegex cheatsheet\nAnother version with examples"
  },
  {
    "objectID": "nlp_lab6.html",
    "href": "nlp_lab6.html",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term"
  },
  {
    "objectID": "nlp_lab6.html#term-document-matrix",
    "href": "nlp_lab6.html#term-document-matrix",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "term-document matrix",
    "text": "term-document matrix\nWord is row, document is column. Each document is a count vector (elements are word counts).\nEach word is also a vector of counts of occurrences in each document."
  },
  {
    "objectID": "nlp_lab6.html#term-term-matrix",
    "href": "nlp_lab6.html#term-term-matrix",
    "title": "NLP: Lab 6 (bag-of-words/PPMI)",
    "section": "term-term matrix",
    "text": "term-term matrix\nRows and columns are rows. Counts are numbers of co-occurrences in a context. Context can either be a whole document, or a window around the word."
  },
  {
    "objectID": "nlp_lab5.html",
    "href": "nlp_lab5.html",
    "title": "NLP: Lab 5 (bag-of-words)",
    "section": "",
    "text": "Let’s agree on terminology:\n\ncorpus (plural: corpora) is a collection of documents\ndocument is a sequence of sentences. Say, a paragraph, or a chapter\nsentence is a sequence of words\nword is synonymous to term"
  },
  {
    "objectID": "nlp_lab5.html#perplexity",
    "href": "nlp_lab5.html#perplexity",
    "title": "NLP: Lab 5 (bag-of-words)",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity of a test set \\(W\\) with an N-gram model is given by (here V=|W|): \\[\nperplexity(W) = \\sqrt[V]{\\frac{1}{P(w_1 w_2 ... w_{V})}} = \\sqrt[V]{\\prod_{i=1}^{V} \\frac{1}{P(w_i | w_{i-N+1:i-1})}}\n\\]"
  },
  {
    "objectID": "nlp_lab5.html#laplace-smoothing",
    "href": "nlp_lab5.html#laplace-smoothing",
    "title": "NLP: Lab 5 (bag-of-words)",
    "section": "Laplace smoothing",
    "text": "Laplace smoothing\nWe have to deal with zero-probability N-grams somehow. One way is to add \\(1\\) to all counts.\nFormula for bigrams: \\[\nP_{Laplace}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}.\n\\]"
  },
  {
    "objectID": "nlp_lab5.html#conditional-interpolation",
    "href": "nlp_lab5.html#conditional-interpolation",
    "title": "NLP: Lab 5 (bag-of-words)",
    "section": "Conditional interpolation",
    "text": "Conditional interpolation\nTrigram example: \\[\\begin{align*}\n&P(w_n | w_{n-2} w_{n-1}) = \\lambda_1(w_{n-2:n-1})P(w_n) +\\\\\n&+ \\lambda_2(w_{n-2:n-1}) P(w_n | w_{n-1}) + \\\\\n&+ \\lambda_3(w_{n-2:n-1})  P(w_n | w_{n-2} w_{n-1})\n\\end{align*}\\] All \\(\\lambda_i\\) should add up to \\(1\\)."
  },
  {
    "objectID": "nlp_lab5.html#stupid-backoff",
    "href": "nlp_lab5.html#stupid-backoff",
    "title": "NLP: Lab 5 (bag-of-words)",
    "section": "Stupid backoff",
    "text": "Stupid backoff\nFormula: \\[\nS(w_i | w_{i-N+1:i-1}) = \\begin{cases}\n\\dfrac{C(w_{i-N+1:i})}{C(w_{i-N+1:i-1})}, & \\text{ if } C(w_{i-N+1:i}) &gt; 0 \\\\\n\\lambda S(w_i | w_{i-N+2:i-1}) & \\text{ otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nValue of \\(\\lambda=0.4\\) seems to be a good default"
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\nInstructions: - Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. - Use np.dot(X,Y) to calculate dot products.\nYou will learn to: - Build the general architecture of a learning algorithm, including: - Initializing parameters - Calculating the cost function and its gradient - Using an optimization algorithm (gradient descent) - Gather all three functions above into a main model function, in the right order."
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Overview of the Problem set\n\nExercise 1\nExercise 2\n\n3 - General Architecture of the learning algorithm\n4 - Building the parts of our algorithm\n\n4.1 - Helper functions\n\nExercise 3 - sigmoid\n\n4.2 - Initializing parameters\n\nExercise 4 - initialize_with_zeros\n\n4.3 - Forward and Backward propagation\n\nExercise 5 - propagate\n\n4.4 - Optimization\n\nExercise 6 - optimize\nExercise 7 - predict\n\n\n5 - Merge all functions into a model\n\nExercise 8 - model\n\n6 - Further analysis (optional/ungraded exercise)\n7 - Test with your own image (optional/ungraded exercise)\n\n ## 1 - Packages ##\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n### v1.2\n\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\nfrom public_tests import *\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Overview of the Problem set ##\nProblem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 29\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\n\n\n\n\nMany software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n ### Exercise 1 Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n#(≈ 3 lines of code)\n# m_train = \n# m_test = \n# num_px = \n# YOUR CODE STARTS HERE\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n# YOUR CODE ENDS HERE\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nExpected Output for m_train, m_test and num_px:\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n ### Exercise 2 Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\n#(≈ 2 lines of code)\n# train_set_x_flatten = ...\n# test_set_x_flatten = ...\n# YOUR CODE STARTS HERE\ntrain_set_x_flatten = train_set_x_orig.reshape((train_set_x_orig.shape[0], -1)).T\ntest_set_x_flatten = test_set_x_orig.reshape((test_set_x_orig.shape[0], -1)).T\n# YOUR CODE ENDS HERE\n\n# Check that the first 10 pixels of the second image are in the correct place\nassert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\nassert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\n\n\nExpected Output:\n\n\n\ntrain_set_x_flatten shape\n\n\n(12288, 209)\n\n\n\n\ntrain_set_y shape\n\n\n(1, 209)\n\n\n\n\ntest_set_x_flatten shape\n\n\n(12288, 50)\n\n\n\n\ntest_set_y shape\n\n\n(1, 50)\n\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\n\nWhat you need to remember:\nCommon steps for pre-processing a new dataset are: - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) - “Standardize” the data\n ## 3 - General Architecture of the learning algorithm ##\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps: - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost\n- Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude\n ## 4 - Building the parts of our algorithm ##\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n ### 4.1 - Helper functions\n ### Exercise 3 - sigmoid Using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute \\(sigmoid(z) = \\frac{1}{1 + e^{-z}}\\) for \\(z = w^T x + b\\) to make predictions. Use np.exp().\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    #(≈ 1 line of code)\n    # s = ...\n    # YOUR CODE STARTS HERE\n    s = 1/(1+np.exp(-z))\n    \n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid_test(sigmoid)\n\nsigmoid([0, 2]) = [0.5        0.88079708]\nAll tests passed!\n\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n[0.62245933 0.5        0.88079708]\n\n\n ### 4.2 - Initializing parameters\n ### Exercise 4 - initialize_with_zeros Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    \n    # (≈ 2 lines of code)\n    # w = ...\n    # b = ...\n    # YOUR CODE STARTS HERE\n    w = np.zeros((dim, 1), dtype=float)\n    b = 0.0\n    # YOUR CODE ENDS HERE\n\n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\ninitialize_with_zeros_test_1(initialize_with_zeros)\ninitialize_with_zeros_test_2(initialize_with_zeros)\n\nw = [[0.]\n [0.]]\nb = 0.0\nFirst test passed!\nSecond test passed!\n\n\n ### 4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n ### Exercise 5 - propagate Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    grads -- dictionary containing the gradients of the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    cost -- negative log-likelihood cost for logistic regression\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    #(≈ 2 lines of code)\n    # compute activation\n    # A = ...\n    # compute cost by using np.dot to perform multiplication. \n    # And don't use loops for the sum.\n    # cost = ...                                \n    # YOUR CODE STARTS HERE\n    A=sigmoid(np.dot(w.T, X) + b)\n    cost =-1/m* np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n    \n    # YOUR CODE ENDS HERE\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    #(≈ 2 lines of code)\n    # dw = ...\n    # db = ...\n    # YOUR CODE STARTS HERE\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    # YOUR CODE ENDS HERE\n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\n\n# X is using 3 examples, with 2 features each\n# Each example is stacked column-wise\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\npropagate_test(propagate)\n\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\nAll tests passed!\n\n\nExpected output\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\n ### 4.4 - Optimization - You have initialized your parameters. - You are also able to compute a cost function and its gradient. - Now, you want to update the parameters using gradient descent.\n ### Exercise 6 - optimize Write down the optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # (≈ 1 lines of code)\n        # Cost and gradient calculation \n        # grads, cost = ...\n        # YOUR CODE STARTS HERE\n        grads, cost = propagate(w,b,X,Y)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        # w = ...\n        # b = ...\n        # YOUR CODE STARTS HERE\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        \n        # YOUR CODE ENDS HERE\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\noptimize_test(optimize)\n\nw = [[0.80956046]\n [2.0508202 ]]\nb = 1.5948713189708588\ndw = [[ 0.17860505]\n [-0.04840656]]\ndb = -0.08888460336847771\nCosts = [array(0.15900538)]\nAll tests passed!\n\n\n ### Exercise 7 - predict The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    #(≈ 1 line of code)\n    # A = ...\n    # YOUR CODE STARTS HERE\n    A = sigmoid(np.dot(w.T,X)+b)\n    \n    # YOUR CODE ENDS HERE\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        #(≈ 4 lines of code)\n        # if A[0, i] &gt; ____ :\n        #     Y_prediction[0,i] = \n        # else:\n        #     Y_prediction[0,i] = \n        # YOUR CODE STARTS HERE\n        if A[0,i] &gt; 0.5:\n            Y_prediction[0,i] = 1\n        else:\n            Y_prediction[0,i] = 0  \n        \n        # YOUR CODE ENDS HERE\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredict_test(predict)\n\npredictions = [[1. 1. 0.]]\nAll tests passed!\n\n\n\nWhat to remember:\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples\n ## 5 - Merge all functions into a model ##\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n ### Exercise 8 - model Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize()\n\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    # (≈ 1 line of code)   \n    # initialize parameters with zeros\n    # and use the \"shape\" function to get the first dimension of X_train\n    # w, b = ...\n    \n    #(≈ 1 line of code)\n    # Gradient descent \n    # params, grads, costs = ...\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    # w = ...\n    # b = ...\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    # Y_prediction_test = ...\n    # Y_prediction_train = ...\n    \n    # YOUR CODE STARTS HERE\n    w, b = initialize_with_zeros(X_train.shape[0])\n    params, grads, costs = optimize(w, b, X_train, Y_train,num_iterations, learning_rate)\n    w = params[\"w\"]\n    b = params[\"b\"]\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    # YOUR CODE ENDS HERE\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nfrom public_tests import *\n\nmodel_test(model)\n\nAll tests passed!\n\n\nIf you pass all the tests, run the following cell to train your model.\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, you predicted that it is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n ## 6 - Further analysis (optional/ungraded exercise) ##\nCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nTraining a model with learning rate: 0.01\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.001\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.0001\n\n-------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nInterpretation: - Different learning rates give different costs and thus different predictions results. - If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). - A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. - In deep learning, we usually recommend that you: - Choose the learning rate that better minimizes the cost function. - If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)\n ## 7 - Test with your own image (optional/ungraded exercise) ##\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: 1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook’s directory, in the “images” folder 3. Change your image’s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n# change this to the name of your image file\nmy_image = \"Cats-image-cats-36712791-1222-917.jpg\"   \n\n# We preprocess the image to fit your algorithm.\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\nmy_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0.0, your algorithm predicts a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nWhat to remember from this assignment: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)\nBibliography: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "nb/Untitled.html",
    "href": "nb/Untitled.html",
    "title": "Deep Learning/NLP course",
    "section": "",
    "text": "import nltk\n\nnltk.corpus.genesis\n\n&lt;PlaintextCorpusReader in '/Users/vitvly/nltk_data/corpora/genesis'&gt;\n\n\n\nl = nltk.corpus.genesis.sents()\n\n\nlen(l)\n\n13640"
  },
  {
    "objectID": "nb/Optimization_methods.html",
    "href": "nb/Optimization_methods.html",
    "title": "Optimization Methods",
    "section": "",
    "text": "Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you’ll gain skills with some more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.\nBy the end of this notebook, you’ll be able to:\nNotations: As usual, $ = $ da for any variable a.\nLet’s get started!"
  },
  {
    "objectID": "nb/Optimization_methods.html#table-of-contents",
    "href": "nb/Optimization_methods.html#table-of-contents",
    "title": "Optimization Methods",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1- Packages\n2 - Gradient Descent\n\nExercise 1 - update_parameters_with_gd\n\n3 - Mini-Batch Gradient Descent\n\nExercise 2 - random_mini_batches\n\n4 - Momentum\n\nExercise 3 - initialize_velocity\nExercise 4 - update_parameters_with_momentum\n\n5 - Adam\n\nExercise 5 - initialize_adam\nExercise 6 - update_parameters_with_adam\n\n6 - Model with different Optimization algorithms\n\n6.1 - Mini-Batch Gradient Descent\n6.2 - Mini-Batch Gradient Descent with Momentum\n6.3 - Mini-Batch with Adam\n6.4 - Summary\n\n7 - Learning Rate Decay and Scheduling\n\n7.1 - Decay on every iteration\n\nExercise 7 - update_lr\n\n7.2 - Fixed Interval Scheduling\n\nExercise 8 - schedule_lr_decay\n\n7.3 - Using Learning Rate Decay for each Optimization Method\n\n7.3.1 - Gradient Descent with Learning Rate Decay\n7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\n7.3.3 - Adam with Learning Rate Decay\n\n7.4 - Achieving similar performance with different methods\n\n\n ## 1- Packages\n\n### v1.1\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport math\nimport sklearn\nimport sklearn.datasets\n\nfrom opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\nfrom opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\nfrom copy import deepcopy\nfrom testCases import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Gradient Descent\nA simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all \\(m\\) examples on each step, it is also called Batch Gradient Descent.\n ### Exercise 1 - update_parameters_with_gd\nImplement the gradient descent update rule. The gradient descent rule is, for \\(l = 1, ..., L\\): \\[ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}\\] \\[ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}\\]\nwhere L is the number of layers and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\n# GRADED FUNCTION: update_parameters_with_gd\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using one step of gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters to be updated:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients to update each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] =  \n        # parameters[\"b\" + str(l)] = \n        # YOUR CODE STARTS HERE\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*grads[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*grads[\"db\" + str(l)]   \n        \n        # YOUR CODE ENDS HERE\n    return parameters\n\n\nparameters, grads, learning_rate = update_parameters_with_gd_test_case()\nlearning_rate = 0.01\nparameters = update_parameters_with_gd(parameters, grads, learning_rate)\n\nprint(\"W1 =\\n\" + str(parameters[\"W1\"]))\nprint(\"b1 =\\n\" + str(parameters[\"b1\"]))\nprint(\"W2 =\\n\" + str(parameters[\"W2\"]))\nprint(\"b2 =\\n\" + str(parameters[\"b2\"]))\n\nupdate_parameters_with_gd_test(update_parameters_with_gd)\n\nW1 =\n[[ 1.63312395 -0.61217855 -0.5339999 ]\n [-1.06196243  0.85396039 -2.3105546 ]]\nb1 =\n[[ 1.73978682]\n [-0.77021546]]\nW2 =\n[[ 0.32587637 -0.24814147]\n [ 1.47146563 -2.05746183]\n [-0.32772076 -0.37713775]]\nb2 =\n[[ 1.13773698]\n [-1.09301954]\n [-0.16397615]]\nAll tests passed\n\n\nA variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent, where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.\n\n(Batch) Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost\n    cost_total = compute_cost(a, Y)  # Cost for m training examples\n    # Backward propagation\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters\n    parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\n        \n\nStochastic Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    cost_total = 0\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters\n        parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\nIn Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here’s what that looks like:\n\n\n\n  Figure 1  : SGD vs GD “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence, but each step is a lot faster to compute for SGD than it is for GD, as it uses only one training example (vs. the whole batch for GD).\n\n\nNote also that implementing SGD requires 3 for-loops in total: 1. Over the number of iterations 2. Over the \\(m\\) training examples 3. Over the layers (to update all parameters, from \\((W^{[1]},b^{[1]})\\) to \\((W^{[L]},b^{[L]})\\))\nIn practice, you’ll often get faster results if you don’t use the entire training set, or just one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.\n\n\n\n  Figure 2 :  SGD vs Mini-Batch GD “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization.\n\n\n ## 3 - Mini-Batch Gradient Descent\nNow you’ll build some mini-batches from the training set (X, Y).\nThere are two steps: - Shuffle: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the \\(i^{th}\\) column of X is the example corresponding to the \\(i^{th}\\) label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.\n\n\nPartition: Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this:\n\n\n ### Exercise 2 - random_mini_batches\nImplement random_mini_batches. The shuffling part has already been coded for you! To help with the partitioning step, you’ve been provided the following code that selects the indexes for the \\(1^{st}\\) and \\(2^{nd}\\) mini-batches:\nfirst_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\nsecond_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n...\nNote that the last mini-batch might end up smaller than mini_batch_size=64. Let \\(\\lfloor s \\rfloor\\) represents \\(s\\) rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be \\(\\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\) mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be \\(\\left(m-mini_\\_batch_\\_size \\times \\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\right)\\).\nHint:\n\\[mini\\_batch\\_X = shuffled\\_X[:, i : j]\\]\nThink of a way in which you can use the for loop variable k help you increment i and j in multiples of mini_batch_size.\nAs an example, if you want to increment in multiples of 3, you could the following:\nn = 3\nfor k in (0 , 5):\n    print(k * n)\n\n# GRADED FUNCTION: random_mini_batches\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1, m))\n    \n    inc = mini_batch_size\n\n    # Step 2 - Partition (shuffled_X, shuffled_Y).\n    # Cases with a complete mini batch size only i.e each of 64 examples.\n    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        # (approx. 2 lines)\n        # mini_batch_X =  \n        # mini_batch_Y =\n        # YOUR CODE STARTS HERE\n        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        \n        # YOUR CODE ENDS HERE\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # For handling the end case (last mini-batch &lt; mini_batch_size i.e less than 64)\n    if m % mini_batch_size != 0:\n        #(approx. 2 lines)\n        # mini_batch_X =\n        # mini_batch_Y =\n        # YOUR CODE STARTS HERE\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]  \n        \n        # YOUR CODE ENDS HERE\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nnp.random.seed(1)\nmini_batch_size = 64\nnx = 12288\nm = 148\nX = np.array([x for x in range(nx * m)]).reshape((m, nx)).T\nY = np.random.randn(1, m) &lt; 0.5\n\nmini_batches = random_mini_batches(X, Y, mini_batch_size)\nn_batches = len(mini_batches)\n\nassert n_batches == math.ceil(m / mini_batch_size), f\"Wrong number of mini batches. {n_batches} != {math.ceil(m / mini_batch_size)}\"\nfor k in range(n_batches - 1):\n    assert mini_batches[k][0].shape == (nx, mini_batch_size), f\"Wrong shape in {k} mini batch for X\"\n    assert mini_batches[k][1].shape == (1, mini_batch_size), f\"Wrong shape in {k} mini batch for Y\"\n    assert np.sum(np.sum(mini_batches[k][0] - mini_batches[k][0][0], axis=0)) == ((nx * (nx - 1) / 2 ) * mini_batch_size), \"Wrong values. It happens if the order of X rows(features) changes\"\nif ( m % mini_batch_size &gt; 0):\n    assert mini_batches[n_batches - 1][0].shape == (nx, m % mini_batch_size), f\"Wrong shape in the last minibatch. {mini_batches[n_batches - 1][0].shape} != {(nx, m % mini_batch_size)}\"\n\nassert np.allclose(mini_batches[0][0][0][0:3], [294912,  86016, 454656]), \"Wrong values. Check the indexes used to form the mini batches\"\nassert np.allclose(mini_batches[-1][0][-1][0:3], [1425407, 1769471, 897023]), \"Wrong values. Check the indexes used to form the mini batches\"\n\nprint(\"\\033[92mAll tests passed!\")\n\nAll tests passed!\n\n\n\nt_X, t_Y, mini_batch_size = random_mini_batches_test_case()\nmini_batches = random_mini_batches(t_X, t_Y, mini_batch_size)\n\nprint (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nrandom_mini_batches_test(random_mini_batches)\n\nshape of the 1st mini_batch_X: (12288, 64)\nshape of the 2nd mini_batch_X: (12288, 64)\nshape of the 3rd mini_batch_X: (12288, 20)\nshape of the 1st mini_batch_Y: (1, 64)\nshape of the 2nd mini_batch_Y: (1, 64)\nshape of the 3rd mini_batch_Y: (1, 20)\nmini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n All tests passed.\n\n\n\nWhat you should remember: - Shuffling and Partitioning are the two steps required to build mini-batches - Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n ## 4 - Momentum\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.\nMomentum takes into account the past gradients to smooth out the update. The ‘direction’ of the previous gradients is stored in the variable \\(v\\). Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of \\(v\\) as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.\n\n\n\nFigure 3 : The red arrows show the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, the gradient is allowed to influence \\(v\\) and then take a step in the direction of \\(v\\). \n\n\n### Exercise 3 - initialize_velocity Initialize the velocity. The velocity, \\(v\\), is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is: for \\(l =1,...,L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\nNote that the iterator l starts at 1 in the for loop as the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript).\n\n# GRADED FUNCTION: initialize_velocity\n\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] =\n        # v[\"db\" + str(l)] =\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)     \n        \n        # YOUR CODE ENDS HERE\n        \n    return v\n\n\nparameters = initialize_velocity_test_case()\n\nv = initialize_velocity(parameters)\nprint(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))\n\ninitialize_velocity_test(initialize_velocity)\n\nv[\"dW1\"] =\n[[0. 0.]\n [0. 0.]\n [0. 0.]]\nv[\"db1\"] =\n[[0.]\n [0.]\n [0.]]\nv[\"dW2\"] =\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] =\n[[0.]\n [0.]\n [0.]]\n All tests passed.\n\n\n\n### Exercise 4 - update_parameters_with_momentum\nNow, implement the parameters update with momentum. The momentum update rule is, for \\(l = 1, ..., L\\):\n\\[ \\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}\\]\n\\[\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}\n\\end{cases}\\tag{4}\\]\nwhere L is the number of layers, \\(\\beta\\) is the momentum and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\) (that’s a “one” on the superscript).\n\n# GRADED FUNCTION: update_parameters_with_momentum\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(1, L + 1):\n        \n        # (approx. 4 lines)\n        # compute velocities\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # update parameters\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = beta*v[\"dW\" + str(l)] + (1-beta)*grads['dW' + str(l)]\n        v[\"db\" + str(l)] = beta*v[\"db\" + str(l)] + (1-beta)*grads['db' + str(l)]\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*v[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*v[\"db\" + str(l)]  \n        \n        # YOUR CODE ENDS HERE\n        \n    return parameters, v\n\n\nparameters, grads, v = update_parameters_with_momentum_test_case()\n\nparameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))\n\nupdate_parameters_with_momentum_test(update_parameters_with_momentum)\n\nW1 = \n[[ 1.62522322 -0.61179863 -0.52875457]\n [-1.071868    0.86426291 -2.30244029]]\nb1 = \n[[ 1.74430927]\n [-0.76210776]]\nW2 = \n[[ 0.31972282 -0.24924749]\n [ 1.46304371 -2.05987282]\n [-0.32294756 -0.38336269]]\nb2 = \n[[ 1.1341662 ]\n [-1.09920409]\n [-0.171583  ]]\nv[\"dW1\"] = \n[[-0.08778584  0.00422137  0.05828152]\n [-0.11006192  0.11447237  0.09015907]]\nv[\"db1\"] = \n[[0.05024943]\n [0.09008559]]\nv[\"dW2\"] = \n[[-0.06837279 -0.01228902]\n [-0.09357694 -0.02678881]\n [ 0.05303555 -0.06916608]]\nv[\"db2\"] = v[[-0.03967535]\n [-0.06871727]\n [-0.08452056]]\n All tests passed.\n\n\nNote that: - The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. - If \\(\\beta = 0\\), then this just becomes standard gradient descent without momentum.\nHow do you choose \\(\\beta\\)?\n\nThe larger the momentum \\(\\beta\\) is, the smoother the update, because it takes the past gradients into account more. But if \\(\\beta\\) is too big, it could also smooth out the updates too much.\nCommon values for \\(\\beta\\) range from 0.8 to 0.999. If you don’t feel inclined to tune this, \\(\\beta = 0.9\\) is often a reasonable default.\nTuning the optimal \\(\\beta\\) for your model might require trying several values to see what works best in terms of reducing the value of the cost function \\(J\\).\n\n\nWhat you should remember: - Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. - You have to tune a momentum hyperparameter \\(\\beta\\) and a learning rate \\(\\alpha\\).\n\n## 5 - Adam\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.\nHow does Adam work? 1. It calculates an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction). 2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction). 3. It updates parameters in a direction based on combining information from “1” and “2”.\nThe update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\] where: - t counts the number of steps taken of Adam - L is the number of layers - \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the two exponentially weighted averages. - \\(\\alpha\\) is the learning rate - \\(\\varepsilon\\) is a very small number to avoid dividing by zero\nAs usual, all parameters are stored in the parameters dictionary\n\n### Exercise 5 - initialize_adam\nInitialize the Adam variables \\(v, s\\) which keep track of the past information.\nInstruction: The variables \\(v, s\\) are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is: for \\(l = 1, ..., L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\ns[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\ns[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n\n# GRADED FUNCTION: initialize_adam\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(1, L + 1):\n    # (approx. 4 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n    # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n        s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n        s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)  \n    \n    # YOUR CODE ENDS HERE\n    \n    return v, s\n\n\nparameters = initialize_adam_test_case()\n\nv, s = initialize_adam(parameters)\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\nprint(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\nprint(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\nprint(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\nprint(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))\n\ninitialize_adam_test(initialize_adam)\n\nv[\"dW1\"] = \n[[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = \n[[0.]\n [0.]]\nv[\"dW2\"] = \n[[0. 0.]\n [0. 0.]\n [0. 0.]]\nv[\"db2\"] = \n[[0.]\n [0.]\n [0.]]\ns[\"dW1\"] = \n[[0. 0. 0.]\n [0. 0. 0.]]\ns[\"db1\"] = \n[[0.]\n [0.]]\ns[\"dW2\"] = \n[[0. 0.]\n [0. 0.]\n [0. 0.]]\ns[\"db2\"] = \n[[0.]\n [0.]\n [0.]]\n All tests passed.\n\n\n\n### Exercise 6 - update_parameters_with_adam\nNow, implement the parameters update with Adam. Recall the general update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\]\nNote that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\n# GRADED FUNCTION: update_parameters_with_adam\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    t -- Adam variable, counts the number of taken steps\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(1, L + 1):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1)*grads['dW' + str(l)]\n        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1)*grads['db' + str(l)]      \n        \n        # YOUR CODE ENDS HERE\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        # (approx. 2 lines)\n        # v_corrected[\"dW\" + str(l)] = ...\n        # v_corrected[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)]/(1-beta1**t)\n        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)]/(1-beta1**t)    \n        \n        # YOUR CODE ENDS HERE\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        #(approx. 2 lines)\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1-beta2)*np.square(grads['dW' + str(l)])\n        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1-beta2)*np.square(grads['db' + str(l)])       \n        \n        # YOUR CODE ENDS HERE\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        # (approx. 2 lines)\n        # s_corrected[\"dW\" + str(l)] = ...\n        # s_corrected[\"db\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)]/(1-beta2**t)\n        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)]/(1-beta2**t) \n        \n        # YOUR CODE ENDS HERE\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)]/(np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)]/(np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)        \n        \n        # YOUR CODE ENDS HERE\n\n    return parameters, v, s, v_corrected, s_corrected\n\n\nparametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon = update_parameters_with_adam_test_case()\n\nparameters, v, s, vc, sc  = update_parameters_with_adam(parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon)\nprint(f\"W1 = \\n{parameters['W1']}\")\nprint(f\"W2 = \\n{parameters['W2']}\")\nprint(f\"b1 = \\n{parameters['b1']}\")\nprint(f\"b2 = \\n{parameters['b2']}\")\n\nupdate_parameters_with_adam_test(update_parameters_with_adam)\n\nW1 = \n[[ 1.63937725 -0.62327448 -0.54308727]\n [-1.0578897   0.85032154 -2.31657668]]\nW2 = \n[[ 0.33400549 -0.23563857]\n [ 1.47715417 -2.04561842]\n [-0.33729882 -0.36908457]]\nb1 = \n[[ 1.72995096]\n [-0.7762447 ]]\nb2 = \n[[ 1.14852557]\n [-1.08492339]\n [-0.15740527]]\nAll tests passed\n\n\nExpected values:\nW1 = \n[[ 1.63937725 -0.62327448 -0.54308727]\n [-1.0578897   0.85032154 -2.31657668]]\nW2 = \n[[ 0.33400549 -0.23563857]\n [ 1.47715417 -2.04561842]\n [-0.33729882 -0.36908457]]\nb1 = \n[[ 1.72995096]\n [-0.7762447 ]]\nb2 = \n[[ 1.14852557]\n [-1.08492339]\n [-0.15740527]]\nYou now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.\n\n## 6 - Model with different Optimization algorithms\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\nA 3-layer neural network has already been implemented for you! You’ll train it with: - Mini-batch Gradient Descent: it will call your function: - update_parameters_with_gd() - Mini-batch Momentum: it will call your functions: - initialize_velocity() and update_parameters_with_momentum() - Mini-batch Adam: it will call your functions: - initialize_adam() and update_parameters_with_adam()\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\nNow, run this 3 layer neural network with each of the 3 optimization methods.\n\n### 6.1 - Mini-Batch Gradient Descent\nRun the following code to see how the model does with mini-batch gradient descent.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702405\nCost after epoch 1000: 0.668101\nCost after epoch 2000: 0.635288\nCost after epoch 3000: 0.600491\nCost after epoch 4000: 0.573367\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7166666666666667\n\n\n\n\n\n\n\n\n\n\n### 6.2 - Mini-Batch Gradient Descent with Momentum\nNext, run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small - but for more complex problems you might see bigger gains.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702413\nCost after epoch 1000: 0.668167\nCost after epoch 2000: 0.635388\nCost after epoch 3000: 0.600591\nCost after epoch 4000: 0.573444\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7166666666666667\n\n\n\n\n\n\n\n\n\n\n### 6.3 - Mini-Batch with Adam\nFinally, run the following code to see how the model does with Adam.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702166\nCost after epoch 1000: 0.167845\nCost after epoch 2000: 0.141316\nCost after epoch 3000: 0.138788\nCost after epoch 4000: 0.136066\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334\n\n\n\n\n\n\n\n\n\n\n### 6.4 - Summary\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smoother\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\ncost shape\n\n\n\n\nMomentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible.\nOn the other hand, Adam clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.\nSome advantages of Adam include:\n\nRelatively low memory requirements (though higher than gradient descent and gradient descent with momentum)\nUsually works well even with little tuning of hyperparameters (except \\(\\alpha\\))\n\nReferences:\n\nAdam paper: https://arxiv.org/pdf/1412.6980.pdf\n\n\n## 7 - Learning Rate Decay and Scheduling\nLastly, the learning rate is another hyperparameter that can help you speed up learning.\nDuring the first part of training, your model can get away with taking large steps, but over time, using a fixed value for the learning rate alpha can cause your model to get stuck in a wide oscillation that never quite converges. But if you were to slowly reduce your learning rate alpha over time, you could then take smaller, slower steps that bring you closer to the minimum. This is the idea behind learning rate decay.\nLearning rate decay can be achieved by using either adaptive methods or pre-defined learning rate schedules.\nNow, you’ll apply scheduled learning rate decay to a 3-layer neural network in three different optimizer modes and see how each one differs, as well as the effect of scheduling at different epochs.\nThis model is essentially the same as the one you used before, except in this one you’ll be able to include learning rate decay. It includes two new parameters, decay and decay_rate.\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    lr_rates = []\n    learning_rate0 = learning_rate   # the original learning rate\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        if decay:\n            learning_rate = decay(learning_rate0, i, decay_rate)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n            if decay:\n                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n### 7.1 - Decay on every iteration\nFor this portion of the assignment, you’ll try one of the pre-defined schedules for learning rate decay, called exponential learning rate decay. It takes this mathematical form:\n\\[\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}\\]\n\n### Exercise 7 - update_lr\nCalculate the new learning rate using exponential weight decay.\n\n# GRADED FUNCTION: update_lr\n\ndef update_lr(learning_rate0, epoch_num, decay_rate):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer\n    decay_rate -- Decay rate. Scalar\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    #(approx. 1 line)\n    # learning_rate = \n    # YOUR CODE STARTS HERE\n    learning_rate = learning_rate0/(1+decay_rate*epoch_num)\n    \n    # YOUR CODE ENDS HERE\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\nepoch_num = 2\ndecay_rate = 1\nlearning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n\nprint(\"Updated learning rate: \", learning_rate_2)\n\nupdate_lr_test(update_lr)\n\nOriginal learning rate:  0.5\nUpdated learning rate:  0.16666666666666666\nAll tests passed\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=update_lr)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.701091\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.661884\nlearning rate after epoch 1000: 0.000100\nCost after epoch 2000: 0.658620\nlearning rate after epoch 2000: 0.000050\nCost after epoch 3000: 0.656765\nlearning rate after epoch 3000: 0.000033\nCost after epoch 4000: 0.655486\nlearning rate after epoch 4000: 0.000025\n\n\n\n\n\n\n\n\n\nAccuracy: 0.6533333333333333\n\n\n\n\n\n\n\n\n\nNotice that if you set the decay to occur at every iteration, the learning rate goes to zero too quickly - even if you start with a higher learning rate.\n\n\n\nEpoch Number\n\n\nLearning Rate\n\n\nCost\n\n\n\n\n0\n\n\n0.100000\n\n\n0.701091\n\n\n\n\n1000\n\n\n0.000100\n\n\n0.661884\n\n\n\n\n2000\n\n\n0.000050\n\n\n0.658620\n\n\n\n\n3000\n\n\n0.000033\n\n\n0.656765\n\n\n\n\n4000\n\n\n0.000025\n\n\n0.655486\n\n\n\n\n5000\n\n\n0.000020\n\n\n0.654514\n\n\n\nWhen you’re training for a few epoch this doesn’t cause a lot of troubles, but when the number of epochs is large the optimization algorithm will stop updating. One common fix to this issue is to decay the learning rate every few steps. This is called fixed interval scheduling.\n ### 7.2 - Fixed Interval Scheduling\nYou can help prevent the learning rate speeding to zero too quickly by scheduling the exponential learning rate decay at a fixed time interval, for example 1000. You can either number the intervals, or divide the epoch by the time interval, which is the size of window with the constant learning rate.\n\n ### Exercise 8 - schedule_lr_decay\nCalculate the new learning rate using exponential weight decay with fixed interval scheduling.\nInstructions: Implement the learning rate scheduling such that it only changes when the epochNum is a multiple of the timeInterval.\nNote: The fraction in the denominator uses the floor operation.\n\\[\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}\\]\nHint: numpy.floor\n\n# GRADED FUNCTION: schedule_lr_decay\n\ndef schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer.\n    decay_rate -- Decay rate. Scalar.\n    time_interval -- Number of epochs where you update the learning rate.\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    # (approx. 1 lines)\n    # learning_rate = ...\n    # YOUR CODE STARTS HERE\n    learning_rate = learning_rate0/(1+decay_rate*math.floor(epoch_num/time_interval))\n    \n    # YOUR CODE ENDS HERE\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\n\nepoch_num_1 = 10\nepoch_num_2 = 100\ndecay_rate = 0.3\ntime_interval = 100\nlearning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\nlearning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)\n\nschedule_lr_decay_test(schedule_lr_decay)\n\nOriginal learning rate:  0.5\nUpdated learning rate after 10 epochs:  0.5\nUpdated learning rate after 100 epochs:  0.3846153846153846\nAll tests passed\n\n\nExpected output\nOriginal learning rate:  0.5\nUpdated learning rate after 10 epochs:  0.5\nUpdated learning rate after 100 epochs:  0.3846153846153846\n ### 7.3 - Using Learning Rate Decay for each Optimization Method\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n #### 7.3.1 - Gradient Descent with Learning Rate Decay\nRun the following code to see how the model does gradient descent and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.701091\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.127161\nlearning rate after epoch 1000: 0.050000\nCost after epoch 2000: 0.120304\nlearning rate after epoch 2000: 0.033333\nCost after epoch 3000: 0.117033\nlearning rate after epoch 3000: 0.025000\nCost after epoch 4000: 0.117512\nlearning rate after epoch 4000: 0.020000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334\n\n\n\n\n\n\n\n\n\n #### 7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\nRun the following code to see how the model does gradient descent with momentum and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent with momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.702226\nlearning rate after epoch 0: 0.100000\nCost after epoch 1000: 0.128974\nlearning rate after epoch 1000: 0.050000\nCost after epoch 2000: 0.125965\nlearning rate after epoch 2000: 0.033333\nCost after epoch 3000: 0.123375\nlearning rate after epoch 3000: 0.025000\nCost after epoch 4000: 0.123218\nlearning rate after epoch 4000: 0.020000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9533333333333334\n\n\n\n\n\n\n\n\n\n #### 7.3.3 - Adam with Learning Rate Decay\nRun the following code to see how the model does Adam and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\", learning_rate = 0.01, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nCost after epoch 0: 0.699346\nlearning rate after epoch 0: 0.010000\nCost after epoch 1000: 0.130074\nlearning rate after epoch 1000: 0.005000\nCost after epoch 2000: 0.129826\nlearning rate after epoch 2000: 0.003333\nCost after epoch 3000: 0.129282\nlearning rate after epoch 3000: 0.002500\nCost after epoch 4000: 0.128361\nlearning rate after epoch 4000: 0.002000\n\n\n\n\n\n\n\n\n\nAccuracy: 0.94\n\n\n\n\n\n\n\n\n\n ### 7.4 - Achieving similar performance with different methods\nWith Mini-batch GD or Mini-batch GD with Momentum, the accuracy is significantly lower than Adam, but when learning rate decay is added on top, either can achieve performance at a speed and accuracy score that’s similar to Adam.\nIn the case of Adam, notice that the learning curve achieves a similar accuracy but faster.\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94.6%\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;95.6%\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    94%\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\n\n\nCongratulations! You’ve made it to the end of the Optimization methods notebook. Here’s a quick recap of everything you’re now able to do:\n\nApply three different optimization methods to your models\nBuild mini-batches for your training set\nUse learning rate decay scheduling to speed up your training\n\nGreat work!"
  },
  {
    "objectID": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "href": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "",
    "text": "Notation: - Superscript \\([l]\\) denotes a quantity associated with the \\(l^{th}\\) layer. - Example: \\(a^{[L]}\\) is the \\(L^{th}\\) layer activation. \\(W^{[L]}\\) and \\(b^{[L]}\\) are the \\(L^{th}\\) layer parameters. - Superscript \\((i)\\) denotes a quantity associated with the \\(i^{th}\\) example. - Example: \\(x^{(i)}\\) is the \\(i^{th}\\) training example. - Lowerscript \\(i\\) denotes the \\(i^{th}\\) entry of a vector. - Example: \\(a^{[l]}_i\\) denotes the \\(i^{th}\\) entry of the \\(l^{th}\\) layer’s activations).\n## 1 - Packages\nFirst, import all the packages you’ll need during this assignment.\n### v1.1\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom testCases import *\nfrom dnn_app_utils_v3 import sigmoid, sigmoid_backward, relu, relu_backward, load_data\nfrom public_tests import *\n\nimport time\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\nimport copy\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\n## 2 - Outline\nTo build your neural network, you’ll be implementing several “helper functions.” Here’s an outline of the steps:\nNote:\nFor every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.\nIn the backpropagation module, you can then use the cache to calculate the gradients.\n## 3 - Initialization\nYou will write two helper functions to initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one generalizes this initialization process to \\(L\\) layers.\n### 3.1 - 2-layer Neural Network\n### Exercise 1 - initialize_parameters\nCreate and initialize the parameters of the 2-layer neural network.\nInstructions:\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters(3,2,1)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_1(initialize_parameters)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters(4,3,2)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_2(initialize_parameters)\nExpected output\n### 3.2 - L-layer Neural Network\nThe initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the initialize_parameters_deep function, you should make sure that your dimensions match between each layer. Recall that \\(n^{[l]}\\) is the number of units in layer \\(l\\). For example, if the size of your input \\(X\\) is \\((12288, 209)\\) (with \\(m=209\\) examples) then:\nRemember that when you compute \\(W X + b\\) in python, it carries out broadcasting. For example, if:\n\\[ W = \\begin{bmatrix}\n    w_{00}  & w_{01} & w_{02} \\\\\n    w_{10}  & w_{11} & w_{12} \\\\\n    w_{20}  & w_{21} & w_{22}\n\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n    x_{00}  & x_{01} & x_{02} \\\\\n    x_{10}  & x_{11} & x_{12} \\\\\n    x_{20}  & x_{21} & x_{22}\n\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n    b_0  \\\\\n    b_1  \\\\\n    b_2\n\\end{bmatrix}\\tag{2}\\]\nThen \\(WX + b\\) will be:\n\\[ WX + b = \\begin{bmatrix}\n    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n\\end{bmatrix}\\tag{3}  \\]\n### Exercise 2 - initialize_parameters_deep\nImplement initialization for an L-layer Neural Network.\nInstructions: - The model’s structure is [LINEAR -&gt; RELU] $ $ (L-1) -&gt; LINEAR -&gt; SIGMOID. I.e., it has \\(L-1\\) layers using a ReLU activation function followed by an output layer with a sigmoid activation function. - Use random initialization for the weight matrices. Use np.random.randn(d0, d1, ..., dn) * 0.01. - Use zeros initialization for the biases. Use np.zeros(shape). - You’ll store \\(n^{[l]}\\), the number of units in different layers, in a variable layer_dims. For example, the layer_dims for last week’s Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means W1’s shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to \\(L\\) layers! - Here is the implementation for \\(L=1\\) (one layer neural network). It should inspire you to implement the general case (L-layer neural network).\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = ...\n        # parameters['b' + str(l)] = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters_deep([5,4,3])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_deep_test_1(initialize_parameters_deep)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters_deep([4,3,2])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_deep_test_2(initialize_parameters_deep)\nExpected output\n## 4 - Forward Propagation Module\n### 4.1 - Linear Forward\nNow that you have initialized your parameters, you can do the forward propagation module. Start by implementing some basic functions that you can use again later when implementing the model. Now, you’ll complete three functions in this order:\nThe linear forward module (vectorized over all the examples) computes the following equations:\n\\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}\\]\nwhere \\(A^{[0]} = X\\).\n### Exercise 3 - linear_forward\nBuild the linear part of forward propagation.\nReminder: The mathematical representation of this unit is \\(Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\). You may also find np.dot() useful. If your dimensions don’t match, printing W.shape may help.\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    # Z = ...\n    # CODE_START\n    \n    # CODE_END\n    cache = (A, W, b)\n    \n    return Z, cache\nt_A, t_W, t_b = linear_forward_test_case()\nt_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\nprint(\"Z = \" + str(t_Z))\n\nlinear_forward_test(linear_forward)\nExpected output\n### 4.2 - Linear-Activation Forward\nIn this notebook, you will use two activation functions:\nFor added convenience, you’re going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you’ll implement a function that does the LINEAR forward step, followed by an ACTIVATION forward step.\n### Exercise 4 - linear_activation_forward\nImplement the forward propagation of the LINEAR-&gt;ACTIVATION layer. Mathematical relation is: \\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\\) where the activation “g” can be sigmoid() or relu(). Use linear_forward() and the correct activation function.\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # CODE_START\n        \n        # CODE_END\n    \n    elif activation == \"relu\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # CODE_START\n        \n        # CODE_END\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\nt_A_prev, t_W, t_b = linear_activation_forward_test_case()\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\nprint(\"With sigmoid: A = \" + str(t_A))\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\nprint(\"With ReLU: A = \" + str(t_A))\n\nlinear_activation_forward_test(linear_activation_forward)\nExpected output\nNote: In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.\n### 4.3 - L-Layer Model\nFor even more convenience when implementing the \\(L\\)-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with RELU) \\(L-1\\) times, then follows that with one linear_activation_forward with SIGMOID.\n### Exercise 5 - L_model_forward\nImplement the forward propagation of the above model.\nInstructions: In the code below, the variable AL will denote \\(A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})\\). (This is sometimes also called Yhat, i.e., this is \\(\\hat{Y}\\).)\nHints: - Use the functions you’ve previously written - Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times - Don’t forget to keep track of the caches in the “caches” list. To add a new value c to a list, you can use list.append(c).\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    # The for loop starts at 1 because layer 0 is the input\n    for l in range(1, L):\n        A_prev = A \n        #(≈ 2 lines of code)\n        # A, cache = ...\n        # caches ...\n        # CODE_START\n\n        # CODE_END\n    \n    # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.\n    #(≈ 2 lines of code)\n    # AL, cache = ...\n    # caches ...\n    # CODE_START\n\n    # CODE_END\n          \n    return AL, caches\nt_X, t_parameters = L_model_forward_test_case_2hidden()\nt_AL, t_caches = L_model_forward(t_X, t_parameters)\n\nprint(\"AL = \" + str(t_AL))\n\nL_model_forward_test(L_model_forward)\nExpected output\nAwesome! You’ve implemented a full forward propagation that takes the input X and outputs a row vector \\(A^{[L]}\\) containing your predictions. It also records all intermediate values in “caches”. Using \\(A^{[L]}\\), you can compute the cost of your predictions.\n## 5 - Cost Function\nNow you can implement forward and backward propagation! You need to compute the cost, in order to check whether your model is actually learning.\n### Exercise 6 - compute_cost Compute the cross-entropy cost \\(J\\), using the following formula: \\[-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}\\]\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    # (≈ 1 lines of code)\n    # cost = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n\n    \n    return cost\nt_Y, t_AL = compute_cost_test_case()\nt_cost = compute_cost(t_AL, t_Y)\n\nprint(\"Cost: \" + str(t_cost))\n\ncompute_cost_test(compute_cost)\nExpected Output:\n## 6 - Backward Propagation Module\nJust as you did for the forward propagation, you’ll implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters."
  },
  {
    "objectID": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html#model-architecture",
    "href": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html#model-architecture",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "8 - Model architecture",
    "text": "8 - Model architecture\n ### 8.1 - 2-layer Neural Network\nNow that you’re familiar with the dataset, it’s time to build a deep neural network to distinguish cat images from non-cat images!\nYou’re going to build two different models:\n\nA 2-layer neural network\nAn L-layer deep neural network\n\nThen, you’ll compare the performance of these models, and try out some different values for \\(L\\).\nLet’s look at the two architectures:\n\n\n\nFigure 2: 2-layer neural network.  The model can be summarized as: INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.\n\n\nDetailed Architecture of Figure 2: - The input is a (64,64,3) image which is flattened to a vector of size \\((12288,1)\\). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) of size \\((n^{[1]}, 12288)\\). - Then, add a bias term and take its relu to get the following vector: \\([a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T\\). - Multiply the resulting vector by \\(W^{[2]}\\) and add the intercept (bias). - Finally, take the sigmoid of the result. If it’s greater than 0.5, classify it as a cat.\n ### 8.2 - L-layer Deep Neural Network\nIt’s pretty difficult to represent an L-layer deep neural network using the above representation. However, here is a simplified network representation:\n\n\n\nFigure 3: L-layer neural network.  The model can be summarized as: [LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID\n\n\nDetailed Architecture of Figure 3: - The input is a (64,64,3) image which is flattened to a vector of size (12288,1). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) and then you add the intercept \\(b^{[1]}\\). The result is called the linear unit. - Next, take the relu of the linear unit. This process could be repeated several times for each \\((W^{[l]}, b^{[l]})\\) depending on the model architecture. - Finally, take the sigmoid of the final linear unit. If it is greater than 0.5, classify it as a cat.\n ### 8.3 - General Methodology\nAs usual, you’ll follow the Deep Learning methodology to build the model:\n\nInitialize parameters / Define hyperparameters\nLoop for num_iterations:\n\nForward propagation\nCompute cost function\nBackward propagation\nUpdate parameters (using parameters, and grads from backprop)\n\nUse trained parameters to predict labels\n\nNow go ahead and implement those two models!\n ## 9 - Two-layer Neural Network\n ### Exercise 11 - two_layer_model\nUse the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)\nlearning_rate = 0.0075\n\n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- If set to True, this will print the cost every 100 iterations \n    \n    Returns:\n    parameters -- a dictionary containing W1, W2, b1, and b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n    \n\n    # CODE_END\n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        #(≈ 2 lines of code)\n        # A1, cache1 = ...\n        # A2, cache2 = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Compute cost\n        #(≈ 1 line of code)\n        # cost = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n        #(≈ 2 lines of code)\n        # dA1, dW2, db2 = ...\n        # dA0, dW1, db1 = ...\n        # CODE_START\n\n        # CODE_END\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n        #(approx. 1 line of code)\n        # parameters = ...\n        # CODE_START\n        \n\n        # CODE_END\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n\n    return parameters, costs\n\ndef plot_costs(costs, learning_rate=0.0075):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\ntwo_layer_model_test(two_layer_model)\n\nExpected output:\ncost after iteration 1 must be around 0.69\n ### 9.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your parameters.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\nplot_costs(costs, learning_rate)\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.6930497356599888\n\n\n\n\nCost after iteration 100\n\n\n0.6464320953428849\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.04421498215868956\n\n\n\nNice! You successfully trained the model. Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.\nNow, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.\n\npredictions_train = predict(train_x, train_y, parameters)\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.9999999999999998\n\n\n\n\npredictions_test = predict(test_x, test_y, parameters)\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.72\n\n\n\n\nCongratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an \\(L\\)-layer model.\nNote: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping”, later we’ll learn more about it. Early stopping is a way to prevent overfitting.\n ## 10 - L-layer Neural Network\n ### Exercise 12 - L_layer_model\nUse the helper functions you implemented previously to build an \\(L\\)-layer neural network with the following structure: [LINEAR -&gt; RELU]\\(\\times\\)(L-1) -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters_deep(layers_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS ###\nlayers_dims = [12288, 20, 7, 5, 1] #  4-layer model\n\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization.\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.\n        #(≈ 1 line of code)\n        # AL, caches = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Compute cost.\n        #(≈ 1 line of code)\n        # cost = ...\n        # CODE_START\n \n        \n        # CODE_END\n    \n        # Backward propagation.\n        #(≈ 1 line of code)\n        # grads = ...    \n        # CODE_START\n\n        \n        # CODE_END\n \n        # Update parameters.\n        #(≈ 1 line of code)\n        # parameters = ...\n        # CODE_START\n\n        \n        # CODE_END\n                \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\nL_layer_model_test(L_layer_model)\n\n ### 10.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your model as a 4-layer neural network.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.771749\n\n\n\n\nCost after iteration 100\n\n\n0.672053\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.088439\n\n\n\n\npred_train = predict(train_x, train_y, parameters)\n\nExpected Output:\n\n\n\nTrain Accuracy\n\n\n0.985645933014\n\n\n\n\npred_test = predict(test_x, test_y, parameters)\n\nExpected Output:\n\n\n\nTest Accuracy\n\n\n0.8\n\n\n\n\n\nCongrats! It seems that your 4-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.\nThis is pretty good performance for this task. Nice job!\nLater, we’ll be able to obtain even higher accuracy by systematically searching for better hyperparameters: learning_rate, layers_dims, or num_iterations, for example.\n ## 11 - Results Analysis\nFirst, take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.\n\nprint_mislabeled_images(classes, test_x, test_y, pred_test)\n\nA few types of images the model tends to do poorly on include: - Cat body in an unusual position - Cat appears against a background of a similar color - Unusual cat color and species - Camera Angle - Brightness of the picture - Scale variation (cat is very large or small in image)\n ## 12 - Test with your own image ##\nFrom this point, if you so choose, you can use your own image to test the output of your model. To do that follow these steps:\n\nClick on “File” in the upper bar of this notebook, then click “Open”.\nAdd your image to this Jupyter Notebook’s directory, in the “images” folder\nChange your image’s name in the following code\nRun the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n\n## CODE_START ##\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)\n## CODE_END ##\n\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\n\nmy_predicted_image = predict(image, my_label_y, parameters)\n\n\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
  },
  {
    "objectID": "nb/dl_lab5/Optimization_methods.html",
    "href": "nb/dl_lab5/Optimization_methods.html",
    "title": "Optimization Methods",
    "section": "",
    "text": "Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you’ll gain skills with some more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.\nBy the end of this notebook, you’ll be able to:\nNotations: As usual, $ = $ da for any variable a.\nLet’s get started!"
  },
  {
    "objectID": "nb/dl_lab5/Optimization_methods.html#table-of-contents",
    "href": "nb/dl_lab5/Optimization_methods.html#table-of-contents",
    "title": "Optimization Methods",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1- Packages\n2 - Gradient Descent\n\nExercise 1 - update_parameters_with_gd\n\n3 - Mini-Batch Gradient Descent\n\nExercise 2 - random_mini_batches\n\n4 - Momentum\n\nExercise 3 - initialize_velocity\nExercise 4 - update_parameters_with_momentum\n\n5 - Adam\n\nExercise 5 - initialize_adam\nExercise 6 - update_parameters_with_adam\n\n6 - Model with different Optimization algorithms\n\n6.1 - Mini-Batch Gradient Descent\n6.2 - Mini-Batch Gradient Descent with Momentum\n6.3 - Mini-Batch with Adam\n6.4 - Summary\n\n7 - Learning Rate Decay and Scheduling\n\n7.1 - Decay on every iteration\n\nExercise 7 - update_lr\n\n7.2 - Fixed Interval Scheduling\n\nExercise 8 - schedule_lr_decay\n\n7.3 - Using Learning Rate Decay for each Optimization Method\n\n7.3.1 - Gradient Descent with Learning Rate Decay\n7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\n7.3.3 - Adam with Learning Rate Decay\n\n7.4 - Achieving similar performance with different methods\n\n\n ## 1- Packages\n\n### v1.1\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport math\nimport sklearn\nimport sklearn.datasets\n\nfrom opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\nfrom opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\nfrom copy import deepcopy\nfrom testCases import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Gradient Descent\nA simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all \\(m\\) examples on each step, it is also called Batch Gradient Descent.\n ### Exercise 1 - update_parameters_with_gd\nImplement the gradient descent update rule. The gradient descent rule is, for \\(l = 1, ..., L\\): \\[ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}\\] \\[ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}\\]\nwhere L is the number of layers and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using one step of gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters to be updated:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients to update each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] =  \n        # parameters[\"b\" + str(l)] = \n        # CODE_START  \n        \n        # CODE_END\n    return parameters\n\n\nparameters, grads, learning_rate = update_parameters_with_gd_test_case()\nlearning_rate = 0.01\nparameters = update_parameters_with_gd(parameters, grads, learning_rate)\n\nprint(\"W1 =\\n\" + str(parameters[\"W1\"]))\nprint(\"b1 =\\n\" + str(parameters[\"b1\"]))\nprint(\"W2 =\\n\" + str(parameters[\"W2\"]))\nprint(\"b2 =\\n\" + str(parameters[\"b2\"]))\n\nupdate_parameters_with_gd_test(update_parameters_with_gd)\n\nA variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent, where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.\n\n(Batch) Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost\n    cost_total = compute_cost(a, Y)  # Cost for m training examples\n    # Backward propagation\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters\n    parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\n        \n\nStochastic Gradient Descent:\n\nX = data_input\nY = labels\nm = X.shape[1]  # Number of training examples\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    cost_total = 0\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters\n        parameters = update_parameters(parameters, grads)\n    # Compute average cost\n    cost_avg = cost_total / m\nIn Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here’s what that looks like:\n\n\n\n  Figure 1  : SGD vs GD “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence, but each step is a lot faster to compute for SGD than it is for GD, as it uses only one training example (vs. the whole batch for GD).\n\n\nNote also that implementing SGD requires 3 for-loops in total: 1. Over the number of iterations 2. Over the \\(m\\) training examples 3. Over the layers (to update all parameters, from \\((W^{[1]},b^{[1]})\\) to \\((W^{[L]},b^{[L]})\\))\nIn practice, you’ll often get faster results if you don’t use the entire training set, or just one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.\n\n\n\n  Figure 2 :  SGD vs Mini-Batch GD “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization.\n\n\n ## 3 - Mini-Batch Gradient Descent\nNow you’ll build some mini-batches from the training set (X, Y).\nThere are two steps: - Shuffle: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the \\(i^{th}\\) column of X is the example corresponding to the \\(i^{th}\\) label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.\n\n\nPartition: Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this:\n\n\n ### Exercise 2 - random_mini_batches\nImplement random_mini_batches. The shuffling part has already been coded for you! To help with the partitioning step, you’ve been provided the following code that selects the indexes for the \\(1^{st}\\) and \\(2^{nd}\\) mini-batches:\nfirst_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\nsecond_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n...\nNote that the last mini-batch might end up smaller than mini_batch_size=64. Let \\(\\lfloor s \\rfloor\\) represents \\(s\\) rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be \\(\\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\) mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be \\(\\left(m-mini_\\_batch_\\_size \\times \\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\right)\\).\nHint:\n\\[mini\\_batch\\_X = shuffled\\_X[:, i : j]\\]\nThink of a way in which you can use the for loop variable k help you increment i and j in multiples of mini_batch_size.\nAs an example, if you want to increment in multiples of 3, you could the following:\nn = 3\nfor k in (0 , 5):\n    print(k * n)\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1, m))\n    \n    inc = mini_batch_size\n\n    # Step 2 - Partition (shuffled_X, shuffled_Y).\n    # Cases with a complete mini batch size only i.e each of 64 examples.\n    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        # (approx. 2 lines)\n        # mini_batch_X =  \n        # mini_batch_Y =\n        # CODE_START\n        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1) * mini_batch_size]\n        # CODE_END\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # For handling the end case (last mini-batch &lt; mini_batch_size i.e less than 64)\n    if m % mini_batch_size != 0:\n        #(approx. 2 lines)\n        # mini_batch_X =\n        # mini_batch_Y =\n        # CODE_START\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]  \n        # CODE_END\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nnp.random.seed(1)\nmini_batch_size = 64\nnx = 12288\nm = 148\nX = np.array([x for x in range(nx * m)]).reshape((m, nx)).T\nY = np.random.randn(1, m) &lt; 0.5\n\nmini_batches = random_mini_batches(X, Y, mini_batch_size)\nn_batches = len(mini_batches)\n\nassert n_batches == math.ceil(m / mini_batch_size), f\"Wrong number of mini batches. {n_batches} != {math.ceil(m / mini_batch_size)}\"\nfor k in range(n_batches - 1):\n    assert mini_batches[k][0].shape == (nx, mini_batch_size), f\"Wrong shape in {k} mini batch for X\"\n    assert mini_batches[k][1].shape == (1, mini_batch_size), f\"Wrong shape in {k} mini batch for Y\"\n    assert np.sum(np.sum(mini_batches[k][0] - mini_batches[k][0][0], axis=0)) == ((nx * (nx - 1) / 2 ) * mini_batch_size), \"Wrong values. It happens if the order of X rows(features) changes\"\nif ( m % mini_batch_size &gt; 0):\n    assert mini_batches[n_batches - 1][0].shape == (nx, m % mini_batch_size), f\"Wrong shape in the last minibatch. {mini_batches[n_batches - 1][0].shape} != {(nx, m % mini_batch_size)}\"\n\nassert np.allclose(mini_batches[0][0][0][0:3], [294912,  86016, 454656]), \"Wrong values. Check the indexes used to form the mini batches\"\nassert np.allclose(mini_batches[-1][0][-1][0:3], [1425407, 1769471, 897023]), \"Wrong values. Check the indexes used to form the mini batches\"\n\nprint(\"\\033[92mAll tests passed!\")\n\nAll tests passed!\n\n\n\npip show numpy\n\nName: numpy\nVersion: 1.26.4\nSummary: Fundamental package for array computing in Python\nHome-page: https://numpy.org\nAuthor: Travis E. Oliphant et al.\nAuthor-email: \nLicense: Copyright (c) 2005-2023, NumPy Developers.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\n       notice, this list of conditions and the following disclaimer.\n\n    * Redistributions in binary form must reproduce the above\n       copyright notice, this list of conditions and the following\n       disclaimer in the documentation and/or other materials provided\n       with the distribution.\n\n    * Neither the name of the NumPy Developers nor the names of any\n       contributors may be used to endorse or promote products derived\n       from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n----\n\nThe NumPy repository and source distributions bundle several libraries that are\ncompatibly licensed.  We list these here.\n\nName: lapack-lite\nFiles: numpy/linalg/lapack_lite/*\nLicense: BSD-3-Clause\n  For details, see numpy/linalg/lapack_lite/LICENSE.txt\n\nName: tempita\nFiles: tools/npy_tempita/*\nLicense: MIT\n  For details, see tools/npy_tempita/license.txt\n\nName: dragon4\nFiles: numpy/core/src/multiarray/dragon4.c\nLicense: MIT\n  For license text, see numpy/core/src/multiarray/dragon4.c\n\nName: libdivide\nFiles: numpy/core/include/numpy/libdivide/*\nLicense: Zlib\n  For license text, see numpy/core/include/numpy/libdivide/LICENSE.txt\n\n\nNote that the following files are vendored in the repository and sdist but not\ninstalled in built numpy packages:\n\nName: Meson\nFiles: vendored-meson/meson/*\nLicense: Apache 2.0\n  For license text, see vendored-meson/meson/COPYING\n\nName: spin\nFiles: .spin/cmds.py\nLicense: BSD-3\n  For license text, see .spin/LICENSE\n\n----\n\nThis binary distribution of NumPy also bundles the following software:\n\nName: OpenBLAS\nFiles: numpy/.dylibs/libopenblas*.so\nDescription: bundled as a dynamically linked library\nAvailability: https://github.com/OpenMathLib/OpenBLAS/\nLicense: BSD-3-Clause\n  Copyright (c) 2011-2014, The OpenBLAS Project\n  All rights reserved.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are\n  met:\n\n     1. Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n\n     2. Redistributions in binary form must reproduce the above copyright\n        notice, this list of conditions and the following disclaimer in\n        the documentation and/or other materials provided with the\n        distribution.\n     3. Neither the name of the OpenBLAS project nor the names of\n        its contributors may be used to endorse or promote products\n        derived from this software without specific prior written\n        permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: LAPACK\nFiles: numpy/.dylibs/libopenblas*.so\nDescription: bundled in OpenBLAS\nAvailability: https://github.com/OpenMathLib/OpenBLAS/\nLicense: BSD-3-Clause-Attribution\n  Copyright (c) 1992-2013 The University of Tennessee and The University\n                          of Tennessee Research Foundation.  All rights\n                          reserved.\n  Copyright (c) 2000-2013 The University of California Berkeley. All\n                          rights reserved.\n  Copyright (c) 2006-2013 The University of Colorado Denver.  All rights\n                          reserved.\n\n  $COPYRIGHT$\n\n  Additional copyrights may follow\n\n  $HEADER$\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are\n  met:\n\n  - Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  - Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer listed\n    in this license in the documentation and/or other materials\n    provided with the distribution.\n\n  - Neither the name of the copyright holders nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\n  The copyright holders provide no reassurances that the source code\n  provided does not infringe any patent, copyright, or any other\n  intellectual property rights of third parties.  The copyright holders\n  disclaim any liability to any recipient for claims brought against\n  recipient by any third party for infringement of that parties\n  intellectual property rights.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nName: GCC runtime library\nFiles: numpy/.dylibs/libgfortran*, numpy/.dylibs/libgcc*\nDescription: dynamically linked to files compiled with gcc\nAvailability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libgfortran\nLicense: GPL-3.0-with-GCC-exception\n  Copyright (C) 2002-2017 Free Software Foundation, Inc.\n\n  Libgfortran is free software; you can redistribute it and/or modify\n  it under the terms of the GNU General Public License as published by\n  the Free Software Foundation; either version 3, or (at your option)\n  any later version.\n\n  Libgfortran is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n  GNU General Public License for more details.\n\n  Under Section 7 of GPL version 3, you are granted additional\n  permissions described in the GCC Runtime Library Exception, version\n  3.1, as published by the Free Software Foundation.\n\n  You should have received a copy of the GNU General Public License and\n  a copy of the GCC Runtime Library Exception along with this program;\n  see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n  &lt;http://www.gnu.org/licenses/&gt;.\n\n----\n\nFull text of license texts referred to above follows (that they are\nlisted below does not necessarily imply the conditions apply to the\npresent binary release):\n\n----\n\nGCC RUNTIME LIBRARY EXCEPTION\n\nVersion 3.1, 31 March 2009\n\nCopyright (C) 2009 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\nThis GCC Runtime Library Exception (\"Exception\") is an additional\npermission under section 7 of the GNU General Public License, version\n3 (\"GPLv3\"). It applies to a given file (the \"Runtime Library\") that\nbears a notice placed by the copyright holder of the file stating that\nthe file is governed by GPLv3 along with this Exception.\n\nWhen you use GCC to compile a program, GCC may combine portions of\ncertain GCC header files and runtime libraries with the compiled\nprogram. The purpose of this Exception is to allow compilation of\nnon-GPL (including proprietary) programs to use, in this way, the\nheader files and runtime libraries covered by this Exception.\n\n0. Definitions.\n\nA file is an \"Independent Module\" if it either requires the Runtime\nLibrary for execution after a Compilation Process, or makes use of an\ninterface provided by the Runtime Library, but is not otherwise based\non the Runtime Library.\n\n\"GCC\" means a version of the GNU Compiler Collection, with or without\nmodifications, governed by version 3 (or a specified later version) of\nthe GNU General Public License (GPL) with the option of using any\nsubsequent versions published by the FSF.\n\n\"GPL-compatible Software\" is software whose conditions of propagation,\nmodification and use would permit combination with GCC in accord with\nthe license of GCC.\n\n\"Target Code\" refers to output from any compiler for a real or virtual\ntarget processor architecture, in executable form or suitable for\ninput to an assembler, loader, linker and/or execution\nphase. Notwithstanding that, Target Code does not include data in any\nformat that is used as a compiler intermediate representation, or used\nfor producing a compiler intermediate representation.\n\nThe \"Compilation Process\" transforms code entirely represented in\nnon-intermediate languages designed for human-written code, and/or in\nJava Virtual Machine byte code, into Target Code. Thus, for example,\nuse of source code generators and preprocessors need not be considered\npart of the Compilation Process, since the Compilation Process can be\nunderstood as starting with the output of the generators or\npreprocessors.\n\nA Compilation Process is \"Eligible\" if it is done using GCC, alone or\nwith other GPL-compatible software, or if it is done without using any\nwork based on GCC. For example, using non-GPL-compatible Software to\noptimize any GCC intermediate representations would not qualify as an\nEligible Compilation Process.\n\n1. Grant of Additional Permission.\n\nYou have permission to propagate a work of Target Code formed by\ncombining the Runtime Library with Independent Modules, even if such\npropagation would otherwise violate the terms of GPLv3, provided that\nall Target Code was generated by Eligible Compilation Processes. You\nmay then convey such a combination under terms of your choice,\nconsistent with the licensing of the Independent Modules.\n\n2. No Weakening of GCC Copyleft.\n\nThe availability of this Exception does not imply any general\npresumption that third-party software is unaffected by the copyleft\nrequirements of the license of GCC.\n\n----\n\n                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n&lt;http://www.gnu.org/licenses/&gt;.\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n&lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;.\n\nName: libquadmath\nFiles: numpy/.dylibs/libquadmath*.so\nDescription: dynamically linked to files compiled with gcc\nAvailability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libquadmath\nLicense: LGPL-2.1-or-later\n\n    GCC Quad-Precision Math Library\n    Copyright (C) 2010-2019 Free Software Foundation, Inc.\n    Written by Francois-Xavier Coudert  &lt;fxcoudert@gcc.gnu.org&gt;\n\n    This file is part of the libquadmath library.\n    Libquadmath is free software; you can redistribute it and/or\n    modify it under the terms of the GNU Library General Public\n    License as published by the Free Software Foundation; either\n    version 2.1 of the License, or (at your option) any later version.\n\n    Libquadmath is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n    Lesser General Public License for more details.\n    https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html\nLocation: /Users/vitvly/c/lnu/md/dl_nlp/venv/lib/python3.12/site-packages\nRequires: \nRequired-by: blis, contourpy, gensim, h5py, keras, matplotlib, ml-dtypes, pandas, patsy, scattertext, scikit-learn, scipy, spacy, statsmodels, tensorboard, thinc\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nt_X, t_Y, mini_batch_size = random_mini_batches_test_case()\nmini_batches = random_mini_batches(t_X, t_Y, mini_batch_size)\n\nprint (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nrandom_mini_batches_test(random_mini_batches)\n\n\nWhat you should remember: - Shuffling and Partitioning are the two steps required to build mini-batches - Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n ## 4 - Momentum\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.\nMomentum takes into account the past gradients to smooth out the update. The ‘direction’ of the previous gradients is stored in the variable \\(v\\). Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of \\(v\\) as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.\n\n\n\nFigure 3 : The red arrows show the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, the gradient is allowed to influence \\(v\\) and then take a step in the direction of \\(v\\). \n\n\n### Exercise 3 - initialize_velocity Initialize the velocity. The velocity, \\(v\\), is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is: for \\(l =1,...,L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\nNote that the iterator l starts at 1 in the for loop as the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript).\n\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(1, L + 1):\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] =\n        # v[\"db\" + str(l)] =\n        # CODE_START   \n        \n        # CODE_END\n        \n    return v\n\n\nparameters = initialize_velocity_test_case()\n\nv = initialize_velocity(parameters)\nprint(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))\n\ninitialize_velocity_test(initialize_velocity)\n\n\n### Exercise 4 - update_parameters_with_momentum\nNow, implement the parameters update with momentum. The momentum update rule is, for \\(l = 1, ..., L\\):\n\\[ \\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}\\]\n\\[\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}\n\\end{cases}\\tag{4}\\]\nwhere L is the number of layers, \\(\\beta\\) is the momentum and \\(\\alpha\\) is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\) (that’s a “one” on the superscript).\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(1, L + 1):\n        \n        # (approx. 4 lines)\n        # compute velocities\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # update parameters\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # CODE_START\n      \n        # CODE_END\n        \n    return parameters, v\n\n\nparameters, grads, v = update_parameters_with_momentum_test_case()\n\nparameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))\n\nupdate_parameters_with_momentum_test(update_parameters_with_momentum)\n\nNote that: - The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. - If \\(\\beta = 0\\), then this just becomes standard gradient descent without momentum.\nHow do you choose \\(\\beta\\)?\n\nThe larger the momentum \\(\\beta\\) is, the smoother the update, because it takes the past gradients into account more. But if \\(\\beta\\) is too big, it could also smooth out the updates too much.\nCommon values for \\(\\beta\\) range from 0.8 to 0.999. If you don’t feel inclined to tune this, \\(\\beta = 0.9\\) is often a reasonable default.\nTuning the optimal \\(\\beta\\) for your model might require trying several values to see what works best in terms of reducing the value of the cost function \\(J\\).\n\n\nWhat you should remember: - Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. - You have to tune a momentum hyperparameter \\(\\beta\\) and a learning rate \\(\\alpha\\).\n\n## 5 - Adam\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.\nHow does Adam work? 1. It calculates an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction). 2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction). 3. It updates parameters in a direction based on combining information from “1” and “2”.\nThe update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\] where: - t counts the number of steps taken of Adam - L is the number of layers - \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the two exponentially weighted averages. - \\(\\alpha\\) is the learning rate - \\(\\varepsilon\\) is a very small number to avoid dividing by zero\nAs usual, all parameters are stored in the parameters dictionary\n\n### Exercise 5 - initialize_adam\nInitialize the Adam variables \\(v, s\\) which keep track of the past information.\nInstruction: The variables \\(v, s\\) are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is: for \\(l = 1, ..., L\\):\nv[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\nv[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\ns[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\ns[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(1, L + 1):\n    # (approx. 4 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n    # CODE_START \n    \n    # CODE_END\n    \n    return v, s\n\n\nparameters = initialize_adam_test_case()\n\nv, s = initialize_adam(parameters)\nprint(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\nprint(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\nprint(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\nprint(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\nprint(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))\n\ninitialize_adam_test(initialize_adam)\n\n\n### Exercise 6 - update_parameters_with_adam\nNow, implement the parameters update with Adam. Recall the general update rule is, for \\(l = 1, ..., L\\):\n\\[\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\\]\nNote that the iterator l starts at 1 in the for loop as the first parameters are \\(W^{[1]}\\) and \\(b^{[1]}\\).\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    t -- Adam variable, counts the number of taken steps\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(1, L + 1):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        # (approx. 2 lines)\n        # v[\"dW\" + str(l)] = ...\n        # v[\"db\" + str(l)] = ...\n        # CODE_START   \n        \n        # CODE_END\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        # (approx. 2 lines)\n        # v_corrected[\"dW\" + str(l)] = ...\n        # v_corrected[\"db\" + str(l)] = ...\n        # CODE_START  \n        \n        # CODE_END\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        #(approx. 2 lines)\n        # s[\"dW\" + str(l)] = ...\n        # s[\"db\" + str(l)] = ...\n        # CODE_START   \n        \n        # CODE_END\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        # (approx. 2 lines)\n        # s_corrected[\"dW\" + str(l)] = ...\n        # s_corrected[\"db\" + str(l)] = ...\n        # CODE_START\n        \n        # CODE_END\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        # (approx. 2 lines)\n        # parameters[\"W\" + str(l)] = ...\n        # parameters[\"b\" + str(l)] = ...\n        # CODE_START\n        \n        # CODE_END\n\n    return parameters, v, s, v_corrected, s_corrected\n\n\nparametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon = update_parameters_with_adam_test_case()\n\nparameters, v, s, vc, sc  = update_parameters_with_adam(parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon)\nprint(f\"W1 = \\n{parameters['W1']}\")\nprint(f\"W2 = \\n{parameters['W2']}\")\nprint(f\"b1 = \\n{parameters['b1']}\")\nprint(f\"b2 = \\n{parameters['b2']}\")\n\nupdate_parameters_with_adam_test(update_parameters_with_adam)\n\nExpected values:\nW1 = \n[[ 1.63937725 -0.62327448 -0.54308727]\n [-1.0578897   0.85032154 -2.31657668]]\nW2 = \n[[ 0.33400549 -0.23563857]\n [ 1.47715417 -2.04561842]\n [-0.33729882 -0.36908457]]\nb1 = \n[[ 1.72995096]\n [-0.7762447 ]]\nb2 = \n[[ 1.14852557]\n [-1.08492339]\n [-0.15740527]]\nYou now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.\n\n## 6 - Model with different Optimization algorithms\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n\ntrain_X, train_Y = load_dataset()\n\nA 3-layer neural network has already been implemented for you! You’ll train it with: - Mini-batch Gradient Descent: it will call your function: - update_parameters_with_gd() - Mini-batch Momentum: it will call your functions: - initialize_velocity() and update_parameters_with_momentum() - Mini-batch Adam: it will call your functions: - initialize_adam() and update_parameters_with_adam()\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\nNow, run this 3 layer neural network with each of the 3 optimization methods.\n\n### 6.1 - Mini-Batch Gradient Descent\nRun the following code to see how the model does with mini-batch gradient descent.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n### 6.2 - Mini-Batch Gradient Descent with Momentum\nNext, run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small - but for more complex problems you might see bigger gains.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n### 6.3 - Mini-Batch with Adam\nFinally, run the following code to see how the model does with Adam.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n### 6.4 - Summary\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;71%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smooth\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94%\n    &lt;/td&gt;\n    &lt;td&gt;\n    smoother\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\ncost shape\n\n\n\n\nMomentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible.\nOn the other hand, Adam clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.\nSome advantages of Adam include:\n\nRelatively low memory requirements (though higher than gradient descent and gradient descent with momentum)\nUsually works well even with little tuning of hyperparameters (except \\(\\alpha\\))\n\nReferences:\n\nAdam paper: https://arxiv.org/pdf/1412.6980.pdf\n\n\n## 7 - Learning Rate Decay and Scheduling\nLastly, the learning rate is another hyperparameter that can help you speed up learning.\nDuring the first part of training, your model can get away with taking large steps, but over time, using a fixed value for the learning rate alpha can cause your model to get stuck in a wide oscillation that never quite converges. But if you were to slowly reduce your learning rate alpha over time, you could then take smaller, slower steps that bring you closer to the minimum. This is the idea behind learning rate decay.\nLearning rate decay can be achieved by using either adaptive methods or pre-defined learning rate schedules.\nNow, you’ll apply scheduled learning rate decay to a 3-layer neural network in three different optimizer modes and see how each one differs, as well as the effect of scheduling at different epochs.\nThis model is essentially the same as the one you used before, except in this one you’ll be able to include learning rate decay. It includes two new parameters, decay and decay_rate.\n\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    m = X.shape[1]                   # number of training examples\n    lr_rates = []\n    learning_rate0 = learning_rate   # the original learning rate\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        cost_total = 0\n        \n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost and add to the cost total\n            cost_total += compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        cost_avg = cost_total / m\n        if decay:\n            learning_rate = decay(learning_rate0, i, decay_rate)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n            if decay:\n                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n        if print_cost and i % 100 == 0:\n            costs.append(cost_avg)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n### 7.1 - Decay on every iteration\nFor this portion of the assignment, you’ll try one of the pre-defined schedules for learning rate decay, called exponential learning rate decay. It takes this mathematical form:\n\\[\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}\\]\n\n### Exercise 7 - update_lr\nCalculate the new learning rate using exponential weight decay.\n\ndef update_lr(learning_rate0, epoch_num, decay_rate):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer\n    decay_rate -- Decay rate. Scalar\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    #(approx. 1 line)\n    # learning_rate = \n    # CODE_START\n    \n    # CODE_END\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\nepoch_num = 2\ndecay_rate = 1\nlearning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n\nprint(\"Updated learning rate: \", learning_rate_2)\n\nupdate_lr_test(update_lr)\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=update_lr)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nNotice that if you set the decay to occur at every iteration, the learning rate goes to zero too quickly - even if you start with a higher learning rate.\n\n\n\nEpoch Number\n\n\nLearning Rate\n\n\nCost\n\n\n\n\n0\n\n\n0.100000\n\n\n0.701091\n\n\n\n\n1000\n\n\n0.000100\n\n\n0.661884\n\n\n\n\n2000\n\n\n0.000050\n\n\n0.658620\n\n\n\n\n3000\n\n\n0.000033\n\n\n0.656765\n\n\n\n\n4000\n\n\n0.000025\n\n\n0.655486\n\n\n\n\n5000\n\n\n0.000020\n\n\n0.654514\n\n\n\nWhen you’re training for a few epoch this doesn’t cause a lot of troubles, but when the number of epochs is large the optimization algorithm will stop updating. One common fix to this issue is to decay the learning rate every few steps. This is called fixed interval scheduling.\n ### 7.2 - Fixed Interval Scheduling\nYou can help prevent the learning rate speeding to zero too quickly by scheduling the exponential learning rate decay at a fixed time interval, for example 1000. You can either number the intervals, or divide the epoch by the time interval, which is the size of window with the constant learning rate.\n\n ### Exercise 8 - schedule_lr_decay\nCalculate the new learning rate using exponential weight decay with fixed interval scheduling.\nInstructions: Implement the learning rate scheduling such that it only changes when the epochNum is a multiple of the timeInterval.\nNote: The fraction in the denominator uses the floor operation.\n\\[\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}\\]\nHint: numpy.floor\n\ndef schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n    \"\"\"\n    Calculates updated the learning rate using exponential weight decay.\n    \n    Arguments:\n    learning_rate0 -- Original learning rate. Scalar\n    epoch_num -- Epoch number. Integer.\n    decay_rate -- Decay rate. Scalar.\n    time_interval -- Number of epochs where you update the learning rate.\n\n    Returns:\n    learning_rate -- Updated learning rate. Scalar \n    \"\"\"\n    # (approx. 1 lines)\n    # learning_rate = ...\n    # CODE_START\n    \n    # CODE_END\n    return learning_rate\n\n\nlearning_rate = 0.5\nprint(\"Original learning rate: \", learning_rate)\n\nepoch_num_1 = 10\nepoch_num_2 = 100\ndecay_rate = 0.3\ntime_interval = 100\nlearning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\nlearning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\nprint(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)\n\nschedule_lr_decay_test(schedule_lr_decay)\n\nExpected output\nOriginal learning rate:  0.5\nUpdated learning rate after 10 epochs:  0.5\nUpdated learning rate after 100 epochs:  0.3846153846153846\n ### 7.3 - Using Learning Rate Decay for each Optimization Method\nBelow, you’ll use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)\n #### 7.3.1 - Gradient Descent with Learning Rate Decay\nRun the following code to see how the model does gradient descent and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n #### 7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\nRun the following code to see how the model does gradient descent with momentum and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent with momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n #### 7.3.3 - Adam with Learning Rate Decay\nRun the following code to see how the model does Adam and weight decay.\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\", learning_rate = 0.01, num_epochs=5000, decay=schedule_lr_decay)\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n ### 7.4 - Achieving similar performance with different methods\nWith Mini-batch GD or Mini-batch GD with Momentum, the accuracy is significantly lower than Adam, but when learning rate decay is added on top, either can achieve performance at a speed and accuracy score that’s similar to Adam.\nIn the case of Adam, notice that the learning curve achieves a similar accuracy but faster.\n&lt;td&gt;\n    Gradient descent\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;94.6%\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Momentum\n    &lt;/td&gt;\n    &lt;td&gt;\n    &gt;95.6%\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    Adam\n    &lt;/td&gt;\n    &lt;td&gt;\n    94%\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\noptimization method\n\n\naccuracy\n\n\n\n\nCongratulations! You’ve made it to the end of the Optimization methods notebook. Here’s a quick recap of everything you’re now able to do:\n\nApply three different optimization methods to your models\nBuild mini-batches for your training set\nUse learning rate decay scheduling to speed up your training\n\nGreat work!"
  },
  {
    "objectID": "nb/dl_lab3_answers/Initialization_Regularization.html",
    "href": "nb/dl_lab3_answers/Initialization_Regularization.html",
    "title": "Deep Learning/NLP course",
    "section": "",
    "text": "## 1 - Packages\n\nv1.1\npip install dlai_tools\npip install tensorflow\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nfrom public_tests import *\nfrom init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\nfrom init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n# load image dataset: blue/red dots in circles\n# train_X, train_Y, test_X, test_Y = load_dataset()\n\n ## 2 - Loading the Dataset\n\ntrain_X, train_Y, test_X, test_Y = load_dataset()\n\n\n\n\n\n\n\n\nFor this classifier, you want to separate the blue dots from the red dots.\n ## 3 - Neural Network Model\nYou’ll use a 3-layer neural network (already implemented for you). These are the initialization methods you’ll experiment with: - Zeros initialization – setting initialization = \"zeros\" in the input argument. - Random initialization – setting initialization = \"random\" in the input argument. This initializes the weights to large random values.\n- He initialization – setting initialization = \"he\" in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.\nInstructions: Instructions: Read over the code below, and run it. In the next part, you’ll implement the three initialization methods that this model() calls.\n\ndef model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n    learning_rate -- learning rate for gradient descent \n    num_iterations -- number of iterations to run gradient descent\n    print_cost -- if True, print the cost every 1000 iterations\n    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n    \n    Returns:\n    parameters -- parameters learnt by the model\n    \"\"\"\n        \n    grads = {}\n    costs = [] # to keep track of the loss\n    m = X.shape[1] # number of examples\n    layers_dims = [X.shape[0], 10, 5, 1]\n    \n    # Initialize parameters dictionary.\n    if initialization == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        a3, cache = forward_propagation(X, parameters)\n        \n        # Loss\n        cost = compute_loss(a3, Y)\n\n        # Backward propagation.\n        grads = backward_propagation(X, Y, cache)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n            costs.append(cost)\n            \n    # plot the loss\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\n ## 4 - Zero Initialization\nThere are two types of parameters to initialize in a neural network: - the weight matrices \\((W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})\\) - the bias vectors \\((b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})\\)\n ### Exercise 1 - initialize_parameters_zeros\nImplement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry,” but try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes.\n\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    parameters = {}\n    L = len(layers_dims)            # number of layers in the network\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] = \n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        \n        # YOUR CODE ENDS HERE\n    return parameters\n\n\nparameters = initialize_parameters_zeros([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_zeros_test(initialize_parameters_zeros)\n\nW1 = [[0. 0. 0.]\n [0. 0. 0.]]\nb1 = [[0.]\n [0.]]\nW2 = [[0. 0.]]\nb2 = [[0.]]\n All tests passed.\n\n\nRun the following code to train your model on 15,000 iterations using zeros initialization.\n\nparameters = model(train_X, train_Y, initialization = \"zeros\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6931471805599453\nCost after iteration 1000: 0.6931471805599453\nCost after iteration 2000: 0.6931471805599453\nCost after iteration 3000: 0.6931471805599453\nCost after iteration 4000: 0.6931471805599453\nCost after iteration 5000: 0.6931471805599453\nCost after iteration 6000: 0.6931471805599453\nCost after iteration 7000: 0.6931471805599453\nCost after iteration 8000: 0.6931471805599453\nCost after iteration 9000: 0.6931471805599453\nCost after iteration 10000: 0.6931471805599453\nCost after iteration 11000: 0.6931471805599453\nCost after iteration 12000: 0.6931471805599453\nCost after iteration 13000: 0.6931471805599453\nCost after iteration 14000: 0.6931471805599453\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.5\nOn the test set:\nAccuracy: 0.5\n\n\nThe performance is terrible, the cost doesn’t decrease, and the algorithm performs no better than random guessing. Why? Take a look at the details of the predictions and the decision boundary:\n\nprint (\"predictions_train = \" + str(predictions_train))\nprint (\"predictions_test = \" + str(predictions_test))\n\npredictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0]]\npredictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\n\n\nplt.title(\"Model with Zeros initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nFor a comprehensive explanation of this, you can read Paul Mielke’s post, Symmetry Breaking versus Zero Initialization.\nA simple explanation is provided below:\nNote: For sake of simplicity calculations below are done using only one example at a time.\nSince the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As z = 0\n\\[a = ReLU(z) = max(0, z) = 0\\]\nAt the classification layer, where the activation function is sigmoid you then get (for either input):\n\\[\\sigma(z) = \\frac{1}{ 1 + e^{-(z)}} = \\frac{1}{2} = y_{pred}\\]\nAs for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.\nYour loss function: \\[ \\mathcal{L}(a, y) =  - y  \\ln(y_{pred}) - (1-y)  \\ln(1-y_{pred})\\]\nFor y=1, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 1) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nFor y=0, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 0) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nAs you can see with the prediction being 0.5 whether the actual (y) value is 1 or 0 you get the same loss value for both, so none of the weights get adjusted and you are stuck with the same old value of the weights.\nThis is why you can see that the model is predicting 0 for every example! No wonder it’s doing so badly.\nIn general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with \\(n^{[l]}=1\\) for every layer. This way, the network is no more powerful than a linear classifier like logistic regression.\n\nWhat you should remember: - The weights \\(W^{[l]}\\) should be initialized randomly to break symmetry. - However, it’s okay to initialize the biases \\(b^{[l]}\\) to zeros. Symmetry is still broken so long as \\(W^{[l]}\\) is initialized randomly.\n ## 5 - Random Initialization\nTo break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you’ll see what happens when the weights are initialized randomly, but to very large values.\n ### Exercise 2 - initialize_parameters_random\nImplement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use np.random.randn(..,..) * 10 for weights and np.zeros((.., ..)) for biases. You’re using a fixed np.random.seed(..) to make sure your “random” weights match ours, so don’t worry if running your code several times always gives you the same initial values for the parameters.\n\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*10\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        \n        # YOUR CODE ENDS HERE\n\n    return parameters\n\n\nparameters = initialize_parameters_random([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_random_test(initialize_parameters_random)\n\nW1 = [[ 17.88628473   4.36509851   0.96497468]\n [-18.63492703  -2.77388203  -3.54758979]]\nb1 = [[0.]\n [0.]]\nW2 = [[-0.82741481 -6.27000677]]\nb2 = [[0.]]\n All tests passed.\n\n\nRun the following code to train your model on 15,000 iterations using random initialization.\n\nparameters = model(train_X, train_Y, initialization = \"random\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: inf\nCost after iteration 1000: 0.6243608083761976\nCost after iteration 2000: 0.5979255420418802\nCost after iteration 3000: 0.5636974033624995\nCost after iteration 4000: 0.550129435783533\nCost after iteration 5000: 0.5444127596753244\nCost after iteration 6000: 0.5374134745412525\nCost after iteration 7000: 0.4738098425592898\nCost after iteration 8000: 0.3977585829307016\nCost after iteration 9000: 0.3934706020369666\nCost after iteration 10000: 0.39202176138493355\nCost after iteration 11000: 0.3892161604894328\nCost after iteration 12000: 0.3861393388253537\nCost after iteration 13000: 0.3849794626808294\nCost after iteration 14000: 0.38278262086755843\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.83\nOn the test set:\nAccuracy: 0.86\n\n\nIf you see “inf” as the cost after the iteration 0, this is because of numerical roundoff. A more numerically sophisticated implementation would fix this, but for the purposes of this notebook, it isn’t really worth worrying about.\nIn any case, you’ve now broken the symmetry, and this gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!\n\nprint (predictions_train)\nprint (predictions_test)\n\n[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0\n  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1\n  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1\n  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1\n  1 1 1 1 0 0 0 1 1 1 1 0]]\n[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1\n  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]\n\n\n\nplt.title(\"Model with large random initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nObservations: - The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when \\(\\log(a^{[3]}) = \\log(0)\\), the loss goes to infinity. - Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. - If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n\nIn summary: - Initializing weights to very large random values doesn’t work well. - Initializing with small random values should do better. The important question is, how small should be these random values be? Let’s find out up next!\n\nOptional Read:\nThe main difference between Gaussian variable (numpy.random.randn()) and uniform random variable is the distribution of the generated random numbers:\n\nnumpy.random.rand() produces numbers in a uniform distribution.\nand numpy.random.randn() produces numbers in a normal distribution.\n\nWhen used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.\nAn intuitive way to see it is, for example, if you take the sigmoid() activation function.\nYou’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.\n ## 6 - He Initialization\nFinally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights \\(W^{[l]}\\) of sqrt(1./layers_dims[l-1]) where He initialization would use sqrt(2./layers_dims[l-1]).)\n ### Exercise 3 - initialize_parameters_he\nImplement the following function to initialize your parameters with He initialization. This function is similar to the previous initialize_parameters_random(...). The only difference is that instead of multiplying np.random.randn(..,..) by 10, you will multiply it by \\(\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}\\), which is what He initialization recommends for layers with a ReLU activation.\n\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))  \n        \n        # YOUR CODE ENDS HERE\n        \n    return parameters\n\n\nparameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_he_test(initialize_parameters_he)\n# parameters\n\nW1 = [[ 1.78862847  0.43650985]\n [ 0.09649747 -1.8634927 ]\n [-0.2773882  -0.35475898]\n [-0.08274148 -0.62700068]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\nb2 = [[0.]]\n All tests passed.\n\n\nExpected output\nW1 = [[ 1.78862847  0.43650985]\n [ 0.09649747 -1.8634927 ]\n [-0.2773882  -0.35475898]\n [-0.08274148 -0.62700068]]\nb1 = [[0.] [0.] [0.] [0.]]\nW2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\nb2 = [[0.]]\nRun the following code to train your model on 15,000 iterations using He initialization.\n\nparameters = model(train_X, train_Y, initialization = \"he\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.8830537463419761\nCost after iteration 1000: 0.6879825919728063\nCost after iteration 2000: 0.6751286264523371\nCost after iteration 3000: 0.6526117768893807\nCost after iteration 4000: 0.6082958970572937\nCost after iteration 5000: 0.5304944491717495\nCost after iteration 6000: 0.41386458170717944\nCost after iteration 7000: 0.31178034648444414\nCost after iteration 8000: 0.23696215330322556\nCost after iteration 9000: 0.18597287209206836\nCost after iteration 10000: 0.15015556280371808\nCost after iteration 11000: 0.12325079292273551\nCost after iteration 12000: 0.09917746546525931\nCost after iteration 13000: 0.08457055954024283\nCost after iteration 14000: 0.07357895962677367\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.9933333333333333\nOn the test set:\nAccuracy: 0.96\n\n\n\nplt.title(\"Model with He initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nObservations: - The model with He initialization separates the blue and the red dots very well in a small number of iterations.\n ## 7 - Conclusions\nYou’ve tried three different types of initializations. For the same number of iterations and same hyperparameters, the comparison is:\n&lt;td&gt;\n    3-layer NN with zeros initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    50%\n    &lt;/td&gt;\n    &lt;td&gt;\n    fails to break symmetry\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with large random initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    83%\n    &lt;/td&gt;\n    &lt;td&gt;\n    too large weights \n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with He initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    99%\n    &lt;/td&gt;\n    &lt;td&gt;\n    recommended method\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\nModel\n\n\nTrain accuracy\n\n\nProblem/Comment\n\n\n\n\nCongratulations! You’ve completed this notebook on Initialization.\nHere’s a quick recap of the main takeaways:\n\n\nDifferent initializations lead to very different results\nRandom initialization is used to break symmetry and make sure different hidden units can learn different things\nResist initializing to values that are too large!\nHe initialization works well for networks with ReLU activations\n\n ## 8. Regularization. Packages.\n\n### v1.1\n\n\n# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\nfrom reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\nfrom reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\nfrom testCases import *\nfrom public_tests import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n ## 9 - Regularization. Problem Statement\nYou have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.\n\n\n\nFigure 1: Football field. The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head \n\n\nThey give you the following 2D dataset from France’s past 10 games.\n ## 10 - Loading the Dataset\n\ntrain_X, train_Y, test_X, test_Y = load_2D_dataset()\n\n\n\n\n\n\n\n\nEach dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field. - If the dot is blue, it means the French player managed to hit the ball with his/her head - If the dot is red, it means the other team’s player hit the ball with their head\nYour goal: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.\nAnalysis of the dataset: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well.\nYou will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem.\n ## 11 - Non-Regularized Model\nYou will use the following neural network (already implemented for you below). This model can be used: - in regularization mode – by setting the lambd input to a non-zero value. We use “lambd” instead of “lambda” because “lambda” is a reserved keyword in Python. - in dropout mode – by setting the keep_prob to a value less than one\nYou will first try the model without any regularization. Then, you will implement: - L2 regularization – functions: “compute_cost_with_regularization()” and “backward_propagation_with_regularization()” - Dropout – functions: “forward_propagation_with_dropout()” and “backward_propagation_with_dropout()”\nIn each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.\n\ndef model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob &lt; 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n                                                # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob &lt; 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\nLet’s train the model without any regularization, and observe the accuracy on the train/test sets.\n\nparameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6557412523481002\nCost after iteration 10000: 0.16329987525724207\nCost after iteration 20000: 0.1385164242326793\n\n\n\n\n\n\n\n\n\nOn the training set:\nAccuracy: 0.9478672985781991\nOn the test set:\nAccuracy: 0.915\n\n\nThe train accuracy is 94.8% while the test accuracy is 91.5%. This is the baseline model (you will observe the impact of regularization on this model). Run the following code to plot the decision boundary of your model.\n\nplt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nThe non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.\n ## 12 - L2 Regularization\nThe standard way to avoid overfitting is called L2 regularization. It consists of appropriately modifying your cost function, from: \\[J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}\\] To: \\[J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}\\]\nLet’s modify your cost and observe the consequences.\n ### Exercise 4 - compute_cost_with_regularization Implement compute_cost_with_regularization() which computes the cost given by formula (2). To calculate \\(\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}\\) , use :\nnp.sum(np.square(Wl))\nNote that you have to do this for \\(W^{[1]}\\), \\(W^{[2]}\\) and \\(W^{[3]}\\), then sum the three terms and multiply by $ $.\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    #(≈ 1 lines of code)\n    # L2_regularization_cost = \n    # YOUR CODE STARTS HERE\n    L2_regularization_cost = lambd/(2*m)*(np.sum(np.square(W3))+np.sum(np.square(W2))+np.sum(np.square(W1)))\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost\n\n\nA3, t_Y, parameters = compute_cost_with_regularization_test_case()\ncost = compute_cost_with_regularization(A3, t_Y, parameters, lambd=0.1)\nprint(\"cost = \" + str(cost))\n\ncompute_cost_with_regularization_test(compute_cost_with_regularization)\n\ncost = 1.7864859451590758\n All tests passed.\n\n\nOf course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost.\n ### Exercise 5 - backward_propagation_with_regularization Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term’s gradient (\\(\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W\\)).\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    #(≈ 1 lines of code)\n    # dW3 = 1./m * np.dot(dZ3, A2.T) + None\n    # YOUR CODE STARTS HERE\n    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m*W3\n    \n    # YOUR CODE ENDS HERE\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    #(≈ 1 lines of code)\n    # dW2 = 1./m * np.dot(dZ2, A1.T) + None\n    # YOUR CODE STARTS HERE\n    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m*W2\n    \n    # YOUR CODE ENDS HERE\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    #(≈ 1 lines of code)\n    # dW1 = 1./m * np.dot(dZ1, X.T) + None\n    # YOUR CODE STARTS HERE\n    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m*W1\n    \n    # YOUR CODE ENDS HERE\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\nt_X, t_Y, cache = backward_propagation_with_regularization_test_case()\n\ngrads = backward_propagation_with_regularization(t_X, t_Y, cache, lambd = 0.7)\nprint (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\nprint (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\nprint (\"dW3 = \\n\"+ str(grads[\"dW3\"]))\nbackward_propagation_with_regularization_test(backward_propagation_with_regularization)\n\ndW1 = \n[[-0.25604646  0.12298827 -0.28297129]\n [-0.17706303  0.34536094 -0.4410571 ]]\ndW2 = \n[[ 0.79276486  0.85133918]\n [-0.0957219  -0.01720463]\n [-0.13100772 -0.03750433]]\ndW3 = \n[[-1.77691347 -0.11832879 -0.09397446]]\n All tests passed.\n\n\nLet’s now run the model with L2 regularization \\((\\lambda = 0.7)\\). The model() function will call: - compute_cost_with_regularization instead of compute_cost - backward_propagation_with_regularization instead of backward_propagation\n\nparameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6974484493131264\nCost after iteration 10000: 0.2684918873282239\nCost after iteration 20000: 0.2680916337127301\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.9383886255924171\nOn the test set:\nAccuracy: 0.93\n\n\nCongrats, the test set accuracy increased to 93%. You have saved the French football team!\nYou are not overfitting the training data anymore. Let’s plot the decision boundary.\n\nplt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nObservations: - The value of \\(\\lambda\\) is a hyperparameter that you can tune using a dev set. - L2 regularization makes your decision boundary smoother. If \\(\\lambda\\) is too large, it is also possible to “oversmooth”, resulting in a model with high bias.\nWhat is L2-regularization actually doing?:\nL2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n \nWhat you should remember: the implications of L2-regularization on: - The cost computation: - A regularization term is added to the cost. - The backpropagation function: - There are extra terms in the gradients with respect to weight matrices. - Weights end up smaller (“weight decay”): - Weights are pushed to smaller values.\n ## 13 - Dropout\nFinally, dropout is a widely used regularization technique that is specific to deep learning. It randomly shuts down some neurons in each iteration. Watch these two videos to see what this means!\n\n\n\n\n\n\n\n\nFigure 2 : Drop-out on the second hidden layer.  At each iteration, you shut down (= set to zero) each neuron of a layer with probability \\(1 - keep\\_prob\\) or keep it with probability \\(keep\\_prob\\) (50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration. \n\n\n\n\n\n\n\n\nFigure 3: Drop-out on the first and third hidden layers.  \\(1^{st}\\) layer: we shut down on average 40% of the neurons. \\(3^{rd}\\) layer: we shut down on average 20% of the neurons. \n\n\nWhen you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.\n ### 13.1 - Forward Propagation with Dropout\n ### Exercise 6 - forward_propagation_with_dropout\nImplement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer.\nInstructions: You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps: 1. In lecture, we dicussed creating a variable \\(d^{[1]}\\) with the same shape as \\(a^{[1]}\\) using np.random.rand() to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{1} d^{1} … d^{1}] $ of the same dimension as \\(A^{[1]}\\). 2. Set each entry of \\(D^{[1]}\\) to be 1 with probability (keep_prob), and 0 otherwise.\nHint: Let’s say that keep_prob = 0.8, which means that we want to keep about 80% of the neurons and drop out about 20% of them. We want to generate a vector that has 1’s and 0’s, where about 80% of them are 1 and about 20% are 0. This python statement:\nX = (X &lt; keep_prob).astype(int)\nis conceptually the same as this if-else statement (for the simple case of a one-dimensional array) :\nfor i,v in enumerate(x):\n    if v &lt; keep_prob:\n        x[i] = 1\n    else: # v &gt;= keep_prob\n        x[i] = 0\nNote that the X = (X &lt; keep_prob).astype(int) works with multi-dimensional arrays, and the resulting output preserves the dimensions of the input array.\nAlso note that without using .astype(int), the result is an array of booleans True and False, which Python automatically converts to 1 and 0 if we multiply it with numbers. (However, it’s better practice to convert data into the data type that we intend, so try using .astype(int).)\n\nSet \\(A^{[1]}\\) to \\(A^{[1]} * D^{[1]}\\). (You are shutting down some neurons). You can think of \\(D^{[1]}\\) as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.\nDivide \\(A^{[1]}\\) by keep_prob. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)\n\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n    # D1 =                                           # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n    # D1 =                                           # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n    # A1 =                                           # Step 3: shut down some neurons of A1\n    # A1 =                                           # Step 4: scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    D1 = np.random.rand(A1.shape[0], A1.shape[1]) \n    D1 = (D1 &lt; keep_prob).astype(int)\n    A1 = np.multiply(A1, D1)\n    A1 = A1/keep_prob\n    # YOUR CODE ENDS HERE\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    #(≈ 4 lines of code)\n    # D2 =                                           # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n    # D2 =                                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n    # A2 =                                           # Step 3: shut down some neurons of A2\n    # A2 =                                           # Step 4: scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n    D2 = (D2 &lt; keep_prob).astype(int)\n    A2 = np.multiply(A2, D2)\n    A2 = A2/keep_prob   \n    # YOUR CODE ENDS HERE\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache\n\n\nt_X, parameters = forward_propagation_with_dropout_test_case()\n\nA3, cache = forward_propagation_with_dropout(t_X, parameters, keep_prob=0.7)\nprint (\"A3 = \" + str(A3))\n\nforward_propagation_with_dropout_test(forward_propagation_with_dropout)\n\nA3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]\n All tests passed.\n\n\n ### 13.2 - Backward Propagation with Dropout\n ### Exercise 7 - backward_propagation_with_dropout Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks \\(D^{[1]}\\) and \\(D^{[2]}\\) stored in the cache.\nInstruction: Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps: 1. You had previously shut down some neurons during forward propagation, by applying a mask \\(D^{[1]}\\) to A1. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask \\(D^{[1]}\\) to dA1. 2. During forward propagation, you had divided A1 by keep_prob. In backpropagation, you’ll therefore have to divide dA1 by keep_prob again (the calculus interpretation is that if \\(A^{[1]}\\) is scaled by keep_prob, then its derivative \\(dA^{[1]}\\) is also scaled by the same keep_prob).\n\n# GRADED FUNCTION: backward_propagation_with_dropout\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1./m * np.dot(dZ3, A2.T)\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    #(≈ 2 lines of code)\n    # dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n    # dA2 =                # Step 2: Scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    dA2 = D2*dA2\n    dA2 = dA2/keep_prob\n    \n    # YOUR CODE ENDS HERE\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = 1./m * np.dot(dZ2, A1.T)\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    #(≈ 2 lines of code)\n    # dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n    # dA1 =                # Step 2: Scale the value of neurons that haven't been shut down\n    # YOUR CODE STARTS HERE\n    dA1 = D1*dA1\n    dA1 = dA1/keep_prob    \n    \n    # YOUR CODE ENDS HERE\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = 1./m * np.dot(dZ1, X.T)\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\nt_X, t_Y, cache = backward_propagation_with_dropout_test_case()\n\ngradients = backward_propagation_with_dropout(t_X, t_Y, cache, keep_prob=0.8)\n\nprint (\"dA1 = \\n\" + str(gradients[\"dA1\"]))\nprint (\"dA2 = \\n\" + str(gradients[\"dA2\"]))\n\nbackward_propagation_with_dropout_test(backward_propagation_with_dropout)\n\ndA1 = \n[[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\ndA2 = \n[[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n [ 0.          0.         -0.00292733  0.         -0.        ]]\n All tests passed.\n\n\nLet’s now run the model with dropout (keep_prob = 0.86). It means at every iteration you shut down each neurons of layer 1 and 2 with 14% probability. The function model() will now call: - forward_propagation_with_dropout instead of forward_propagation. - backward_propagation_with_dropout instead of backward_propagation.\n\nparameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6543912405149825\n\n\n/Users/vitvly/c/lnu/2023-2024.2/dl_nlp/dl_lab3/reg_utils.py:236: RuntimeWarning: divide by zero encountered in log\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n/Users/vitvly/c/lnu/2023-2024.2/dl_nlp/dl_lab3/reg_utils.py:236: RuntimeWarning: invalid value encountered in multiply\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n\n\nCost after iteration 10000: 0.0610169865749056\nCost after iteration 20000: 0.060582435798513114\n\n\n\n\n\n\n\n\n\nOn the train set:\nAccuracy: 0.9289099526066351\nOn the test set:\nAccuracy: 0.95\n\n\nDropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set. The French football team will be forever grateful to you!\nRun the code below to plot the decision boundary.\n\nplt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\n\n\nNote: - A common mistake when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. - Deep learning frameworks like TensorFlow, PaddlePaddle, Keras or caffe come with a dropout layer implementation. Don’t stress - you will soon learn some of these frameworks.\n\nWhat you should remember about dropout: - Dropout is a regularization technique. - You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time. - Apply dropout both during forward and backward propagation. - During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.\n ## 14 - Conclusions\nHere are the results of our three models:\n&lt;td&gt;\n    3-layer NN without regularization\n    &lt;/td&gt;\n    &lt;td&gt;\n    95%\n    &lt;/td&gt;\n    &lt;td&gt;\n    91.5%\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with L2-regularization\n    &lt;/td&gt;\n    &lt;td&gt;\n    94%\n    &lt;/td&gt;\n    &lt;td&gt;\n    93%\n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with dropout\n    &lt;/td&gt;\n    &lt;td&gt;\n    93%\n    &lt;/td&gt;\n    &lt;td&gt;\n    95%\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\nmodel\n\n\ntrain accuracy\n\n\ntest accuracy\n\n\n\n\nNote that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.\nCongratulations for finishing this assignment! And also for revolutionizing French football. :-)\n\nWhat we want you to remember from this notebook: - Regularization will help you reduce overfitting. - Regularization will drive your weights to lower values. - L2 regularization and Dropout are two very effective regularization techniques."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html",
    "href": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html",
    "title": "Gradient Checking",
    "section": "",
    "text": "Welcome to the final assignment for this week! In this assignment you’ll be implementing gradient checking.\nBy the end of this notebook, you’ll be able to:\nImplement gradient checking to verify the accuracy of your backprop implementation."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html#important-note-on-submission-to-the-autograder",
    "title": "Gradient Checking",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html#table-of-contents",
    "href": "nb/coursera/deep_learning_2/W1A3/Gradient_Checking.html#table-of-contents",
    "title": "Gradient Checking",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Problem Statement\n3 - How does Gradient Checking work?\n4 - 1-Dimensional Gradient Checking\n\nExercise 1 - forward_propagation\nExercise 2 - backward_propagation\nExercise 3 - gradient_check\n\n5 - N-Dimensional Gradient Checking\n\nExercise 4 - gradient_check_n\n\n\n ## 1 - Packages\n\n### v1.1\n\n\nimport numpy as np\nfrom testCases import *\nfrom public_tests import *\nfrom gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Problem Statement\nYou are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud–whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user’s account has been taken over by a hacker.\nYou already know that backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company’s CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, “Give me proof that your backpropagation is actually working!” To give this reassurance, you are going to use “gradient checking.”\nLet’s do it!\n ## 3 - How does Gradient Checking work? Backpropagation computes the gradients \\(\\frac{\\partial J}{\\partial \\theta}\\), where \\(\\theta\\) denotes the parameters of the model. \\(J\\) is computed using forward propagation and your loss function.\nBecause forward propagation is relatively easy to implement, you’re confident you got that right, and so you’re almost 100% sure that you’re computing the cost \\(J\\) correctly. Thus, you can use your code for computing \\(J\\) to verify the code for computing \\(\\frac{\\partial J}{\\partial \\theta}\\).\nLet’s look back at the definition of a derivative (or gradient):\\[ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}\\]\nIf you’re not familiar with the “\\(\\displaystyle \\lim_{\\varepsilon \\to 0}\\)” notation, it’s just a way of saying “when \\(\\varepsilon\\) is really, really small.”\nYou know the following:\n\\(\\frac{\\partial J}{\\partial \\theta}\\) is what you want to make sure you’re computing correctly. You can compute \\(J(\\theta + \\varepsilon)\\) and \\(J(\\theta - \\varepsilon)\\) (in the case that \\(\\theta\\) is a real number), since you’re confident your implementation for \\(J\\) is correct. Let’s use equation (1) and a small value for \\(\\varepsilon\\) to convince your CEO that your code for computing \\(\\frac{\\partial J}{\\partial \\theta}\\) is correct!\n ## 4 - 1-Dimensional Gradient Checking\nConsider a 1D linear function \\(J(\\theta) = \\theta x\\). The model contains only a single real-valued parameter \\(\\theta\\), and takes \\(x\\) as input.\nYou will implement code to compute \\(J(.)\\) and its derivative \\(\\frac{\\partial J}{\\partial \\theta}\\). You will then use gradient checking to make sure your derivative computation for \\(J\\) is correct.\n\n\n\nFigure 1:1D linear model \n\n\nThe diagram above shows the key computation steps: First start with \\(x\\), then evaluate the function \\(J(x)\\) (“forward propagation”). Then compute the derivative \\(\\frac{\\partial J}{\\partial \\theta}\\) (“backward propagation”).\n ### Exercise 1 - forward_propagation\nImplement forward propagation. For this simple function compute \\(J(.)\\)\n\n# GRADED FUNCTION: forward_propagation\n\ndef forward_propagation(x, theta):\n    \"\"\"\n    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n    \n    Arguments:\n    x -- a real-valued input\n    theta -- our parameter, a real number as well\n    \n    Returns:\n    J -- the value of function J, computed using the formula J(theta) = theta * x\n    \"\"\"\n    \n    # (approx. 1 line)\n    # J = \n    # YOUR CODE STARTS HERE\n    J = x*theta\n    \n    # YOUR CODE ENDS HERE\n    \n    return J\n\n\nx, theta = 2, 4\nJ = forward_propagation(x, theta)\nprint (\"J = \" + str(J))\nforward_propagation_test(forward_propagation)\n\nJ = 8\n All tests passed.\n\n\n ### Exercise 2 - backward_propagation\nNow, implement the backward propagation step (derivative computation) of Figure 1. That is, compute the derivative of \\(J(\\theta) = \\theta x\\) with respect to \\(\\theta\\). To save you from doing the calculus, you should get \\(dtheta = \\frac { \\partial J }{ \\partial \\theta} = x\\).\n\n# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(x, theta):\n    \"\"\"\n    Computes the derivative of J with respect to theta (see Figure 1).\n    \n    Arguments:\n    x -- a real-valued input\n    theta -- our parameter, a real number as well\n    \n    Returns:\n    dtheta -- the gradient of the cost with respect to theta\n    \"\"\"\n    \n    # (approx. 1 line)\n    # dtheta = \n    # YOUR CODE STARTS HERE\n    dtheta = x\n    \n    # YOUR CODE ENDS HERE\n    \n    return dtheta\n\n\nx, theta = 3, 4\ndtheta = backward_propagation(x, theta)\nprint (\"dtheta = \" + str(dtheta))\nbackward_propagation_test(backward_propagation)\n\ndtheta = 3\n All tests passed.\n\n\n\nExpected output:\ndtheta = 3\n All tests passed.\n ### Exercise 3 - gradient_check\nTo show that the backward_propagation() function is correctly computing the gradient \\(\\frac{\\partial J}{\\partial \\theta}\\), let’s implement gradient checking.\nInstructions: - First compute “gradapprox” using the formula above (1) and a small value of \\(\\varepsilon\\). Here are the Steps to follow: 1. \\(\\theta^{+} = \\theta + \\varepsilon\\) 2. \\(\\theta^{-} = \\theta - \\varepsilon\\) 3. \\(J^{+} = J(\\theta^{+})\\) 4. \\(J^{-} = J(\\theta^{-})\\) 5. \\(gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}\\) - Then compute the gradient using backward propagation, and store the result in a variable “grad” - Finally, compute the relative difference between “gradapprox” and the “grad” using the following formula: \\[ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}\\] You will need 3 Steps to compute this formula: - 1’. compute the numerator using np.linalg.norm(…) - 2’. compute the denominator. You will need to call np.linalg.norm(…) twice. - 3’. divide them. - If this difference is small (say less than \\(10^{-7}\\)), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation.\n\n# GRADED FUNCTION: gradient_check\n\ndef gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n    \"\"\"\n    Implement the gradient checking presented in Figure 1.\n    \n    Arguments:\n    x -- a float input\n    theta -- our parameter, a float as well\n    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n    \n    Returns:\n    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output\n    \"\"\"\n    \n    # Compute gradapprox using right side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n    # (approx. 5 lines)\n    # theta_plus =                                 # Step 1\n    # theta_minus =                                # Step 2\n    # J_plus =                                    # Step 3\n    # J_minus =                                   # Step 4\n    # gradapprox =                                # Step 5\n    # YOUR CODE STARTS HERE\n    theta_plus = theta+epsilon\n    theta_minus = theta-epsilon\n    J_plus = forward_propagation(x, theta_plus)\n    J_minus = forward_propagation(x, theta_minus)\n    gradapprox = (J_plus-J_minus)/(2*epsilon)\n    # YOUR CODE ENDS HERE\n    \n    # Check if gradapprox is close enough to the output of backward_propagation()\n    #(approx. 1 line) DO NOT USE \"grad = gradapprox\"\n    # grad =\n    # YOUR CODE STARTS HERE\n    grad = backward_propagation(x, theta)\n    \n    # YOUR CODE ENDS HERE\n    \n    #(approx. 3 lines)\n    # numerator =                                 # Step 1'\n    # denominator =                               # Step 2'\n    # difference =                                # Step 3'\n    # YOUR CODE STARTS HERE\n    numerator = np.linalg.norm(grad-gradapprox)\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n    difference = numerator/denominator\n    \n    # YOUR CODE ENDS HERE\n    if print_msg:\n        if difference &gt; 2e-7:\n            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n        else:\n            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n    \n    return difference\n\n\nx, theta = 3, 4\ndifference = gradient_check(x, theta, print_msg=True)\n\nYour backward propagation works perfectly fine! difference = 7.814075313343006e-11\n\n\nExpected output:\n\n\n\n Your backward propagation works perfectly fine!\n\n\ndifference = 7.814075313343006e-11\n\n\n\nCongrats, the difference is smaller than the \\(2 * 10^{-7}\\) threshold. So you can have high confidence that you’ve correctly computed the gradient in backward_propagation().\nNow, in the more general case, your cost function \\(J\\) has more than a single 1D input. When you are training a neural network, \\(\\theta\\) actually consists of multiple matrices \\(W^{[l]}\\) and biases \\(b^{[l]}\\)! It is important to know how to do a gradient check with higher-dimensional inputs. Let’s do it!\n ## 5 - N-Dimensional Gradient Checking\nThe following figure describes the forward and backward propagation of your fraud detection model.\n\n\n\nFigure 2: Deep neural network. LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n\n\nLet’s look at your implementations for forward_propagation and backward_propagation.\nBelow is the code provided for forward_propagation_n. Note here that n in the name implies it is for n-dimensions\n\ndef forward_propagation_n(X, Y, parameters):\n    \"\"\"\n    Implements the forward propagation (and computes the cost) presented in Figure 3.\n    \n    Arguments:\n    X -- training set for m examples\n    Y -- labels for m examples \n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (5, 4)\n                    b1 -- bias vector of shape (5, 1)\n                    W2 -- weight matrix of shape (3, 5)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    \n    Returns:\n    cost -- the cost function (logistic cost for m examples)\n    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n\n    \"\"\"\n    \n    # retrieve parameters\n    m = X.shape[1]\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n\n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n\n    # Cost\n    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n    cost = 1. / m * np.sum(log_probs)\n    \n    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return cost, cache\n\nNow, let’s look at the code for backward propagation.\nBelow is the code provided for backward_propagation_n. Note here that n in the name implies it is for n-dimensions\n\ndef backward_propagation_n(X, Y, cache):\n    \"\"\"\n    Implement the backward propagation presented in figure 2.\n    \n    Arguments:\n    X -- input datapoint, of shape (input size, 1)\n    Y -- true \"label\"\n    cache -- cache output from forward_propagation_n()\n    \n    Returns:\n    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1. / m * np.dot(dZ3, A2.T)\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = 1. / m * np.dot(dZ2, A1.T)\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = 1. / m * np.dot(dZ1, X.T)\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\nIf you had just implemented these functions, you might not have high confidence whether they work correctly. So let’s implement gradient checking to help verify the performance.\nHow does gradient checking work?.\nAs in Section 3 and 4, you want to compare “gradapprox” to the gradient computed by backpropagation. The formula is still:\n\\[ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}\\]\nHowever, \\(\\theta\\) is not a scalar anymore. It is a dictionary called “parameters”. The function “dictionary_to_vector()” has been implemented for you. It converts the “parameters” dictionary into a vector called “values”, obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3) into vectors and concatenating them.\nThe inverse function is “vector_to_dictionary” which outputs back the “parameters” dictionary.\n\n\n\nFigure 2: dictionary_to_vector() and vector_to_dictionary(). You will need these functions in gradient_check_n()\n\n\nThe “gradients” dictionary has also been converted into a vector “grad” using gradients_to_vector(), so you don’t need to worry about that.\nNow, for every single parameter in your vector, you will apply the same procedure as for the gradient_check exercise. You will store each gradient approximation in a vector gradapprox. If the check goes as expected, each value in this approximation must match the real gradient values stored in the grad vector.\nNote that grad is calculated using the function gradients_to_vector, which uses the gradients outputs of the backward_propagation_n function.\n ### Exercise 4 - gradient_check_n\nImplement the function below.\nInstructions: Here is pseudo-code that will help you implement the gradient check.\nFor each i in num_parameters: - To compute J_plus[i]: 1. Set \\(\\theta^{+}\\) to np.copy(parameters_values) 2. Set \\(\\theta^{+}_i\\) to \\(\\theta^{+}_i + \\varepsilon\\) 3. Calculate \\(J^{+}_i\\) using to forward_propagation_n(x, y, vector_to_dictionary(\\(\\theta^{+}\\) )).\n- To compute J_minus[i]: do the same thing with \\(\\theta^{-}\\) - Compute \\(gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}\\)\nThus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to parameter_values[i]. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1’, 2’, 3’), compute: \\[ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}\\]\nNote: Use np.linalg.norm to get the norms\n\n# GRADED FUNCTION: gradient_check_n\n\ndef gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n    \"\"\"\n    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters \n    X -- input datapoint, of shape (input size, number of examples)\n    Y -- true \"label\"\n    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n    \n    Returns:\n    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n    \"\"\"\n    \n    # Set-up variables\n    parameters_values, _ = dictionary_to_vector(parameters)\n    \n    grad = gradients_to_vector(gradients)\n    num_parameters = parameters_values.shape[0]\n    J_plus = np.zeros((num_parameters, 1))\n    J_minus = np.zeros((num_parameters, 1))\n    gradapprox = np.zeros((num_parameters, 1))\n    \n    # Compute gradapprox\n    for i in range(num_parameters):\n        \n        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n        # \"_\" is used because the function you have outputs two parameters but we only care about the first one\n        #(approx. 3 lines)\n        # theta_plus =                                        # Step 1\n        # theta_plus[i] =                                     # Step 2\n        # J_plus[i], _ =                                     # Step 3\n        # YOUR CODE STARTS HERE\n        theta_plus = np.copy(parameters_values)\n        theta_plus[i] = theta_plus[i]+epsilon\n        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n        \n        # YOUR CODE ENDS HERE\n        \n        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n        #(approx. 3 lines)\n        # theta_minus =                                    # Step 1\n        # theta_minus[i] =                                 # Step 2        \n        # J_minus[i], _ =                                 # Step 3\n        # YOUR CODE STARTS HERE\n        theta_minus = np.copy(parameters_values)\n        theta_minus[i] = theta_minus[i]-epsilon\n        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))       \n        \n        # YOUR CODE ENDS HERE\n        \n        # Compute gradapprox[i]\n        # (approx. 1 line)\n        # gradapprox[i] = \n        # YOUR CODE STARTS HERE\n        gradapprox[i] = (J_plus[i]-J_minus[i])/(2*epsilon)\n        \n        # YOUR CODE ENDS HERE\n    \n    # Compare gradapprox to backward propagation gradients by computing difference.\n    # (approx. 3 line)\n    # numerator =                                             # Step 1'\n    # denominator =                                           # Step 2'\n    # difference =                                            # Step 3'\n    # YOUR CODE STARTS HERE\n    numerator = np.linalg.norm(grad-gradapprox)\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n    difference = numerator/denominator\n    # YOUR CODE ENDS HERE\n    if print_msg:\n        if difference &gt; 2e-7:\n            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n        else:\n            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n\n    return difference\n\n\nX, Y, parameters = gradient_check_n_test_case()\n\ncost, cache = forward_propagation_n(X, Y, parameters)\ngradients = backward_propagation_n(X, Y, cache)\ndifference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)\nexpected_values = [0.2850931567761623, 1.1890913024229996e-07]\nassert not(type(difference) == np.ndarray), \"You are not using np.linalg.norm for numerator or denominator\"\nassert np.any(np.isclose(difference, expected_values)), \"Wrong value. It is not one of the expected values\"\n\nYour backward propagation works perfectly fine! difference = 1.1890913024229996e-07\n\n\nExpected output:\n\n\n\n There is a mistake in the backward propagation!\n\n\ndifference = 0.2850931567761623\n\n\n\nIt seems that there were errors in the backward_propagation_n code! Good thing you’ve implemented the gradient check. Go back to backward_propagation_n and try to find/correct the errors (Hint: check dW2 and db1). Rerun the gradient check when you think you’ve fixed it. Remember, you’ll need to re-execute the cell defining backward_propagation_n() if you modify the code.\nCan you get gradient check to declare your derivative computation correct? Even though this part of the assignment isn’t graded, you should try to find the bug and re-run gradient check until you’re convinced backprop is now correctly implemented.\nNotes - Gradient Checking is slow! Approximating the gradient with \\(\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}\\) is computationally costly. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. - Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.\nCongrats! Now you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :)  \nWhat you should remember from this notebook: - Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation). - Gradient checking is slow, so you don’t want to run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A1/Initialization.html",
    "href": "nb/coursera/deep_learning_2/W1A1/Initialization.html",
    "title": "Initialization",
    "section": "",
    "text": "Welcome to the first assignment of Improving Deep Neural Networks!\nTraining your neural network requires specifying an initial value of the weights. A well-chosen initialization method helps the learning process.\nIf you completed the previous course of this specialization, you probably followed the instructions for weight initialization, and seen that it’s worked pretty well so far. But how do you choose the initialization for a new neural network? In this notebook, you’ll try out a few different initializations, including random, zeros, and He initialization, and see how each leads to different results.\nA well-chosen initialization can: - Speed up the convergence of gradient descent - Increase the odds of gradient descent converging to a lower training (and generalization) error\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A1/Initialization.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_2/W1A1/Initialization.html#important-note-on-submission-to-the-autograder",
    "title": "Initialization",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_2/W1A1/Initialization.html#table-of-contents",
    "href": "nb/coursera/deep_learning_2/W1A1/Initialization.html#table-of-contents",
    "title": "Initialization",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Loading the Dataset\n3 - Neural Network Model\n4 - Zero Initialization\n\nExercise 1 - initialize_parameters_zeros\n\n5 - Random Initialization\n\nExercise 2 - initialize_parameters_random\n\n6 - He Initialization\n\nExercise 3 - initialize_parameters_he\n\n7 - Conclusions\n\n ## 1 - Packages\n\n### v1.1\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nfrom public_tests import *\nfrom init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\nfrom init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n# load image dataset: blue/red dots in circles\n# train_X, train_Y, test_X, test_Y = load_dataset()\n\n ## 2 - Loading the Dataset\n\ntrain_X, train_Y, test_X, test_Y = load_dataset()\n\n\n\n\n\n\n\n\nFor this classifier, you want to separate the blue dots from the red dots.\n ## 3 - Neural Network Model\nYou’ll use a 3-layer neural network (already implemented for you). These are the initialization methods you’ll experiment with: - Zeros initialization – setting initialization = \"zeros\" in the input argument. - Random initialization – setting initialization = \"random\" in the input argument. This initializes the weights to large random values.\n- He initialization – setting initialization = \"he\" in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.\nInstructions: Instructions: Read over the code below, and run it. In the next part, you’ll implement the three initialization methods that this model() calls.\n\ndef model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n    learning_rate -- learning rate for gradient descent \n    num_iterations -- number of iterations to run gradient descent\n    print_cost -- if True, print the cost every 1000 iterations\n    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n    \n    Returns:\n    parameters -- parameters learnt by the model\n    \"\"\"\n        \n    grads = {}\n    costs = [] # to keep track of the loss\n    m = X.shape[1] # number of examples\n    layers_dims = [X.shape[0], 10, 5, 1]\n    \n    # Initialize parameters dictionary.\n    if initialization == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        a3, cache = forward_propagation(X, parameters)\n        \n        # Loss\n        cost = compute_loss(a3, Y)\n\n        # Backward propagation.\n        grads = backward_propagation(X, Y, cache)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n            costs.append(cost)\n            \n    # plot the loss\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\n ## 4 - Zero Initialization\nThere are two types of parameters to initialize in a neural network: - the weight matrices \\((W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})\\) - the bias vectors \\((b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})\\)\n ### Exercise 1 - initialize_parameters_zeros\nImplement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry,” but try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes.\n\n# GRADED FUNCTION: initialize_parameters_zeros \n\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    parameters = {}\n    L = len(layers_dims)            # number of layers in the network\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] = \n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        \n        # YOUR CODE ENDS HERE\n    return parameters\n\n\nparameters = initialize_parameters_zeros([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_zeros_test(initialize_parameters_zeros)\n\nW1 = [[0. 0. 0.]\n [0. 0. 0.]]\nb1 = [[0.]\n [0.]]\nW2 = [[0. 0.]]\nb2 = [[0.]]\n Error: Datatype mismatch\n Error: Wrong shape\n Error: Wrong output\n 0  Tests passed\n 3  Tests failed\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[6], line 6\n      4 print(\"W2 = \" + str(parameters[\"W2\"]))\n      5 print(\"b2 = \" + str(parameters[\"b2\"]))\n----&gt; 6 initialize_parameters_zeros_test(initialize_parameters_zeros)\n\nFile ~/c/lnu/2023-2024.2/dl_nlp/coursera/deep_learning_2/W1A1/public_tests.py:98, in initialize_parameters_zeros_test(target)\n     70    expected_output = {'W1': np.array([[0., 0., 0.],\n     71        [0., 0., 0.]]),\n     72 'b1': np.array([[0.],\n     73        [0.]]),\n     74 'W2': np.array([[0., 0.]]),\n     75 'b2': np.array([[0.]])}\n     77    test_cases = [\n     78        {\n     79            \"name\":\"datatype_check\",\n   (...)\n     95        }\n     96    ]\n---&gt; 98    multiple_test(test_cases, target)\n\nFile ~/c/lnu/2023-2024.2/dl_nlp/coursera/deep_learning_2/W1A1/public_tests.py:62, in multiple_test(test_cases, target)\n     60 print('\\033[92m', success, \" Tests passed\")\n     61 print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n---&gt; 62 raise AssertionError(\n     63     \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n\nAssertionError: Not all tests were passed for initialize_parameters_zeros. Check your equations and avoid using global variables inside the function.\n\n\n\nRun the following code to train your model on 15,000 iterations using zeros initialization.\n\nparameters = model(train_X, train_Y, initialization = \"zeros\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nThe performance is terrible, the cost doesn’t decrease, and the algorithm performs no better than random guessing. Why? Take a look at the details of the predictions and the decision boundary:\n\nprint (\"predictions_train = \" + str(predictions_train))\nprint (\"predictions_test = \" + str(predictions_test))\n\n\nplt.title(\"Model with Zeros initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nFor a comprehensive explanation of this, you can read Paul Mielke’s post, Symmetry Breaking versus Zero Initialization.\nA simple explanation is provided below:\nNote: For sake of simplicity calculations below are done using only one example at a time.\nSince the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As z = 0\n\\[a = ReLU(z) = max(0, z) = 0\\]\nAt the classification layer, where the activation function is sigmoid you then get (for either input):\n\\[\\sigma(z) = \\frac{1}{ 1 + e^{-(z)}} = \\frac{1}{2} = y_{pred}\\]\nAs for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.\nYour loss function: \\[ \\mathcal{L}(a, y) =  - y  \\ln(y_{pred}) - (1-y)  \\ln(1-y_{pred})\\]\nFor y=1, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 1) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nFor y=0, y_pred=0.5 it becomes:\n\\[ \\mathcal{L}(0, 0) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453\\]\nAs you can see with the prediction being 0.5 whether the actual (y) value is 1 or 0 you get the same loss value for both, so none of the weights get adjusted and you are stuck with the same old value of the weights.\nThis is why you can see that the model is predicting 0 for every example! No wonder it’s doing so badly.\nIn general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with \\(n^{[l]}=1\\) for every layer. This way, the network is no more powerful than a linear classifier like logistic regression.\n\nWhat you should remember: - The weights \\(W^{[l]}\\) should be initialized randomly to break symmetry. - However, it’s okay to initialize the biases \\(b^{[l]}\\) to zeros. Symmetry is still broken so long as \\(W^{[l]}\\) is initialized randomly.\n ## 5 - Random Initialization\nTo break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you’ll see what happens when the weights are initialized randomly, but to very large values.\n ### Exercise 2 - initialize_parameters_random\nImplement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use np.random.randn(..,..) * 10 for weights and np.zeros((.., ..)) for biases. You’re using a fixed np.random.seed(..) to make sure your “random” weights match ours, so don’t worry if running your code several times always gives you the same initial values for the parameters.\n\n# GRADED FUNCTION: initialize_parameters_random\n\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*10\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        \n        # YOUR CODE ENDS HERE\n\n    return parameters\n\n\nparameters = initialize_parameters_random([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_random_test(initialize_parameters_random)\n\nRun the following code to train your model on 15,000 iterations using random initialization.\n\nparameters = model(train_X, train_Y, initialization = \"random\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nIf you see “inf” as the cost after the iteration 0, this is because of numerical roundoff. A more numerically sophisticated implementation would fix this, but for the purposes of this notebook, it isn’t really worth worrying about.\nIn any case, you’ve now broken the symmetry, and this gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!\n\nprint (predictions_train)\nprint (predictions_test)\n\n\nplt.title(\"Model with large random initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nObservations: - The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when \\(\\log(a^{[3]}) = \\log(0)\\), the loss goes to infinity. - Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. - If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n\nIn summary: - Initializing weights to very large random values doesn’t work well. - Initializing with small random values should do better. The important question is, how small should be these random values be? Let’s find out up next!\n\nOptional Read:\nThe main difference between Gaussian variable (numpy.random.randn()) and uniform random variable is the distribution of the generated random numbers:\n\nnumpy.random.rand() produces numbers in a uniform distribution.\nand numpy.random.randn() produces numbers in a normal distribution.\n\nWhen used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.\nAn intuitive way to see it is, for example, if you take the sigmoid() activation function.\nYou’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.\n ## 6 - He Initialization\nFinally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights \\(W^{[l]}\\) of sqrt(1./layers_dims[l-1]) where He initialization would use sqrt(2./layers_dims[l-1]).)\n ### Exercise 3 - initialize_parameters_he\nImplement the following function to initialize your parameters with He initialization. This function is similar to the previous initialize_parameters_random(...). The only difference is that instead of multiplying np.random.randn(..,..) by 10, you will multiply it by \\(\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}\\), which is what He initialization recommends for layers with a ReLU activation.\n\n# GRADED FUNCTION: initialize_parameters_he\n\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = \n        # parameters['b' + str(l)] =\n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))  \n        \n        # YOUR CODE ENDS HERE\n        \n    return parameters\n\n\nparameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_he_test(initialize_parameters_he)\n# parameters\n\nExpected output\nW1 = [[ 1.78862847  0.43650985]\n [ 0.09649747 -1.8634927 ]\n [-0.2773882  -0.35475898]\n [-0.08274148 -0.62700068]]\nb1 = [[0.] [0.] [0.] [0.]]\nW2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\nb2 = [[0.]]\nRun the following code to train your model on 15,000 iterations using He initialization.\n\nparameters = model(train_X, train_Y, initialization = \"he\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\n\nplt.title(\"Model with He initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\nObservations: - The model with He initialization separates the blue and the red dots very well in a small number of iterations.\n ## 7 - Conclusions\nYou’ve tried three different types of initializations. For the same number of iterations and same hyperparameters, the comparison is:\n&lt;td&gt;\n    3-layer NN with zeros initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    50%\n    &lt;/td&gt;\n    &lt;td&gt;\n    fails to break symmetry\n    &lt;/td&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with large random initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    83%\n    &lt;/td&gt;\n    &lt;td&gt;\n    too large weights \n    &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;\n    3-layer NN with He initialization\n    &lt;/td&gt;\n    &lt;td&gt;\n    99%\n    &lt;/td&gt;\n    &lt;td&gt;\n    recommended method\n    &lt;/td&gt;\n&lt;/tr&gt;\n\n\nModel\n\n\nTrain accuracy\n\n\nProblem/Comment\n\n\n\n\nCongratulations! You’ve completed this notebook on Initialization.\nHere’s a quick recap of the main takeaways:\n\n\nDifferent initializations lead to very different results\nRandom initialization is used to break symmetry and make sure different hidden units can learn different things\nResist initializing to values that are too large!\nHe initialization works well for networks with ReLU activations"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "href": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "",
    "text": "Welcome to your week 4 assignment (part 1 of 2)! Previously you trained a 2-layer Neural Network with a single hidden layer. This week, you will build a deep neural network with as many layers as you want!\nBy the end of this assignment, you’ll be able to:\nNotation: - Superscript \\([l]\\) denotes a quantity associated with the \\(l^{th}\\) layer. - Example: \\(a^{[L]}\\) is the \\(L^{th}\\) layer activation. \\(W^{[L]}\\) and \\(b^{[L]}\\) are the \\(L^{th}\\) layer parameters. - Superscript \\((i)\\) denotes a quantity associated with the \\(i^{th}\\) example. - Example: \\(x^{(i)}\\) is the \\(i^{th}\\) training example. - Lowerscript \\(i\\) denotes the \\(i^{th}\\) entry of a vector. - Example: \\(a^{[l]}_i\\) denotes the \\(i^{th}\\) entry of the \\(l^{th}\\) layer’s activations).\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html#important-note-on-submission-to-the-autograder",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html#table-of-contents",
    "href": "nb/coursera/deep_learning_1/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.html#table-of-contents",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Outline\n3 - Initialization\n\n3.1 - 2-layer Neural Network\n\nExercise 1 - initialize_parameters\n\n3.2 - L-layer Neural Network\n\nExercise 2 - initialize_parameters_deep\n\n\n4 - Forward Propagation Module\n\n4.1 - Linear Forward\n\nExercise 3 - linear_forward\n\n4.2 - Linear-Activation Forward\n\nExercise 4 - linear_activation_forward\n\n4.3 - L-Layer Model\n\nExercise 5 - L_model_forward\n\n\n5 - Cost Function\n\nExercise 6 - compute_cost\n\n6 - Backward Propagation Module\n\n6.1 - Linear Backward\n\nExercise 7 - linear_backward\n\n6.2 - Linear-Activation Backward\n\nExercise 8 - linear_activation_backward\n\n6.3 - L-Model Backward\n\nExercise 9 - L_model_backward\n\n6.4 - Update Parameters\n\nExercise 10 - update_parameters\n\n\n\n ## 1 - Packages\nFirst, import all the packages you’ll need during this assignment.\n\nnumpy is the main package for scientific computing with Python.\nmatplotlib is a library to plot graphs in Python.\ndnn_utils provides some necessary functions for this notebook.\ntestCases provides some test cases to assess the correctness of your functions\nnp.random.seed(1) is used to keep all the random function calls consistent. It helps grade your work. Please don’t change the seed!\n\n\n### v1.1\n\n\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom testCases import *\nfrom dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\nfrom public_tests import *\n\nimport copy\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\n\n ## 2 - Outline\nTo build your neural network, you’ll be implementing several “helper functions.” These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network.\nEach small helper function will have detailed instructions to walk you through the necessary steps. Here’s an outline of the steps in this assignment:\n\nInitialize the parameters for a two-layer network and for an \\(L\\)-layer neural network\nImplement the forward propagation module (shown in purple in the figure below)\n\nComplete the LINEAR part of a layer’s forward propagation step (resulting in \\(Z^{[l]}\\)).\nThe ACTIVATION function is provided for you (relu/sigmoid)\nCombine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.\nStack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer \\(L\\)). This gives you a new L_model_forward function.\n\nCompute the loss\nImplement the backward propagation module (denoted in red in the figure below)\n\nComplete the LINEAR part of a layer’s backward propagation step\nThe gradient of the ACTIVATION function is provided for you(relu_backward/sigmoid_backward)\nCombine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function\nStack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function\n\nFinally, update the parameters\n\n\n\n\nFigure 1\n\n\n\nNote:\nFor every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.\nIn the backpropagation module, you can then use the cache to calculate the gradients. Don’t worry, this assignment will show you exactly how to carry out each of these steps!\n ## 3 - Initialization\nYou will write two helper functions to initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one generalizes this initialization process to \\(L\\) layers.\n ### 3.1 - 2-layer Neural Network\n ### Exercise 1 - initialize_parameters\nCreate and initialize the parameters of the 2-layer neural network.\nInstructions:\n\nThe model’s structure is: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\nUse this random initialization for the weight matrices: np.random.randn(d0, d1, ..., dn) * 0.01 with the correct shape. The documentation for np.random.randn\nUse zero initialization for the biases: np.zeros(shape). The documentation for np.zeros\n\n\n# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = np.random.randn(n_h, n_x)*0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h)*0.01\n    b2 = np.zeros((n_y, 1))\n    \n    # YOUR CODE ENDS HERE\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters    \n\n\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters(3,2,1)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_1(initialize_parameters)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters(4,3,2)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_2(initialize_parameters)\n\nTest Case 1:\n\nW1 = [[ 0.01624345 -0.00611756 -0.00528172]\n [-0.01072969  0.00865408 -0.02301539]]\nb1 = [[0.]\n [0.]]\nW2 = [[ 0.01744812 -0.00761207]]\nb2 = [[0.]]\n All tests passed.\n\nTest Case 2:\n\nW1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]\n [ 0.00865408 -0.02301539  0.01744812 -0.00761207]\n [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]\nb1 = [[0.]\n [0.]\n [0.]]\nW2 = [[-0.00322417 -0.00384054  0.01133769]\n [-0.01099891 -0.00172428 -0.00877858]]\nb2 = [[0.]\n [0.]]\n All tests passed.\n\n\nExpected output\nTest Case 1:\n\nW1 = [[ 0.01624345 -0.00611756 -0.00528172]\n [-0.01072969  0.00865408 -0.02301539]]\nb1 = [[0.]\n [0.]]\nW2 = [[ 0.01744812 -0.00761207]]\nb2 = [[0.]]\n All tests passed.\n\nTest Case 2:\n\nW1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]\n [ 0.00865408 -0.02301539  0.01744812 -0.00761207]\n [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]\nb1 = [[0.]\n [0.]\n [0.]]\nW2 = [[-0.00322417 -0.00384054  0.01133769]\n [-0.01099891 -0.00172428 -0.00877858]]\nb2 = [[0.]\n [0.]]\n All tests passed.\n ### 3.2 - L-layer Neural Network\nThe initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the initialize_parameters_deep function, you should make sure that your dimensions match between each layer. Recall that \\(n^{[l]}\\) is the number of units in layer \\(l\\). For example, if the size of your input \\(X\\) is \\((12288, 209)\\) (with \\(m=209\\) examples) then:\n\n\n\n\n\nShape of W\n\n\nShape of b\n\n\nActivation\n\n\nShape of Activation\n\n\n\n\nLayer 1\n\n\n\\((n^{[1]},12288)\\)\n\n\n\\((n^{[1]},1)\\)\n\n\n$Z^{[1]} = W^{[1]} X + b^{[1]} $\n\n\n\\((n^{[1]},209)\\)\n\n\n\n\nLayer 2\n\n\n\\((n^{[2]}, n^{[1]})\\)\n\n\n\\((n^{[2]},1)\\)\n\n\n\\(Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\)\n\n\n\\((n^{[2]}, 209)\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\nLayer L-1\n\n\n\\((n^{[L-1]}, n^{[L-2]})\\)\n\n\n\\((n^{[L-1]}, 1)\\)\n\n\n\\(Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}\\)\n\n\n\\((n^{[L-1]}, 209)\\)\n\n\n\n\nLayer L\n\n\n\\((n^{[L]}, n^{[L-1]})\\)\n\n\n\\((n^{[L]}, 1)\\)\n\n\n\\(Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}\\)\n\n\n\\((n^{[L]}, 209)\\)\n\n\n\nRemember that when you compute \\(W X + b\\) in python, it carries out broadcasting. For example, if:\n\\[ W = \\begin{bmatrix}\n    w_{00}  & w_{01} & w_{02} \\\\\n    w_{10}  & w_{11} & w_{12} \\\\\n    w_{20}  & w_{21} & w_{22}\n\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n    x_{00}  & x_{01} & x_{02} \\\\\n    x_{10}  & x_{11} & x_{12} \\\\\n    x_{20}  & x_{21} & x_{22}\n\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n    b_0  \\\\\n    b_1  \\\\\n    b_2\n\\end{bmatrix}\\tag{2}\\]\nThen \\(WX + b\\) will be:\n\\[ WX + b = \\begin{bmatrix}\n    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n\\end{bmatrix}\\tag{3}  \\]\n ### Exercise 2 - initialize_parameters_deep\nImplement initialization for an L-layer Neural Network.\nInstructions: - The model’s structure is [LINEAR -&gt; RELU] $ $ (L-1) -&gt; LINEAR -&gt; SIGMOID. I.e., it has \\(L-1\\) layers using a ReLU activation function followed by an output layer with a sigmoid activation function. - Use random initialization for the weight matrices. Use np.random.randn(d0, d1, ..., dn) * 0.01. - Use zeros initialization for the biases. Use np.zeros(shape). - You’ll store \\(n^{[l]}\\), the number of units in different layers, in a variable layer_dims. For example, the layer_dims for last week’s Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means W1’s shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to \\(L\\) layers! - Here is the implementation for \\(L=1\\) (one layer neural network). It should inspire you to implement the general case (L-layer neural network).\n    if L == 1:\n        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n\n# GRADED FUNCTION: initialize_parameters_deep\n\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = ...\n        # parameters['b' + str(l)] = ...\n        # YOUR CODE STARTS HERE\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        # YOUR CODE ENDS HERE\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\n\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters_deep([5,4,3])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_deep_test_1(initialize_parameters_deep)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters_deep([4,3,2])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_deep_test_2(initialize_parameters_deep)\n\nTest Case 1:\n\nW1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n [-0.00768836 -0.00230031  0.00745056  0.01976111]]\nb2 = [[0.]\n [0.]\n [0.]]\n All tests passed.\n\nTest Case 2:\n\nW1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493]\n [-0.00277388 -0.00354759 -0.00082741 -0.00627001]\n [-0.00043818 -0.00477218 -0.01313865  0.00884622]]\nb1 = [[0.]\n [0.]\n [0.]]\nW2 = [[ 0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477]]\nb2 = [[0.]\n [0.]]\n All tests passed.\n\n\nExpected output\nTest Case 1:\n\nW1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n [-0.00768836 -0.00230031  0.00745056  0.01976111]]\nb2 = [[0.]\n [0.]\n [0.]]\n All tests passed.\n\nTest Case 2:\n\nW1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493]\n [-0.00277388 -0.00354759 -0.00082741 -0.00627001]\n [-0.00043818 -0.00477218 -0.01313865  0.00884622]]\nb1 = [[0.]\n [0.]\n [0.]]\nW2 = [[ 0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477]]\nb2 = [[0.]\n [0.]]\n All tests passed.\n ## 4 - Forward Propagation Module\n ### 4.1 - Linear Forward\nNow that you have initialized your parameters, you can do the forward propagation module. Start by implementing some basic functions that you can use again later when implementing the model. Now, you’ll complete three functions in this order:\n\nLINEAR\nLINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.\n[LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)\n\nThe linear forward module (vectorized over all the examples) computes the following equations:\n\\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}\\]\nwhere \\(A^{[0]} = X\\).\n ### Exercise 3 - linear_forward\nBuild the linear part of forward propagation.\nReminder: The mathematical representation of this unit is \\(Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\). You may also find np.dot() useful. If your dimensions don’t match, printing W.shape may help.\n\n# GRADED FUNCTION: linear_forward\n\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    # Z = ...\n    # YOUR CODE STARTS HERE\n    Z = np.dot(W, A) + b\n    \n    # YOUR CODE ENDS HERE\n    cache = (A, W, b)\n    \n    return Z, cache\n\n\nt_A, t_W, t_b = linear_forward_test_case()\nt_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\nprint(\"Z = \" + str(t_Z))\n\nlinear_forward_test(linear_forward)\n\nZ = [[ 3.26295337 -1.23429987]]\n All tests passed.\n\n\nExpected output\nZ = [[ 3.26295337 -1.23429987]]\n ### 4.2 - Linear-Activation Forward\nIn this notebook, you will use two activation functions:\n\nSigmoid: \\(\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}\\). You’ve been provided with the sigmoid function which returns two items: the activation value “a” and a “cache” that contains “Z” (it’s what we will feed in to the corresponding backward function). To use it you could just call:\n\nA, activation_cache = sigmoid(Z)\n\nReLU: The mathematical formula for ReLu is \\(A = RELU(Z) = max(0, Z)\\). You’ve been provided with the relu function. This function returns two items: the activation value “A” and a “cache” that contains “Z” (it’s what you’ll feed in to the corresponding backward function). To use it you could just call:\n\nA, activation_cache = relu(Z)\nFor added convenience, you’re going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you’ll implement a function that does the LINEAR forward step, followed by an ACTIVATION forward step.\n ### Exercise 4 - linear_activation_forward\nImplement the forward propagation of the LINEAR-&gt;ACTIVATION layer. Mathematical relation is: \\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\\) where the activation “g” can be sigmoid() or relu(). Use linear_forward() and the correct activation function.\n\n# GRADED FUNCTION: linear_activation_forward\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # YOUR CODE STARTS HERE\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n        # YOUR CODE ENDS HERE\n    \n    elif activation == \"relu\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # YOUR CODE STARTS HERE\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n        \n        # YOUR CODE ENDS HERE\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\n\nt_A_prev, t_W, t_b = linear_activation_forward_test_case()\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\nprint(\"With sigmoid: A = \" + str(t_A))\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\nprint(\"With ReLU: A = \" + str(t_A))\n\nlinear_activation_forward_test(linear_activation_forward)\n\nWith sigmoid: A = [[0.96890023 0.11013289]]\nWith ReLU: A = [[3.43896131 0.        ]]\n All tests passed.\n\n\nExpected output\nWith sigmoid: A = [[0.96890023 0.11013289]]\nWith ReLU: A = [[3.43896131 0.        ]]\nNote: In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.\n ### 4.3 - L-Layer Model\nFor even more convenience when implementing the \\(L\\)-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with RELU) \\(L-1\\) times, then follows that with one linear_activation_forward with SIGMOID.\n\n\n\nFigure 2 : [LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID model\n\n\n\n ### Exercise 5 - L_model_forward\nImplement the forward propagation of the above model.\nInstructions: In the code below, the variable AL will denote \\(A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})\\). (This is sometimes also called Yhat, i.e., this is \\(\\hat{Y}\\).)\nHints: - Use the functions you’ve previously written - Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times - Don’t forget to keep track of the caches in the “caches” list. To add a new value c to a list, you can use list.append(c).\n\n# GRADED FUNCTION: L_model_forward\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    # The for loop starts at 1 because layer 0 is the input\n    for l in range(1, L):\n        A_prev = A \n        #(≈ 2 lines of code)\n        # A, cache = ...\n        # caches ...\n        # YOUR CODE STARTS HERE\n        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n        caches.append(cache)\n        # YOUR CODE ENDS HERE\n    \n    # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.\n    #(≈ 2 lines of code)\n    # AL, cache = ...\n    # caches ...\n    # YOUR CODE STARTS HERE\n    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n    caches.append(cache)\n    # YOUR CODE ENDS HERE\n          \n    return AL, caches\n\n\nt_X, t_parameters = L_model_forward_test_case_2hidden()\nt_AL, t_caches = L_model_forward(t_X, t_parameters)\n\nprint(\"AL = \" + str(t_AL))\n\nL_model_forward_test(L_model_forward)\n\nAL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n All tests passed.\n\n\nExpected output\nAL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\nAwesome! You’ve implemented a full forward propagation that takes the input X and outputs a row vector \\(A^{[L]}\\) containing your predictions. It also records all intermediate values in “caches”. Using \\(A^{[L]}\\), you can compute the cost of your predictions.\n ## 5 - Cost Function\nNow you can implement forward and backward propagation! You need to compute the cost, in order to check whether your model is actually learning.\n ### Exercise 6 - compute_cost Compute the cross-entropy cost \\(J\\), using the following formula: \\[-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}\\]\n\n# GRADED FUNCTION: compute_cost\n\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    # (≈ 1 lines of code)\n    # cost = ...\n    # YOUR CODE STARTS HERE\n    cost = -np.sum(np.multiply(np.log(AL), Y) + np.multiply(1-Y, np.log(1-AL)))/m\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n\n    \n    return cost\n\n\nt_Y, t_AL = compute_cost_test_case()\nt_cost = compute_cost(t_AL, t_Y)\n\nprint(\"Cost: \" + str(t_cost))\n\ncompute_cost_test(compute_cost)\n\nCost: 0.2797765635793423\n All tests passed.\n\n\nExpected Output:\n\n\n\ncost\n\n\n0.2797765635793422\n\n\n\n ## 6 - Backward Propagation Module\nJust as you did for the forward propagation, you’ll implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\nReminder: \n\n\nFigure 3: Forward and Backward propagation for LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID  The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.\n\n\n\nNow, similarly to forward propagation, you’re going to build the backward propagation in three steps: 1. LINEAR backward 2. LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation 3. [LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)\nFor the next exercise, you will need to remember that:\n\nb is a matrix(np.ndarray) with 1 column and n rows, i.e: b = [[1.0], [2.0]] (remember that b is a constant)\nnp.sum performs a sum over the elements of a ndarray\naxis=1 or axis=0 specify if the sum is carried out by rows or by columns respectively\nkeepdims specifies if the original dimensions of the matrix must be kept.\nLook at the following example to clarify:\n\n\nA = np.array([[1, 2], [3, 4]])\n\nprint('axis=1 and keepdims=True')\nprint(np.sum(A, axis=1, keepdims=True))\nprint('axis=1 and keepdims=False')\nprint(np.sum(A, axis=1, keepdims=False))\nprint('axis=0 and keepdims=True')\nprint(np.sum(A, axis=0, keepdims=True))\nprint('axis=0 and keepdims=False')\nprint(np.sum(A, axis=0, keepdims=False))\n\naxis=1 and keepdims=True\n[[3]\n [7]]\naxis=1 and keepdims=False\n[3 7]\naxis=0 and keepdims=True\n[[4 6]]\naxis=0 and keepdims=False\n[4 6]\n\n\n ### 6.1 - Linear Backward\nFor layer \\(l\\), the linear part is: \\(Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\\) (followed by an activation).\nSuppose you have already calculated the derivative \\(dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}\\). You want to get \\((dW^{[l]}, db^{[l]}, dA^{[l-1]})\\).\n\n\n\nFigure 4\n\n\nThe three outputs \\((dW^{[l]}, db^{[l]}, dA^{[l-1]})\\) are computed using the input \\(dZ^{[l]}\\).\nHere are the formulas you need: \\[ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}\\] \\[ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}\\] \\[ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}\\]\n\\(A^{[l-1] T}\\) is the transpose of \\(A^{[l-1]}\\).\n ### Exercise 7 - linear_backward\nUse the 3 formulas above to implement linear_backward().\nHint:\n\nIn numpy you can get the transpose of an ndarray A using A.T or A.transpose()\n\n\n# GRADED FUNCTION: linear_backward\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    ### START CODE HERE ### (≈ 3 lines of code)\n    # dW = ...\n    # db = ... sum by the rows of dZ with keepdims=True\n    # dA_prev = ...\n    # YOUR CODE STARTS HERE\n    dW = np.dot(dZ, cache[0].T)/m\n    db = np.sum(dZ, axis=1, keepdims=True)/m\n    dA_prev = np.dot(cache[1].T, dZ)\n    \n    # YOUR CODE ENDS HERE\n    \n    return dA_prev, dW, db\n\n\nt_dZ, t_linear_cache = linear_backward_test_case()\nt_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n\nprint(\"dA_prev: \" + str(t_dA_prev))\nprint(\"dW: \" + str(t_dW))\nprint(\"db: \" + str(t_db))\n\nlinear_backward_test(linear_backward)\n\ndA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n [-0.4319552  -1.30987417  1.72354705  0.05070578]\n [-0.38981415  0.60811244 -1.25938424  1.47191593]\n [-2.52214926  2.67882552 -0.67947465  1.48119548]]\ndW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\ndb: [[-0.14713786]\n [-0.11313155]\n [-0.13209101]]\n All tests passed.\n\n\nExpected Output:\ndA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n [-0.4319552  -1.30987417  1.72354705  0.05070578]\n [-0.38981415  0.60811244 -1.25938424  1.47191593]\n [-2.52214926  2.67882552 -0.67947465  1.48119548]]\ndW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\ndb: [[-0.14713786]\n [-0.11313155]\n [-0.13209101]]\n ### 6.2 - Linear-Activation Backward\nNext, you will create a function that merges the two helper functions: linear_backward and the backward step for the activation linear_activation_backward.\nTo help you implement linear_activation_backward, two backward functions have been provided: - sigmoid_backward: Implements the backward propagation for SIGMOID unit. You can call it as follows:\ndZ = sigmoid_backward(dA, activation_cache)\n\nrelu_backward: Implements the backward propagation for RELU unit. You can call it as follows:\n\ndZ = relu_backward(dA, activation_cache)\nIf \\(g(.)\\) is the activation function, sigmoid_backward and relu_backward compute \\[dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{11}\\]\n ### Exercise 8 - linear_activation_backward\nImplement the backpropagation for the LINEAR-&gt;ACTIVATION layer.\n\n# GRADED FUNCTION: linear_activation_backward\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        #(≈ 2 lines of code)\n        # dZ =  ...\n        # dA_prev, dW, db =  ...\n        # YOUR CODE STARTS HERE\n        dZ = relu_backward(dA, cache[1])\n        dA_prev, dW, db = linear_backward(dZ, cache[0])\n        \n        # YOUR CODE ENDS HERE\n        \n    elif activation == \"sigmoid\":\n        #(≈ 2 lines of code)\n        # dZ =  ...\n        # dA_prev, dW, db =  ...\n        # YOUR CODE STARTS HERE\n        dZ = sigmoid_backward(dA, cache[1])\n        dA_prev, dW, db = linear_backward(dZ, cache[0])\n        \n        # YOUR CODE ENDS HERE\n    \n    return dA_prev, dW, db\n\n\nt_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n\nt_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\nprint(\"With sigmoid: dA_prev = \" + str(t_dA_prev))\nprint(\"With sigmoid: dW = \" + str(t_dW))\nprint(\"With sigmoid: db = \" + str(t_db))\n\nt_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\nprint(\"With relu: dA_prev = \" + str(t_dA_prev))\nprint(\"With relu: dW = \" + str(t_dW))\nprint(\"With relu: db = \" + str(t_db))\n\nlinear_activation_backward_test(linear_activation_backward)\n\nWith sigmoid: dA_prev = [[ 0.11017994  0.01105339]\n [ 0.09466817  0.00949723]\n [-0.05743092 -0.00576154]]\nWith sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]\nWith sigmoid: db = [[-0.05729622]]\nWith relu: dA_prev = [[ 0.44090989  0.        ]\n [ 0.37883606  0.        ]\n [-0.2298228   0.        ]]\nWith relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]\nWith relu: db = [[-0.20837892]]\n All tests passed.\n\n\nExpected output:\nWith sigmoid: dA_prev = [[ 0.11017994  0.01105339]\n [ 0.09466817  0.00949723]\n [-0.05743092 -0.00576154]]\nWith sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]\nWith sigmoid: db = [[-0.05729622]]\nWith relu: dA_prev = [[ 0.44090989  0.        ]\n [ 0.37883606  0.        ]\n [-0.2298228   0.        ]]\nWith relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]\nWith relu: db = [[-0.20837892]]\n ### 6.3 - L-Model Backward\nNow you will implement the backward function for the whole network!\nRecall that when you implemented the L_model_forward function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you’ll use those variables to compute the gradients. Therefore, in the L_model_backward function, you’ll iterate through all the hidden layers backward, starting from layer \\(L\\). On each step, you will use the cached values for layer \\(l\\) to backpropagate through layer \\(l\\). Figure 5 below shows the backward pass.\n\n\n\nFigure 5: Backward pass\n\n\nInitializing backpropagation:\nTo backpropagate through this network, you know that the output is: \\(A^{[L]} = \\sigma(Z^{[L]})\\). Your code thus needs to compute dAL \\(= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}\\). To do so, use this formula (derived using calculus which, again, you don’t need in-depth knowledge of!):\ndAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\nYou can then use this post-activation gradient dAL to keep going backward. As seen in Figure 5, you can now feed in dAL into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function).\nAfter that, you will have to use a for loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :\n\\[grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} \\]\nFor example, for \\(l=3\\) this would store \\(dW^{[l]}\\) in grads[\"dW3\"].\n ### Exercise 9 - L_model_backward\nImplement backpropagation for the [LINEAR-&gt;RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID model.\n\n# GRADED FUNCTION: L_model_backward\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    #(1 line of code)\n    # dAL = ...\n    # YOUR CODE STARTS HERE\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    # YOUR CODE ENDS HERE\n    \n    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n    #(approx. 5 lines)\n    # current_cache = ...\n    # dA_prev_temp, dW_temp, db_temp = ...\n    # grads[\"dA\" + str(L-1)] = ...\n    # grads[\"dW\" + str(L)] = ...\n    # grads[\"db\" + str(L)] = ...\n    # YOUR CODE STARTS HERE\n    current_cache = caches[L-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    # YOUR CODE ENDS HERE\n    \n    # Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -&gt; LINEAR) gradients.\n        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n        #(approx. 5 lines)\n        # current_cache = ...\n        # dA_prev_temp, dW_temp, db_temp = ...\n        # grads[\"dA\" + str(l)] = ...\n        # grads[\"dW\" + str(l + 1)] = ...\n        # grads[\"db\" + str(l + 1)] = ...\n        # YOUR CODE STARTS HERE\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l+1)] = dW_temp\n        grads[\"db\" + str(l+1)] = db_temp       \n        \n        # YOUR CODE ENDS HERE\n\n    return grads\n\n\nt_AL, t_Y_assess, t_caches = L_model_backward_test_case()\ngrads = L_model_backward(t_AL, t_Y_assess, t_caches)\n\nprint(\"dA0 = \" + str(grads['dA0']))\nprint(\"dA1 = \" + str(grads['dA1']))\nprint(\"dW1 = \" + str(grads['dW1']))\nprint(\"dW2 = \" + str(grads['dW2']))\nprint(\"db1 = \" + str(grads['db1']))\nprint(\"db2 = \" + str(grads['db2']))\n\nL_model_backward_test(L_model_backward)\n\ndA0 = [[ 0.          0.52257901]\n [ 0.         -0.3269206 ]\n [ 0.         -0.32070404]\n [ 0.         -0.74079187]]\ndA1 = [[ 0.12913162 -0.44014127]\n [-0.14175655  0.48317296]\n [ 0.01663708 -0.05670698]]\ndW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n [0.         0.         0.         0.        ]\n [0.05283652 0.01005865 0.01777766 0.0135308 ]]\ndW2 = [[-0.39202432 -0.13325855 -0.04601089]]\ndb1 = [[-0.22007063]\n [ 0.        ]\n [-0.02835349]]\ndb2 = [[0.15187861]]\n All tests passed.\n\n\nExpected output:\ndA0 = [[ 0.          0.52257901]\n [ 0.         -0.3269206 ]\n [ 0.         -0.32070404]\n [ 0.         -0.74079187]]\ndA1 = [[ 0.12913162 -0.44014127]\n [-0.14175655  0.48317296]\n [ 0.01663708 -0.05670698]]\ndW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n [0.         0.         0.         0.        ]\n [0.05283652 0.01005865 0.01777766 0.0135308 ]]\ndW2 = [[-0.39202432 -0.13325855 -0.04601089]]\ndb1 = [[-0.22007063]\n [ 0.        ]\n [-0.02835349]]\ndb2 = [[0.15187861]]\n ### 6.4 - Update Parameters\nIn this section, you’ll update the parameters of the model, using gradient descent:\n\\[ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}\\] \\[ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}\\]\nwhere \\(\\alpha\\) is the learning rate.\nAfter computing the updated parameters, store them in the parameters dictionary.\n ### Exercise 10 - update_parameters\nImplement update_parameters() to update your parameters using gradient descent.\nInstructions: Update parameters using gradient descent on every \\(W^{[l]}\\) and \\(b^{[l]}\\) for \\(l = 1, 2, ..., L\\).\n\n# GRADED FUNCTION: update_parameters\n\ndef update_parameters(params, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    params -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    parameters = copy.deepcopy(params)\n    L = len(parameters) // 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    #(≈ 2 lines of code)\n    for l in range(L):\n        # parameters[\"W\" + str(l+1)] = ...\n        # parameters[\"b\" + str(l+1)] = ...\n        # YOUR CODE STARTS HERE\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+str(l+1)]        \n        \n        # YOUR CODE ENDS HERE\n    return parameters\n\n\nt_parameters, grads = update_parameters_test_case()\nt_parameters = update_parameters(t_parameters, grads, 0.1)\n\nprint (\"W1 = \"+ str(t_parameters[\"W1\"]))\nprint (\"b1 = \"+ str(t_parameters[\"b1\"]))\nprint (\"W2 = \"+ str(t_parameters[\"W2\"]))\nprint (\"b2 = \"+ str(t_parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nW1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n [-1.0535704  -0.86128581  0.68284052  2.20374577]]\nb1 = [[-0.04659241]\n [-1.28888275]\n [ 0.53405496]]\nW2 = [[-0.55569196  0.0354055   1.32964895]]\nb2 = [[-0.84610769]]\n All tests passed.\n\n\nExpected output:\nW1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n [-1.0535704  -0.86128581  0.68284052  2.20374577]]\nb1 = [[-0.04659241]\n [-1.28888275]\n [ 0.53405496]]\nW2 = [[-0.55569196  0.0354055   1.32964895]]\nb2 = [[-0.84610769]]\n\nCongratulations!\nYou’ve just implemented all the functions required for building a deep neural network, including:\n\nUsing non-linear units improve your model\nBuilding a deeper neural network (with more than 1 hidden layer)\nImplementing an easy-to-use neural network class\n\nThis was indeed a long assignment, but the next part of the assignment is easier. ;)\nIn the next assignment, you’ll be putting all these together to build two models:\n\nA two-layer neural network\nAn L-layer neural network\n\nYou will in fact use these models to classify cat vs non-cat images! (Meow!) Great work and see you next time."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html",
    "href": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html",
    "title": "Python Basics with Numpy (optional assignment)",
    "section": "",
    "text": "Welcome to your first assignment. This exercise gives you a brief introduction to Python. Even if you’ve used Python before, this will help familiarize you with the functions we’ll need.\nInstructions: - You will be using Python 3. - Avoid using for-loops and while-loops, unless you are explicitly told to do so. - After coding your function, run the cell right below it to check if your result is correct.\nAfter this assignment you will: - Be able to use iPython Notebooks - Be able to use numpy functions and numpy matrix/vector operations - Understand the concept of “broadcasting” - Be able to vectorize code\nLet’s get started!"
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html#important-note-on-submission-to-the-autograder",
    "href": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html#important-note-on-submission-to-the-autograder",
    "title": "Python Basics with Numpy (optional assignment)",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html#table-of-contents",
    "href": "nb/coursera/deep_learning_1/W2A1/Python_Basics_with_Numpy.html#table-of-contents",
    "title": "Python Basics with Numpy (optional assignment)",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nAbout iPython Notebooks\n\nExercise 1\n\n1 - Building basic functions with numpy\n\n1.1 - sigmoid function, np.exp()\n\nExercise 2 - basic_sigmoid\nExercise 3 - sigmoid\n\n1.2 - Sigmoid Gradient\n\nExercise 4 - sigmoid_derivative\n\n1.3 - Reshaping arrays\n\nExercise 5 - image2vector\n\n1.4 - Normalizing rows\n\nExercise 6 - normalize_rows\nExercise 7 - softmax\n\n\n2 - Vectorization\n\n2.1 Implement the L1 and L2 loss functions\n\nExercise 8 - L1\nExercise 9 - L2\n\n\n\n ## About iPython Notebooks ##\niPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the # your code here comment. After writing your code, you can run the cell by either pressing “SHIFT”+“ENTER” or by clicking on “Run Cell” (denoted by a play symbol) in the upper bar of the notebook.\nWe will often specify “(≈ X lines of code)” in the comments to tell you about how much code you need to write. It is just a rough estimate, so don’t feel bad if your code is longer or shorter.\n\n### v1.2\n\n ### Exercise 1 Set test to \"Hello World\" in the cell below to print “Hello World” and run the two cells below.\n\n# (≈ 1 line of code)\n\n# YOUR CODE STARTS HERE\ntest = \"Hello world\"\n\n# YOUR CODE ENDS HERE\n\n\nprint (\"test: \" + test)\n\ntest: Hello world\n\n\nExpected output: test: Hello World\n What you need to remember :\n\nRun your cells using SHIFT+ENTER (or “Run cell”)\nWrite code in the designated areas using Python 3 only\nDo not modify the code outside of the designated areas\n\n ## 1 - Building basic functions with numpy ##\nNumpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments.\n ### 1.1 - sigmoid function, np.exp() ###\nBefore using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n ### Exercise 2 - basic_sigmoid Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\nReminder: \\(sigmoid(x) = \\frac{1}{1+e^{-x}}\\) is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n\nTo refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp().\n\nimport math\nfrom public_tests import *\n\n# GRADED FUNCTION: basic_sigmoid\n\ndef basic_sigmoid(x):\n    \"\"\"\n    Compute sigmoid of x.\n\n    Arguments:\n    x -- A scalar\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    # (≈ 1 line of code)\n    \n    # YOUR CODE STARTS HERE\n    s = 1/(1+math.exp(-x)) \n    \n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nprint(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))\n\nbasic_sigmoid_test(basic_sigmoid)\n\nbasic_sigmoid(1) = 0.7310585786300049\n All tests passed.\n\n\nActually, we rarely use the “math” library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful.\n\n### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n\nx = [1, 2, 3] # x becomes a python list object\nbasic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-27-f93a844e80fe&gt; in &lt;module&gt;\n      2 \n      3 x = [1, 2, 3] # x becomes a python list object\n----&gt; 4 basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.\n\n&lt;ipython-input-25-b3029fc7646f&gt; in basic_sigmoid(x)\n     17 \n     18     # YOUR CODE STARTS HERE\n---&gt; 19     s = 1/(1+math.exp(-x))\n     20 \n     21     # YOUR CODE ENDS HERE\n\nTypeError: bad operand type for unary -: 'list'\n\n\n\nIn fact, if $ x = (x_1, x_2, …, x_n)$ is a row vector then np.exp(x) will apply the exponential function to every element of x. The output will thus be: np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})\n\nimport numpy as np\n\n# example of np.exp\nt_x = np.array([1, 2, 3])\nprint(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))\n\n[ 2.71828183  7.3890561  20.08553692]\n\n\nFurthermore, if x is a vector, then a Python operation such as \\(s = x + 3\\) or \\(s = \\frac{1}{x}\\) will output s as a vector of the same size as x.\n\n# example of vector operation\nt_x = np.array([1, 2, 3])\nprint (t_x + 3)\n\n[4 5 6]\n\n\nAny time you need more info on a numpy function, we encourage you to look at the official documentation.\nYou can also create a new cell in the notebook and write np.exp? (for example) to get quick access to the documentation.\n ### Exercise 3 - sigmoid Implement the sigmoid function using numpy.\nInstructions: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don’t need to know more for now. \\[ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n    x_1  \\\\\n    x_2  \\\\\n    ...  \\\\\n    x_n  \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n    \\frac{1}{1+e^{-x_1}}  \\\\\n    \\frac{1}{1+e^{-x_2}}  \\\\\n    ...  \\\\\n    \\frac{1}{1+e^{-x_n}}  \\\\\n\\end{pmatrix}\\tag{1} \\]\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(x):\n    \"\"\"\n    Compute the sigmoid of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    \n    # (≈ 1 line of code)\n    \n    # YOUR CODE STARTS HERE\n    s = 1/(1+np.exp(-x))\n    \n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nt_x = np.array([1, 2, 3])\nprint(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))\n\nsigmoid_test(sigmoid)\n\nsigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n All tests passed.\n\n\n ### 1.2 - Sigmoid Gradient\nAs you’ve seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let’s code your first gradient function.\n ### Exercise 4 - sigmoid_derivative Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is:\n\\[sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}\\]\nYou often code this function in two steps: 1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful. 2. Compute \\(\\sigma'(x) = s(1-s)\\)\n\n# GRADED FUNCTION: sigmoid_derivative\n\ndef sigmoid_derivative(x):\n    \"\"\"\n    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n    \n    Arguments:\n    x -- A scalar or numpy array\n\n    Return:\n    ds -- Your computed gradient.\n    \"\"\"\n    \n    #(≈ 2 lines of code)\n\n    # YOUR CODE STARTS HERE\n    s = sigmoid(x)\n    ds = s*(1-s)   \n    \n    # YOUR CODE ENDS HERE\n    \n    return ds\n\n\nt_x = np.array([1, 2, 3])\nprint (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))\n\nsigmoid_derivative_test(sigmoid_derivative)\n\nsigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n All tests passed.\n\n\n ### 1.3 - Reshaping arrays ###\nTwo common numpy functions used in deep learning are np.shape and np.reshape(). - X.shape is used to get the shape (dimension) of a matrix/vector X. - X.reshape(…) is used to reshape X into some other dimension.\nFor example, in computer science, an image is represented by a 3D array of shape \\((length, height, depth = 3)\\). However, when you read an image as the input of an algorithm you convert it to a vector of shape \\((length*height*3, 1)\\). In other words, you “unroll”, or reshape, the 3D array into a 1D vector.\n\n ### Exercise 5 - image2vector Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\nv = v.reshape((v.shape[0] * v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n\nPlease don’t hardcode the dimensions of image as a constant. Instead look up the quantities you need with image.shape[0], etc.\nYou can use v = v.reshape(-1, 1). Just make sure you understand why it works.\n\n\n# GRADED FUNCTION:image2vector\n\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n    \n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    \n    # (≈ 1 line of code)\n    \n    # YOUR CODE STARTS HERE\n    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2], 1))\n    \n    # YOUR CODE ENDS HERE\n    \n    return v\n\n\n# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\nt_image = np.array([[[ 0.67826139,  0.29380381],\n                     [ 0.90714982,  0.52835647],\n                     [ 0.4215251 ,  0.45017551]],\n\n                   [[ 0.92814219,  0.96677647],\n                    [ 0.85304703,  0.52351845],\n                    [ 0.19981397,  0.27417313]],\n\n                   [[ 0.60659855,  0.00533165],\n                    [ 0.10820313,  0.49978937],\n                    [ 0.34144279,  0.94630077]]])\n\nprint (\"image2vector(image) = \" + str(image2vector(t_image)))\n\nimage2vector_test(image2vector)\n\nimage2vector(image) = [[0.67826139]\n [0.29380381]\n [0.90714982]\n [0.52835647]\n [0.4215251 ]\n [0.45017551]\n [0.92814219]\n [0.96677647]\n [0.85304703]\n [0.52351845]\n [0.19981397]\n [0.27417313]\n [0.60659855]\n [0.00533165]\n [0.10820313]\n [0.49978937]\n [0.34144279]\n [0.94630077]]\n All tests passed.\n\n\n ### 1.4 - Normalizing rows\nAnother common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ $ (dividing each row vector of x by its norm).\nFor example, if \\[x = \\begin{bmatrix}\n        0 & 3 & 4 \\\\\n        2 & 6 & 4 \\\\\n\\end{bmatrix}\\tag{3}\\] then \\[\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n    5 \\\\\n    \\sqrt{56} \\\\\n\\end{bmatrix}\\tag{4} \\] and \\[ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n\\end{bmatrix}\\tag{5}\\]\nNote that you can divide matrices of different sizes and it works fine: this is called broadcasting and you’re going to learn about it in part 5.\nWith keepdims=True the result will broadcast correctly against the original x.\naxis=1 means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set axis=0.\nnumpy.linalg.norm has another parameter ord where we specify the type of normalization to be done (in the exercise below you’ll do 2-norm). To get familiar with the types of normalization you can visit numpy.linalg.norm\n ### Exercise 6 - normalize_rows Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).\n\n# GRADED FUNCTION: normalize_rows\n\ndef normalize_rows(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n    \n    Argument:\n    x -- A numpy matrix of shape (n, m)\n    \n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    \n    #(≈ 2 lines of code)\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    \n    # Divide x by its norm.\n    \n    # YOUR CODE STARTS HERE\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n    x = x/x_norm\n    # YOUR CODE ENDS HERE\n\n    return x\n\n\nx = np.array([[0., 3., 4.],\n              [1., 6., 4.]])\nprint(\"normalizeRows(x) = \" + str(normalize_rows(x)))\n\nnormalizeRows_test(normalize_rows)\n\nnormalizeRows(x) = [[0.         0.6        0.8       ]\n [0.13736056 0.82416338 0.54944226]]\n All tests passed.\n\n\nNote: In normalize_rows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You’ll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we’ll talk about it now!\n ### Exercise 7 - softmax Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\nInstructions: - \\(\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }\\)\n\\[\\begin{align*}\nsoftmax(x) &= softmax\\left(\\begin{bmatrix}\n    x_1  &&\n    x_2 &&\n    ...  &&\n    x_n  \n\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n    ...  &&\n    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}}\n\\end{bmatrix}\n\\end{align*}\\]\n\n\\(\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }\\)\n\n\\[\\begin{align*}\nsoftmax(x) &= softmax\\begin{bmatrix}\n            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n            \\end{bmatrix} \\\\ \\\\&=\n\\begin{bmatrix}\n    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n    softmax\\text{(first row of x)}  \\\\\n    softmax\\text{(second row of x)} \\\\\n    \\vdots  \\\\\n    softmax\\text{(last row of x)} \\\\\n\\end{pmatrix}\n\\end{align*}\\]\nNotes: Note that later in the course, you’ll see “m” used to represent the “number of training examples”, and each training example is in its own column of the matrix. Also, each feature will be in its own row (each row has data for the same feature).\nSoftmax should be performed for all features of each training example, so softmax would be performed on the columns (once we switch to that representation later in this course).\nHowever, in this coding practice, we’re just focusing on getting familiar with Python, so we’re using the common math notation \\(m \\times n\\)\nwhere \\(m\\) is the number of rows and \\(n\\) is the number of columns.\n\n# GRADED FUNCTION: softmax\n\ndef softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (m,n).\n\n    Argument:\n    x -- A numpy matrix of shape (m,n)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n    \"\"\"\n    \n    #(≈ 3 lines of code)\n    # Apply exp() element-wise to x. Use np.exp(...).\n    # x_exp = ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    # x_sum = ...\n    \n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    # s = ...\n    \n    # YOUR CODE STARTS HERE\n    x_exp = np.exp(x)\n    x_sum=np.sum(x_exp, axis=1, keepdims=True)\n    s = x_exp/x_sum\n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nt_x = np.array([[9, 2, 5, 0, 0],\n                [7, 5, 0, 0 ,0]])\nprint(\"softmax(x) = \" + str(softmax(t_x)))\n\nsoftmax_test(softmax)\n\nsoftmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n  1.21052389e-04]\n [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n  8.01252314e-04]]\n All tests passed.\n\n\n\nNotes\n\nIf you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). x_exp/x_sum works due to python broadcasting.\n\nCongratulations! You now have a pretty good understanding of python numpy and have implemented a few useful functions that you will be using in deep learning.\n What you need to remember:\n\nnp.exp(x) works for any np.array x and applies the exponential function to every coordinate\nthe sigmoid function and its gradient\nimage2vector is commonly used in deep learning\nnp.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.\nnumpy has efficient built-in functions\nbroadcasting is extremely useful\n\n ## 2 - Vectorization\nIn deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.\n\nimport time\n\nx1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\ntic = time.process_time()\ndot = 0\n\nfor i in range(len(x1)):\n    dot += x1[i] * x2[i]\ntoc = time.process_time()\nprint (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\n### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\ntic = time.process_time()\nouter = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n\nfor i in range(len(x1)):\n    for j in range(len(x2)):\n        outer[i,j] = x1[i] * x2[j]\ntoc = time.process_time()\nprint (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\n### CLASSIC ELEMENTWISE IMPLEMENTATION ###\ntic = time.process_time()\nmul = np.zeros(len(x1))\n\nfor i in range(len(x1)):\n    mul[i] = x1[i] * x2[i]\ntoc = time.process_time()\nprint (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\n### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\nW = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\ntic = time.process_time()\ngdot = np.zeros(W.shape[0])\n\nfor i in range(W.shape[0]):\n    for j in range(len(x1)):\n        gdot[i] += W[i,j] * x1[j]\ntoc = time.process_time()\nprint (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\ndot = 278\n ----- Computation time = 0.10518699999995107ms\nouter = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n ----- Computation time = 0.2146759999999137ms\nelementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n ----- Computation time = 0.11882400000007287ms\ngdot = [19.99988862 24.03445885 20.36375209]\n ----- Computation time = 0.18460800000008604ms\n\n\n\nx1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### VECTORIZED DOT PRODUCT OF VECTORS ###\ntic = time.process_time()\ndot = np.dot(x1,x2)\ntoc = time.process_time()\nprint (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\n### VECTORIZED OUTER PRODUCT ###\ntic = time.process_time()\nouter = np.outer(x1,x2)\ntoc = time.process_time()\nprint (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\n### VECTORIZED ELEMENTWISE MULTIPLICATION ###\ntic = time.process_time()\nmul = np.multiply(x1,x2)\ntoc = time.process_time()\nprint (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### VECTORIZED GENERAL DOT PRODUCT ###\ntic = time.process_time()\ndot = np.dot(W,x1)\ntoc = time.process_time()\nprint (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n\ndot = 278\n ----- Computation time = 0.452246000000045ms\nouter = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n ----- Computation time = 0.1794619999999636ms\nelementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n ----- Computation time = 0.07722999999981717ms\ngdot = [19.99988862 24.03445885 20.36375209]\n ----- Computation time = 0.9557090000000823ms\n\n\nAs you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger.\nNote that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication.\n ### 2.1 Implement the L1 and L2 loss functions\n ### Exercise 8 - L1 Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\nReminder: - The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\() are from the true values (\\)y\\(). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n- L1 loss is defined as:\\)\\(\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}\\)$\n\n# GRADED FUNCTION: L1\n\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    # loss = \n    # YOUR CODE STARTS HERE\n    \n    loss = np.sum(abs(yhat-y))\n    # YOUR CODE ENDS HERE\n    \n    return loss\n\n\nyhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L1 = \" + str(L1(yhat, y)))\n\nL1_test(L1)\n\nL1 = 1.1\n All tests passed.\n\n\n ### Exercise 9 - L2 Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if \\(x = [x_1, x_2, ..., x_n]\\), then np.dot(x,x) = \\(\\sum_{j=1}^n x_j^{2}\\).\n\nL2 loss is defined as \\[\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}\\]\n\n\n# GRADED FUNCTION: L2\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    \n    # YOUR CODE STARTS HERE\n    loss = np.sum(np.dot(yhat-y,yhat-y))\n    \n    # YOUR CODE ENDS HERE\n    \n    return loss\n\n\nyhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\n\nprint(\"L2 = \" + str(L2(yhat, y)))\n\nL2_test(L2)\n\nL2 = 0.43\n All tests passed.\n\n\nCongratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!\n What to remember:\n\nVectorization is very important in deep learning. It provides computational efficiency and clarity.\nYou have reviewed the L1 and L2 loss.\nYou are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…"
  },
  {
    "objectID": "nlp_lab4.html",
    "href": "nlp_lab4.html",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "nlp_lab4.html#plan",
    "href": "nlp_lab4.html#plan",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "dl_lab1.html",
    "href": "dl_lab1.html",
    "title": "DL: Lab 1",
    "section": "",
    "text": "Lab overview\nWe’ll implement a binary classifier using logistic regression, but via a neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lab2.html",
    "href": "dl_lab2.html",
    "title": "DL: Lab 2",
    "section": "",
    "text": "Lab overview\nWe’ll work with a planar data classifier using a two-layer neural network (using one hidden layer.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec3.html#deep-networks",
    "href": "dl_lec3.html#deep-networks",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep networks",
    "text": "Deep networks\nSuppose we have this network:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-1",
    "href": "dl_lec3.html#deep-networks-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nWhat happens in Hidden layer 1 and Output layer?\nWe will use this notation: \\[\\begin{align*}\n  & z^{[1]} = W^{[1]}x+b^{[1]} \\\\\n  & a^{[1]} = \\sigma(z^{[1]})\\\\\n  & z^{[2]} = W^{[2]}a^{[1]}+b^{[2]} \\\\\n  & a^{[2]} = \\sigma(z^{[2]})\n\\end{align*}\\] And then we compute \\(L(a^{[2]}, y)\\).\nAnd then, similarly, for backpropagation, we will compute \\(da^{[2]}, dz^{[2]}, dW^{[2]}\\).\n\n\nDo not confuse round and square brackets."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-2",
    "href": "dl_lec3.html#deep-networks-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nAlternative notation: \\(a^{[0]} = x\\). So in the picture \\[\\begin{align*}\n  a = \\begin{bmatrix}\n    a_1^{[1]} \\\\\n    a_2^{[1]}\\\\\n    a_3^{[1]}\\\\\n    a_4^{[1]}\n  \\end{bmatrix}\n\\end{align*}\\] 2-layer network (input layer not counted). Input layer is layer 0."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-3",
    "href": "dl_lec3.html#deep-networks-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\n\n\n\n\nFirst hidden layer params\n\n\n\n\\(W^{[1]}\\) ((4,3) matrix)\n\\(b^{[1]}\\) ((4,1) matrix).\n\n\n\n\n\n\n\nSecond hidden layer params\n\n\n\n\\(W^{[2]}\\) is a (1,4) matrix\n\\(b^{[2]}\\) is a (1,1) matrix."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-4",
    "href": "dl_lec3.html#deep-networks-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor all nodes: \\[\\begin{align*}\n  & z_1^{[1]} = W_1^{[1]T}x + b_1^{[1]},\\; & a_1^{[1]} = \\sigma(z_1^{[1]}), \\\\\n  & z_2^{[1]} = W_2^{[1]T}x + b_2^{[1]},\\; & a_2^{[1]} = \\sigma(z_2^{[1]}), \\\\\n  & z_3^{[1]} = W_3^{[1]T}x + b_3^{[1]},\\; & a_3^{[1]} = \\sigma(z_3^{[1]}), \\\\\n  & z_4^{[1]} = W_4^{[1]T}x + b_4^{[1]},\\; & a_4^{[1]} = \\sigma(z_4^{[1]})\n\\end{align*}\\] So, if we have \\(a_i^{[l]}\\), then \\(l\\) means layer, and \\(i\\) means node number in a layer."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-5",
    "href": "dl_lec3.html#deep-networks-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-6",
    "href": "dl_lec3.html#deep-networks-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor \\(m\\) training examples, we compute \\(x^{(m)} \\rightarrow a^{[2](m)} = \\hat{y}^{(m)}\\).\nThe block of code:\nfor i = 1 to m:\n\\[\\begin{align*}\n  &z^{[1](i)} = W^{[1]}x^{(i)}+b^{[1]},\\\\\n  &a^{[1](i)} = \\sigma(z^{[1](i)}),\\\\\n  &z^{[2](i)} = W^{[2]}a^{[1](i)}+b^{[2]},\\\\\n  &a^{[2](i)} = \\sigma(z^{[2](i)})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-7",
    "href": "dl_lec3.html#deep-networks-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we vectorize it across multiple training examples? We compute by going to matrices: \\[\\begin{align*}\n  &Z^{[1]} = W^{[1]}X+b^{[1]},\\; &A^{[1]} = \\sigma(Z^{[1]}),\\\\\n  &Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]},\\; &A^{[2]} = \\sigma(Z^{[2]})\n\\end{align*}\\]\n\\[\\begin{align*}\n&Z^{[1]} =\\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  z^{[1](1)} & z^{[1](2)} & \\dots & z^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix},\\\\\n&A^{[1]} = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  a^{[1](1)} & a^{[1](2)} & \\dots & a^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#activation-functions",
    "href": "dl_lec3.html#activation-functions",
    "title": "Deep learning: multi-layer NNs",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\n\n\n\n\n\n\n\nSigmoid derivative\n\n\n\\[\n\\dfrac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))\n\\]\n\n\n\n\n\n\nTanh derivative\n\n\n\\[\n\\dfrac{d\\tanh}{dz} = 1-(\\tanh(z))^2\n\\]\n\n\n\n\n\n\nReLU derivative\n\n\n\\[\\begin{align*}\n  &g'(z) = \\begin{cases}\n    0 , & \\text{if } z &lt; 0,\\\\\n    1, & \\text{if } z &gt; 0, \\\\\n    \\text{undefined}, & \\text{if } z = 0\n  \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-8",
    "href": "dl_lec3.html#deep-networks-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nParameters\n\n\nReview:\n\\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\).\n\\(n_x = n^{[0]}\\), \\(n^{[2]} = 1\\).\n\n\n\n\n\n\nCost function\n\n\nIn our case will be \\[\\begin{align*}\n  &J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})= \\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y},y).\n\\end{align*}\\]\n\n\n\nDimensions for \\(W^{[1]}\\) are \\((n^{[1]}, n^{[0]})\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-9",
    "href": "dl_lec3.html#deep-networks-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor gradient descent we compute\n\n\\(\\hat{y}^{(i)}\\)\n\\(dW^{[1]} \\equiv \\dfrac{dJ}{dW^{[1]}}, dB^{[1]} \\equiv \\dfrac{dJ}{dB^{[1]}}\\)\n\\(W^{[1]} = W^{[1]} - \\alpha dW^{[1]}\\)\n\\(B^{[1]} = B^{[1]} - \\alpha dB^{[1]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-10",
    "href": "dl_lec3.html#deep-networks-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we compute the derivatives? \\[\\begin{align*}\n  &dZ^{[2]} = A^{[2]} - Y,\\\\\n  &dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T},\\\\\n  &dB^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-11",
    "href": "dl_lec3.html#deep-networks-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nNext step: \\[\\begin{align*}\n  &dZ^{[1]} = W^{[2]T}dZ^{[2]} \\odot g^{[1]'}(Z^{[1]}),\\\\\n  &dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{T},\\\\\n  &dB^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-12",
    "href": "dl_lec3.html#deep-networks-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nForward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-13",
    "href": "dl_lec3.html#deep-networks-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nBackward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-14",
    "href": "dl_lec3.html#deep-networks-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nVectorized versions: \\[\\begin{align*}\n  &d\\vec{Z}^{[2]} = A^{[2]} - Y,\\\\\n  &d\\vec{W}^{[2]} = \\frac{1}{m}d\\vec{Z}^{[2]}\\vec{A}^{[1]T},\\\\\n  &d\\vec{b}^{[2]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[2]}, axis=1,keepdims=True),\\\\\n  &d\\vec{Z}^{[1]} = W^{[2]T}d\\vec{Z}^{[2]} \\odot g^{[1]'}(\\vec{Z}^{[1]}),\\\\\n  &d\\vec{W}^{[1]} = \\frac{1}{m}d\\vec{Z}^{[1]}\\vec{X}^{T},\\\\\n  &d\\vec{b}^{[1]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[1]}, axis=1,keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-random-initialization",
    "href": "dl_lec3.html#deep-networks-random-initialization",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks: Random Initialization",
    "text": "Deep Networks: Random Initialization\n\n\n\nSymmetry-breaking problem\n\n\nIf we initialize weights to zero, the hidden units will be symmetric. \\[\nW^{[1]} = np.random.randn((2,2))*0.01,\\\\\nb^{[1]} = np.zero((2,1))\n\\]\n\n\n\n\n\nThe 0.01 multiplier is because we don’t want to end up at flat parts of the activation function."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-15",
    "href": "dl_lec3.html#deep-networks-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\\(a^{[l]}\\) - activations in layer \\(l\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-16",
    "href": "dl_lec3.html#deep-networks-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nGeneral rule\n\n\n\\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\\), \\(a^{[l]} = g^{[l]}(z^{[l]})\\).\n\n\n\n\n\n\nVectorized versions\n\n\n\\[\\begin{align*}\n&Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}, \\\\\n&A^{[l]} = g^{[l]}(Z^{[l]})\n\\end{align*}\\]\n\n\n\n\n\n\nImportant\n\n\nFor loop is necessary for multiple layers."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-17",
    "href": "dl_lec3.html#deep-networks-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-18",
    "href": "dl_lec3.html#deep-networks-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-19",
    "href": "dl_lec3.html#deep-networks-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\nGetting your Matrix Dimensions Right\n\n\n\nDimensions of \\(W^{[l]}\\) are \\((n^{[l]}, n^{[l-1]})\\).\nDimensions of \\(b^{[l]}\\) should be \\((n^{[l]}, 1)\\).\nDimensions of dW and db should be identical to the ones for W and b.\nDimension of \\(Z^{[1]}\\) is \\((n^{[1]}, m)\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-20",
    "href": "dl_lec3.html#deep-networks-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\nIntuition from circuit theory\n\n\nSmall L-layer network requires exponentially less hidden units than shallower networks.\n\n\n\n\n\n\nExample\n\n\nTo compute XOR, we’ll need \\(O(\\log \\, n)\\) layers.\nWith a single hidden layer, we’ll need \\(2^{n-1}\\) hidden units."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-21",
    "href": "dl_lec3.html#deep-networks-21",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nForward propagation:\n\n\n\nInputs: \\(a^{[l-1]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(a^{[l]}\\)\nCache: \\(z^{[l]}\\)\n\n\n\n\n\n\n\nBackward propagation:\n\n\n\nInputs: \\(da^{[l]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(da^{[l-1]}\\)\nCache: \\(dz^{[l]}\\), \\(dW^{[l]}\\), \\(db^{[l]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-22",
    "href": "dl_lec3.html#deep-networks-22",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nBackward propagation steps:\n\n\n\\[\\begin{align*}\n  &dz^{[l]} = da^{[l]}\\odot g^{[l]'}(z^{[l]})\\\\\n  &dW^{[l]} = dz^{[l]} \\cdot (a^{[l-1]T})\\\\\n  &db^{[l]} = dz^{[l]} \\\\\n  &da^{[l-1]} = W^{[l]T} \\cdot dz^{[l]}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-23",
    "href": "dl_lec3.html#deep-networks-23",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nVectorized versions:\n\n\n\\[\\begin{align*}\n  &dZ^{[l]} = dA^{[l]}\\odot g^{[l]'}(Z^{[l]})\\\\\n  &dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot (A^{[l-1]T})\\\\\n  &db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\\\\n  &dA^{[l-1]} = W^{[l]T} \\cdot dZ^{[l]}\n\\end{align*}\\]\n\n\n\n\n\n\nFinal layer\n\n\n\\[\\begin{align*}\n  &da^{[l]} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#parameters-vs-hyperparameters",
    "href": "dl_lec3.html#parameters-vs-hyperparameters",
    "title": "Deep learning: multi-layer NNs",
    "section": "Parameters vs Hyperparameters",
    "text": "Parameters vs Hyperparameters\n\n\n\nParameters\n\n\n\n\\(W^{[i]}\\)\n\\(b^{[i]}\\)\n\n\n\n\n\n\n\nHyperparameters\n\n\n\nlearning rate\nn of iterations\nn of hidden layers (\\(L\\))\nn of hidden units (\\(n^{[i]}\\))\nchoice of activation functions"
  },
  {
    "objectID": "dl_lec3.html#backpropagation",
    "href": "dl_lec3.html#backpropagation",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align}\nz_{j, i}^{[l]} &= \\sum_k w_{j, k}^{[l]} a_{k, i}^{[l - 1]} + b_j^{[l]}, \\label{eq:z_scalar} \\\\\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}). \\label{eq:a_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-1",
    "href": "dl_lec3.html#backpropagation-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\n\nEntity\nDescription\n\n\n\n\n\\(l\\)\nThe current layer \\(l = 1, \\dots, L\\)\n\n\n\\(n^{[l]}\\)\nThe number of nodes in the current layer\n\n\n\\(n^{[l - 1]}\\)\nThe number of nodes in the previous layer\n\n\n\\(j\\)\nThe \\(j\\)-th node of the current layer, \\(j = 1, \\dots, n^{[l]}\\)\n\n\n\\(k\\)\nThe \\(k\\)-th node of the previous layer,\\(k = 1, \\dots, n^{[l - 1]}\\)\n\n\n\\(i\\)\nThe current training example \\(i = 1, \\dots, m\\)\n\n\n\\(z_{j, i}^{[l]}\\)\nA weighted sum of the activations of the previous layer"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-2",
    "href": "dl_lec3.html#backpropagation-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\n\nEntity\nDescription\n\n\n\n\n\\(w_{j, k}^{[l]}\\)\nA weight that scales the \\(k\\)-th activation of the previous layer\n\n\n\\(b_j^{[l]}\\)\nA bias in the current layer\n\n\n\\(a_{j, i}^{[l]}\\)\nAn activation in the current layer\n\n\n\\(a_{k, i}^{[l - 1]}\\)\nAn activation in the previous layer\n\n\n\\(g_j^{[l]}\\)\nAn activation function \\(g_j^{[l]} \\colon \\mathbb{R}^{n^{[l]}} \\to \\mathbb{R}\\)"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-3",
    "href": "dl_lec3.html#backpropagation-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe vectorize the nodes:\n\\[\\begin{align*}\n\\begin{bmatrix}\nz_{1, i}^{[l]} \\\\\n\\vdots \\\\\nz_{j, i}^{[l]} \\\\\n\\vdots \\\\\nz_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{1, k}^{[l]} & \\dots & w_{1, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{j, 1}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{n^{[l]}, 1}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix}\n\n\\begin{bmatrix}\na_{1, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{k, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{n^{[l - 1]}, i}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_1^{[l]} \\\\\n\\vdots \\\\\nb_j^{[l]} \\\\\n\\vdots \\\\\nb_{n^{[l]}}^{[l]}\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-4",
    "href": "dl_lec3.html#backpropagation-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align*}\n\\begin{bmatrix}\na_{1, i}^{[l]} \\\\\n\\vdots \\\\\na_{j, i}^{[l]} \\\\\n\\vdots \\\\\na_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\ng_1^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_{n^{[l]}}^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-5",
    "href": "dl_lec3.html#backpropagation-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nwhich we can write as\n\\[\\begin{align}\n\\vec{z}_{:, i}^{[l]} &= \\vec{W}^{[l]} \\vec{a}_{:, i}^{[l - 1]} + \\vec{b}^{[l]}, \\label{eq:z} \\\\\n\\vec{a}_{:, i}^{[l]} &= \\vec{g}^{[l]}(\\vec{z}_{:, i}^{[l]}), \\label{eq:a}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{z}_{:, i}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\,\n  &\\vec{W}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l - 1]}}, \\,\n  &\\vec{b}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\\\\n  &\\vec{a}_{:, i}^{[l]} \\in \\mathbb{R}^{n^{[l]}}, \\;\n  &\\vec{a}_{:, i}^{[l - 1]} \\in \\mathbb{R}^{n^{[l - 1]}}, \\;\n  &\\vec{g}^{[l]} \\colon \\mathbb{R}^{n^{[l]}} \\to \\mathbb{R}^{n^{[l]}}.\n\\end{align*}\\]\n\n\nWe have used a colon to clarify that \\(\\vec{z}_{:, i}^{[l]}\\) is the \\(i\\)-th column of \\(\\vec{Z}^{[l]}\\), and so on."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-6",
    "href": "dl_lec3.html#backpropagation-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nNext, we vectorize the training examples:\n\\[\\begin{align}\n\\vec{Z}^{[l]} &=\n\\begin{bmatrix}\n\\vec{z}_{:, 1}^{[l]} & \\dots & \\vec{z}_{:, i}^{[l]} & \\dots & \\vec{z}_{:, m}^{[l]}\n\\end{bmatrix} \\label{eq:Z} \\\\\n&= \\vec{W}^{[l]}\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l - 1]} & \\dots & \\vec{a}_{:, i}^{[l - 1]} & \\dots & \\vec{a}_{:, m}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]}\n\\end{bmatrix} \\notag \\\\\n&= \\vec{W}^{[l]} \\vec{A}^{[l - 1]} + \\text{broadcast}(\\vec{b}^{[l]}), \\notag \\\\\n\\vec{A}^{[l]} &=\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l]} & \\dots & \\vec{a}_{:, i}^{[l]} & \\dots & \\vec{a}_{:, m}^{[l]}\n\\end{bmatrix}, \\label{eq:A}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{Z}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l - 1]} \\in \\mathbb{R}^{n^{[l - 1]} \\times m}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-7",
    "href": "dl_lec3.html#backpropagation-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nNumpy broadcasting\n\n\nSmaller array is “broadcast” across the larger array so that they have compatible shapes.\nBroadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-8",
    "href": "dl_lec3.html#backpropagation-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe would also like to establish two additional notations:\n\\[\\begin{align}\n\\vec{A}^{[0]} &= \\vec{X}, \\label{eq:A_zero} \\\\\n\\vec{A}^{[L]} &= \\vec{\\hat{Y}}, \\label{eq:A_L}\n\\end{align}\\]\nwhere \\(\\vec{X} \\in \\mathbb{R}^{n^{[0]} \\times m}\\) denotes the inputs and \\(\\vec{\\hat{Y}} \\in \\mathbb{R}^{n^{[L]} \\times m}\\) denotes the predictions/outputs.\nFinally, we are ready to define the cost function:\n\\[\\begin{equation}\nJ = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}), \\label{eq:J}\n\\end{equation}\\]\nwhere \\(\\vec{Y} \\in \\mathbb{R}^{n^{[L]} \\times m}\\) denotes the targets and \\(f \\colon \\mathbb{R}^{2 n^{[L]}} \\to \\mathbb{R}\\) can be tailored to our needs."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-9",
    "href": "dl_lec3.html#backpropagation-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\n\\def\\pdv#1#2{\\frac{\\partial #1}{\\partial #2}}\n\\def\\dpdv#1#2{\\frac{\\partial #1}{\\partial #2}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\peq}{\\phantom{=}}\n\\]\n\n\n\nChain rule\n\n\n\\[\\begin{align}\nu_i &= g_i(x_1, \\dots, x_j, \\dots, x_n), \\label{eq:example_u_scalar} \\\\\ny_k &= f_k(u_1, \\dots, u_i, \\dots, u_m). \\label{eq:example_y_scalar}\n\\end{align}\\]\n\\[\\begin{equation}\n\\pdv{y_k}{x_j} = \\sum_i \\pdv{y_k}{u_i} \\pdv{u_i}{x_j}. \\label{eq:chain_rule}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-10",
    "href": "dl_lec3.html#backpropagation-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nLet’s write out first derivatives of \\(J\\) with respect to parameters \\(w\\) and \\(b\\):\n\\[\\begin{align}\n\\pdv{J}{w_{j, k}^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{w_{j, k}^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}} a_{k, i}^{[l - 1]}, \\label{eq:dw_scalar} \\\\\n\\pdv{J}{b_j^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{b_j^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}}. \\label{eq:db_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-11",
    "href": "dl_lec3.html#backpropagation-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nVectorization results in\n\\[\\begin{align*}\n&\n\\begin{bmatrix}\n\\dpdv{J}{w_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{1, k}^{[l]}} & \\dots & \\dpdv{J}{w_{1, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{j, k}^{[l]}} & \\dots & \\dpdv{J}{w_{j, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, k}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, n^{[l - 1]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix} \\notag \\\\\n&\\peq{} \\cdot\n\\begin{bmatrix}\na_{1, 1}^{[l - 1]} & \\dots & a_{k, 1}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, 1}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, i}^{[l - 1]} & \\dots & a_{k, i}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, i}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, m}^{[l - 1]} & \\dots & a_{k, m}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, m}^{[l - 1]}\n\\end{bmatrix}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-12",
    "href": "dl_lec3.html#backpropagation-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\dpdv{J}{b_1^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_j^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_{n^{[l]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-13",
    "href": "dl_lec3.html#backpropagation-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\\[\\begin{align}\n\\pdv{J}{\\vec{W}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} \\vec{a}_{:, i}^{[l - 1]T} = \\pdv{J}{\\vec{Z}^{[l]}} \\vec{A}^{[l - 1]^T}, \\label{eq:dW} \\\\\n\\pdv{J}{\\vec{b}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\underbrace{\\sum_{\\text{axis} = 1} \\pdv{J}{\\vec{Z}^{[l]}}}_\\text{column vector}, \\label{eq:db}\n\\end{align}\\] where \\(\\pdv{J}{\\vec{z}_{:, i}^{[l]}} \\in \\R^{n^{[l]}}\\), \\(\\pdv{J}{\\vec{Z}^{[l]}} \\in \\R^{n^{[l]} \\times m}\\), \\(\\pdv{J}{\\vec{W}^{[l]}} \\in \\R^{n^{[l]} \\times n^{[l - 1]}}\\), and \\(\\pdv{J}{\\vec{b}^{[l]}} \\in \\R^{n^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-14",
    "href": "dl_lec3.html#backpropagation-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nLooking back at \\(\\eqref{eq:dw_scalar}\\) and \\(\\eqref{eq:db_scalar}\\), we see that the only unknown entity is \\(\\pdv{J}{z_{j, i}^{[l]}}\\). By applying the chain rule once again, we get\n\\[\\begin{equation}\n\\pdv{J}{z_{j, i}^{[l]}} = \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}}, \\label{eq:dz_scalar}\n\\end{equation}\\]\nwhere \\(p = 1, \\dots, n^{[l]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-15",
    "href": "dl_lec3.html#backpropagation-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nNext, we present the vectorized version of \\(\\eqref{eq:dz_scalar}\\):\n\\[\\begin{equation*}\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{a_{1, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{1, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix},\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-16",
    "href": "dl_lec3.html#backpropagation-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWhich can be converted into\n\\[\\begin{equation}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\pdv{\\vec{a}_{:, i}^{[l]}}{\\vec{z}_{:, i}^{[l]}} \\pdv{J}{\\vec{a}_{:, i}^{[l]}}, \\label{eq:dz}\n\\end{equation}\\]\nwhere \\(\\pdv{J}{\\vec{a}_{:, i}^{[l]}} \\in \\R^{n^{[l]}}\\) and \\(\\pdv{\\vec{a}_{:, i}^{[l]}}{\\vec{z}_{:, i}^{[l]}} \\in \\R^{n^{[l]} \\times n^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-17",
    "href": "dl_lec3.html#backpropagation-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nWe have already encountered\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{z}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dZ}\n\\end{equation}\\]\nand for the sake of completeness, we also clarify that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{a}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dA}\n\\end{equation}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l]}} \\in \\R^{n^{[l]} \\times m}\\).\nOn purpose, we have omitted the details of \\(g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]})\\); consequently, we cannot derive an analytic expression for \\(\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\), which we depend on in \\(\\eqref{eq:dz_scalar}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-18",
    "href": "dl_lec3.html#backpropagation-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nFurthermore, according to \\(\\eqref{eq:dz_scalar}\\), we see that \\(\\pdv{J}{z_{j, i}^{[l]}}\\) also depends on \\(\\pdv{J}{a_{j, i}^{[l]}}\\).\n\\(\\pdv{J}{a_{j, i}^{[l]}}\\) has already been computed when we reach the \\(l\\)-th layer during backward propagation.\nHow? Each layer paves the way for the previous layer by also computing \\(\\pdv{J}{a_{k, i}^{[l - 1]}}\\), which we shall do now:\n\\[\\begin{equation}\n\\pdv{J}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} w_{j, k}^{[l]}. \\label{eq:da_prev_scalar}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-19",
    "href": "dl_lec3.html#backpropagation-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\\[\\begin{equation*}\n\\begin{split}\n&\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{k, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{n^{[l - 1]}, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, m}^{[l - 1]}}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{j, 1}^{[l]} & \\dots & w_{n^{[l]}, 1}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, k}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, n^{[l - 1]}}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-20",
    "href": "dl_lec3.html#backpropagation-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nwhich we can write as \\[\\begin{align}\n&\\pdv{J}{\\vec{A}^{[l - 1]}} =\\vec{W}^{[l]T} \\pdv{J}{\\vec{Z}^{[l]}}, \\label{eq:dA_prev}\n\\end{align}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l - 1]}} \\in \\R^{n^{[l - 1]} \\times m}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-21",
    "href": "dl_lec3.html#backpropagation-21",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nInitial values\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}\\)\nBackward: \\(\\pdv{J}{\\vec{A}^{[L]}} = \\pdv{J}{\\vec{\\hat{Y}}}\\)\n\n\n\n\n\n\n\nComputations\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}, \\vec{A}^{[L]} = \\vec{\\hat{Y}}, J = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y})\\)\nBackward: \\(\\dfrac{\\partial J}{\\partial \\vec{A}^{[L]}} = \\dfrac{\\partial J}{\\partial \\vec{\\hat{Y}}}, \\dfrac{\\partial J}{\\partial \\vec{W}^{[l]}}, \\dfrac{\\partial J}{\\partial \\vec{b}^{[l]}}\\)"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-22",
    "href": "dl_lec3.html#backpropagation-22",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\nBackpropagation seed\n\n\nWe have yet to derive an analytic expression for the backpropagation seed \\(\\dpdv{J}{\\vec{A}^{[L]}} = \\dpdv{J}{\\vec{\\hat{Y}}}\\).\n\n\n\nLet’s derive an analytic expression for \\(\\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\) or, by extension, \\(\\dpdv{J}{z_{j, i}^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu",
    "href": "dl_lec3.html#backpropagation-relu",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nThe rectified linear unit, or ReLU for short, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\max(0, z_{j, i}^{[l]}) = \\\\\n&=\n\\begin{cases}\nz_{j, i}^{[l]} &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\max(0, \\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu-1",
    "href": "dl_lec3.html#backpropagation-relu-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nCompute the partial derivatives of the activations in the current layer:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &:=\n\\begin{cases}\n1 &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise,}\n\\end{cases} \\\\\n&= I(z_{j, i}^{[l]} &gt; 0), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= 0, \\quad \\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-relu-2",
    "href": "dl_lec3.html#backpropagation-relu-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: ReLU",
    "text": "Backpropagation: ReLU\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} I(z_{j, i}^{[l]} &gt; 0),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot I(\\vec{Z}^{[l]} &gt; 0),\n\\end{equation}\\]\nwhere \\(\\odot\\) denotes element-wise multiplication (Hadamard product)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid",
    "href": "dl_lec3.html#backpropagation-sigmoid",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nThe sigmoid activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\sigma(z_{j, i}^{[l]}) = \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization yields\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{1 + \\exp(-\\vec{Z}^{[l]})}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-1",
    "href": "dl_lec3.html#backpropagation-sigmoid-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nTo practice backward propagation, first, we construct a computation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-2",
    "href": "dl_lec3.html#backpropagation-sigmoid-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nCompute, starting from outside:\n\\[\\begin{align*}\n&\\pdv{a_{j, i}^{[l]}}{u_4} = 1, \\; \\pdv{a_{j, i}^{[l]}}{u_3} = \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_2} = \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_2} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_1} = \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_1} = -\\frac{1}{u_3^2} \\exp(u_1) = -\\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_0} = \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} = \\frac{1}{u_3^2} \\exp(u_1) = \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-3",
    "href": "dl_lec3.html#backpropagation-sigmoid-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nLet us simplify:\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{1 + \\exp(-z_{j, i}^{[l]}) - 1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})} - \\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-4",
    "href": "dl_lec3.html#backpropagation-sigmoid-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nWe also note that\n\\[\\begin{equation*}\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\n\\end{equation*}\\]\nConsequently,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-sigmoid-5",
    "href": "dl_lec3.html#backpropagation-sigmoid-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: Sigmoid",
    "text": "Backpropagation: Sigmoid\nLastly, no summations mean trivial vectorization:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]} \\odot (1 - \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh",
    "href": "dl_lec3.html#backpropagation-tanh",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nThe hyperbolic tangent function, i.e., the tanh activation function, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\tanh(z_{j, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nBy utilizing element-wise multiplication, we get\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\exp(\\vec{Z}^{[l]}) + \\exp(-\\vec{Z}^{[l]})} \\odot (\\exp(\\vec{Z}^{[l]}) - \\exp(-\\vec{Z}^{[l]})).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-1",
    "href": "dl_lec3.html#backpropagation-tanh-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nComputation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-2",
    "href": "dl_lec3.html#backpropagation-tanh-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nWe compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_7} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_6} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_6} = u_4 = \\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_5} &= \\pdv{a_{j, i}^{[l]}}{u_6} \\pdv{u_6}{u_5} = -u_4 \\frac{1}{u_5^2} = -\\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_4} = u_6 = \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-3",
    "href": "dl_lec3.html#backpropagation-tanh-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_3} \\\\\n&= -u_6 - u_4 \\frac{1}{u_5^2} \\notag \\\\\n&= -\\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_2} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_2} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_2} = u_6 - u_4 \\frac{1}{u_5^2} =\\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-4",
    "href": "dl_lec3.html#backpropagation-tanh-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n&= \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{2 \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} \\\\\n&= \\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-5",
    "href": "dl_lec3.html#backpropagation-tanh-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_0} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} + \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_0} \\\\\n&= -\\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) + \\Bigl(u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_0) \\notag \\\\\n&= \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} + \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-6",
    "href": "dl_lec3.html#backpropagation-tanh-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})^2 + 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&\\peq\\negmedspace{} - \\frac{\\exp(z_{j, i}^{[l]})^2 - 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= 1 - \\frac{(\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}))^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} = 1 - a_{j, i}^{[l]} a_{j, i}^{[l]}.\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-tanh-7",
    "href": "dl_lec3.html#backpropagation-tanh-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: tanh",
    "text": "Backpropagation: tanh\nSimilarly to the sigmoid activation function: \\(\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\\)\nThus,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\left(1 - a_{j, i}^{[l]} a_{j, i}^{[l]}\\right),\n\\end{split}\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot (1 - \\vec{A}^{[l]} \\odot \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax",
    "href": "dl_lec3.html#backpropagation-softmax",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nThe softmax activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization results in\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\text{broadcast}(\\underbrace{\\sum_{\\text{axis} = 0} \\exp(\\vec{Z}^{[l]})}_\\text{row vector})} \\odot \\exp(\\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-1",
    "href": "dl_lec3.html#backpropagation-softmax-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nTo begin with, we construct a computation graph for the \\(j\\)-th activation of the current layer:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-2",
    "href": "dl_lec3.html#backpropagation-softmax-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nBy applying the chain rule, we get\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_1 = \\exp(z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_1 \\frac{1}{u_3^2} = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_1} \\\\\n&= -u_1 \\frac{1}{u_3^2} + u_4 = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{1}{\\sum_p \\exp(z_{p, i}^{[l]})}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-3",
    "href": "dl_lec3.html#backpropagation-softmax-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_{-1}} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = \\Bigl(-u_1 \\frac{1}{u_3^2} + u_4\\Bigr) \\exp(u_{-1}) \\notag \\\\\n&= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-4",
    "href": "dl_lec3.html#backpropagation-softmax-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nNext, we need to take into account that (z_{j, i}^{[l]}) also affects other activations in the same layer:\n\\[\\begin{align*}\nu_{-1} &= z_{j, i}^{[l]}, \\\\\nu_{0, p} &= z_{p, i}^{[l]}, &&\\forall p \\ne j, \\\\\nu_1 &= \\exp(u_{-1}), \\\\\nu_{2, p} &= \\exp(u_{0, p}), &&\\forall p \\ne j, \\\\\nu_3 &= u_1 + \\sum_{p \\ne j} u_{2, p}, \\\\\nu_4 &= \\frac{1}{u_3}, \\\\\nu_5 &= u_{2, p} u_4 = a_{p, i}^{[l]}, &&\\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-5",
    "href": "dl_lec3.html#backpropagation-softmax-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{align*}\n\\pdv{a_{p, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_4} &= \\pdv{a_{p, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_{2, p} = \\exp(z_{p, i}^{[l]}), \\\\\n\\pdv{a_{p, i}^{[l]}}{u_3} &= \\pdv{a_{p, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_1} &= \\pdv{a_{p, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_{-1}} &= \\pdv{a_{p, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = -u_{2, p} \\frac{1}{u_3^2} \\exp(u_{-1}) = -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-6",
    "href": "dl_lec3.html#backpropagation-softmax-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nWe now know that\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})} \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} \\\\\n&= -a_{p, i}^{[l]} a_{j, i}^{[l]}, \\quad \\forall p \\ne j. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-7",
    "href": "dl_lec3.html#backpropagation-softmax-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} a_{j, i}^{[l]} \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} + \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr),\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-softmax-8",
    "href": "dl_lec3.html#backpropagation-softmax-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: softmax",
    "text": "Backpropagation: softmax\nVectorized version: \\[\\begin{equation*}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\vec{a}_{:, i}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{a}_{:, i}^{[l]}} - \\underbrace{{\\vec{a}_{:, i}^{[l]}}^T \\pdv{J}{\\vec{a}_{:, i}^{[l]}}}_{\\text{scalar}}\\Bigr).\n\\end{equation*}\\]\nLet us not stop with the vectorization just yet:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\vec{A}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{A}^{[l]}} - \\text{broadcast}\\bigl(\\underbrace{\\sum_{\\text{axis} = 0} \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]}}_\\text{row vector}\\bigr)\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost",
    "href": "dl_lec3.html#backpropagation-binary-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nIn binary classification, the cost function is given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(a_i^{[L]}) + (1 - y_i) \\log(1 - a_i^{[L]})),\n\\end{split}\n\\end{equation*}\\]\nwhich we can write as\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\text{axis} = 1} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-1",
    "href": "dl_lec3.html#backpropagation-binary-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nNext, we construct a computation graph:"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-2",
    "href": "dl_lec3.html#backpropagation-binary-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nLet’s compute derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_5} &= 1, \\; \\pdv{J}{u_{4, i}} = \\pdv{J}{u_5} \\pdv{u_5}{u_{4, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{3, i}} = -\\frac{1}{m} (1 - y_i), \\\\\n\\pdv{J}{u_{2, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{2, i}} = -\\frac{1}{m} y_i, \\\\\n\\pdv{J}{u_{1, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{1, i}} = -\\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} = -\\frac{1}{m} \\frac{1 - y_i}{1 - a_i^{[L]}}, \\\\\n\\pdv{J}{u_{0, i}} &= \\pdv{J}{u_{1, i}} \\pdv{u_{1, i}}{u_{0, i}} + \\pdv{J}{u_{2, i}} \\pdv{u_{2, i}}{u_{0, i}} = \\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} - \\frac{1}{m} y_i \\frac{1}{u_{0, i}} = \\\\\n&=\\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-3",
    "href": "dl_lec3.html#backpropagation-binary-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nThus,\n\\[\\begin{equation*}\n\\pdv{J}{a_i^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr),\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-binary-cost-4",
    "href": "dl_lec3.html#backpropagation-binary-cost-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: binary cost",
    "text": "Backpropagation: binary cost\nIn addition, since the sigmoid activation function is used in the output layer, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_i^{[L]}} &= \\pdv{J}{a_i^{[L]}} a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr) a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_i) a_i^{[L]} - y_i (1 - a_i^{[L]})) \\\\\n&= \\frac{1}{m} (a_i^{[L]} - y_i).\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]\nNote that both \\(\\pdv{J}{\\vec{Z}^{[L]}} \\in \\R^{1 \\times m}\\) and \\(\\pdv{J}{\\vec{A}^{[L]}} \\in \\R^{1 \\times m}\\), because \\(n^{[L]} = 1\\) in this case."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost",
    "href": "dl_lec3.html#backpropagation-multiclass-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nIn multiclass classification, the cost function is instead given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(\\hat{y}_{j, i}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(a_{j, i}^{[L]}),\n\\end{split}\n\\end{equation*}\\]\nwhere \\(j = 1, \\dots, n^{[L]}\\). We can vectorize the cost expression:\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 0 \\\\ \\text{axis} = 1}} \\vec{Y} \\odot \\log(\\vec{A}^{[L]})}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-1",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-cost",
    "href": "dl_lec3.html#backpropagation-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: cost",
    "text": "Backpropagation: cost\nWith the computation graph in place, we can perform backward propagation:\n\\[\\begin{align*}\n\\pdv{J}{u_4} &= 1, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_4} \\pdv{u_4}{u_{3, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{2, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{1, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} = -\\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-2",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nHence,\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{equation*}\\]\nVectorization is trivial:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = -\\frac{1}{m} \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multiclass-cost-3",
    "href": "dl_lec3.html#backpropagation-multiclass-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multiclass cost",
    "text": "Backpropagation: multiclass cost\nFurthermore, since the output layer uses the softmax activation function, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= a_{j, i}^{[L]} \\Bigl(\\pdv{J}{a_{j, i}^{[L]}} - \\sum_p \\pdv{J}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) = a_{j, i}^{[L]} \\Bigl(-\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}} + \\sum_p \\frac{1}{m} \\frac{y_{p, i}}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) \\\\\n&= \\frac{1}{m} \\Bigl(-y_{j, i} + a_{j, i}^{[L]} \\underbrace{\\sum_p y_{p, i}}_{\\sum \\text{probabilities} = 1}\\Bigr) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}).\n\\end{split}\n\\end{equation*}\\]\nNote that \\(p = 1, \\dots, n^{[L]}\\). To conclude,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost",
    "href": "dl_lec3.html#backpropagation-multilabel-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nWe can view multi-label classification as \\(j\\) binary classification problems:\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(\\hat{y}_{j, i}) + (1 - y_{j, i}) \\log(1 - \\hat{y}_{j, i}))\\Bigr) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(a_{j, i}^{[L]}) + (1 - y_{j, i}) \\log(1 - a_{j, i}^{[L]}))\\Bigr),\n\\end{split}\n\\end{equation*}\\]\nwhere once again \\(j = 1, \\dots, n^{[L]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-1",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nVectorization gives\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 1 \\\\ \\text{axis} = 0}} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-2",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-3",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nNext, we compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_6} &= 1, \\\\\n\\pdv{J}{u_{5, j}} &= \\pdv{J}{u_6} \\pdv{u_6}{u_{5, j}} = 1, \\\\\n\\pdv{J}{u_{4, j, i}} &= \\pdv{J}{u_{5, j}} \\pdv{u_{5, j}}{u_{4, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{3, j, i}} = -\\frac{1}{m} (1 - y_{j, i}), \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{2, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{3, j, i}} \\pdv{u_{3, j, i}}{u_{1, j, i}} = -\\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} = -\\frac{1}{m} \\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-4",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\n\\[\\begin{align*}\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} + \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{0, j, i}} \\\\\n&= \\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} - \\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} \\notag \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-5",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nSimply put, we have\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr),\n\\end{equation*}\\]\nand\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-multilabel-cost-6",
    "href": "dl_lec3.html#backpropagation-multilabel-cost-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation: multilabel cost",
    "text": "Backpropagation: multilabel cost\nBearing in mind that we view multi-label classification as \\(j\\) binary classification problems, we also know that the output layer uses the sigmoid activation function.\nAs a result,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= \\pdv{J}{a_{j, i}^{[L]}} a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr) a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_{j, i}) a_{j, i}^{[L]} - y_{j, i} (1 - a_{j, i}^{[L]})) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lab4.html",
    "href": "dl_lab4.html",
    "title": "DL: Lab 4",
    "section": "",
    "text": "Lab overview\nImplement various initialization/regularization approaches for a deep neural network.\nCode in attached Jupyter notebook."
  }
]