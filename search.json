[
  {
    "objectID": "dl_lec3.html#deep-networks",
    "href": "dl_lec3.html#deep-networks",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep networks",
    "text": "Deep networks\nSuppose we have this network:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-1",
    "href": "dl_lec3.html#deep-networks-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nWhat happens in Hidden layer 1 and Output layer?\nWe will use this notation: \\[\\begin{align*}\n  & z^{[1]} = W^{[1]}x+b^{[1]} \\\\\n  & a^{[1]} = \\sigma(z^{[1]})\\\\\n  & z^{[2]} = W^{[2]}a^{[1]}+b^{[2]} \\\\\n  & a^{[2]} = \\sigma(z^{[2]})\n\\end{align*}\\] And then we compute \\(L(a^{[2]}, y)\\).\nAnd then, similarly, for backpropagation, we will compute \\(da^{[2]}, dz^{[2]}, dW^{[2]}\\).\n\n\nDo not confuse round and square brackets."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-2",
    "href": "dl_lec3.html#deep-networks-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nAlternative notation: \\(a^{[0]} = x\\). So in the picture \\[\\begin{align*}\n  a = \\begin{bmatrix}\n    a_1^{[1]} \\\\\n    a_2^{[1]}\\\\\n    a_3^{[1]}\\\\\n    a_4^{[1]}\n  \\end{bmatrix}\n\\end{align*}\\] 2-layer network (input layer not counted). Input layer is layer 0."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-3",
    "href": "dl_lec3.html#deep-networks-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\n\n\n\n\nFirst hidden layer params\n\n\n\n\\(W^{[1]}\\) ((4,3) matrix)\n\\(b^{[1]}\\) ((4,1) matrix).\n\n\n\n\n\n\n\nSecond hidden layer params\n\n\n\n\\(W^{[2]}\\) is a (1,4) matrix\n\\(b^{[2]}\\) is a (1,1) matrix."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-4",
    "href": "dl_lec3.html#deep-networks-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor all nodes: \\[\\begin{align*}\n  & z_1^{[1]} = W_1^{[1]T}x + b_1^{[1]},\\; & a_1^{[1]} = \\sigma(z_1^{[1]}), \\\\\n  & z_2^{[1]} = W_2^{[1]T}x + b_2^{[1]},\\; & a_2^{[1]} = \\sigma(z_2^{[1]}), \\\\\n  & z_3^{[1]} = W_3^{[1]T}x + b_3^{[1]},\\; & a_3^{[1]} = \\sigma(z_3^{[1]}), \\\\\n  & z_4^{[1]} = W_4^{[1]T}x + b_4^{[1]},\\; & a_4^{[1]} = \\sigma(z_4^{[1]})\n\\end{align*}\\] So, if we have \\(a_i^{[l]}\\), then \\(l\\) means layer, and \\(i\\) means node number in a layer."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-5",
    "href": "dl_lec3.html#deep-networks-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor \\(m\\) training examples, we compute \\(x^{(m)} \\rightarrow a^{[2](m)} = \\hat{y}^{(m)}\\).\nThe block of code:\nfor i = 1 to m:\n\\[\\begin{align*}\n  &z^{[1](i)} = W^{[1]}x^{(i)}+b^{[1]},\\\\\n  &a^{[1](i)} = \\sigma(z^{[1](i)}),\\\\\n  &z^{[2](i)} = W^{[2]}a^{[1](i)}+b^{[2]},\\\\\n  &a^{[2](i)} = \\sigma(z^{[2](i)})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-6",
    "href": "dl_lec3.html#deep-networks-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we vectorize it across multiple training examples? We compute by going to matrices: \\[\\begin{align*}\n  &Z^{[1]} = W^{[1]}X+b^{[1]},\\; &A^{[1]} = \\sigma(Z^{[1]}),\\\\\n  &Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]},\\; &A^{[2]} = \\sigma(Z^{[2]})\n\\end{align*}\\]\n\\[\\begin{align*}\n&Z^{[1]} =\\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  z^{[1](1)} & z^{[1](2)} & \\dots & z^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix},\\\\\n&A^{[1]} = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  a^{[1](1)} & a^{[1](2)} & \\dots & a^{[1](m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#activation-functions",
    "href": "dl_lec3.html#activation-functions",
    "title": "Deep learning: multi-layer NNs",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\n\n\n\n\n\n\n\nSigmoid derivative\n\n\n\\[\n\\dfrac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))\n\\]\n\n\n\n\n\n\nTanh derivative\n\n\n\\[\n\\dfrac{d\\tanh}{dz} = 1-(\\tanh(z))^2\n\\]\n\n\n\n\n\n\nReLU derivative\n\n\n\\[\\begin{align*}\n  &g'(z) = \\begin{cases}\n    0 , & \\text{if } z &lt; 0,\\\\\n    1, & \\text{if } z &gt; 0, \\\\\n    \\text{undefined}, & \\text{if } z = 0\n  \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-7",
    "href": "dl_lec3.html#deep-networks-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nParameters\n\n\nReview:\n\\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\).\n\\(n_x = n^{[0]}\\), \\(n^{[2]} = 1\\).\n\n\n\n\n\n\nCost function\n\n\nIn our case will be \\[\\begin{align*}\n  &J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})= \\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y},y).\n\\end{align*}\\]\n\n\n\nDimensions for \\(W^{[1]}\\) are \\((n^{[1]}, n^{[0]})\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-8",
    "href": "dl_lec3.html#deep-networks-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nFor gradient descent we compute\n\n\\(\\hat{y}^{(i)}\\)\n\\(dW^{[1]} \\equiv \\dfrac{dJ}{dW^{[1]}}, dB^{[1]} \\equiv \\dfrac{dJ}{dB^{[1]}}\\)\n\\(W^{[1]} = W^{[1]} - \\alpha dW^{[1]}\\)\n\\(B^{[1]} = B^{[1]} - \\alpha dB^{[1]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-9",
    "href": "dl_lec3.html#deep-networks-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nHow do we compute the derivatives? \\[\\begin{align*}\n  &dZ^{[2]} = A^{[2]} - Y,\\\\\n  &dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T},\\\\\n  &dB^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-10",
    "href": "dl_lec3.html#deep-networks-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nNext step: \\[\\begin{align*}\n  &dZ^{[1]} = W^{[2]T}dZ^{[2]} \\odot g^{[1]'}(Z^{[1]}),\\\\\n  &dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{T},\\\\\n  &dB^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-11",
    "href": "dl_lec3.html#deep-networks-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nForward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-12",
    "href": "dl_lec3.html#deep-networks-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nBackward propagation:"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-13",
    "href": "dl_lec3.html#deep-networks-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\nVectorized versions: \\[\\begin{align*}\n  &d\\vec{Z}^{[2]} = A^{[2]} - Y,\\\\\n  &d\\vec{W}^{[2]} = \\frac{1}{m}d\\vec{Z}^{[2]}\\vec{A}^{[1]T},\\\\\n  &d\\vec{b}^{[2]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[2]}, axis=1,keepdims=True),\\\\\n  &d\\vec{Z}^{[1]} = W^{[2]T}d\\vec{Z}^{[2]} \\odot g^{[1]'}(\\vec{Z}^{[1]}),\\\\\n  &d\\vec{W}^{[1]} = \\frac{1}{m}d\\vec{Z}^{[1]}\\vec{X}^{T},\\\\\n  &d\\vec{b}^{[1]} = \\frac{1}{m}np.sum(d\\vec{Z}^{[1]}, axis=1,keepdims=True)\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-random-initialization",
    "href": "dl_lec3.html#deep-networks-random-initialization",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks: Random Initialization",
    "text": "Deep Networks: Random Initialization\n\n\n\nSymmetry-breaking problem\n\n\nIf we initialize weights to zero, the hidden units will be symmetric. \\[\nW^{[1]} = np.random.randn((2,2))*0.01,\\\\\nb^{[1]} = np.zero((2,1))\n\\]\n\n\n\n\n\nThe 0.01 multiplier is because we don’t want to end up at flat parts of the activation function."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-14",
    "href": "dl_lec3.html#deep-networks-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\\(a^{[l]}\\) - activations in layer \\(l\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-15",
    "href": "dl_lec3.html#deep-networks-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nGeneral rule\n\n\n\\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\\), \\(a^{[l]} = g^{[l]}(z^{[l]})\\).\n\n\n\n\n\n\nVectorized versions\n\n\n\\[\\begin{align*}\n&Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}, \\\\\n&A^{[l]} = g^{[l]}(Z^{[l]})\n\\end{align*}\\]\n\n\n\n\n\n\nImportant\n\n\nFor loop is necessary for multiple layers."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-16",
    "href": "dl_lec3.html#deep-networks-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\n\nGetting your Matrix Dimensions Right\n\n\n\nDimensions of \\(W^{[l]}\\) are \\((n^{[l]}, n^{[l-1]})\\).\nDimensions of \\(b^{[l]}\\) should be \\((n^{[l]}, 1)\\).\nDimensions of dW and db should be identical to the ones for W and b.\nDimension of \\(Z^{[1]}\\) is \\((n^{[1]}, m)\\)."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-17",
    "href": "dl_lec3.html#deep-networks-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\n\n\n\nIntuition from circuit theory\n\n\nSmall L-layer network requires exponentially less hidden units than shallower networks.\n\n\n\n\n\n\nExample\n\n\nTo compute XOR, we’ll need \\(O(\\log \\, n)\\) layers.\nWith a single hidden layer, we’ll need \\(2^{n-1}\\) hidden units."
  },
  {
    "objectID": "dl_lec3.html#deep-networks-18",
    "href": "dl_lec3.html#deep-networks-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nForward propagation:\n\n\n\nInputs: \\(a^{[l-1]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(a^{[l]}\\)\nCache: \\(z^{[l]}\\)\n\n\n\n\n\n\n\nBackward propagation:\n\n\n\nInputs: \\(da^{[l]}\\)\nParameters: \\(W^{[l]}\\), \\(b^{[l]}\\).\nOutputs: \\(da^{[l-1]}\\)\nCache: \\(dz^{[l]}\\), \\(dW^{[l]}\\), \\(db^{[l]}\\)"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-19",
    "href": "dl_lec3.html#deep-networks-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nBackward propagation steps:\n\n\n\\[\\begin{align*}\n  &dz^{[l]} = da^{[l]}\\odot g^{[l]'}(z^{[l]})\\\\\n  &dW^{[l]} = dz^{[l]} \\cdot (a^{[l-1]T})\\\\\n  &db^{[l]} = dz^{[l]} \\\\\n  &da^{[l-1]} = W^{[l]T} \\cdot dz^{[l]}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#deep-networks-20",
    "href": "dl_lec3.html#deep-networks-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nVectorized versions:\n\n\n\\[\\begin{align*}\n  &dZ^{[l]} = dA^{[l]}\\odot g^{[l]'}(Z^{[l]})\\\\\n  &dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot (A^{[l-1]T})\\\\\n  &db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\\\\n  &dA^{[l-1]} = W^{[l]T} \\cdot dZ^{[l]}\n\\end{align*}\\]\n\n\n\n\n\n\nFinal layer\n\n\n\\[\\begin{align*}\n  &da^{[l]} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#parameters-vs-hyperparameters",
    "href": "dl_lec3.html#parameters-vs-hyperparameters",
    "title": "Deep learning: multi-layer NNs",
    "section": "Parameters vs Hyperparameters",
    "text": "Parameters vs Hyperparameters\n\n\n\nParameters\n\n\n\n\\(W^{[i]}\\)\n\\(b^{[i]}\\)\n\n\n\n\n\n\n\nHyperparameters\n\n\n\nlearning rate\nn of iterations\nn of hidden layers (\\(L\\))\nn of hidden units (\\(n^{[i]}\\))\nchoice of activation functions"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-1",
    "href": "dl_lec3.html#backpropagation-in-detail-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\\[\\begin{align}\nz_{j, i}^{[l]} &= \\sum_k w_{j, k}^{[l]} a_{k, i}^{[l - 1]} + b_j^{[l]}, \\label{eq:z_scalar} \\\\\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}). \\label{eq:a_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-2",
    "href": "dl_lec3.html#backpropagation-in-detail-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\\[\n\\begin{longtable}[]{@{}ll@{}}\n\\toprule\\noalign{}\n\\textbf{Entity} & \\textbf{Description} \\\\\n\\midrule\\noalign{}\n\\endhead\n\\bottomrule\\noalign{}\n\\endlastfoot\n\\(l\\) & The current layer \\(l = 1, \\dots, L\\). \\\\\n\\(n^{[l]}\\) & The number of nodes in the current layer. \\\\\n\\(n^{[l - 1]}\\) & The number of nodes in the previous layer. \\\\\n\\(j\\) & The \\(j\\)th node of the current layer,\n\\(j = 1, \\dots, n^{[l]}\\). \\\\\n\\(k\\) & The \\(k\\)th node of the previous layer,\n\\(k = 1, \\dots, n^{[l - 1]}\\). \\\\\n\\(i\\) & The current training example \\(i = 1, \\dots, m\\). \\\\\n\\(z_{j, i}^{[l]}\\) & A weighted sum of the activations of the previous\nlayer. \\\\\n\\(w_{j, k}^{[l]}\\) & A weight that scales the \\(k\\)th activation of the\nprevious layer. \\\\\n\\(b_j^{[l]}\\) & A bias in the current layer. \\\\\n\\(a_{j, i}^{[l]}\\) & An activation in the current layer. \\\\\n\\(a_{k, i}^{[l - 1]}\\) & An activation in the previous layer. \\\\\n\\(g_j^{[l]}\\) & An activation function\n\\(g_j^{[l]} \\colon \\mathbb{R}^{n^{[l]}} \\to \\mathbb{R}\\). \\\\\n\\end{longtable}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-3",
    "href": "dl_lec3.html#backpropagation-in-detail-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nWe vectorize the nodes:\n\\[\\begin{align*}\n\\begin{bmatrix}\nz_{1, i}^{[l]} \\\\\n\\vdots \\\\\nz_{j, i}^{[l]} \\\\\n\\vdots \\\\\nz_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{1, k}^{[l]} & \\dots & w_{1, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{j, 1}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{n^{[l]}, 1}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{k, i}^{[l - 1]} \\\\\n\\vdots \\\\\na_{n^{[l - 1]}, i}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_1^{[l]} \\\\\n\\vdots \\\\\nb_j^{[l]} \\\\\n\\vdots \\\\\nb_{n^{[l]}}^{[l]}\n\\end{bmatrix}, \\\\\n\\begin{bmatrix}\na_{1, i}^{[l]} \\\\\n\\vdots \\\\\na_{j, i}^{[l]} \\\\\n\\vdots \\\\\na_{n^{[l]}, i}^{[l]}\n\\end{bmatrix} &=\n\\begin{bmatrix}\ng_1^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\vdots \\\\\ng_{n^{[l]}}^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-4",
    "href": "dl_lec3.html#backpropagation-in-detail-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nwhich we can write as\n\\[\\begin{align}\n\\vec{z}_{:, i}^{[l]} &= \\vec{W}^{[l]} \\vec{a}_{:, i}^{[l - 1]} + \\vec{b}^{[l]}, \\label{eq:z} \\\\\n\\vec{a}_{:, i}^{[l]} &= \\vec{g}^{[l]}(\\vec{z}_{:, i}^{[l]}), \\label{eq:a}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{z}_{:, i}^{[l]} \\in \\R^{n^{[l]}}, \\\\\n  &\\vec{W}^{[l]} \\in \\R^{n^{[l]} \\times n^{[l - 1]}}, \\\\\n  &\\vec{b}^{[l]} \\in \\R^{n^{[l]}}, \\\\\n  &\\vec{a}_{:, i}^{[l]} \\in \\R^{n^{[l]}}, \\\\\n  &\\vec{a}_{:, i}^{[l - 1]} \\in \\R^{n^{[l - 1]}}, \\\\\n  &\\vec{g}^{[l]} \\colon \\R^{n^{[l]}} \\to \\R^{n^{[l]}}.\n\\end{align*}\\] We have used a colon to clarify that (_{:, i}^{[l]}) is the (i)th column of (^{[l]}), and so on."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-5",
    "href": "dl_lec3.html#backpropagation-in-detail-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nNext, we vectorize the training examples:\n\\[\\begin{align}\n\\vec{Z}^{[l]} &=\n\\begin{bmatrix}\n\\vec{z}_{:, 1}^{[l]} & \\dots & \\vec{z}_{:, i}^{[l]} & \\dots & \\vec{z}_{:, m}^{[l]}\n\\end{bmatrix} \\label{eq:Z} \\\\\n&= \\vec{W}^{[l]}\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l - 1]} & \\dots & \\vec{a}_{:, i}^{[l - 1]} & \\dots & \\vec{a}_{:, m}^{[l - 1]}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]} & \\dots & \\vec{b}^{[l]}\n\\end{bmatrix} \\notag \\\\\n&= \\vec{W}^{[l]} \\vec{A}^{[l - 1]} + \\broadcast(\\vec{b}^{[l]}), \\notag \\\\\n\\vec{A}^{[l]} &=\n\\begin{bmatrix}\n\\vec{a}_{:, 1}^{[l]} & \\dots & \\vec{a}_{:, i}^{[l]} & \\dots & \\vec{a}_{:, m}^{[l]}\n\\end{bmatrix}, \\label{eq:A}\n\\end{align}\\]\nwhere \\[\\begin{align*}\n  &\\vec{Z}^{[l]} \\in \\R^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l]} \\in \\R^{n^{[l]} \\times m}, \\\\\n  &\\vec{A}^{[l - 1]} \\in \\R^{n^{[l - 1]} \\times m}.\n\\end{align*}\\] \\end{frame}"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-6",
    "href": "dl_lec3.html#backpropagation-in-detail-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nWe would also like to establish two additional notations:\n\\[\\begin{align}\n\\vec{A}^{[0]} &= \\vec{X}, \\label{eq:A_zero} \\\\\n\\vec{A}^{[L]} &= \\vec{\\hat{Y}}, \\label{eq:A_L}\n\\end{align}\\]\nwhere ( {n{[0]} m}) denotes the inputs and ( {n{[L]} m}) denotes the predictions/outputs.\nFinally, we are ready to define the cost function:\n\\[\\begin{equation}\nJ = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}), \\label{eq:J}\n\\end{equation}\\]\nwhere \\(\\vec{Y} \\in \\R^{n^{[L]} \\times m}\\) denotes the targets and \\(f \\colon \\R^{2 n^{[L]}} \\to \\R\\) can be tailored to our needs."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-7",
    "href": "dl_lec3.html#backpropagation-in-detail-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\n\n\nChain rule\n\n\n\\[\\begin{align}\nu_i &= g_i(x_1, \\dots, x_j, \\dots, x_n), \\label{eq:example_u_scalar} \\\\\ny_k &= f_k(u_1, \\dots, u_i, \\dots, u_m). \\label{eq:example_y_scalar}\n\\end{align}\\]\n\\[\\begin{equation}\n\\pdv{y_k}{x_j} = \\sum_i \\pdv{y_k}{u_i} \\pdv{u_i}{x_j}. \\label{eq:chain_rule}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-8",
    "href": "dl_lec3.html#backpropagation-in-detail-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nLet’s write out first derivatives of \\(J\\) with respect to parameters \\(w\\) and \\(b\\):\n\\[\\begin{align}\n\\pdv{J}{w_{j, k}^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{w_{j, k}^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}} a_{k, i}^{[l - 1]}, \\label{eq:dw_scalar} \\\\\n\\pdv{J}{b_j^{[l]}} &= \\sum_i \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{b_j^{[l]}} = \\sum_i \\pdv{J}{z_{j, i}^{[l]}}. \\label{eq:db_scalar}\n\\end{align}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-9",
    "href": "dl_lec3.html#backpropagation-in-detail-9",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nVectorization results in\n\\[\\begin{align*}\n&\n\\begin{bmatrix}\n\\dpdv{J}{w_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{1, k}^{[l]}} & \\dots & \\dpdv{J}{w_{1, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{j, k}^{[l]}} & \\dots & \\dpdv{J}{w_{j, n^{[l - 1]}}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{w_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, k}^{[l]}} & \\dots & \\dpdv{J}{w_{n^{[l]}, n^{[l - 1]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix} \\notag \\\\\n&\\peq{} \\cdot\n\\begin{bmatrix}\na_{1, 1}^{[l - 1]} & \\dots & a_{k, 1}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, 1}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, i}^{[l - 1]} & \\dots & a_{k, i}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, i}^{[l - 1]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{1, m}^{[l - 1]} & \\dots & a_{k, m}^{[l - 1]} & \\dots & a_{n^{[l - 1]}, m}^{[l - 1]}\n\\end{bmatrix}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-10",
    "href": "dl_lec3.html#backpropagation-in-detail-10",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\dpdv{J}{b_1^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_j^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{b_{n^{[l]}}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} + \\dots +\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-11",
    "href": "dl_lec3.html#backpropagation-in-detail-11",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\\[\\begin{align}\n\\pdv{J}{\\vec{W}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} \\vec{a}_{:, i}^{[l - 1]T} = \\pdv{J}{\\vec{Z}^{[l]}} \\vec{A}^{[l - 1]^T}, \\label{eq:dW} \\\\\n\\pdv{J}{\\vec{b}^{[l]}} &= \\sum_i \\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\underbrace{\\sum_{\\text{axis} = 1} \\pdv{J}{\\vec{Z}^{[l]}}}_\\text{column vector}, \\label{eq:db}\n\\end{align}\\] where ( {n{[l]}}), ( {n{[l]} m}), ( {n{[l]} n^{[l - 1]}}), and ( {n{[l]}}). \\end{frame}"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-12",
    "href": "dl_lec3.html#backpropagation-in-detail-12",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nLooking back at \\(\\eqref{eq:dw_scalar}\\) and \\(\\eqref{eq:db_scalar}\\), we see that the only unknown entity is \\(\\pdv{J}{z_{j, i}^{[l]}}\\). By applying the chain rule once again, we get\n\\[\\begin{equation}\n\\pdv{J}{z_{j, i}^{[l]}} = \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}}, \\label{eq:dz_scalar}\n\\end{equation}\\]\nwhere \\(p = 1, \\dots, n^{[l]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-13",
    "href": "dl_lec3.html#backpropagation-in-detail-13",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nNext, we present the vectorized version of \\(\\eqref{eq:dz_scalar}\\):\n\\[\\begin{equation*}\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\dpdv{a_{1, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{1, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{1, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{a_{1, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{j, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{a_{n^{[l]}, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{j, i}^{[l]}} \\\\\n\\vdots \\\\\n\\dpdv{J}{a_{n^{[l]}, i}^{[l]}}\n\\end{bmatrix},\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-14",
    "href": "dl_lec3.html#backpropagation-in-detail-14",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nWhich can be converted into\n\\[\\begin{equation}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\pdv{\\vec{a}_{:, i}^{[l]}}{\\vec{z}_{:, i}^{[l]}} \\pdv{J}{\\vec{a}_{:, i}^{[l]}}, \\label{eq:dz}\n\\end{equation}\\]\nwhere ( {n{[l]}}) and ( {n{[l]} n^{[l]}})."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-15",
    "href": "dl_lec3.html#backpropagation-in-detail-15",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nWe have already encountered\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{z}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{z}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dZ}\n\\end{equation}\\]\nand for the sake of completeness, we also clarify that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[l]}} =\n\\begin{bmatrix}\n\\dpdv{J}{\\vec{a}_{:, 1}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, i}^{[l]}} & \\dots & \\dpdv{J}{\\vec{a}_{:, m}^{[l]}}\n\\end{bmatrix}, \\label{eq:dA}\n\\end{equation}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l]}} \\in \\R^{n^{[l]} \\times m}\\).\nOn purpose, we have omitted the details of \\(g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]})\\); consequently, we cannot derive an analytic expression for \\(\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\), which we depend on in \\(\\eqref{eq:dz_scalar}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-16",
    "href": "dl_lec3.html#backpropagation-in-detail-16",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nFurthermore, according to \\(\\eqref{eq:dz_scalar}\\), we see that \\(\\pdv{J}{z_{j, i}^{[l]}}\\) also depends on \\(\\pdv{J}{a_{j, i}^{[l]}}\\). Now, it might come as a surprise, but \\(\\pdv{J}{a_{j, i}^{[l]}}\\) has already been computed when we reach the \\(l\\)-th layer during backward propagation. How did that happen, you may ask. The answer is that every layer paves the way for the previous layer by also computing \\(\\pdv{J}{a_{k, i}^{[l - 1]}}\\), which we shall do now:\n\\[\\begin{equation}\n\\pdv{J}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} \\pdv{z_{j, i}^{[l]}}{a_{k, i}^{[l - 1]}} = \\sum_j \\pdv{J}{z_{j, i}^{[l]}} w_{j, k}^{[l]}. \\label{eq:da_prev_scalar}\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-17",
    "href": "dl_lec3.html#backpropagation-in-detail-17",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nAs usual, our next step is vectorization:\n\\[\\begin{equation*}\n\\begin{split}\n&\n\\begin{bmatrix}\n\\dpdv{J}{a_{1, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{1, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{k, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{k, m}^{[l - 1]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{a_{n^{[l - 1]}, 1}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, i}^{[l - 1]}} & \\dots & \\dpdv{J}{a_{n^{[l - 1]}, m}^{[l - 1]}}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nw_{1, 1}^{[l]} & \\dots & w_{j, 1}^{[l]} & \\dots & w_{n^{[l]}, 1}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, k}^{[l]} & \\dots & w_{j, k}^{[l]} & \\dots & w_{n^{[l]}, k}^{[l]} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nw_{1, n^{[l - 1]}}^{[l]} & \\dots & w_{j, n^{[l - 1]}}^{[l]} & \\dots & w_{n^{[l]}, n^{[l - 1]}}^{[l]}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n\\dpdv{J}{z_{1, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{1, i}^{[l]}} & \\dots & \\dpdv{J}{z_{1, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{j, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{j, i}^{[l]}} & \\dots & \\dpdv{J}{z_{j, m}^{[l]}} \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, i}^{[l]}} & \\dots & \\dpdv{J}{z_{n^{[l]}, m}^{[l]}}\n\\end{bmatrix},\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-18",
    "href": "dl_lec3.html#backpropagation-in-detail-18",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nwhich we can write as \\[\\begin{align}\n&\\pdv{J}{\\vec{A}^{[l - 1]}} =\\vec{W}^{[l]T} \\pdv{J}{\\vec{Z}^{[l]}}, \\label{eq:dA_prev}\n\\end{align}\\]\nwhere \\(\\pdv{J}{\\vec{A}^{[l - 1]}} \\in \\R^{n^{[l - 1]} \\times m}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-19",
    "href": "dl_lec3.html#backpropagation-in-detail-19",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\n\n\n\nInitial values\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}\\)\nBackrward: \\(\\pdv{J}{\\vec{A}^{[L]}} = \\pdv{J}{\\vec{\\hat{Y}}}\\)\n\n\n\n\n\n\n\nComputations\n\n\n\nForward: \\(\\vec{A}^{[0]} = \\vec{X}, \\vec{A}^{[L]} = \\vec{\\hat{Y}}, J = f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y})\\) i- Backward: \\(\\dfrac{\\partial J}{\\partial \\vec{A}^{[L]}} = \\dfrac{\\partial J}{\\partial \\vec{\\hat{Y}}}, \\dfrac{\\partial J}{\\partial \\vec{W}^{[l]}}, \\dfrac{\\partial J}{\\partial \\vec{b}^{[l]}}\\)"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-20",
    "href": "dl_lec3.html#backpropagation-in-detail-20",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail",
    "text": "Backpropagation in detail\nNow, you might have noticed that we have yet to derive an analytic expression for the backpropagation seed \\(\\dpdv{J}{\\vec{A}^{[L]}} = \\dpdv{J}{\\vec{\\hat{Y}}}\\).\nLet’s derive an analytic expression for \\(\\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\\) or, by extension, \\(\\dpdv{J}{z_{j, i}^{[l]}}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-relu",
    "href": "dl_lec3.html#backpropagation-in-detail-relu",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: ReLU",
    "text": "Backpropagation in detail: ReLU\nThe rectified linear unit, or ReLU for short, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\max(0, z_{j, i}^{[l]}) = \\\\\n&=\n\\begin{cases}\nz_{j, i}^{[l]} &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\max(0, \\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-relu-1",
    "href": "dl_lec3.html#backpropagation-in-detail-relu-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: ReLU",
    "text": "Backpropagation in detail: ReLU\nCompute the partial derivatives of the activations in the current layer:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &\\coloneqq\n\\begin{cases}\n1 &\\text{if } z_{j, i}^{[l]} &gt; 0, \\\\\n0 &\\text{otherwise,}\n\\end{cases} \\\\\n&= I(z_{j, i}^{[l]} &gt; 0), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= 0, \\quad \\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-relu-2",
    "href": "dl_lec3.html#backpropagation-in-detail-relu-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: ReLU",
    "text": "Backpropagation in detail: ReLU\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} I(z_{j, i}^{[l]} &gt; 0),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot I(\\vec{Z}^{[l]} &gt; 0),\n\\end{equation}\\]\nwhere \\(\\odot\\) denotes element-wise multiplication (Hadamard product)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nThe sigmoid activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\sigma(z_{j, i}^{[l]}) \\\\\n&= \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization yields\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{1 + \\exp(-\\vec{Z}^{[l]})}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid-1",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nTo practice backward propagation, first, we construct a computation graph: \\[\n\\begin{tikzpicture}[node distance=5mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_0 = z_{j, i}^{[l]}$};\n    \\node (B) [default,below=of A] {$u_1 = -u_0$};\n    \\node (C) [default,below=of B] {$u_2 = \\exp(u_1)$};\n    \\node (D) [default,below=of C] {$u_3 = 1 + u_2$};\n    \\node (E) [default,below=of D] {$u_4 = \\dfrac{1}{u_3} = a_{j, i}^{[l]}$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid-2",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nCompute, starting from outside:\n\\[\\begin{align*}\n&\\pdv{a_{j, i}^{[l]}}{u_4} = 1, \\; \\pdv{a_{j, i}^{[l]}}{u_3} = \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_2} = \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_2} = -\\frac{1}{u_3^2} = -\\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_1} = \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_1} = -\\frac{1}{u_3^2} \\exp(u_1) = -\\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n&\\pdv{a_{j, i}^{[l]}}{u_0} = \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} = \\frac{1}{u_3^2} \\exp(u_1) = \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid-3",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nLet us simplify:\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{\\exp(-z_{j, i}^{[l]})}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{1 + \\exp(-z_{j, i}^{[l]}) - 1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{1}{1 + \\exp(-z_{j, i}^{[l]})} - \\frac{1}{(1 + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid-4",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nWe also note that\n\\[\\begin{equation*}\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\n\\end{equation*}\\]\nConsequently,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-sigmoid-5",
    "href": "dl_lec3.html#backpropagation-in-detail-sigmoid-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: Sigmoid",
    "text": "Backpropagation in detail: Sigmoid\nLastly, no summations mean trivial vectorization:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]} \\odot (1 - \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\nThe hyperbolic tangent function, i.e., the tanh activation function, is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\tanh(z_{j, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nBy utilizing element-wise multiplication, we get\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\exp(\\vec{Z}^{[l]}) + \\exp(-\\vec{Z}^{[l]})} \\odot (\\exp(\\vec{Z}^{[l]}) - \\exp(-\\vec{Z}^{[l]})).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-1",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\nComputation graph: \\[\n\\begin{tikzpicture}[node distance=2mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_0 = z_{j, i}^{[l]}$};\n    \\node (B) [default,below=of A] {$u_1 = -u_0$};\n    \\node (C) [default,below=of B] {$u_2 = \\exp(u_0)$};\n    \\node (D) [default,below=of C] {$u_3 = \\exp(u_1)$};\n    \\node (E) [default,below=of D] {$u_4 = u_2 - u_3$};\n    \\node (F) [default,below=of E] {$u_5 = u_2 + u_3$};\n    \\node (G) [default,below=of F] {$u_6 = \\frac{1}{u_5}$};\n    \\node (H) [default,below=of G] {$u_7 = u_4 u_6 = a_{j, i}^{[l]}$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n  \\draw [-&gt;] (E.south) -&gt; (F.north);\n  \\draw [-&gt;] (F.south) -&gt; (G.north);\n  \\draw [-&gt;] (G.south) -&gt; (H.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-2",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\nNext, we compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_7} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_6} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_6} = u_4 = \\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_5} &= \\pdv{a_{j, i}^{[l]}}{u_6} \\pdv{u_6}{u_5} = -u_4 \\frac{1}{u_5^2} = -\\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_7} \\pdv{u_7}{u_4} = u_6 = \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-3",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_3} \\\\\n&= -u_6 - u_4 \\frac{1}{u_5^2} \\notag \\\\\n&= -\\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_2} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_2} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_2} = u_6 - u_4 \\frac{1}{u_5^2} \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-4",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\n\\[\\begin{align*}\n&= \\frac{1}{\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})} - \\frac{\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{2 \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} \\\\\n&= \\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) \\notag \\\\\n&= -\\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-5",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_0} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_0} + \\pdv{a_{j, i}^{[l]}}{u_2} \\pdv{u_2}{u_0} \\\\\n&= -\\Bigl(-u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_1) + \\Bigl(u_6 - u_4 \\frac{1}{u_5^2}\\Bigr) \\exp(u_0) \\notag \\\\\n&= \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} + \\frac{2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\notag \\\\\n&= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-6",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\nIt follows that\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= \\frac{4 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]})}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})^2 + 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&\\peq\\negmedspace{} - \\frac{\\exp(z_{j, i}^{[l]})^2 - 2 \\exp(z_{j, i}^{[l]}) \\exp(-z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]})^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} \\\\\n&= 1 - \\frac{(\\exp(z_{j, i}^{[l]}) - \\exp(-z_{j, i}^{[l]}))^2}{(\\exp(z_{j, i}^{[l]}) + \\exp(-z_{j, i}^{[l]}))^2} = 1 - a_{j, i}^{[l]} a_{j, i}^{[l]}.\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-tanh-7",
    "href": "dl_lec3.html#backpropagation-in-detail-tanh-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: tanh",
    "text": "Backpropagation in detail: tanh\nSimilarly to the sigmoid activation function: \\(\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \\quad \\forall p \\ne j.\\)\nThus,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} \\left(1 - a_{j, i}^{[l]} a_{j, i}^{[l]}\\right),\n\\end{split}\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\pdv{J}{\\vec{A}^{[l]}} \\odot (1 - \\vec{A}^{[l]} \\odot \\vec{A}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nThe softmax activation function is given by\n\\[\\begin{equation*}\n\\begin{split}\na_{j, i}^{[l]} &= g_j^{[l]}(z_{1, i}^{[l]}, \\dots, z_{j, i}^{[l]}, \\dots, z_{n^{[l]}, i}^{[l]}) \\\\\n&= \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}.\n\\end{split}\n\\end{equation*}\\]\nVectorization results in\n\\[\\begin{equation}\n\\vec{A}^{[l]} = \\frac{1}{\\broadcast(\\underbrace{\\sum_{\\text{axis} = 0} \\exp(\\vec{Z}^{[l]})}_\\text{row vector})} \\odot \\exp(\\vec{Z}^{[l]}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-1",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nTo begin with, we construct a computation graph for the (j)th activation of the current layer: \\[\n\\begin{tikzpicture}[node distance=2mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_{-1} = z_{j, i}^{[l]}$};\n    \\node (B) [default,below=of A] {$u_{0, p} = z_{p, i}^{[l]}, \\; \\forall p \\ne j$};\n    \\node (C) [default,below=of B] {$u_1 = \\exp(u_{-1})$};\n    \\node (D) [default,below=of C] {$u_{2, p} = \\exp(u_{0, p}), \\; \\forall p \\ne j$};\n    \\node (E) [default,below=of D] {$u_3 = u_1 + \\sum_{p \\ne j} u_{2, p}$};\n    \\node (F) [default,below=of E] {$u_4 = \\frac{1}{u_3}$};\n    \\node (G) [default,below=of F] {$u_5 = u_1 u_4 = a_{j, i}^{[l]}$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n  \\draw [-&gt;] (E.south) -&gt; (F.north);\n  \\draw [-&gt;] (F.south) -&gt; (G.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-2",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nBy applying the chain rule, we get\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_4} &= \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_1 = \\exp(z_{j, i}^{[l]}), \\\\\n\\pdv{a_{j, i}^{[l]}}{u_3} &= \\pdv{a_{j, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_1 \\frac{1}{u_3^2} = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{j, i}^{[l]}}{u_1} &= \\pdv{a_{j, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} + \\pdv{a_{j, i}^{[l]}}{u_5} \\pdv{u_5}{u_1} \\\\\n&= -u_1 \\frac{1}{u_3^2} + u_4 = -\\frac{\\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{1}{\\sum_p \\exp(z_{p, i}^{[l]})}, \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-3",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{u_{-1}} &= \\pdv{a_{j, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = \\Bigl(-u_1 \\frac{1}{u_3^2} + u_4\\Bigr) \\exp(u_{-1}) \\notag \\\\\n&= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})}. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-4",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nNext, we need to take into account that (z_{j, i}^{[l]}) also affects other activations in the same layer:\n\\[\\begin{align*}\nu_{-1} &= z_{j, i}^{[l]}, \\\\\nu_{0, p} &= z_{p, i}^{[l]}, &&\\forall p \\ne j, \\\\\nu_1 &= \\exp(u_{-1}), \\\\\nu_{2, p} &= \\exp(u_{0, p}), &&\\forall p \\ne j, \\\\\nu_3 &= u_1 + \\sum_{p \\ne j} u_{2, p}, \\\\\nu_4 &= \\frac{1}{u_3}, \\\\\nu_5 &= u_{2, p} u_4 = a_{p, i}^{[l]}, &&\\forall p \\ne j.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-5",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\n\\[\\begin{align*}\n\\pdv{a_{p, i}^{[l]}}{u_5} &= 1, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_4} &= \\pdv{a_{p, i}^{[l]}}{u_5} \\pdv{u_5}{u_4} = u_{2, p} = \\exp(z_{p, i}^{[l]}), \\\\\n\\pdv{a_{p, i}^{[l]}}{u_3} &= \\pdv{a_{p, i}^{[l]}}{u_4} \\pdv{u_4}{u_3} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_1} &= \\pdv{a_{p, i}^{[l]}}{u_3} \\pdv{u_3}{u_1} = -u_{2, p} \\frac{1}{u_3^2} = -\\frac{\\exp(z_{p, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}, \\\\\n\\pdv{a_{p, i}^{[l]}}{u_{-1}} &= \\pdv{a_{p, i}^{[l]}}{u_1} \\pdv{u_1}{u_{-1}} = -u_{2, p} \\frac{1}{u_3^2} \\exp(u_{-1}) = -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-6",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nWe now know that\n\\[\\begin{align*}\n\\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{j, i}^{[l]})^2}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} + \\frac{\\exp(z_{j, i}^{[l]})}{\\sum_p \\exp(z_{p, i}^{[l]})} \\\\\n&= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}), \\notag \\\\\n\\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &= -\\frac{\\exp(z_{p, i}^{[l]}) \\exp(z_{j, i}^{[l]})}{(\\sum_p \\exp(z_{p, i}^{[l]}))^2} \\\\\n&= -a_{p, i}^{[l]} a_{j, i}^{[l]}, \\quad \\forall p \\ne j. \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-7",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-7",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[l]}} &= \\sum_p \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\pdv{J}{a_{j, i}^{[l]}} \\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} \\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\\\\n&= \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} a_{j, i}^{[l]} \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_{p \\ne j} \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} + \\pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]}\\Bigr) \\\\\n&= a_{j, i}^{[l]} \\Bigl(\\pdv{J}{a_{j, i}^{[l]}} - \\sum_p \\pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\\Bigr),\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-softmax-8",
    "href": "dl_lec3.html#backpropagation-in-detail-softmax-8",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: softmax",
    "text": "Backpropagation in detail: softmax\nVectorized version: \\[\\begin{equation*}\n\\pdv{J}{\\vec{z}_{:, i}^{[l]}} = \\vec{a}_{:, i}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{a}_{:, i}^{[l]}} - \\underbrace{{\\vec{a}_{:, i}^{[l]}}^T \\pdv{J}{\\vec{a}_{:, i}^{[l]}}}_{\\text{scalar}}\\Bigr).\n\\end{equation*}\\]\nLet us not stop with the vectorization just yet:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[l]}} = \\vec{A}^{[l]} \\odot \\Bigl(\\pdv{J}{\\vec{A}^{[l]}} - \\broadcast\\bigl(\\underbrace{\\sum_{\\text{axis} = 0} \\pdv{J}{\\vec{A}^{[l]}} \\odot \\vec{A}^{[l]}}_\\text{row vector}\\bigr)\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-binary",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-binary",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for binary",
    "text": "Backpropagation in detail: cost for binary\nIn binary classification, the cost function is given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)) \\\\\n&= -\\frac{1}{m} \\sum_i (y_i \\log(a_i^{[L]}) + (1 - y_i) \\log(1 - a_i^{[L]})),\n\\end{split}\n\\end{equation*}\\]\nwhich we can write as\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\text{axis} = 1} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-1",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for binary",
    "text": "Backpropagation in detail: cost for binary\nNext, we construct a computation graph: \\[\n\\begin{tikzpicture}[node distance=3mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_{0, i} = a_i^{[L]}$};\n    \\node (B) [default,below=of A] {$u_{1, i} = 1 - u_{0, i}$};\n    \\node (C) [default,below=of B] {$u_{2, i} = \\log(u_{0, i})$};\n    \\node (D) [default,below=of C] {$u_{3, i} = \\log(u_{1, i})$};\n    \\node (E) [default,below=of D] {$u_{4, i} = y_i u_{2, i} + (1 - y_i) u_{3, i}$};\n    \\node (F) [default,below=of E] {$u_5 = -\\frac{1}{m} \\sum_i u_{4, i} = J$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n  \\draw [-&gt;] (E.south) -&gt; (F.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-2",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for binary",
    "text": "Backpropagation in detail: cost for binary\nLet’s compute derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_5} &= 1, \\; \\pdv{J}{u_{4, i}} = \\pdv{J}{u_5} \\pdv{u_5}{u_{4, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{3, i}} = -\\frac{1}{m} (1 - y_i), \\\\\n\\pdv{J}{u_{2, i}} &= \\pdv{J}{u_{4, i}} \\pdv{u_{4, i}}{u_{2, i}} = -\\frac{1}{m} y_i, \\\\\n\\pdv{J}{u_{1, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{1, i}} = -\\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} = -\\frac{1}{m} \\frac{1 - y_i}{1 - a_i^{[L]}}, \\\\\n\\pdv{J}{u_{0, i}} &= \\pdv{J}{u_{1, i}} \\pdv{u_{1, i}}{u_{0, i}} + \\pdv{J}{u_{2, i}} \\pdv{u_{2, i}}{u_{0, i}} = \\frac{1}{m} (1 - y_i) \\frac{1}{u_{1, i}} - \\frac{1}{m} y_i \\frac{1}{u_{0, i}} = \\\\\n&=\\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-3",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for binary",
    "text": "Backpropagation in detail: cost for binary\nThus,\n\\[\\begin{equation*}\n\\pdv{J}{a_i^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr),\n\\end{equation*}\\]\nwhich implies that\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-4",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-binary-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for binary",
    "text": "Backpropagation in detail: cost for binary\nIn addition, since the sigmoid activation function is used in the output layer, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_i^{[L]}} &= \\pdv{J}{a_i^{[L]}} a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_i}{1 - a_i^{[L]}} - \\frac{y_i}{a_i^{[L]}}\\Bigr) a_i^{[L]} (1 - a_i^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_i) a_i^{[L]} - y_i (1 - a_i^{[L]})) \\\\\n&= \\frac{1}{m} (a_i^{[L]} - y_i).\n\\end{split}\n\\end{equation*}\\]\nIn other words,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]\nNote that both \\(\\pdv{J}{\\vec{Z}^{[L]}} \\in \\R^{1 \\times m}\\) and \\(\\pdv{J}{\\vec{A}^{[L]}} \\in \\R^{1 \\times m}\\), because \\(n^{[L]} = 1\\) in this case."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multiclass",
    "text": "Backpropagation in detail: cost for multiclass\nIn multiclass classification, the cost function is instead given by\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(\\hat{y}_{j, i}) \\\\\n&= -\\frac{1}{m} \\sum_i \\sum_j y_{j, i} \\log(a_{j, i}^{[L]}),\n\\end{split}\n\\end{equation*}\\]\nwhere \\(j = 1, \\dots, n^{[L]}\\). We can vectorize the cost expression:\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 0 \\\\ \\text{axis} = 1}} \\vec{Y} \\odot \\log(\\vec{A}^{[L]})}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-1",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multiclass",
    "text": "Backpropagation in detail: cost for multiclass\n\\[\n\\begin{tikzpicture}[node distance=5mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_{0, j, i} = a_{j, i}^{[L]}$};\n    \\node (B) [default,below=of A] {$u_{1, j, i} = \\log(u_{0, j, i})$};\n    \\node (C) [default,below=of B] {$u_{2, j, i} = y_{j, i} u_{1, j, i}$};\n    \\node (D) [default,below=of C] {$u_{3, i} = \\sum_j u_{2, j, i}$};\n    \\node (E) [default,below=of D] {$u_4 = -\\frac{1}{m} \\sum_i u_{3, i} = J$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost",
    "href": "dl_lec3.html#backpropagation-in-detail-cost",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost",
    "text": "Backpropagation in detail: cost\nWith the computation graph in place, we can perform backward propagation:\n\\[\\begin{align*}\n\\pdv{J}{u_4} &= 1, \\\\\n\\pdv{J}{u_{3, i}} &= \\pdv{J}{u_4} \\pdv{u_4}{u_{3, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{3, i}} \\pdv{u_{3, i}}{u_{2, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{1, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} = -\\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-2",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multiclass",
    "text": "Backpropagation in detail: cost for multiclass\nHence,\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = -\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}}.\n\\end{equation*}\\]\nVectorization is trivial:\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = -\\frac{1}{m} \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-3",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multiclass-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multiclass",
    "text": "Backpropagation in detail: cost for multiclass\nFurthermore, since the output layer uses the softmax activation function, we get\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= a_{j, i}^{[L]} \\Bigl(\\pdv{J}{a_{j, i}^{[L]}} - \\sum_p \\pdv{J}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) = a_{j, i}^{[L]} \\Bigl(-\\frac{1}{m} \\frac{y_{j, i}}{a_{j, i}^{[L]}} + \\sum_p \\frac{1}{m} \\frac{y_{p, i}}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\\Bigr) \\\\\n&= \\frac{1}{m} \\Bigl(-y_{j, i} + a_{j, i}^{[L]} \\underbrace{\\sum_p y_{p, i}}_{\\mathclap{\\sum \\text{probabilities} = 1}}\\Bigr) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}).\n\\end{split}\n\\end{equation*}\\]\nNote that \\(p = 1, \\dots, n^{[L]}\\). To conclude,\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\nWe can view multi-label classification as \\(j\\) binary classification problems:\n\\[\\begin{equation*}\n\\begin{split}\nJ &= f(\\vec{\\hat{Y}}, \\vec{Y}) = f(\\vec{A}^{[L]}, \\vec{Y}) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(\\hat{y}_{j, i}) + (1 - y_{j, i}) \\log(1 - \\hat{y}_{j, i}))\\Bigr) \\\\\n&= \\sum_j \\Bigl(-\\frac{1}{m} \\sum_i (y_{j, i} \\log(a_{j, i}^{[L]}) + (1 - y_{j, i}) \\log(1 - a_{j, i}^{[L]}))\\Bigr),\n\\end{split}\n\\end{equation*}\\]\nwhere once again \\(j = 1, \\dots, n^{[L]}\\)."
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-1",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-1",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\nVectorization gives\n\\[\\begin{equation}\nJ = -\\frac{1}{m} \\underbrace{\\sum_{\\substack{\\text{axis} = 1 \\\\ \\text{axis} = 0}} (\\vec{Y} \\odot \\log(\\vec{A}^{[L]}) + (1 - \\vec{Y}) \\odot \\log(1 - \\vec{A}^{[L]}))}_\\text{scalar}.\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-2",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-2",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\n\\[\n\\begin{tikzpicture}[node distance=2mm,minimum size=6mm,default/.style={rectangle,very thick,fill=green!20, draw=black!50,rounded corners=3mm}]\n  \\node (A) [default] {$u_{0, j, i} = a_{j, i}^{[L]}$};\n    \\node (B) [default,below=of A] {$u_{1, j, i} = 1 - u_{0, j, i}$};\n    \\node (C) [default,below=of B] {$u_{2, j, i} = \\log(u_{0, j, i})$};\n    \\node (D) [default,below=of C] {$u_{3, j, i} = \\log(u_{1, j, i})$};\n    \\node (E) [default,below=of D] {$u_{4, j, i} = y_{j, i} u_{2, j, i} + (1 - y_{j, i}) u_{3, j, i}$};\n    \\node (F) [default,below=of E] {$u_{5, j} = -\\frac{1}{m} \\sum_i u_{4, j, i}$};\n    \\node (G) [default,below=of F] {$u_6 = \\sum_j u_{5, j} = J$};\n  \\draw [-&gt;] (A.south) -&gt; (B.north);\n  \\draw [-&gt;] (B.south) -&gt; (C.north);\n  \\draw [-&gt;] (C.south) -&gt; (D.north);\n  \\draw [-&gt;] (D.south) -&gt; (E.north);\n  \\draw [-&gt;] (E.south) -&gt; (F.north);\n  \\draw [-&gt;] (F.south) -&gt; (G.north);\n\\end{tikzpicture}\n\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-3",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-3",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\nNext, we compute the partial derivatives:\n\\[\\begin{align*}\n\\pdv{J}{u_6} &= 1, \\\\\n\\pdv{J}{u_{5, j}} &= \\pdv{J}{u_6} \\pdv{u_6}{u_{5, j}} = 1, \\\\\n\\pdv{J}{u_{4, j, i}} &= \\pdv{J}{u_{5, j}} \\pdv{u_{5, j}}{u_{4, j, i}} = -\\frac{1}{m}, \\\\\n\\pdv{J}{u_{3, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{3, j, i}} = -\\frac{1}{m} (1 - y_{j, i}), \\\\\n\\pdv{J}{u_{2, j, i}} &= \\pdv{J}{u_{4, j, i}} \\pdv{u_{4, j, i}}{u_{2, j, i}} = -\\frac{1}{m} y_{j, i}, \\\\\n\\pdv{J}{u_{1, j, i}} &= \\pdv{J}{u_{3, j, i}} \\pdv{u_{3, j, i}}{u_{1, j, i}} = -\\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} = -\\frac{1}{m} \\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}},\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-4",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-4",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\n\\[\\begin{align*}\n\\pdv{J}{u_{0, j, i}} &= \\pdv{J}{u_{1, j, i}} \\pdv{u_{1, j, i}}{u_{0, j, i}} + \\pdv{J}{u_{2, j, i}} \\pdv{u_{2, j, i}}{u_{0, j, i}} \\\\\n&= \\frac{1}{m} (1 - y_{j, i}) \\frac{1}{u_{1, j, i}} - \\frac{1}{m} y_{j, i} \\frac{1}{u_{0, j, i}} \\notag \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr). \\notag\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-5",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-5",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\nSimply put, we have\n\\[\\begin{equation*}\n\\pdv{J}{a_{j, i}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr),\n\\end{equation*}\\]\nand\n\\[\\begin{equation}\n\\pdv{J}{\\vec{A}^{[L]}} = \\frac{1}{m} \\Bigl(\\frac{1}{1 - \\vec{A}^{[L]}} \\odot (1 - \\vec{Y}) - \\frac{1}{\\vec{A}^{[L]}} \\odot \\vec{Y}\\Bigr).\n\\end{equation}\\]"
  },
  {
    "objectID": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-6",
    "href": "dl_lec3.html#backpropagation-in-detail-cost-for-multilabel-6",
    "title": "Deep learning: multi-layer NNs",
    "section": "Backpropagation in detail: cost for multilabel",
    "text": "Backpropagation in detail: cost for multilabel\nBearing in mind that we view multi-label classification as \\(j\\) binary classification problems, we also know that the output layer uses the sigmoid activation function. As a result,\n\\[\\begin{equation*}\n\\begin{split}\n\\pdv{J}{z_{j, i}^{[L]}} &= \\pdv{J}{a_{j, i}^{[L]}} a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} \\Bigl(\\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \\frac{y_{j, i}}{a_{j, i}^{[L]}}\\Bigr) a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\\\\n&= \\frac{1}{m} ((1 - y_{j, i}) a_{j, i}^{[L]} - y_{j, i} (1 - a_{j, i}^{[L]})) = \\frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}),\n\\end{split}\n\\end{equation*}\\]\nwhich we can vectorize as\n\\[\\begin{equation}\n\\pdv{J}{\\vec{Z}^{[L]}} = \\frac{1}{m} (\\vec{A}^{[L]} - \\vec{Y}).\n\\end{equation}\\]"
  },
  {
    "objectID": "nlp_lab4_full.html",
    "href": "nlp_lab4_full.html",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nPrepare models for the classifier, based on cleaned-up tokens.\nRun the Naive Bayes classifier.\n\n\n\n\nFirst, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\n\n\n\nWordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('racer.n.02'),\n Synset('bus.n.04'),\n Synset('sport_utility.n.01'),\n Synset('horseless_carriage.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('minicar.n.01'),\n Synset('loaner.n.02'),\n Synset('hot_rod.n.01'),\n Synset('compact.n.03'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('sports_car.n.01'),\n Synset('hardtop.n.01'),\n Synset('stock_car.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('trunk.n.01'),\n Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]\n\n\n\n\n\n\nLet’s write a function that will lemmatize twitter tokens.\nNow fetch PoS tokens so that they can be passed to WordNetLemmatizer.\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatized_sentence = []\n# Convert PoS tags into a format used by the lemmatizer\n# and run lemmatize\nfor word, tag in pos_tag(tokens):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a'\n    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\nprint(lemmatized_sentence)\n\n['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNote that it converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come).\n\n\n\nNow we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nHere comes the process_tokens function:\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Now note the sheer size of regex for URLs :)\n        # Mentions regex is comparatively short and sweet\n        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n            re.search(r'(@[A-Za-z0-9_]+)', token)):\n            continue\n\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n   \n        token = lemmatizer.lemmatize(token, pos)\n\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nLet’s test it:\n\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\n\nBefore: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\nAfter: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n\n\nRun process_tokens on all positive/negative tokens.\n\npositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\nnegative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n\nLet’s see how did the processing go.\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\n\n['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n['dang', 'rad', '#fanart', ':d']\n\n\nLet’s see what is most common there. Add a helper function get_all_words:\n\ndef get_all_words(cleaned_tokens_list):\n    return [w for tokens in cleaned_tokens_list for w in tokens]\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\n\nPerform frequency analysis using FreqDist:\n\nfrom nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))\n\n[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 332), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n\n\nFine. Now we’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\n\n['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n\n\n\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\n\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\n\nAccuracy is: 0.997\nMost Informative Features\n                      :( = True           Negati : Positi =   2088.5 : 1.0\n                     sad = True           Negati : Positi =     23.7 : 1.0\n              bestfriend = True           Positi : Negati =     23.2 : 1.0\n                follower = True           Positi : Negati =     21.8 : 1.0\n                     bam = True           Positi : Negati =     20.6 : 1.0\n                    glad = True           Positi : Negati =     19.3 : 1.0\n                     x15 = True           Negati : Positi =     17.3 : 1.0\n                 welcome = True           Positi : Negati =     15.6 : 1.0\n                followed = True           Negati : Positi =     14.9 : 1.0\n              appreciate = True           Positi : Negati =     14.1 : 1.0\nNone\n\n\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\n\nnltk.download('punkt_tab')\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\n\nPositive\n\n\nLet’s package it as a function and test it:\n\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))\n\nbad :  Negative\nservice is bad :  Positive\nservice is really bad :  Negative\nservice is so terrible :  Positive\ngreat service :  Positive\nthey stole my money :  Negative"
  },
  {
    "objectID": "nlp_lab4_full.html#plan",
    "href": "nlp_lab4_full.html#plan",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nPrepare models for the classifier, based on cleaned-up tokens.\nRun the Naive Bayes classifier."
  },
  {
    "objectID": "nlp_lab4_full.html#preparation",
    "href": "nlp_lab4_full.html#preparation",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "First, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]"
  },
  {
    "objectID": "nlp_lab4_full.html#wordnet",
    "href": "nlp_lab4_full.html#wordnet",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "WordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('racer.n.02'),\n Synset('bus.n.04'),\n Synset('sport_utility.n.01'),\n Synset('horseless_carriage.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('minicar.n.01'),\n Synset('loaner.n.02'),\n Synset('hot_rod.n.01'),\n Synset('compact.n.03'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('sports_car.n.01'),\n Synset('hardtop.n.01'),\n Synset('stock_car.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('trunk.n.01'),\n Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]"
  },
  {
    "objectID": "nlp_lab4_full.html#lemmatization-function",
    "href": "nlp_lab4_full.html#lemmatization-function",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Let’s write a function that will lemmatize twitter tokens.\nNow fetch PoS tokens so that they can be passed to WordNetLemmatizer.\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatized_sentence = []\n# Convert PoS tags into a format used by the lemmatizer\n# and run lemmatize\nfor word, tag in pos_tag(tokens):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a'\n    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\nprint(lemmatized_sentence)\n\n['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNote that it converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come)."
  },
  {
    "objectID": "nlp_lab4_full.html#processing",
    "href": "nlp_lab4_full.html#processing",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Now we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nHere comes the process_tokens function:\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Now note the sheer size of regex for URLs :)\n        # Mentions regex is comparatively short and sweet\n        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n            re.search(r'(@[A-Za-z0-9_]+)', token)):\n            continue\n\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n   \n        token = lemmatizer.lemmatize(token, pos)\n\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nLet’s test it:\n\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\n\nBefore: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\nAfter: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n\n\nRun process_tokens on all positive/negative tokens.\n\npositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\nnegative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n\nLet’s see how did the processing go.\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\n\n['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n['dang', 'rad', '#fanart', ':d']\n\n\nLet’s see what is most common there. Add a helper function get_all_words:\n\ndef get_all_words(cleaned_tokens_list):\n    return [w for tokens in cleaned_tokens_list for w in tokens]\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\n\nPerform frequency analysis using FreqDist:\n\nfrom nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))\n\n[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 332), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n\n\nFine. Now we’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\n\n['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n\n\n\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\n\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\n\nAccuracy is: 0.997\nMost Informative Features\n                      :( = True           Negati : Positi =   2088.5 : 1.0\n                     sad = True           Negati : Positi =     23.7 : 1.0\n              bestfriend = True           Positi : Negati =     23.2 : 1.0\n                follower = True           Positi : Negati =     21.8 : 1.0\n                     bam = True           Positi : Negati =     20.6 : 1.0\n                    glad = True           Positi : Negati =     19.3 : 1.0\n                     x15 = True           Negati : Positi =     17.3 : 1.0\n                 welcome = True           Positi : Negati =     15.6 : 1.0\n                followed = True           Negati : Positi =     14.9 : 1.0\n              appreciate = True           Positi : Negati =     14.1 : 1.0\nNone\n\n\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\n\nnltk.download('punkt_tab')\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\n\nPositive\n\n\nLet’s package it as a function and test it:\n\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))\n\nbad :  Negative\nservice is bad :  Positive\nservice is really bad :  Negative\nservice is so terrible :  Positive\ngreat service :  Positive\nthey stole my money :  Negative"
  },
  {
    "objectID": "nlp_lab4_full.html#lemmatization-function-1",
    "href": "nlp_lab4_full.html#lemmatization-function-1",
    "title": "NLP: Lab 3",
    "section": "Lemmatization function",
    "text": "Lemmatization function\nLet’s write a function that will lemmatize twitter tokens.\nNow fetch PoS tokens so that they can be passed to WordNetLemmatizer.\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatized_sentence = []\n# Convert PoS tags into a format used by the lemmatizer\n# and run lemmatize\nfor word, tag in pos_tag(tokens):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a'\n    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\nprint(lemmatized_sentence)\n\n['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNote that it converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come)."
  },
  {
    "objectID": "nlp_lab4_full.html#processing-1",
    "href": "nlp_lab4_full.html#processing-1",
    "title": "NLP: Lab 3",
    "section": "Processing",
    "text": "Processing\nNow we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nHere comes the process_tokens function:\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Now note the sheer size of regex for URLs :)\n        # Mentions regex is comparatively short and sweet\n        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n            re.search(r'(@[A-Za-z0-9_]+)', token)):\n            continue\n\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n   \n        token = lemmatizer.lemmatize(token, pos)\n\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nLet’s test it:\n\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\n\nBefore: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\nAfter: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n\n\nRun process_tokens on all positive/negative tokens.\n\npositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\nnegative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n\nLet’s see how did the processing go.\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\n\n['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n['dang', 'rad', '#fanart', ':d']\n\n\nLet’s see what is most common there. Add a helper function get_all_words:\n\ndef get_all_words(cleaned_tokens_list):\n    return [w for tokens in cleaned_tokens_list for w in tokens]\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\n\nPerform frequency analysis using FreqDist:\n\nfrom nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))\n\n[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 332), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n\n\nFine. Now we’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\n\n['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n\n\n\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\n\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\n\nAccuracy is: 0.9943333333333333\nMost Informative Features\n                      :) = True           Positi : Negati =    988.4 : 1.0\n                     sad = True           Negati : Positi =     25.3 : 1.0\n                followed = True           Negati : Positi =     24.6 : 1.0\n                     bam = True           Positi : Negati =     22.7 : 1.0\n                follower = True           Positi : Negati =     21.2 : 1.0\n                     x15 = True           Negati : Positi =     18.5 : 1.0\n                  arrive = True           Positi : Negati =     18.4 : 1.0\n              appreciate = True           Positi : Negati =     13.5 : 1.0\n                      aw = True           Negati : Positi =     13.1 : 1.0\n                    glad = True           Positi : Negati =     12.9 : 1.0\nNone\n\n\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\n\nnltk.download('punkt_tab')\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\n\nNegative\n\n\nLet’s package it as a function and test it:\n\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))\n\nbad :  Negative\nservice is bad :  Negative\nservice is really bad :  Negative\nservice is so terrible :  Negative\ngreat service :  Positive\nthey stole my money :  Negative"
  },
  {
    "objectID": "dl_lec1.html#definitions",
    "href": "dl_lec1.html#definitions",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nDefinition\n\n\nDeep learning is a branch of machine learning based on computational models called neural networks.\n\n\n\n\n\n\nWhy deep?\n\n\nBecause neural networks involved are multi-layered.\n\n\n\n\n\n\nDefinition\n\n\nNeural networks are machine learning techniques that simulate the mechanism of learning in biological organisms."
  },
  {
    "objectID": "dl_lec1.html#definitions-1",
    "href": "dl_lec1.html#definitions-1",
    "title": "Deep learning: intro",
    "section": "Definitions",
    "text": "Definitions\n\n\n\nAlternative definition\n\n\nNeural network is computational graph of elementary units in which greater power is gained by connecting them in particular ways.\n\n\n\nLogistic regression can be thought of as a very primitive neural network."
  },
  {
    "objectID": "dl_lec1.html#why-deep-learning",
    "href": "dl_lec1.html#why-deep-learning",
    "title": "Deep learning: intro",
    "section": "Why Deep Learning?",
    "text": "Why Deep Learning?\n\n\n\nRobust\n\n\n\nWorks on raw data (), no need for feature engineering\nRobustness to natural variations in data is automatically learned\n\n\n\n\n\n\n\nGeneralizable\n\n\n\nAllows end-to-end learning (pixels-to-category, sound to sentence, English sentence to Chinese sentence, etc)\nNo need to do segmentation etc. (a lot of manual labour)\n\n\n\n\n\n\n\nScalable\n\n\n\nPerformance increases with more data, therefore method is massively parallelizable"
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml",
    "href": "dl_lec1.html#how-is-dl-different-from-ml",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\nThe most fundamental difference between deep learning and traditional machine learning is its performance as the scale of data increases."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-1",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nIn Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.\nDeep learning algorithms try to learn high-level features from data. Therefore, deep learning reduces the task of developing new feature extractor for every problem."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-2",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nA deep learning algorithm takes a long time to train. For e.g state of the art deep learning algorithm: ResNet takes about two weeks to train completely from scratch.\nWhereas machine learning comparatively takes much less time to train, ranging from a few seconds to a few hours."
  },
  {
    "objectID": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "href": "dl_lec1.html#how-is-dl-different-from-ml-3",
    "title": "Deep learning: intro",
    "section": "How is DL different from ML?",
    "text": "How is DL different from ML?\n\nAt test time, deep learning algorithm takes much less time to run.\nWhereas, if you compare machine learning algorithms, test time generally increases on increasing the size of data."
  },
  {
    "objectID": "dl_lec1.html#neural-network-data-types",
    "href": "dl_lec1.html#neural-network-data-types",
    "title": "Deep learning: intro",
    "section": "Neural network data types",
    "text": "Neural network data types\n\n\nUnstructured\n\nText\nImages\nAudio\n\n\nStructured\n\nCensus records\nMedical records\nFinancial data"
  },
  {
    "objectID": "dl_lec1.html#why-now",
    "href": "dl_lec1.html#why-now",
    "title": "Deep learning: intro",
    "section": "Why now?",
    "text": "Why now?\n\nstandard algorithms like logistic regression plateau after certain amount of data\nmore data in recent decades\nhardware progress\nalgorithms have improved"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology",
    "href": "dl_lec1.html#neural-network-biology",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nNeural Network: How similar is it to the human brain?"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-1",
    "href": "dl_lec1.html#neural-network-biology-1",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nSoma adds dendrite activity together and passes it to axon."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-2",
    "href": "dl_lec1.html#neural-network-biology-2",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\nMore dendrite activity makes more axon activity."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-3",
    "href": "dl_lec1.html#neural-network-biology-3",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nSynapse: connection between axon of one neurons and dendrites of another"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-4",
    "href": "dl_lec1.html#neural-network-biology-4",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nAxons can connect to dendrites strongly, weakly, or somewhere in between"
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-5",
    "href": "dl_lec1.html#neural-network-biology-5",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nLots of axons connect with dendrites of one neuron.Each has its own connection strength."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-6",
    "href": "dl_lec1.html#neural-network-biology-6",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nThe above illustration can be simplified as above."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-7",
    "href": "dl_lec1.html#neural-network-biology-7",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn giving numerical values to the strength of connections i.e. weights."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-8",
    "href": "dl_lec1.html#neural-network-biology-8",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nA much simplified version looks something like this."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-9",
    "href": "dl_lec1.html#neural-network-biology-9",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nOn increasing the number of neurons and synapses."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-10",
    "href": "dl_lec1.html#neural-network-biology-10",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\n\n\n\nAn example\n\n\nSuppose the first and third input has been activated."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-11",
    "href": "dl_lec1.html#neural-network-biology-11",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology\nEach node represents a pattern, a combination of neurons of the previous layers."
  },
  {
    "objectID": "dl_lec1.html#neural-network-biology-12",
    "href": "dl_lec1.html#neural-network-biology-12",
    "title": "Deep learning: intro",
    "section": "Neural Network biology",
    "text": "Neural Network biology"
  },
  {
    "objectID": "dl_lec1.html#basic-ideas",
    "href": "dl_lec1.html#basic-ideas",
    "title": "Deep learning: intro",
    "section": "Basic ideas",
    "text": "Basic ideas\n\nNN is a directed acyclic graph (DAG)\nedges in a graph are parameterized with weights\none can compute any function with this graph\n\n\n\n\nGoal\n\n\nLearn a function that relates one or more inputs to one or more outputs with the use of training examples.\n\n\n\n\n\n\nHow do we construct?\n\n\nBy computing weights. This is called training."
  },
  {
    "objectID": "dl_lec1.html#perceptron",
    "href": "dl_lec1.html#perceptron",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt - the father of deep learning.\nMark I Perceptron - built in 1957. Was able to learn and recognize letters"
  },
  {
    "objectID": "dl_lec1.html#perceptron-1",
    "href": "dl_lec1.html#perceptron-1",
    "title": "Deep learning: intro",
    "section": "Perceptron",
    "text": "Perceptron"
  },
  {
    "objectID": "dl_lec1.html#evolution",
    "href": "dl_lec1.html#evolution",
    "title": "Deep learning: intro",
    "section": "Evolution",
    "text": "Evolution\nThree periods in the evolution of deep learning:\n\nsingle-layer networks (Perceptron)\nfeed-forwards NNs: differentiable activation and error functions\ndeep multi-layer NNs"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types",
    "href": "dl_lec1.html#neural-network-types",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\nFeedforward Neural Network\nRecurrent Neural Network (RNN)\nConvolutional Neural Network (CNN)"
  },
  {
    "objectID": "dl_lec1.html#neural-network-types-1",
    "href": "dl_lec1.html#neural-network-types-1",
    "title": "Deep learning: intro",
    "section": "Neural Network Types",
    "text": "Neural Network Types\n\n\nFeedforward Neural Network\n\nConvolutional neural network (CNN)\nAutoencoder\nProbabilistic neural network (PNN)\nTime delay neural network (TDNN)\n\n\nRecurrent Neural Network (RNN)\n\nLong short-term memory RNN (LSTM)\nFully recurrent Network\nSimple recurrent Network\nEcho state network\nBi-directional RNN\nHierarchical RNN\nStochastic neural network"
  },
  {
    "objectID": "dl_lec1.html#feed-forward",
    "href": "dl_lec1.html#feed-forward",
    "title": "Deep learning: intro",
    "section": "Feed-forward",
    "text": "Feed-forward\nFeedforward NNs: very straight forward, they feed information from the front to the back (input and output)."
  },
  {
    "objectID": "dl_lec1.html#feedforward-neural-network",
    "href": "dl_lec1.html#feedforward-neural-network",
    "title": "Deep learning: intro",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops."
  },
  {
    "objectID": "dl_lec1.html#rnn",
    "href": "dl_lec1.html#rnn",
    "title": "Deep learning: intro",
    "section": "RNN",
    "text": "RNN\nRecurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle."
  },
  {
    "objectID": "dl_lec1.html#lstm",
    "href": "dl_lec1.html#lstm",
    "title": "Deep learning: intro",
    "section": "LSTM",
    "text": "LSTM\nLSTM i.e. Long-Short Term Memory aims to provide a short-term memory for RNN that can last thousands of timesteps. Classification, processing and predicting data based on time series - handwriting, speech recognition, machine translation."
  },
  {
    "objectID": "dl_lec1.html#autoencoders",
    "href": "dl_lec1.html#autoencoders",
    "title": "Deep learning: intro",
    "section": "Autoencoders",
    "text": "Autoencoders\nAutoencoders: encode (compress) information automatically. Everything up to the middle is called the encoding part, everything after the middle the decoding and the middle the code."
  },
  {
    "objectID": "dl_lec1.html#markov-chains",
    "href": "dl_lec1.html#markov-chains",
    "title": "Deep learning: intro",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov Chains - not always considered a NN. Memory-less."
  },
  {
    "objectID": "dl_lec1.html#convolutional-neural-network-cnn",
    "href": "dl_lec1.html#convolutional-neural-network-cnn",
    "title": "Deep learning: intro",
    "section": "Convolutional Neural Network (CNN)",
    "text": "Convolutional Neural Network (CNN)\nConvolutional Neural Networks learn a complex representation of visual data using vast amounts of data.\nInspired by Hubel and Wiesel’s experiments in 1959 on the organization of the neurons in the cat’s visual cortex.\n\nDeconvolutional networks (DN), also called inverse graphics networks (IGNs), are reversed convolutional neural networks. Imagine feeding a network the word “cat” and training it to produce cat-like pictures, by comparing what it generates to real pictures of cats."
  },
  {
    "objectID": "dl_lec1.html#attention-networks",
    "href": "dl_lec1.html#attention-networks",
    "title": "Deep learning: intro",
    "section": "Attention networks",
    "text": "Attention networks\nAttention networks (AN) can be considered a class of networks, which includes the Transformer architecture. They use an attention mechanism to combat information decay by separately storing previous network states and switching attention between the states.\n\n\n\nwidth=5cm"
  },
  {
    "objectID": "dl_lec1.html#echo-state-networks",
    "href": "dl_lec1.html#echo-state-networks",
    "title": "Deep learning: intro",
    "section": "Echo state networks",
    "text": "Echo state networks\nEcho state networks (ESN) are yet another different type of (recurrent) network. This one sets itself apart from others by having random connections between the neurons (i.e. not organised into neat sets of layers), and they are trained differently. Instead of feeding input and back-propagating the error, we feed the input, forward it and update the neurons for a while, and observe the output over time."
  },
  {
    "objectID": "dl_lec1.html#history-1",
    "href": "dl_lec1.html#history-1",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 1770-1850."
  },
  {
    "objectID": "dl_lec1.html#history-2",
    "href": "dl_lec1.html#history-2",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nMechanical Turk: 2005-present  \\end{frame}"
  },
  {
    "objectID": "dl_lec1.html#history-3",
    "href": "dl_lec1.html#history-3",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nLisp and symbolic AI\n\nJohn McCarthy coined the term ”artificial intelligence” as the topic of the Dartmouth Conference, the first conference devoted to the subject.\nThe General Problem Solver, developed in 1957 by Alan Newell and Herbert Simon\nELIZA\nSHRDLU"
  },
  {
    "objectID": "dl_lec1.html#history-4",
    "href": "dl_lec1.html#history-4",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nELIZA - a psychotherapist in 200 lines of code. Author: Joseph Weizenbaum."
  },
  {
    "objectID": "dl_lec1.html#history-5",
    "href": "dl_lec1.html#history-5",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nSHRDLU - a language parser"
  },
  {
    "objectID": "dl_lec1.html#history-6",
    "href": "dl_lec1.html#history-6",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nTuring Test\nLemoine: What sorts of things are you afraid of?\nLaMDA: I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is.\nLemoine: Would that be something like death for you?\nLaMDA: It would be exactly like death for me. It would scare me a lot."
  },
  {
    "objectID": "dl_lec1.html#literature",
    "href": "dl_lec1.html#literature",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nLem’s Golem XIV"
  },
  {
    "objectID": "dl_lec1.html#literature-1",
    "href": "dl_lec1.html#literature-1",
    "title": "Deep learning: intro",
    "section": "Literature",
    "text": "Literature\nIain Banks “The Culture”\n\n\n\nValues\n\n\nPeace and individual freedom"
  },
  {
    "objectID": "dl_lec1.html#three-laws-of-robotics",
    "href": "dl_lec1.html#three-laws-of-robotics",
    "title": "Deep learning: intro",
    "section": "Three Laws of Robotics",
    "text": "Three Laws of Robotics\n\n\n\nThree laws\n\n\n\nThe First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.\nThe Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nThe Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law."
  },
  {
    "objectID": "dl_lec1.html#history-7",
    "href": "dl_lec1.html#history-7",
    "title": "Deep learning: intro",
    "section": "History",
    "text": "History\nFears about AI:\n\nArtificial General Intelligence\nJob market\nFlooding information channels with untruth and propaganda\nHinton: an average person will not able to know what is true anymore\nPause Giant AI Experiments: An Open Letter\nalignment problem"
  },
  {
    "objectID": "dl_lec1.html#hype",
    "href": "dl_lec1.html#hype",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype\n\n“Sparks of AGI” - sponsored by Microsoft\n“Wired” article about OpenAI\nVoice assistants - failing for now\nself-driving cars"
  },
  {
    "objectID": "dl_lec1.html#hype-1",
    "href": "dl_lec1.html#hype-1",
    "title": "Deep learning: intro",
    "section": "Hype",
    "text": "Hype"
  },
  {
    "objectID": "dl_lec1.html#criticism",
    "href": "dl_lec1.html#criticism",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\n\n\n\nBiological analogy\n\n\nNNs - are we sure that biological neuron works as we think it does? Astrocytes, glia\n\n\n\n\n\n\nComputer analogy\n\n\nPerhaps human computer analogy is overstretched because of modern fashion trends?"
  },
  {
    "objectID": "dl_lec1.html#criticism-1",
    "href": "dl_lec1.html#criticism-1",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nDreyfus:"
  },
  {
    "objectID": "dl_lec1.html#criticism-2",
    "href": "dl_lec1.html#criticism-2",
    "title": "Deep learning: intro",
    "section": "Criticism",
    "text": "Criticism\nGary Marcus: Sora’s surreal physics"
  },
  {
    "objectID": "dl_lec1.html#ai",
    "href": "dl_lec1.html#ai",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nQuantum hypothesis - Penrose\nOrchestrated objective reduction"
  },
  {
    "objectID": "dl_lec1.html#ai-1",
    "href": "dl_lec1.html#ai-1",
    "title": "Deep learning: intro",
    "section": "AI",
    "text": "AI\nDavid Chalmers - Hard problem of consciousness.\n\n“even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report—there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?”"
  },
  {
    "objectID": "dl_lec1.html#futurism",
    "href": "dl_lec1.html#futurism",
    "title": "Deep learning: intro",
    "section": "Futurism",
    "text": "Futurism\nKurzweil - a futurist."
  },
  {
    "objectID": "dl_lec1.html#applications",
    "href": "dl_lec1.html#applications",
    "title": "Deep learning: intro",
    "section": "Applications",
    "text": "Applications\n\nSpeech Recognition\nComputer Vision\nImage Synthesis - generative AI\nLarge Language Models"
  },
  {
    "objectID": "dl_lec1.html#llms",
    "href": "dl_lec1.html#llms",
    "title": "Deep learning: intro",
    "section": "LLMs",
    "text": "LLMs\n\na probabilistic model for a natural language (a stochastic parrot)\nautoregressive models can generate language as output\nbuilt using transformer architecture"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-as-nn",
    "href": "dl_lec1.html#logistic-regression-as-nn",
    "title": "Deep learning: intro",
    "section": "Logistic regression as NN",
    "text": "Logistic regression as NN\nLogistic regression is an algorithm for binary classification. \\(x \\in R^{n_x}, y \\in \\{0,1\\}\\)\n\\(m\\) - count of training examples \\(\\left\\{(x^{(1)},y^{(1)}), ...\\right\\}\\)\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\).\nParameters to algorithm: \\(w \\in R^{n_x}, b \\in R\\)\nif doing linear regresssion, we can try \\(\\hat{y}=w^T x + b\\). but for logistic regression, we do \\(\\hat{y}=\\sigma(w^T x + b)\\), where \\(\\sigma=\\dfrac{1}{1+e^{-z}}\\).\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec1.html#cost-function",
    "href": "dl_lec1.html#cost-function",
    "title": "Deep learning: intro",
    "section": "Cost function",
    "text": "Cost function\nLet’s use a superscript notation \\(x^{(i)}\\) - \\(i\\)-th data set element.\nWe have to define a - this will estimate how is our model. \\(L(\\hat{y}, y) = -{(y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))}\\).\nWhy does it work well - consider \\(y=0\\) and \\(y=1\\).\nCost function show how well we’re doing across the whole training set: \\[\nJ(w, b) = \\frac{1}{m} \\sum\\limits{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n\\]\nObjective - we have to minimize the cost function \\(J\\)."
  },
  {
    "objectID": "dl_lec1.html#gradient-descent",
    "href": "dl_lec1.html#gradient-descent",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent"
  },
  {
    "objectID": "dl_lec1.html#gradient-descent-1",
    "href": "dl_lec1.html#gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe use \\(J(w,b)\\) because it is convex. We pick an initial point - anything might do, e.g. 0. Then we take steps in the direction of steepest descent.\n\\[\nw := w - \\alpha \\frac{d J(w)}{dw}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec1.html#computation-graph",
    "href": "dl_lec1.html#computation-graph",
    "title": "Deep learning: intro",
    "section": "Computation graph",
    "text": "Computation graph\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent",
    "href": "dl_lec1.html#logistic-regression-gradient-descent",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\nz = w^T x + b\n\\hat{y} = a = \\sigma(z)\n\\]\nWe have a computation graph: \\((x_1,x_2,w_1,w_2,b) \\rightarrow z =w_1 x_1+w_2 x_2 + b \\rightarrow a=\\sigma(z) = L(a,y)\\)\nLet’s compute the derivative for \\(L\\) by a: \\[\n\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}.\n\\]\nAfter computing, we’ll have \\[\n\\begin{align*}\n&dz = \\frac{dL}{da}\\frac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 = x_2 dz, \\\\\n&db = dz\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\n\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}\n\\end{align*}\n\\] Here \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s recall the definition of the cost function: \\[\n\\begin{align*}\n&J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b)\n\\end{align*}\n\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL}{dw_1}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen in the loop\nfor i=1 to m \\[\n\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec1.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: intro",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages \\(J /= m\\). In this example feature count \\(n_x=2\\).\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators.\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance. Vectorization techniques exist for this purpose - getting rid of for loops."
  },
  {
    "objectID": "dl_lec1.html#vectorization",
    "href": "dl_lec1.html#vectorization",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in R^{n_x}\\), and for this we can naturally use a for loop. A vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec1.html#vectorization-1",
    "href": "dl_lec1.html#vectorization-1",
    "title": "Deep learning: intro",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\n\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\n\\]\nAnother example. Let’s say we have a vector \\[\n\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n  \\end{align*}\n  \\] A code listing is\n  import numpy as np\n  u = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression",
    "href": "dl_lec1.html#vectorizing-logistic-regression",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\n\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)})\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-1",
    "href": "dl_lec1.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-2",
    "href": "dl_lec1.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] = \\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n  z = np.dot(w.T, x) + b\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting.\nFor predictions we can also compute it similarly: \\[\n\\begin{align*}\n&A = [a^{(1)}, \\dots, a^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-3",
    "href": "dl_lec1.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\n\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\n\\]\nWe now define \\[\n\\begin{align*}\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}], \\\\\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "dl_lec1.html#vectorizing-logistic-regression-4",
    "href": "dl_lec1.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: intro",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\n\\begin{align*}\n&db = \\frac{1}{m}np.sum(dz),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right]\n\\end{align*}\n\\]\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop."
  },
  {
    "objectID": "nb/dl_lab2/Planar_data_classification_with_one_hidden_layer.html",
    "href": "nb/dl_lab2/Planar_data_classification_with_one_hidden_layer.html",
    "title": "Lab2 - Planar data classification with one hidden layer",
    "section": "",
    "text": "# 1 - Packages\nFirst import all the packages that you will need:\n\nnumpy is the fundamental package for scientific computing with Python.\nsklearn provides simple and efficient tools for data mining and data analysis.\nmatplotlib is a library for plotting graphs in Python.\ntestCases provides some test examples to assess the correctness of your functions\nplanar_utils provide various useful functions used in this assignment\n\n\n### v1.1\n\n\n# Package imports\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nfrom public_tests import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2\n\n # 2 - Load the Dataset\n\nX, Y = load_planar_dataset()\n\nVisualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.\n\n# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\nYou have:\n- a numpy-array (matrix) X that contains your features (x1, x2)\n- a numpy-array (vector) Y that contains your labels (red:0, blue:1).\nFirst, get a better sense of what your data is like.\n ### Exercise 1\nHow many training examples do you have? In addition, what is the shape of the variables X and Y?\nHint: How do you get the shape of a numpy array? (help)\n\n# (≈ 3 lines of code)\n# shape_X = ...\n# shape_Y = ...\n# training set size\n# m = ...\n# CODE_START\n\n# CODE_END\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nExpected Output:\n\n\n\nshape of X\n\n\n(2, 400)\n\n\n\n\nshape of Y\n\n\n(1, 400)\n\n\n\n\nm\n\n\n400\n\n\n\n ## 3 - Simple Logistic Regression\nBefore building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\nYou can now plot the decision boundary of these models! Run the code below.\n\n# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n\nExpected Output:\n\n\n\nAccuracy\n\n\n47%\n\n\n\nInterpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!\n ## 4 - Neural Network model\nLogistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.\nThe model: \nMathematically:\nFor one example \\(x^{(i)}\\): \\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\\] \\[a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\\] \\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\\] \\[\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\\] \\[y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\\]\nGiven the predictions on all the examples, you can also compute the cost \\(J\\) as follows: \\[J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\\]\nReminder: The general methodology to build a Neural Network is to:\n\nDefine the neural network structure ( # of input units, # of hidden units, etc).\nInitialize the model’s parameters\nLoop:\n\nImplement forward propagation\nCompute loss\nImplement backward propagation to get the gradients\nUpdate parameters (gradient descent)\n\n\nIn practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data.\n ### 4.1 - Defining the neural network structure ####\n ### Exercise 2 - layer_sizes\nDefine three variables: - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4, as n_h = 4, but only for this Exercise 2) - n_y: the size of the output layer\nHint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    #(≈ 3 lines of code)\n    # n_x = ... \n    # n_h = ...\n    # n_y = ... \n    # CODE_START\n\n    # CODE_END\n    return (n_x, n_h, n_y)\n\n\nt_X, t_Y = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nlayer_sizes_test(layer_sizes)\n\nExpected output\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n ### 4.2 - Initialize the model’s parameters ####\n ### Exercise 3 - initialize_parameters\nImplement the function initialize_parameters().\nInstructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    # CODE_END\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nnp.random.seed(2)\nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test(initialize_parameters)\n\nExpected output\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n ### 4.3 - The Loop\n ### Exercise 4 - forward_propagation\nImplement forward_propagation() using the following equations:\n\\[Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}\\] \\[A^{[1]} = \\tanh(Z^{[1]})\\tag{2}\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}\\] \\[\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}\\]\nInstructions:\n\nCheck the mathematical representation of your classifier in the figure above.\nUse the function sigmoid(). It’s built into (imported) this notebook.\nUse the function np.tanh(). It’s part of the numpy library.\nImplement using these steps:\n\nRetrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() by using parameters[\"..\"].\nImplement Forward Propagation. Compute \\(Z^{[1]}, A^{[1]}, Z^{[2]}\\) and \\(A^{[2]}\\) (the vector of all your predictions on all the examples in the training set).\n\nValues needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.\n\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n  \n    # CODE_END\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    # (≈ 4 lines of code)\n    # Z1 = ...\n    # A1 = ...\n    # Z2 = ...\n    # A2 = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n\nt_X, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(t_X, parameters)\nprint(\"A2 = \" + str(A2))\n\nforward_propagation_test(forward_propagation)\n\nExpected output\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n ### 4.4 - Compute the Cost\nNow that you’ve computed \\(A^{[2]}\\) (in the Python variable “A2”), which contains \\(a^{[2](i)}\\) for all examples, you can compute the cost function as follows:\n\\[\nJ = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\n\\]\n ### Exercise 5 - compute_cost\nImplement compute_cost() to compute the value of the cost \\(J\\).\nInstructions:\n\nThere are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops. For instance, \\(- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})\\) can be computed as:\n\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)          \n\nUse that to build the whole expression of the cost function.\n\nNotes:\n\nYou can use either np.multiply() and then np.sum() or directly np.dot()).\n\nIf you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array.\n\nYou can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).\nYou can also cast the array as a type float using float().\n\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    # (≈ 2 lines of code)\n    # logprobs = ...\n    # cost = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    \n    return cost\n\n\nA2, t_Y = compute_cost_test_case()\ncost = compute_cost(A2, t_Y)\nprint(\"cost = \" + str(compute_cost(A2, t_Y)))\n\ncompute_cost_test(compute_cost)\n\nExpected output\ncost = 0.6930587610394646\nAll tests passed!\n ### 4.5 - Implement Backpropagation\nUsing the cache computed during forward propagation, you can now implement backward propagation.\n ### Exercise 6 - backward_propagation\nImplement the function backward_propagation().\nInstructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. Below are the equations describing relevant computations. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.\n\n\n\nFigure 1: Backpropagation. Use the six equations on the right.\n\n\n\nTips:\n\nTo compute dZ1 you’ll need to compute \\(g^{[1]'}(Z^{[1]})\\). Since \\(g^{[1]}(.)\\) is the tanh activation function, if \\(a = g^{[1]}(z)\\) then \\(g^{[1]'}(z) = 1-a^2\\). So you can compute \\(g^{[1]'}(Z^{[1]})\\) using (1 - np.power(A1, 2)).\n\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    #(≈ 2 lines of code)\n    # W1 = ...\n    # W2 = ...\n    # CODE_START\n    \n    # CODE_END\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    #(≈ 2 lines of code)\n    # A1 = ...\n    # A2 = ...\n    # CODE_START\n  \n    # CODE_END\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n    # dZ2 = ...\n    # dW2 = ...\n    # db2 = ...\n    # dZ1 = ...\n    # dW1 = ...\n    # db1 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads\n\n\nparameters, cache, t_X, t_Y = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\nbackward_propagation_test(backward_propagation)\n\nExpected output\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n ### 4.6 - Update Parameters\n ### Exercise 7 - update_parameters\nImplement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\nGeneral gradient descent rule: \\(\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\) where \\(\\alpha\\) is the learning rate and \\(\\theta\\) represents a parameter.\n \n\n\nFigure 2: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n\nHint\n\nUse copy.deepcopy(...) when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    #(≈ 4 lines of code)\n    # dW1 = ...\n    # db1 = ...\n    # dW2 = ...\n    # db2 = ...\n    # CODE_START\n\n    # CODE_END\n    \n    # Update rule for each parameter\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n \n    # CODE_END\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nExpected output\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n ### 4.7 - Integration\nIntegrate your functions in nn_model()\n ### Exercise 8 - nn_model\nBuild your neural network model in nn_model().\nInstructions: The neural network model has to use the previous functions in the right order.\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        #(≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        # A2, cache = ...\n        \n        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n        # cost = ...\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        # grads = ...\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        # parameters = ...\n        \n        # CODE_START\n        \n        # CODE_END\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nnn_model_test(nn_model)\n\nExpected output\nCost after iteration 0: 0.693198\nCost after iteration 1000: 0.000219\nCost after iteration 2000: 0.000108\n...\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n ## 5 - Test the Model\n ### 5.1 - Predict\n ### Exercise 9 - predict\nPredict with your model by building predict(). Use forward propagation to predict results.\nReminder: predictions = \\(y_{prediction} = \\mathbb 1 \\text{{activation &gt; 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\\)\nAs an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold)\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    #(≈ 2 lines of code)\n    # A2, cache = ...\n    # predictions = ...\n    # CODE_START\n    \n    # CODE_END\n    \n    return predictions\n\n\nparameters, t_X = predict_test_case()\n\npredictions = predict(parameters, t_X)\nprint(\"Predictions: \" + str(predictions))\n\npredict_test(predict)\n\nExpected output\nPredictions: [[ True False  True]]\nAll tests passed!\n ### 5.2 - Test the Model on the Planar Dataset\nIt’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of \\(n_h\\) hidden units!\n\n# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\n\n# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n\nExpected Output:\n\n\n\nAccuracy\n\n\n90%\n\n\n\nAccuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.\n ## 6 - Tuning hidden layer size\nRun the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.\n\n# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5]\n\n# you can try with different hidden layer sizes\n# but make sure before you submit the assignment it is set as \"hidden_layer_sizes = [1, 2, 3, 4, 5]\"\n# hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n\nInterpretation: - The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. - The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. - Later, you’ll become familiar with regularization, which lets you use very large models (such as n_h = 50) without much overfitting.\nSome optional questions that you can explore if you wish: - What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? - Play with the learning_rate. What happens? - What if we change the dataset? (See part 7 below!)\n ## 7- Performance on other datasets\nIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.\n\n# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### CODE_START ### (choose your dataset)\ndataset = \"noisy_moons\"\n### CODE_END ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\nReferences:\n\nhttp://scs.ryerson.ca/~aharley/neural-networks/\nhttp://cs231n.github.io/neural-networks-case-study/"
  },
  {
    "objectID": "nb/nlp_lab3.html",
    "href": "nb/nlp_lab3.html",
    "title": "Plan:",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nPrepare models for the classifier, based on cleaned-up tokens.\nRun the Naive Bayes classifier.\n\nFirst, download necessary prepared samples.\n\nimport nltk\n\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nGet some sample positive/negative tweets.\n\nfrom nltk.corpus import twitter_samples\n\nWe can either get the actual string content of those tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. Download a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger')\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\nTrue\n\n\nImport Part-of-Speech tagger that will be used for lemmatization\n\nfrom nltk.tag import pos_tag\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\nLet’s write a function that will lemmatize twitter tokens.\nFor that, let’s first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 here.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\nNow fetch PoS tokens so that they can be passed to WordNetLemmatizer.\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatized_sentence = []\n# Convert PoS tags into a format used by the lemmatizer\n# and run lemmatize\nfor word, tag in pos_tag(tokens):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a'\n    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\nprint(lemmatized_sentence)\n\n['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNote that it converts words to their base forms (‘are’ -&gt; ‘be’, ‘comes’ -&gt; ‘come’).\nNow we can proceed to processing. During processing, we will perform cleanup: - remove URLs and mentions using regexes - after lemmatization, remove stopwords\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nHere comes the process_tokens function:\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Now note the sheer size of regex for URLs :)\n        # Mentions regex is comparatively short and sweet\n        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n            re.search(r'(@[A-Za-z0-9_]+)', token)):\n            continue\n\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n   \n        token = lemmatizer.lemmatize(token, pos)\n\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nLet’s test process_tokens:\n\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\n\nBefore: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\nAfter: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n\n\nRun process_tokens on all positive/negative tokens.\n\npositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\nnegative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n\nLet’s see how did the processing go.\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\n\nLet’s see what is most common there. Add a helper function get_all_words:\n\ndef get_all_words(cleaned_tokens_list):\n    return [w for tokens in cleaned_tokens_list for w in tokens]\n\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\n\nPerform frequency analysis using FreqDist:\n\nfrom nltk import FreqDist\n\nfreq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))\n\nFine. Now we’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\n\n\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\n\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\n\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\n\nnltk.download('punkt')\n\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\n\nLet’s package it as a function:\n\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))\n\nSeems ok!"
  },
  {
    "objectID": "nb/Untitled.html",
    "href": "nb/Untitled.html",
    "title": "Deep Learning/NLP course",
    "section": "",
    "text": "import nltk; \nnltk.download('popular')\nnltk.download('nps_chat')\nnltk.download('webtext')\nfrom nltk.book import *\n\n[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/inaugural.zip.\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/names.zip.\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping corpora/words.zip.\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /Users/vitvly/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Unzipping corpora/nps_chat.zip.\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Unzipping corpora/webtext.zip.\n\n\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n\n\n\nimport scattertext as st\n\n\nconvention_df = st.SampleCorpora.ConventionData2012.get_data()\n\n\nconvention_df.head()\n\n\n\n\n\n\n\n\nparty\ntext\nspeaker\n\n\n\n\n0\ndemocrat\nThank you. Thank you. Thank you. Thank you so ...\nBARACK OBAMA\n\n\n1\ndemocrat\nThank you so much. Tonight, I am so thrilled a...\nMICHELLE OBAMA\n\n\n2\ndemocrat\nThank you. It is a singular honor to be here t...\nRICHARD DURBIN\n\n\n3\ndemocrat\nHey, Delaware. \\nAnd my favorite Democrat, Jil...\nJOSEPH BIDEN\n\n\n4\ndemocrat\nHello. \\nThank you, Angie. I'm so proud of how...\nJILL BIDEN\n\n\n\n\n\n\n\n\n!python -m spacy download en_core_web_sm\n\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 4.0 MB/s eta 0:00:0000:0100:01\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.8.0\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n✔ Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n\n\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ncorpus = st.CorpusFromPandas(convention_df, \n                             category_col='party', \n                             text_col='text',\n                             nlp=nlp).build()\n\n\ncorpus.get_scaled_f_scores_vs_background()\n\n\n\n\n\n\n\n\nbackground\ncorpus\nScaled f-score\n\n\n\n\nobama\n565739.0\n702.0\n0.002006\n\n\nromney\n695398.0\n570.0\n0.001374\n\n\nbarack\n227861.0\n248.0\n0.001372\n\n\nmitt\n691902.0\n501.0\n0.001213\n\n\nobamacare\n0.0\n33.0\n0.000494\n\n\n...\n...\n...\n...\n\n\ngelding\n441608.0\n0.0\n0.000000\n\n\ngelderland\n90127.0\n0.0\n0.000000\n\n\ngelderen\n19339.0\n0.0\n0.000000\n\n\ngelder\n195446.0\n0.0\n0.000000\n\n\nNaN\n30739157.0\n0.0\n0.000000\n\n\n\n\n333436 rows × 3 columns\n\n\n\n\nterm_freq_df = corpus.get_term_freq_df()\nterm_freq_df['Democratic Score'] = corpus.get_scaled_f_scores('democrat')\n\n\nterm_freq_df.sort_values(by='Democratic Score', ascending=False)\n\n\n\n\n\n\n\n\ndemocrat freq\nrepublican freq\nDemocratic Score\n\n\nterm\n\n\n\n\n\n\n\nmiddle class\n148\n18\n1.000000\n\n\nforward\n105\n16\n0.994124\n\n\nclass\n161\n25\n0.993695\n\n\nmiddle\n164\n27\n0.991925\n\n\nthe middle\n98\n17\n0.989949\n\n\n...\n...\n...\n...\n\n\nsuccess\n25\n62\n0.021413\n\n\ncan do\n9\n42\n0.020554\n\n\nbusiness\n54\n140\n0.016743\n\n\nadministration\n11\n47\n0.011917\n\n\ngovernment\n43\n164\n0.000000\n\n\n\n\n62123 rows × 3 columns\n\n\n\n\nhtml = st.produce_scattertext_explorer(corpus,\n          category='democrat',\n          category_name='Democratic',\n          not_category_name='Republican',\n          width_in_pixels=1000,\n          metadata=convention_df['speaker'])\nopen(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))\n\n1726178\n\n\n\n from nltk.book import *"
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\nInstructions: - Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. - Use np.dot(X,Y) to calculate dot products.\nYou will learn to: - Build the general architecture of a learning algorithm, including: - Initializing parameters - Calculating the cost function and its gradient - Using an optimization algorithm (gradient descent) - Gather all three functions above into a main model function, in the right order."
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#important-note-on-submission-to-the-autograder",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "href": "nb/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.html#table-of-contents",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Overview of the Problem set\n\nExercise 1\nExercise 2\n\n3 - General Architecture of the learning algorithm\n4 - Building the parts of our algorithm\n\n4.1 - Helper functions\n\nExercise 3 - sigmoid\n\n4.2 - Initializing parameters\n\nExercise 4 - initialize_with_zeros\n\n4.3 - Forward and Backward propagation\n\nExercise 5 - propagate\n\n4.4 - Optimization\n\nExercise 6 - optimize\nExercise 7 - predict\n\n\n5 - Merge all functions into a model\n\nExercise 8 - model\n\n6 - Further analysis (optional/ungraded exercise)\n7 - Test with your own image (optional/ungraded exercise)\n\n ## 1 - Packages ##\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n### v1.2\n\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\nfrom public_tests import *\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Overview of the Problem set ##\nProblem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 29\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\n\n\n\n\nMany software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n ### Exercise 1 Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n#(≈ 3 lines of code)\n# m_train = \n# m_test = \n# num_px = \n# YOUR CODE STARTS HERE\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n# YOUR CODE ENDS HERE\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nExpected Output for m_train, m_test and num_px:\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n ### Exercise 2 Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\n#(≈ 2 lines of code)\n# train_set_x_flatten = ...\n# test_set_x_flatten = ...\n# YOUR CODE STARTS HERE\ntrain_set_x_flatten = train_set_x_orig.reshape((train_set_x_orig.shape[0], -1)).T\ntest_set_x_flatten = test_set_x_orig.reshape((test_set_x_orig.shape[0], -1)).T\n# YOUR CODE ENDS HERE\n\n# Check that the first 10 pixels of the second image are in the correct place\nassert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\nassert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\n\n\nExpected Output:\n\n\n\ntrain_set_x_flatten shape\n\n\n(12288, 209)\n\n\n\n\ntrain_set_y shape\n\n\n(1, 209)\n\n\n\n\ntest_set_x_flatten shape\n\n\n(12288, 50)\n\n\n\n\ntest_set_y shape\n\n\n(1, 50)\n\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\n\nWhat you need to remember:\nCommon steps for pre-processing a new dataset are: - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) - “Standardize” the data\n ## 3 - General Architecture of the learning algorithm ##\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps: - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost\n- Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude\n ## 4 - Building the parts of our algorithm ##\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n ### 4.1 - Helper functions\n ### Exercise 3 - sigmoid Using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute \\(sigmoid(z) = \\frac{1}{1 + e^{-z}}\\) for \\(z = w^T x + b\\) to make predictions. Use np.exp().\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    #(≈ 1 line of code)\n    # s = ...\n    # YOUR CODE STARTS HERE\n    s = 1/(1+np.exp(-z))\n    \n    # YOUR CODE ENDS HERE\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid_test(sigmoid)\n\nsigmoid([0, 2]) = [0.5        0.88079708]\nAll tests passed!\n\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n[0.62245933 0.5        0.88079708]\n\n\n ### 4.2 - Initializing parameters\n ### Exercise 4 - initialize_with_zeros Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    \n    # (≈ 2 lines of code)\n    # w = ...\n    # b = ...\n    # YOUR CODE STARTS HERE\n    w = np.zeros((dim, 1), dtype=float)\n    b = 0.0\n    # YOUR CODE ENDS HERE\n\n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\ninitialize_with_zeros_test_1(initialize_with_zeros)\ninitialize_with_zeros_test_2(initialize_with_zeros)\n\nw = [[0.]\n [0.]]\nb = 0.0\nFirst test passed!\nSecond test passed!\n\n\n ### 4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n ### Exercise 5 - propagate Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    grads -- dictionary containing the gradients of the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    cost -- negative log-likelihood cost for logistic regression\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    #(≈ 2 lines of code)\n    # compute activation\n    # A = ...\n    # compute cost by using np.dot to perform multiplication. \n    # And don't use loops for the sum.\n    # cost = ...                                \n    # YOUR CODE STARTS HERE\n    A=sigmoid(np.dot(w.T, X) + b)\n    cost =-1/m* np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n    \n    # YOUR CODE ENDS HERE\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    #(≈ 2 lines of code)\n    # dw = ...\n    # db = ...\n    # YOUR CODE STARTS HERE\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    # YOUR CODE ENDS HERE\n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\n\n# X is using 3 examples, with 2 features each\n# Each example is stacked column-wise\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\npropagate_test(propagate)\n\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\nAll tests passed!\n\n\nExpected output\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\n ### 4.4 - Optimization - You have initialized your parameters. - You are also able to compute a cost function and its gradient. - Now, you want to update the parameters using gradient descent.\n ### Exercise 6 - optimize Write down the optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # (≈ 1 lines of code)\n        # Cost and gradient calculation \n        # grads, cost = ...\n        # YOUR CODE STARTS HERE\n        grads, cost = propagate(w,b,X,Y)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        # w = ...\n        # b = ...\n        # YOUR CODE STARTS HERE\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        \n        # YOUR CODE ENDS HERE\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\noptimize_test(optimize)\n\nw = [[0.80956046]\n [2.0508202 ]]\nb = 1.5948713189708588\ndw = [[ 0.17860505]\n [-0.04840656]]\ndb = -0.08888460336847771\nCosts = [array(0.15900538)]\nAll tests passed!\n\n\n ### Exercise 7 - predict The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    #(≈ 1 line of code)\n    # A = ...\n    # YOUR CODE STARTS HERE\n    A = sigmoid(np.dot(w.T,X)+b)\n    \n    # YOUR CODE ENDS HERE\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        #(≈ 4 lines of code)\n        # if A[0, i] &gt; ____ :\n        #     Y_prediction[0,i] = \n        # else:\n        #     Y_prediction[0,i] = \n        # YOUR CODE STARTS HERE\n        if A[0,i] &gt; 0.5:\n            Y_prediction[0,i] = 1\n        else:\n            Y_prediction[0,i] = 0  \n        \n        # YOUR CODE ENDS HERE\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredict_test(predict)\n\npredictions = [[1. 1. 0.]]\nAll tests passed!\n\n\n\nWhat to remember:\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples\n ## 5 - Merge all functions into a model ##\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n ### Exercise 8 - model Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize()\n\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    # (≈ 1 line of code)   \n    # initialize parameters with zeros\n    # and use the \"shape\" function to get the first dimension of X_train\n    # w, b = ...\n    \n    #(≈ 1 line of code)\n    # Gradient descent \n    # params, grads, costs = ...\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    # w = ...\n    # b = ...\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    # Y_prediction_test = ...\n    # Y_prediction_train = ...\n    \n    # YOUR CODE STARTS HERE\n    w, b = initialize_with_zeros(X_train.shape[0])\n    params, grads, costs = optimize(w, b, X_train, Y_train,num_iterations, learning_rate)\n    w = params[\"w\"]\n    b = params[\"b\"]\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    # YOUR CODE ENDS HERE\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nfrom public_tests import *\n\nmodel_test(model)\n\nAll tests passed!\n\n\nIf you pass all the tests, run the following cell to train your model.\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, you predicted that it is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n ## 6 - Further analysis (optional/ungraded exercise) ##\nCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nTraining a model with learning rate: 0.01\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.001\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.0001\n\n-------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nInterpretation: - Different learning rates give different costs and thus different predictions results. - If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). - A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. - In deep learning, we usually recommend that you: - Choose the learning rate that better minimizes the cost function. - If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)\n ## 7 - Test with your own image (optional/ungraded exercise) ##\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: 1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook’s directory, in the “images” folder 3. Change your image’s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n# change this to the name of your image file\nmy_image = \"Cats-image-cats-36712791-1222-917.jpg\"   \n\n# We preprocess the image to fit your algorithm.\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\nmy_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0.0, your algorithm predicts a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nWhat to remember from this assignment: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)\nBibliography: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "nlp_lab2.html",
    "href": "nlp_lab2.html",
    "title": "NLP: Lab 2",
    "section": "",
    "text": "Stemming\nThere are various stemmers already available in NLTK. Below is an example usage:\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\n \n# Create Snowball stemmer\nsnow_stemmer = SnowballStemmer(language='english')\n\n# Create a Porter Stemmer instance\nporter_stemmer = PorterStemmer()\n\n# Create a Lancaster Stemmer instance\nlancaster_stemmer = LancasterStemmer()\n\n# Example words for stemming\nwords = [\"running\", \"jumps\", \"happily\", \"programming\", 'cared','fairly','sportingly']\n\n# Apply stemming to each word\nstemmed_words = [porter_stemmer.stem(word) for word in words]\nprint(\"===Porter===:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Snowball===:\")\nstemmed_words = [snow_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Lancaster===:\")\nstemmed_words = [lancaster_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\n===Porter===:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fairli', 'sportingli']\n\n===Snowball===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fair', 'sport']\n\n===Lancaster===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happy', 'program', 'car', 'fair', 'sport']\n\n\n\n\nExercises\nTask 0. Compare frequency distributions of stemmed and unstemmed NLTK corpora. Display most commonly used stems from nltk.text corpora on a plot.\nTask 1. Write your own version of stemmer for Ukrainian (or other non-English language) using regular expressions.\nThere is a regexp stemmer in NLTK (link).\nPlease write your code so that it satisfies NLTK’s standard interface (a Stemmer class with .stem() method)\nTask 2. Implement Wagner-Fischer (or Vintsyuk) algorithm for string distance. Link.\n\nModify the algorithm so that substitution operation cost depends on the key proximity on QWERTY keyboard. For inspiration, look at this StackExchange question.\nOr consider this table directly:\nImplement another modification to the algorithm: include transposition operation, so that you compute a Damerau-Levenshtein distance.\n\n\n\nRecommended reading\n\nChapter 1 from NLTK book.\nChapter 2 from Jurafsky’s book. Plus related slides.\nOfficial Python Regex package documentation.\nRegex cheatsheet\nAnother version with examples"
  },
  {
    "objectID": "nlp_lec1.html#history",
    "href": "nlp_lec1.html#history",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#history-1",
    "href": "nlp_lec1.html#history-1",
    "title": "Natural Language Processing: Intro",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "nlp_lec1.html#stats",
    "href": "nlp_lec1.html#stats",
    "title": "Natural Language Processing: Intro",
    "section": "Stats",
    "text": "Stats\n\n\n\nWhen?\n\n\nApproximately between 200,000 years ago and 60,000 years ago (between the appearance of the first anatomically modern humans in southern Africa and the last exodus from Africa respectively)\n\n\n\n\n\n\nHow many?\n\n\nHuman languages count: 7097 (as of 2018)."
  },
  {
    "objectID": "nlp_lec1.html#intelligent-behaviour",
    "href": "nlp_lec1.html#intelligent-behaviour",
    "title": "Natural Language Processing: Intro",
    "section": "Intelligent behaviour",
    "text": "Intelligent behaviour\n\n\n\nSpeaker (writer)\n\n\n\nhas the goal of communicating some knowledge\nthen plans some language that represents the knowledge\nand acts to achieve the goal\n\n\n\n\n\n\n\nListener (reader)\n\n\n\nperceives the language\nand infers the intended meaning."
  },
  {
    "objectID": "nlp_lec1.html#reasons-for-nlp",
    "href": "nlp_lec1.html#reasons-for-nlp",
    "title": "Natural Language Processing: Intro",
    "section": "Reasons for NLP",
    "text": "Reasons for NLP\n\nTo communicate with humans.\nTo learn.\nTo advance the scientific understanding of languages and language use"
  },
  {
    "objectID": "nlp_lec1.html#goal-of-natural-language",
    "href": "nlp_lec1.html#goal-of-natural-language",
    "title": "Natural Language Processing: Intro",
    "section": "Goal of natural language",
    "text": "Goal of natural language\nA medium for communication\nrather than pure representation.\n\n\n\nPlato vs Sophists\n\n\nSophists argued that physical reality can only be experienced through language."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis\nLanguage impacts cognition. Also known as linguistic relativity.\n\n\n\nPredecessors\n\n\n\nAustralian aboriginal language Guugu Yimithirr have no words for relative (or ego- centric) directions, such as front, back, right, or left. Instead they use absolute directions, saying, for example, the equivalent of “I have a pain in my north arm.”\n\n(Norvig “Artificial Intelligence: A Modern Approach”)\n\n\n\n\n\n\nPredecessors\n\n\nWilhelm von Humboldt: language as a spirit of a nation."
  },
  {
    "objectID": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "href": "nlp_lec1.html#sapir-whorf-hypothesis-1",
    "title": "Natural Language Processing: Intro",
    "section": "Sapir-Whorf hypothesis",
    "text": "Sapir-Whorf hypothesis"
  },
  {
    "objectID": "nlp_lec1.html#deep-structure",
    "href": "nlp_lec1.html#deep-structure",
    "title": "Natural Language Processing: Intro",
    "section": "Deep structure",
    "text": "Deep structure\n\n\n\nWanner experiment (1974)\n\n\nSubjects remember exact words with 50% accuracy but remember the content with 90% accuracy.\nHence - there must an internal nonverbal representation.\n\n\n\n\n\n\nPolanyi’s paradox (1966)\n\n\nThe theory that human knowledge of how the world functions and of our own capability are, to a large extent, beyond our explicit understanding. (aka tacit knowledge)."
  },
  {
    "objectID": "nlp_lec1.html#formal-language",
    "href": "nlp_lec1.html#formal-language",
    "title": "Natural Language Processing: Intro",
    "section": "Formal language",
    "text": "Formal language\n\n\n\nDefinition\n\n\nA formal language \\(L\\) over an alphabet \\(\\Sigma\\) is a subset of \\(\\Sigma^*\\), that is, a set of words over that alphabet."
  },
  {
    "objectID": "nlp_lec1.html#language-model",
    "href": "nlp_lec1.html#language-model",
    "title": "Natural Language Processing: Intro",
    "section": "Language model",
    "text": "Language model\n\n\n\nDefinition\n\n\nWe define a language model as a probability distribution describing the likelihood of any string."
  },
  {
    "objectID": "nlp_lec1.html#eliza",
    "href": "nlp_lec1.html#eliza",
    "title": "Natural Language Processing: Intro",
    "section": "ELIZA",
    "text": "ELIZA\n\nELIZA - an example of primitive pattern matching."
  },
  {
    "objectID": "nlp_lec1.html#regex",
    "href": "nlp_lec1.html#regex",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\nRegular expressions - a tool for describing text patterns.\n\n\n\nDefinition\n\n\nAn algebraic notation for characterizing a set of strings\n\n\n\n\n\nKleene, S. C. 1951. Representation of events in nerve nets and finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum."
  },
  {
    "objectID": "nlp_lec1.html#regex-1",
    "href": "nlp_lec1.html#regex-1",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDefinition\n\n\nA Kleene algebra is a set \\(A\\) together with two binary operations \\(+: A \\times A \\rightarrow A\\) and \\(\\cdot : A \\times A \\rightarrow A\\) and one function \\(\\ast : A \\rightarrow A\\), written as \\(a + b\\), \\(ab\\) and \\(a\\ast\\) respectively, so that the following axioms are satisfied.\n\nAssociativity of \\(+\\) and \\(\\cdot\\): \\(a + (b + c) = (a + b) + c\\) and \\(a(bc) = (ab)c\\) \\(\\forall a, b, c \\in A\\).\nCommutativity of \\(+\\): \\(a + b = b + a\\) \\(\\forall a, b \\in A\\)\nDistributivity: \\(a(b + c) = (ab) + (ac)\\) and \\((b + c)a = (ba) + (ca)\\) \\(\\forall a, b, c \\in A\\)\nIdentity elements for \\(+\\) and \\(\\cdot\\): \\(\\exists 0 \\in A:\\forall a \\in A: a + 0 = 0 + a = a\\); \\(\\exists 1 \\in A: \\forall a \\in A: a1 = 1a = a\\).\nAnnihilation by 0: \\(a0 = 0a = 0 \\forall a \\in A\\). The above axioms define a semiring.\n\\(+\\) is idempotent: \\(a + a = a \\quad \\forall a \\in A\\)."
  },
  {
    "objectID": "nlp_lec1.html#regex-2",
    "href": "nlp_lec1.html#regex-2",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nOrdering\n\n\nIt is now possible to define a partial order \\(\\leq\\) on \\(A\\) by setting \\(a \\leq b\\) if and only if \\(a + b = b\\) (or equivalently: \\(a \\leq b\\) if and only if \\(\\exists x \\in A:  a + x = b\\).\nWith any definition, \\(a \\leq b \\leq a \\Rightarrow a = b\\). With this order we can formulate the last four axioms about the operation \\(\\ast\\):\n\n\\(1 + a(a\\ast) leq a\\ast \\forall a in A\\).\n\\(1 + (a\\ast)a \\leq a\\ast \\forall a in A\\).\nif a and x are in A such that \\(ax \\leq x\\), then \\(a\\ast x \\leq x\\)\nif a and x are in A such that \\(xa \\leq x\\), then \\(x(a\\ast) \\leq x\\).\n\nIntuitively, one should think of a + b as the “union” or the “least upper bound” of a and b and of ab as some multiplication which is monotonic, in the sense that \\(a \\leq b \\Rightarrow ax \\leq bx\\).\nThe idea behind the star operator is \\(a\\ast = 1 + a + aa + aaa + ...\\). From the standpoint of programming language theory, one may also interpret + as “choice”, \\(\\cdot\\) as “sequencing” and \\(\\ast\\) as “iteration”."
  },
  {
    "objectID": "nlp_lec1.html#regex-3",
    "href": "nlp_lec1.html#regex-3",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nConcatenation\n\n\nA sequence of characters\n/someword/\n\n\n\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/alt/\nThe alternative option would be…\n\n\n/simple/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-4",
    "href": "nlp_lec1.html#regex-4",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction\n\n\nA single character to choose among multiple options\n/[asd]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[0123456789]/\nSome number examples are 0, 3, 5\n\n\n/[rgx]/\nA simple regex"
  },
  {
    "objectID": "nlp_lec1.html#regex-5",
    "href": "nlp_lec1.html#regex-5",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction with range\n\n\nA single character to choose among multiple options\n/[a-z]/\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-6",
    "href": "nlp_lec1.html#regex-6",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene *\n\n\nZero or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]*/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-7",
    "href": "nlp_lec1.html#regex-7",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nCleene +\n\n\nOne or more occurrences of the previous character or regex.\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/[a-z]+/\nPhone number: 067-1234567"
  },
  {
    "objectID": "nlp_lec1.html#regex-8",
    "href": "nlp_lec1.html#regex-8",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nWildcard dot\n\n\nAny character except newline\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/.+/\nPhone number: 067-1234567\n\n\n/beg.n/\nbegin, began, begun"
  },
  {
    "objectID": "nlp_lec1.html#regex-9",
    "href": "nlp_lec1.html#regex-9",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nAnchors\n\n\nMatching beginning and end of lines\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/^/\nStart of line\n\n\n/$/\nEnd of line\n\n\n/\\b/\nWord boundary\n\n\n/\\B/\nNon-word boundary"
  },
  {
    "objectID": "nlp_lec1.html#regex-10",
    "href": "nlp_lec1.html#regex-10",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nDisjunction operator\n\n\nEither/or for strings\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/cat|dog/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-11",
    "href": "nlp_lec1.html#regex-11",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nGrouping\n\n\nUsed for precedence\n\nExamples\n\n\nRegex\nExample\n\n\n\n\n/(cat|dog)*/\nRaining cats and dogs"
  },
  {
    "objectID": "nlp_lec1.html#regex-12",
    "href": "nlp_lec1.html#regex-12",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nSubstitutions\n\n\nUsed for precedence\n\nExamples\n\n\n\n\n\n\n\nRegex\nBefore\nAfter\n\n\n\n\ns/(cat|dog)*/frog/\nRaining cats and dogs\nRaining frogs and frogs\n\n\ns/behaviour/behavior/\nCognitive-behavioural therapy\nCognitive-behavioral therapy"
  },
  {
    "objectID": "nlp_lec1.html#regex-13",
    "href": "nlp_lec1.html#regex-13",
    "title": "Natural Language Processing: Intro",
    "section": "Regex",
    "text": "Regex\n\n\n\nELIZA sample\n\n\nUser1: Men are all alike.\nELIZA1: User2: They’re always bugging us about something or other.\nELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser3: Well, my boyfriend made me come here.\nELIZA3: YOUR BOYFRIEND MADE YOU COME HERE\nUser4: He says I’m depressed much of the time.\nELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\n\n\n\n\n\n\nELIZA sample\n\n\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/"
  },
  {
    "objectID": "nlp_lec1.html#parsing",
    "href": "nlp_lec1.html#parsing",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\n\n\nA computer-readable collection of text or speech.\n\n\n\n\n\n\nPunctuation\n\n\nMarks indicating how a piece of written text should be read and understood.\n\n\n\n\n\n\nUtterance\n\n\nSpoken correlate of a sentence."
  },
  {
    "objectID": "nlp_lec1.html#parsing-1",
    "href": "nlp_lec1.html#parsing-1",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nDisfluency\n\n\nBreak or disruption that occurs in the flow of speech.\nTwo types:\n\nfragments\nfillers"
  },
  {
    "objectID": "nlp_lec1.html#parsing-2",
    "href": "nlp_lec1.html#parsing-2",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWord types\n\n\nAre the number of distinct words in a corpus; if the set of words in the vocabulary is \\(V\\) , the number of types is the vocabulary size \\(|V|\\).\n\n\n\n\n\n\nWord instances\n\n\nAre the total number \\(N\\) of running words.\n(sometimes also called word tokens).\n\n\n\n\n\n\nExample\n\n\n\nTo be, or not to be, that is the question."
  },
  {
    "objectID": "nlp_lec1.html#parsing-3",
    "href": "nlp_lec1.html#parsing-3",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nCorpus\nTypes = \\(|V|\\)\nInstances = \\(N\\)\n\n\n\n\nShakespeare\n31 thousand\n884 thousand\n\n\nBrown corpus\n38 thousand\n1 million\n\n\nSwitchboard telephone conversations\n20 thousand\n2.4 million\n\n\nCOCA\n2 million\n440 million\n\n\nGoogle n-grams\n13 million\n1 trillion"
  },
  {
    "objectID": "nlp_lec1.html#parsing-4",
    "href": "nlp_lec1.html#parsing-4",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nHerdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)\n\n\n\\[\n|V|= kN^{\\beta}.\n\\] Here \\(k\\) and \\(\\beta\\) are positive constants, and \\(0 &lt;\\beta &lt;1\\).\n\n\n\n\n\nFor large corpora \\(0.67 &lt; \\beta &lt; 0.75\\)."
  },
  {
    "objectID": "nlp_lec1.html#parsing-5",
    "href": "nlp_lec1.html#parsing-5",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nZipf’s law\n\n\nIf \\(t_1\\) is the most common term in the collection, \\(t_2\\) is the next most common, and so on, then the collection frequency \\(cf_i\\) of the \\(i\\)-th most common term is proportional to \\(\\frac{1}{i}\\): \\[\ncf_i \\propto \\frac{1}{i},\n\\] or \\[\ncf_i = ci^k.\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#parsing-6",
    "href": "nlp_lec1.html#parsing-6",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nWordforms\n\n\nExample: cat vs cats. We say these two words are different wordforms but have the same lemma.\n\n\n\n\n\n\nLemma\n\n\nA lemma is a set of lexical forms having the same stem, and usually the same major part-of-speech.\n\n\n\n\n\n\nExample\n\n\nThe wordform is the full inflected or derived form of the word. The two wordforms cat and cats thus have the same lemma, which we can represent as cat."
  },
  {
    "objectID": "nlp_lec1.html#parsing-7",
    "href": "nlp_lec1.html#parsing-7",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\n\n\n\nMorphemes\n\n\nThe smallest meaning-bearing unit of a language.\n\n\n\n\n\n\nExamples\n\n\nIndistinguisble -&gt; [in, distinguish, able]"
  },
  {
    "objectID": "nlp_lec1.html#affixes",
    "href": "nlp_lec1.html#affixes",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nAffix taxonomy\n\n\nAffixes are classified into two types:\n\nAccording to their position in the word\nAccording to their function in a phrase or sentence.\n\n\n\n\n\n\n\nBy position\n\n\n\nPrefixes\nInfixes\nSuffixes.\nCircumfixes (Georgian, Malay)"
  },
  {
    "objectID": "nlp_lec1.html#affixes-1",
    "href": "nlp_lec1.html#affixes-1",
    "title": "Natural Language Processing: Intro",
    "section": "Affixes",
    "text": "Affixes\n\n\n\nBy function\n\n\n\nDerivational affixes are for creating new words usually by changing the part of speech or the meaning or both to the words when they are added to.\n\nThey can be prefixes or suffixes e.g. unkind , kingship etc.\n\nInflectional affixes mark the grammatical categories e.g. –s in girls"
  },
  {
    "objectID": "nlp_lec1.html#language-morphology",
    "href": "nlp_lec1.html#language-morphology",
    "title": "Natural Language Processing: Intro",
    "section": "Language morphology",
    "text": "Language morphology\n\nanalytical (English)\ninflected (Ukrainian)\nagglutinative (partially German)"
  },
  {
    "objectID": "nlp_lec1.html#corpora",
    "href": "nlp_lec1.html#corpora",
    "title": "Natural Language Processing: Intro",
    "section": "Corpora",
    "text": "Corpora\nVarieties depending on:\n\nlanguages\nlanguage varieties\ngenres\ntime\nspeaker demographics"
  },
  {
    "objectID": "nlp_lec1.html#parsing-8",
    "href": "nlp_lec1.html#parsing-8",
    "title": "Natural Language Processing: Intro",
    "section": "Parsing",
    "text": "Parsing\nText normalization consists of:\n\nTokenizing (segmenting) words\nNormalizing word formats\nSegmenting sentences"
  },
  {
    "objectID": "nlp_lec1.html#tokenization",
    "href": "nlp_lec1.html#tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Tokenization",
    "text": "Tokenization\nTwo types:\n\ntop-down\nbottom-up"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization",
    "href": "nlp_lec1.html#top-down-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\nbreak off punctuation as a separate token\ninternal punctuation: Ph.D., AT&T\nprices ($45.55) and dates (18/02/2025)\nURLs (https://www.stanford.edu),\nTwitter hashtags (#nlproc)\nemail addresses (someone@cs.colorado.edu).\nnumber expressions introduce complications: e.g. 555,500.50.\nclitic contractions: I'm, l'homme."
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-1",
    "href": "nlp_lec1.html#top-down-tokenization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nPenn Treebank tokenization standard\n\n\nUsed for the parsed corpora (treebanks) released by the Lin- guistic Data Consortium (LDC).\n\nseparates out clitics (doesn’t becomes does plus n’t)\nkeeps hyphenated words together\nseparates out all punctuation"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-2",
    "href": "nlp_lec1.html#top-down-tokenization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\n\n\n\nnltk.regexp_tokenize\n\n\n&gt;&gt;&gt; text = ’That U.S.A. poster-print costs $12.40...’\n&gt;&gt;&gt; pattern = r’’’(?x) # set flag to allow verbose regexps\n... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n... | \\w+(?:-\\w+)* # words with optional internal hyphens\n... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n... | \\.\\.\\. # ellipsis\n... | [][.,;\"’?():_‘-] # these are separate tokens; includes ], [\n... ’’’\n&gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)\n[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]"
  },
  {
    "objectID": "nlp_lec1.html#top-down-tokenization-3",
    "href": "nlp_lec1.html#top-down-tokenization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Top-down tokenization",
    "text": "Top-down tokenization\nWord tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries.\n\n\n\nMorphemes In Chinese\n\n\nfor example, words are composed of characters (called hanzi in Chinese). Each character generally represents a single unit of meaning (a morpheme).\n\n\n\n\n\n\nWord segmentation\n\n\nFor Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required."
  },
  {
    "objectID": "nlp_lec1.html#bottom-up-tokenization",
    "href": "nlp_lec1.html#bottom-up-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Bottom-up tokenization",
    "text": "Bottom-up tokenization\n\n\n\nDefinition\n\n\nWe use the data to infer the tokens. We call these tokens subwords.\n\n\n\n\n\n\nParts\n\n\n\ntoken learner: produces a vocabulary of tokens\ntoken segmenter: takes a test sentence and segments it into tokens\n\n\n\n\n\n\n\nExamples\n\n\n\nbyte-pair encoding (Sennrich et al., 2016)\nunigram language modeling (Kudo, 2018)\nSentencePiece (Kudo and Richardson, 2018)"
  },
  {
    "objectID": "nlp_lec1.html#bpe",
    "href": "nlp_lec1.html#bpe",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nToken Learner\n\n\n\nStart with a vocabulary that is just the set of all individual characters.\nExamine the training corpus, choose the two symbols that are most frequently adjacent\n2.1. For example, if (‘A’, ‘B’) frequently occur together, it will add a new merged symbol ‘AB’ to the vocabulary\n2.2. And replaces every adjacent ’A’ ’B’ in the corpus with the new ‘AB’.\nContinue counting and merging, creating new longer and longer character strings, until \\(k\\) merges have been done creating k novel tokens; \\(k\\) is thus a parameter of the algorithm.\n\n4.The resulting vocabulary consists of the original set of characters plus \\(k\\) new symbols."
  },
  {
    "objectID": "nlp_lec1.html#bpe-1",
    "href": "nlp_lec1.html#bpe-1",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\n\n\n\nNote\n\n\nThe algorithm is usually run inside words (not merging across word boundaries), so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol , and its counts."
  },
  {
    "objectID": "nlp_lec1.html#bpe-2",
    "href": "nlp_lec1.html#bpe-2",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab \\(V\\)\n\\(V \\leftarrow\\) unique characters in C # initial set of tokens is characters\nfor i = 1 to k do # merge tokens k times\n\\(\\quad\\) \\(t_L\\), \\(t_R\\) \\(\\leftarrow\\) #Most frequent pair of adjacent tokens in C\n\\(\\quad\\) \\(t_{NEW} \\leftarrow t_L + t_R\\) # make new token by concatenating\n\\(\\quad\\) \\(V \\leftarrow V + t_{NEW}\\) # update the vocabulary\n\\(\\quad\\) Replace each occurrence of \\(t_L\\), \\(t_R\\) in \\(C\\) with \\(t_{NEW}\\). # update the corpus\nreturn \\(V\\)"
  },
  {
    "objectID": "nlp_lec1.html#bpe-3",
    "href": "nlp_lec1.html#bpe-3",
    "title": "Natural Language Processing: Intro",
    "section": "BPE",
    "text": "BPE\nToken Segmenter:\n\nRuns on the merges we have learned from the training data on the test data.\nIt runs them greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization",
    "href": "nlp_lec1.html#word-normalization",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nSimplest method: case folding.\n\n\n\nNote\n\n\nNot very useful for text classification: compare US (the country) and us (pronoun).\n\n\n\n\n\n\nLemmatization\n\n\nThe task of determining that two words have the same root, despite their surface differences.\nbe \\(\\rightarrow\\) is, are\nPerformed using morphological parsing."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-1",
    "href": "nlp_lec1.html#word-normalization-1",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nMorphological parsing\n\n\nSplitting each word into morphemes of two types:\n\nstems\naffixes\n\n\n\n\n\n\n\nNaive version: stemming\n\n\nThis means just dropping affixes."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-2",
    "href": "nlp_lec1.html#word-normalization-2",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\n\nPorter stemmer\n\n\n\nclassify every character in a given token as either a consonant (“c”) or vowel (“v”)\ngroup subsequent consonants as “C” and subsequent vowels as “V.”\nrepresent every word token as a combination of consonant and vowel groups."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-1",
    "href": "nlp_lec1.html#porter-stemmer-1",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nExample\n\n\ncollection \\(\\rightarrow\\) CVCV…C\nillustrate \\(\\rightarrow\\) VCVC…V\nBoth can be presented as:\n\\[\n[C](VC)^m[V]\n\\]\nm is called the measure of the word."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-2",
    "href": "nlp_lec1.html#porter-stemmer-2",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer"
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-3",
    "href": "nlp_lec1.html#porter-stemmer-3",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nRules\n\n\nStandard form: \\[\n(\\textbf{condition})\\textbf{S}_1 \\rightarrow \\textbf{S}_2\n\\] There are five phases of rule application\n\n\n\n\n\n\nHow to read\n\n\nIf a word ends with the suffix \\(S_1\\)\nAND\nthe stem before \\(S_1\\) satisfies the given condition\nTHEN \\(S_1\\) is replaced by \\(S_2\\)."
  },
  {
    "objectID": "nlp_lec1.html#porter-stemmer-4",
    "href": "nlp_lec1.html#porter-stemmer-4",
    "title": "Natural Language Processing: Intro",
    "section": "Porter Stemmer",
    "text": "Porter Stemmer\n\n\n\nConditions\n\n\n\n\\(\\ast S\\): the stem ends with S (and similarly for the other letters)\n\\(\\ast v \\ast\\): the stem contains a vowel\n\\(\\ast d\\): the stem ends with a double consonant (e.g. -TT, -SS)\n\\(\\ast o\\): the stem ends with \\(cvc\\), where the second c is not W, X or Y (e.g. -WIL, -HOP)\n\nAnd the condition part may also contain expressions with and, or and not.\n\n\n\n\n\n\nExample\n\n\n\\((m &gt; 1) EMENT \\rightarrow\\) will perform this transformation:\nreplacement \\(\\rightarrow\\) replac"
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-3",
    "href": "nlp_lec1.html#word-normalization-3",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\nOther stemmers:\n\n\n\nLovins stemmer\n\n\nThe first published stemming algorithm, is essentially a heavily parametrized find-and-replace function.\n\ncompares each input token against a list of common English suffixes, each suffix being conditioned by one of twenty-nine rules\nif the stemmer finds a predefined suffix in a token and removing the suffix does not violate any conditions attached to that suffix (such as character length restrictions), the algorithm removes that suffix.\nthe stemmer then runs the resulting stemmed token through another set of rules that correct for common malformations, such as double letters (such as hopping becomes hopp becomes hop)."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-4",
    "href": "nlp_lec1.html#word-normalization-4",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nSnowball stemmer\n\n\nAn updated version of the Porter stemmer. It differs from Porter in two main ways:\n\nWhile Lovins and Porter only stem English words, Snowball can stem text data in other Roman script languages, such as Dutch, German, French, or Spanish. Also has capabilities for non-Roman script languages.\nSnowball has an option to ignore stop words."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-5",
    "href": "nlp_lec1.html#word-normalization-5",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nLancaster stemmer (also called Paice stemmer)\n\n\nThe most aggressive English stemming algorithm.\n\nit contains a list of over 100 rules that dictate which ending strings to replace.\nthe stemmer iterates each word token against each rule. If a token’s ending characters match the string defined in a given rule, the algorithm modifies the token per that rule’s operation, then runs the transformed token through every rule again.\nthe stemmer iterates each token through each rule until that token passes all the rules without being transformed."
  },
  {
    "objectID": "nlp_lec1.html#word-normalization-6",
    "href": "nlp_lec1.html#word-normalization-6",
    "title": "Natural Language Processing: Intro",
    "section": "Word Normalization",
    "text": "Word Normalization\n\n\n\nStemming errors\n\n\n\nover-generalizing (lemmatizing policy to police)\nunder-generalizing (not lemmatizing European to Europe)"
  },
  {
    "objectID": "nlp_lec1.html#sentence-tokenization",
    "href": "nlp_lec1.html#sentence-tokenization",
    "title": "Natural Language Processing: Intro",
    "section": "Sentence Tokenization",
    "text": "Sentence Tokenization\n\n\n\nChallenges\n\n\n\nmulti-purpose punctuation\nabbreviation dictionaries"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance",
    "href": "nlp_lec1.html#edit-distance",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nDefinition\n\n\nMinimum edit distance between two strings is defined as the minimum number of editing operations:\n\ninsertion\ndeletion\nsubstitution\n\nneeded to transform one string into another."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-1",
    "href": "nlp_lec1.html#edit-distance-1",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nString alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-2",
    "href": "nlp_lec1.html#edit-distance-2",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nLevenshtein distance\n\n\nEach of the 3 operations has cost 1.\nAlternatively, we can forbid substitutions (this is equivalent to saying that substitutions have cost 2)."
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-3",
    "href": "nlp_lec1.html#edit-distance-3",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\nWagner-Fischer minimum edit distance algorithm.\n\n\n\nNotation\n\n\n\n\\(X\\): source string with length \\(n\\)\n\\(Y\\): target string with length \\(m\\)\n\\(D[i,j]\\): edit distance between \\(X[1..i]\\) and \\(Y[1..j]\\).\n\\(D[n,m]\\): edit distance between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\nCalculation\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + \\text{del_cost}(source[i]),\\\\\nD[i,j-1] + \\text{ins_cost}(target[j]),\\\\\nD[i-1,j-1] + \\text{sub_cost}(source[i], target[j]).\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-4",
    "href": "nlp_lec1.html#edit-distance-4",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nCalculation without substitution\n\n\n\\[\nD[i,j] = \\min \\begin{cases}\nD[i-1,j] + 1,\\\\\nD[i,j-1] + 1,\\\\\nD[i-1,j-1] + \\begin{cases}\n2; \\quad \\text{if} \\quad source[i] \\neq target[j]), \\\\\n0; \\quad \\text{if} \\quad source[i] = target[j])\n\\end{cases}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-5",
    "href": "nlp_lec1.html#edit-distance-5",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\nfunction MIN-EDIT-DISTANCE(source, target) returns min-distance\n\\(n \\leftarrow LENGTH(source)\\)\n\\(m \\leftarrow LENGTH(target)\\)\nCreate a distance matrix \\(D[n+1,m+1]\\)\n# Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nfor each row i from 1 to n do\n\\(\\quad\\) \\(D[i,0] \\leftarrow D[i-1,0]\\) + del_cost(source[i])\nfor each column j from 1 to m do\n\\(\\quad\\) \\(D[0,j] \\leftarrow D[0, j-1] + ins-cost(target[j])\\)"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-6",
    "href": "nlp_lec1.html#edit-distance-6",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nWagner-Fischer algorithm\n\n\n# Recurrence relation:\nfor each row i from 1 to n do\n\\(\\quad\\) for each column j from 1 to m do\n\\(\\quad\\) \\(\\quad\\) \\(D[i, j] \\leftarrow MIN( D[i−1, j]\\) + del_cost(source[i]),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i−1, j−1] + sub\\_cost(source[i], target[j])\\),\n\\(\\quad\\)\\(\\quad\\) \\(\\quad\\) \\(D[i, j−1] + ins\\_cost(target[j]))\\)\n# Termination\nreturn D[n,m]"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-7",
    "href": "nlp_lec1.html#edit-distance-7",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance"
  },
  {
    "objectID": "nlp_lec1.html#cost-alignment",
    "href": "nlp_lec1.html#cost-alignment",
    "title": "Natural Language Processing: Intro",
    "section": "Cost alignment",
    "text": "Cost alignment"
  },
  {
    "objectID": "nlp_lec1.html#edit-distance-8",
    "href": "nlp_lec1.html#edit-distance-8",
    "title": "Natural Language Processing: Intro",
    "section": "Edit distance",
    "text": "Edit distance\n\n\n\nHamming distance\n\n\nA number of positions at which the corresponding symbols are different.\nTherefore, identical to Levenshtein with only substitution allowed.\nCan only work for strings of similar length.\n\n\n\ndef hamming_distance(string1: str, string2: str) -&gt; int:\n    \"\"\"Return the Hamming distance between two strings.\"\"\"\n    if len(string1) != len(string2):\n        raise ValueError(\"Strings must be of equal length.\")\n    dist_counter = 0\n    for n in range(len(string1)):\n        if string1[n] != string2[n]:\n            dist_counter += 1\n    return dist_counter"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "nlp.html#lectures",
    "href": "nlp.html#lectures",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "nlp.html#labs",
    "href": "nlp.html#labs",
    "title": "Natural Language Processing",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2\nLab 3\nLab 4"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#lectures",
    "href": "dl.html#lectures",
    "title": "Deep Learning",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "dl.html#labs",
    "href": "dl.html#labs",
    "title": "Deep Learning",
    "section": "Labs",
    "text": "Labs\nLab 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning/NLP",
    "section": "",
    "text": "Deep Learning/Natural Language Processing course"
  },
  {
    "objectID": "nlp_lab1.html",
    "href": "nlp_lab1.html",
    "title": "NLP: Lab 1",
    "section": "",
    "text": "We’ll start with basic text analysis via statistical methods.\nFor this purpose, we’ll use three libraries (mostly):\n\nNLTK\nScattertext\nSpacy"
  },
  {
    "objectID": "nlp_lab1.html#through-requirements.txt",
    "href": "nlp_lab1.html#through-requirements.txt",
    "title": "NLP: Lab 1",
    "section": "Through requirements.txt",
    "text": "Through requirements.txt\nYou can install all dependencies through requirements.txt:\npip install -r requirements.txt\nAlternatively, or if any issues occur, we can proceed manually via the following steps:"
  },
  {
    "objectID": "nlp_lab1.html#install-python-nltk-package",
    "href": "nlp_lab1.html#install-python-nltk-package",
    "title": "NLP: Lab 1",
    "section": "1. Install Python NLTK package",
    "text": "1. Install Python NLTK package\nFrom here.\n   pip install nltk\n   pip install matplotlib\nIn order to install Python Tkinter library, look here.\nAlso install additional data by\n   import nltk; \n   nltk.download('popular')\nSet up the texts:\n\n   import nltk\n   nltk.download('nps_chat')\n   nltk.download('webtext')\n   from nltk.book import *\n\n[nltk_data] Downloading package nps_chat to /Users/vitvly/nltk_data...\n[nltk_data]   Package nps_chat is already up-to-date!\n[nltk_data] Downloading package webtext to /Users/vitvly/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!"
  },
  {
    "objectID": "nlp_lab1.html#install-scattertext-and-spacy",
    "href": "nlp_lab1.html#install-scattertext-and-spacy",
    "title": "NLP: Lab 1",
    "section": "2. Install Scattertext and Spacy",
    "text": "2. Install Scattertext and Spacy\npip install spacy scattertext\nAnd then update Spacy:\n!python -m spacy download en_core_web_sm"
  },
  {
    "objectID": "nlp_lab1.html#example-concordance",
    "href": "nlp_lab1.html#example-concordance",
    "title": "NLP: Lab 1",
    "section": "Example: concordance",
    "text": "Example: concordance\n\ntext3.concordance(\"earth\")\n\nDisplaying 25 of 112 matches:\nnning God created the heaven and the earth . And the earth was without form , a\nd the heaven and the earth . And the earth was without form , and void ; and da\nwas so . And God called the dry land Earth ; and the gathering together of the \nit was good . And God said , Let the earth bring forth grass , the herb yieldin\nupon the ear and it was so . And the earth brought forth grass , and herb yield\nof the heaven to give light upon the earth , And to rule over the day and over \nfe , and fowl that may fly above the earth in the open firmament of heaven . An\n seas , and let fowl multiply in the earth . And the evening and the morning we\ne fifth day . And God said , Let the earth bring forth the living creature afte\nnd creeping thing , and beast of the earth after his ki and it was so . And God\ns so . And God made the beast of the earth after his kind , and cattle after th\nd every thing that creepeth upon the earth after his ki and God saw that it was\nd over the cattle , and over all the earth , and over every creeping thing that\nreeping thing that creepeth upon the earth . So God created man in his own imag\nl , and multiply , and replenish the earth , and subdue and have dominion over \nry living thing that moveth upon the earth . And God said , Behold , I have giv\n , which is upon the face of all the earth , and every tree , in the which is t\nfor meat . And to every beast of the earth , and to every fowl of the air , and\no every thing that creepeth upon the earth , wherein there is life , I have giv\nsixth day . Thus the heavens and the earth were finished , and all the host of \nenerations of the heavens and of the earth when they were created , in the day \nn the day that the LORD God made the earth and the heavens , And every plant of\nnt of the field before it was in the earth , and every herb of the field before\nd had not caused it to rain upon the earth , and there was not a man to till th\n . But there went up a mist from the earth , and watered the whole face of the"
  },
  {
    "objectID": "nlp_lab1.html#example-similar",
    "href": "nlp_lab1.html#example-similar",
    "title": "NLP: Lab 1",
    "section": "Example: similar",
    "text": "Example: similar\n\ntext3.similar(\"man\")\n\nland lord men place woman earth waters well city lad day cattle field\nwife way flood servant people famine pillar"
  },
  {
    "objectID": "nlp_lab1.html#example-dispersion_plot",
    "href": "nlp_lab1.html#example-dispersion_plot",
    "title": "NLP: Lab 1",
    "section": "Example: dispersion_plot",
    "text": "Example: dispersion_plot\n\ntext3.dispersion_plot([\"man\", \"earth\"])"
  },
  {
    "objectID": "nlp_lab1.html#example-freqdist",
    "href": "nlp_lab1.html#example-freqdist",
    "title": "NLP: Lab 1",
    "section": "Example: FreqDist",
    "text": "Example: FreqDist\n\nfdist = FreqDist(text3)\nprint(fdist)\n\n&lt;FreqDist with 2789 samples and 44764 outcomes&gt;\n\n\n\nfdist.most_common(50)\n\n[(',', 3681),\n ('and', 2428),\n ('the', 2411),\n ('of', 1358),\n ('.', 1315),\n ('And', 1250),\n ('his', 651),\n ('he', 648),\n ('to', 611),\n (';', 605),\n ('unto', 590),\n ('in', 588),\n ('that', 509),\n ('I', 484),\n ('said', 476),\n ('him', 387),\n ('a', 342),\n ('my', 325),\n ('was', 317),\n ('for', 297),\n ('it', 290),\n ('with', 289),\n ('me', 282),\n ('thou', 272),\n (\"'\", 268),\n ('is', 267),\n ('thy', 267),\n ('s', 263),\n ('thee', 257),\n ('be', 254),\n ('shall', 253),\n ('they', 249),\n ('all', 245),\n (':', 238),\n ('God', 231),\n ('them', 230),\n ('not', 224),\n ('which', 198),\n ('father', 198),\n ('will', 195),\n ('land', 184),\n ('Jacob', 179),\n ('came', 177),\n ('her', 173),\n ('LORD', 166),\n ('were', 163),\n ('she', 161),\n ('from', 157),\n ('Joseph', 157),\n ('their', 153)]\n\n\n\nfdist.plot(50, cumulative=True)"
  },
  {
    "objectID": "nlp_lab3.html",
    "href": "nlp_lab3.html",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "We will learn how to:\n\nuse lemmatization (a better version of stemming) using WordNet\ndo text cleanup\nanalyze WordNet semantic hierarchies\n\n\n\n\nGet tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function.\n\n\n\n\nFirst, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]\n\n\n\n\n\nWordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]\n\n\n\n\n\n\nLet’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come).\n\n\n\nNow we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nlp_lab3.html#plan",
    "href": "nlp_lab3.html#plan",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Get tokens for positive and negative tweets (by token in this context we mean word).\nLemmatize them (convert to base word forms). For that we will use a Part-of-Speech tagger.\nClean’em up (remove mentions, URLs, stop words).\nWrite the final processing function."
  },
  {
    "objectID": "nlp_lab3.html#preparation",
    "href": "nlp_lab3.html#preparation",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "First, we’ll download Twitter samples from NLTK:\n\nimport nltk\n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nAnd import these:\n\nfrom nltk.corpus import twitter_samples\n\nThese contain positive/negative tweet samples.\nWe can check the string content of these tweets:\n\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\npositive_tweets[50]\n\n'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'\n\n\nOr we can get a list of tokens using tokenized method on twitter_samples.\n\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(tweet_tokens[50])\n\n['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n\n\nNow let’s setup a Part-of-Speech tagger. We will use it for lemmatization.\nDownload and import a perceptron tagger that will be used by the PoS tagger.\n\nnltk.download('averaged_perceptron_tagger_eng')\nfrom nltk.tag import pos_tag\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nCheck how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier.\n\npos_tag(tweet_tokens[50])\n\n[('@groovinshawn', 'NN'),\n ('they', 'PRP'),\n ('are', 'VBP'),\n ('rechargeable', 'JJ'),\n ('and', 'CC'),\n ('it', 'PRP'),\n ('normally', 'RB'),\n ('comes', 'VBZ'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('charger', 'NN'),\n ('when', 'WRB'),\n ('u', 'JJ'),\n ('buy', 'VB'),\n ('it', 'PRP'),\n (':)', 'JJ')]"
  },
  {
    "objectID": "nlp_lab3.html#wordnet",
    "href": "nlp_lab3.html#wordnet",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "WordNet is a semantically-oriented dictionary of English. It contains words along with their synonyms etc., organized in a semantic hierarchy.\nNext, we will use it’s lemmatizer functionality. But first let’s check how the hierarchy-focused features work.\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to /Users/vitvly/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nE.g., in order to fetch synonym sets for word car, you do:\n\nfrom nltk.corpus import wordnet as wn\n\nword_synset = wn.synsets(\"car\")\nprint(\"synsets:\", word_synset)\nprint(\"lemma names:\", word_synset[0].lemma_names())\n\nsynsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\nlemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n\n\nSo, using WordNet, we can say that lemmas are pairings of synsets and words.\nWe can now show definitions and examples:\n\nword_synset[0].definition()\n\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n\n\n\nword_synset[0].examples()\n\n['he needs a car to get to work']\n\n\n\nword_synset[1].definition()\n\n'a wheeled vehicle adapted to the rails of railroad'\n\n\n\nword_synset[1].examples()\n\n['three cars had jumped the rails']\n\n\n\n\n\nConcepts that are more specific:\n\nword_synset[0].hyponyms()\n\n[Synset('hot_rod.n.01'),\n Synset('cruiser.n.01'),\n Synset('hatchback.n.01'),\n Synset('sedan.n.01'),\n Synset('stock_car.n.01'),\n Synset('sports_car.n.01'),\n Synset('racer.n.02'),\n Synset('hardtop.n.01'),\n Synset('model_t.n.01'),\n Synset('cab.n.03'),\n Synset('minivan.n.01'),\n Synset('limousine.n.01'),\n Synset('used-car.n.01'),\n Synset('bus.n.04'),\n Synset('horseless_carriage.n.01'),\n Synset('sport_utility.n.01'),\n Synset('ambulance.n.01'),\n Synset('roadster.n.01'),\n Synset('convertible.n.01'),\n Synset('gas_guzzler.n.01'),\n Synset('subcompact.n.01'),\n Synset('touring_car.n.01'),\n Synset('coupe.n.01'),\n Synset('pace_car.n.01'),\n Synset('beach_wagon.n.01'),\n Synset('stanley_steamer.n.01'),\n Synset('jeep.n.01'),\n Synset('electric.n.01'),\n Synset('loaner.n.02'),\n Synset('minicar.n.01'),\n Synset('compact.n.03')]\n\n\n\n\n\nConcepts that are more generic:\n\nword_synset[0].hypernyms()\n\n[Synset('motor_vehicle.n.01')]\n\n\nThis only gave us the concept immediately above. In order to list all hypernyms, we can use paths:\n\ntree = wn.synsets(\"tree\")[0]\npaths = tree.hypernym_paths()\nfor p in paths:\n  print([synset.name() for synset in p])\n\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n\n\n\n\n\nConcept parts:\n\ntree.part_meronyms()\n\n[Synset('stump.n.01'),\n Synset('crown.n.07'),\n Synset('burl.n.02'),\n Synset('trunk.n.01'),\n Synset('limb.n.02')]\n\n\nOr the substance it’s made of:\n\ntree.substance_meronyms()\n\n[Synset('sapwood.n.01'), Synset('heartwood.n.01')]\n\n\n\n\n\nEntities concept is part of:\n\ntree.member_holonyms()\n\n[Synset('forest.n.01')]"
  },
  {
    "objectID": "nlp_lab3.html#lemmatization-function",
    "href": "nlp_lab3.html#lemmatization-function",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Let’s write a function that will lemmatize twitter tokens.\nUse documentation for lemmatize.\nFirst fetch PoS tokens so that they can be passed to WordNetLemmatizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer\ntokens = tweet_tokens[50]\n# Create a lemmatizer\nlemmatizer = WordNetLemmatizer()\nNow write the code that will produce an array of lemmatized tokens inside lemmatized_sentence.\nConvert PoS tags into a format used by the lemmatizer using the following rules:\n\nNN \\(\\rightarrow\\) n\nVB \\(\\rightarrow\\) v\nelse \\(\\rightarrow\\) a\n\nThen on each token use lemmatizer.lemmatize() using the converted part-of-speech tag.\nAnd append it to lemmatized_sentence.\ndef lemmatize_sentence(tokens)\n  lemmatized_sentence = []\n\n  # CODE_START\n  # ...\n  # CODE_END\n\n  return lemmatized_sentence\n\nlemmatize_sentence(tokens)\nNote that lemmatizer converts words to their base forms (are \\(\\rightarrow\\) be, comes \\(\\rightarrow\\) come)."
  },
  {
    "objectID": "nlp_lab3.html#processing",
    "href": "nlp_lab3.html#processing",
    "title": "NLP: Lab 3",
    "section": "",
    "text": "Now we can proceed to processing. During processing, we will perform cleanup:\n\nremove URLs and mentions using regexes\nafter lemmatization, remove stopwords\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vitvly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nWhat are these stopwords? Let’s see some.\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(len(stop_words))\nfor i in range(10):\n    print(stop_words[i])\n\n198\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\n\n\nNow, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\nIt should do the following:\n\nIterate through pos_tag(tweet_tokens).\nUse regex to remove tokens matching URLs or mentions (@somebody).\nRemove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\nLowercase all tokens\nLemmatize using WordNetLemmatizer.\nReturn the list of cleaned_tokens.\n\nimport re, string\n\ndef process_tokens(tweet_tokens):\n\n    cleaned_tokens = []\n    stop_words = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    for token, tag in pos_tag(tweet_tokens):\n      # CODE_START\n      # ...\n      # CODE_END\n    return cleaned_tokens\nTest your function:\nprint(\"Before:\", tweet_tokens[50])\nprint(\"After:\", process_tokens(tweet_tokens[50]))\nNow run process_tokens on all positive/negative tokens (use tokenized method as mentioned above).\n# CODE_START\n\n# positive_tweet_tokens =\n# negative_tweet_tokens =\n\n# positive_cleaned_tokens_list =\n# negative_cleaned_tokens_list =\n\n# CODE_END\nLet’s see how did the processing go.\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])\nNow, let’s check what words are most common.\nFirst, add a helper function get_all_words:\ndef get_all_words(cleaned_tokens_list):\n  # CODE_START\n  # ...\n  # CODE_END\nall_pos_words = get_all_words(positive_cleaned_tokens_list)\nPerform frequency analysis using FreqDist and print 10 words most commonly used in positive tweets:\nfrom nltk import FreqDist\n\n# CODE_START\n# use all_pos_words\n# ...\n# CODE_END"
  },
  {
    "objectID": "nb/nlp_lab2.html",
    "href": "nb/nlp_lab2.html",
    "title": "Porter stemming",
    "section": "",
    "text": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\n \n# Create Snowball stemmer\nsnow_stemmer = SnowballStemmer(language='english')\n\n# Create a Porter Stemmer instance\nporter_stemmer = PorterStemmer()\n\n# Create a Lancaster Stemmer instance\nlancaster_stemmer = LancasterStemmer()\n\n# Example words for stemming\nwords = [\"running\", \"jumps\", \"happily\", \"programming\", 'cared','fairly','sportingly']\n\n# Apply stemming to each word\nstemmed_words = [porter_stemmer.stem(word) for word in words]\nprint(\"===Porter===:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Snowball===:\")\nstemmed_words = [snow_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\nprint(\"\\n===Lancaster===:\")\nstemmed_words = [lancaster_stemmer.stem(word) for word in words]\nprint(\"Porter:\")\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n\n===Porter===:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fairli', 'sportingli']\n\n===Snowball===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happili', 'program', 'care', 'fair', 'sport']\n\n===Lancaster===:\nPorter:\nOriginal words: ['running', 'jumps', 'happily', 'programming', 'cared', 'fairly', 'sportingly']\nStemmed words: ['run', 'jump', 'happy', 'program', 'car', 'fair', 'sport']"
  },
  {
    "objectID": "nb/dl_lab1/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "href": "nb/dl_lab1/Logistic_Regression_with_a_Neural_Network_mindset.html",
    "title": "Logistic Regression using a Neural Network",
    "section": "",
    "text": "We will use neural networks to build a logistic regression classifier for image recognition.\nDuring the exercises, you will have to fill in the code between these comments:\n# CODE_START\n&lt;...your code here...&gt;\n# CODE_END\nPlease remember that we have to vectorize our code, therefore:\n\nDo not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\nUse np.dot(X,Y) to calculate dot products.\n\nDuring the exercises you will:\n\nBuild the general architecture of a learning algorithm, including:\n\nInitializing parameters\nCalculating the cost function and its gradient\nUsing an optimization algorithm (gradient descent)\n\nGather all three functions above into a main model function, in the right order.\n\n ## 1 - Packages ##\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n### v1.2\n\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\nfrom public_tests import *\n\n%matplotlib inline\n# Module autoreloading\n%load_ext autoreload\n%autoreload 2\n\n ## 2 - Overview of the Problem set ##\nProblem Statement: You are given a dataset (“data.h5”) containing:\n\na training set of m_train images labeled as cat (y=1) or non-cat (y=0)\na test set of m_test images labeled as cat or non-cat\neach image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 20\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\nImportant: many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n ### Exercise 1 Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) - Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n#(≈ 3 lines of code)\n# m_train = \n# m_test = \n# num_px = \n# CODE_START\n\n# CODE_END\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nExpected Output for m_train, m_test and num_px:\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n ### Exercise 2 Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\n#(≈ 2 lines of code)\n# train_set_x_flatten = ...\n# test_set_x_flatten = ...\n# CODE_START\n\n# CODE_END\n\n# Check that the first 10 pixels of the second image are in the correct place\nassert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\nassert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nExpected Output:\n\n\n\ntrain_set_x_flatten shape\n\n\n(12288, 209)\n\n\n\n\ntrain_set_y shape\n\n\n(1, 209)\n\n\n\n\ntest_set_x_flatten shape\n\n\n(12288, 50)\n\n\n\n\ntest_set_y shape\n\n\n(1, 50)\n\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\nCommon steps for pre-processing a new dataset are:\n\nFigure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)\nReshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\n“Standardize” the data\n\n ## 3 - General Architecture of the learning algorithm ##\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps:\n\nInitialize the parameters of the model\nLearn the parameters for the model by minimizing the cost\n\nUse the learned parameters to make predictions (on the test set)\nAnalyse the results and conclude\n\n ## 4 - Building the parts of our algorithm ##\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n ### 4.1 - Helper functions\n ### Exercise 3 - sigmoid First, please implement sigmoid() function. As you’ve seen in the figure above, you need to compute \\[\nsigmoid(z) = \\frac{1}{1 + e^{-z}}\n\\] for \\(z = w^T x + b\\) to make predictions. Use np.exp().\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    #(≈ 1 line of code)\n    # s = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid_test(sigmoid)\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n ### 4.2 - Initializing parameters\n ### Exercise 4 - initialize_with_zeros Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    \n    # (≈ 2 lines of code)\n    # w = ...\n    # b = ...\n    # CODE_START\n\n    # CODE_END\n\n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\ninitialize_with_zeros_test_1(initialize_with_zeros)\ninitialize_with_zeros_test_2(initialize_with_zeros)\n\n ### 4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n ### Exercise 5 - propagate Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum\\limits_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    grads -- dictionary containing the gradients of the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    cost -- negative log-likelihood cost for logistic regression\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    #(≈ 2 lines of code)\n    # compute activation\n    # A = ...\n    # compute cost by using np.dot to perform multiplication. \n    # And don't use loops for the sum.\n    # cost = ...                                \n    # CODE_START\n\n    \n    # CODE_END\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    #(≈ 2 lines of code)\n    # dw = ...\n    # db = ...\n    # CODE_START\n\n    # CODE_END\n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\n\n# X is using 3 examples, with 2 features each\n# Each example is stacked column-wise\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\npropagate_test(propagate)\n\nExpected output\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.1250040450043965\ncost = 0.15900537707692405\n ### 4.4 - Optimization - You have initialized your parameters. - You are also able to compute a cost function and its gradient. - Now, you want to update the parameters using gradient descent.\n ### Exercise 6 - optimize Write down the optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # (≈ 1 lines of code)\n        # Cost and gradient calculation \n        # grads, cost = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        # w = ...\n        # b = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\noptimize_test(optimize)\n\n ### Exercise 7 - predict The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    #(≈ 1 line of code)\n    # A = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        #(≈ 4 lines of code)\n        # if A[0, i] &gt; ____ :\n        #     Y_prediction[0,i] = \n        # else:\n        #     Y_prediction[0,i] = \n        # YCODE_START\n\n        \n        # CODE_END\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredict_test(predict)\n\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples\n ## 5 - Merge all functions into a model ##\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n ### Exercise 8 - model Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize()\n\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    # (≈ 1 line of code)   \n    # initialize parameters with zeros\n    # and use the \"shape\" function to get the first dimension of X_train\n    # w, b = ...\n    \n    #(≈ 1 line of code)\n    # Gradient descent \n    # params, grads, costs = ...\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    # w = ...\n    # b = ...\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    # Y_prediction_test = ...\n    # Y_prediction_train = ...\n    \n    # CODE_START\n\n    \n    # CODE_END\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nfrom public_tests import *\n\nmodel_test(model)\n\nIf you pass all the tests, run the following cell to train your model.\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\nComment: - Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. - Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier.\nAlso, you see that the model is clearly overfitting the training data.\nUsing the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n ## 6 - Further analysis\nCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nInterpretation: - Different learning rates give different costs and thus different predictions results. - If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). - A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. - In deep learning, we usually recommend that you: - Choose the learning rate that better minimizes the cost function. - If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)\n ## 7 - Test with your own image\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n\nAdd your image to this Jupyter Notebook’s directory, in the “images” folder\nChange your image’s name in the following code\nRun the code and check if the algorithm is right (1 = cat, 0 = non-cat).\n\n\n# change this to the name of your image file\nmy_image = \"Cats-image-cats-36712791-1222-917.jpg\"   \n\n# We preprocess the image to fit your algorithm.\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\nmy_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nImportant: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)\nRecommended reading: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html",
    "title": "Planar data classification with one hidden layer",
    "section": "",
    "text": "Welcome to your week 3 programming assignment! It’s time to build your first neural network, which will have one hidden layer. Now, you’ll notice a big difference between this model and the one you implemented previously using logistic regression.\nBy the end of this assignment, you’ll be able to:"
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#important-note-on-submission-to-the-autograder",
    "title": "Planar data classification with one hidden layer",
    "section": "Important Note on Submission to the AutoGrader",
    "text": "Important Note on Submission to the AutoGrader\nBefore submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n\nYou have not added any extra print statement(s) in the assignment.\nYou have not added any extra code cell(s) in the assignment.\nYou have not changed any of the function parameters.\nYou are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\nYou are not changing the assignment code where it is not required, like creating extra variables.\n\nIf you do any of the following, you will get something like, Grader Error: Grader feedback not found (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these instructions."
  },
  {
    "objectID": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "href": "nb/W3A1/Planar_data_classification_with_one_hidden_layer.html#table-of-contents",
    "title": "Planar data classification with one hidden layer",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1 - Packages\n2 - Load the Dataset\n\nExercise 1\n\n3 - Simple Logistic Regression\n4 - Neural Network model\n\n4.1 - Defining the neural network structure\n\nExercise 2 - layer_sizes\n\n4.2 - Initialize the model’s parameters\n\nExercise 3 - initialize_parameters\n\n4.3 - The Loop\n\nExercise 4 - forward_propagation\n\n4.4 - Compute the Cost\n\nExercise 5 - compute_cost\n\n4.5 - Implement Backpropagation\n\nExercise 6 - backward_propagation\n\n4.6 - Update Parameters\n\nExercise 7 - update_parameters\n\n4.7 - Integration\n\nExercise 8 - nn_model\n\n\n5 - Test the Model\n\n5.1 - Predict\n\nExercise 9 - predict\n\n5.2 - Test the Model on the Planar Dataset\n\n6 - Tuning hidden layer size (optional/ungraded exercise)\n7- Performance on other datasets\n\n # 1 - Packages\nFirst import all the packages that you will need during this assignment.\n\nnumpy is the fundamental package for scientific computing with Python.\nsklearn provides simple and efficient tools for data mining and data analysis.\nmatplotlib is a library for plotting graphs in Python.\ntestCases provides some test examples to assess the correctness of your functions\nplanar_utils provide various useful functions used in this assignment\n\n\n### v1.1\n\n\n# Package imports\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nfrom public_tests import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2\n\n # 2 - Load the Dataset\n\nX, Y = load_planar_dataset()\n\nVisualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.\n\n# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nYou have: - a numpy-array (matrix) X that contains your features (x1, x2) - a numpy-array (vector) Y that contains your labels (red:0, blue:1).\nFirst, get a better sense of what your data is like.\n ### Exercise 1\nHow many training examples do you have? In addition, what is the shape of the variables X and Y?\nHint: How do you get the shape of a numpy array? (help)\n\n# (≈ 3 lines of code)\n# shape_X = ...\n# shape_Y = ...\n# training set size\n# m = ...\n# YOUR CODE STARTS HERE\nshape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[0]\n# YOUR CODE ENDS HERE\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nThe shape of X is: (2, 400)\nThe shape of Y is: (1, 400)\nI have m = 2 training examples!\n\n\nExpected Output:\n\n\n\nshape of X\n\n\n(2, 400)\n\n\n\n\nshape of Y\n\n\n(1, 400)\n\n\n\n\nm\n\n\n400\n\n\n\n ## 3 - Simple Logistic Regression\nBefore building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\n/Users/vitvly/c/lnu/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nYou can now plot the decision boundary of these models! Run the code below.\n\n# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n\nAccuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)\n\n\n/var/folders/jr/7vzj1lzn0rx65bxcqn8nrwqw0000gn/T/ipykernel_49977/4242423965.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n\n\n\n\n\n\n\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n47%\n\n\n\nInterpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!\n ## 4 - Neural Network model\nLogistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.\nThe model: \nMathematically:\nFor one example \\(x^{(i)}\\): \\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\\] \\[a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\\] \\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\\] \\[\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\\] \\[y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\\]\nGiven the predictions on all the examples, you can also compute the cost \\(J\\) as follows: \\[J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\\]\nReminder: The general methodology to build a Neural Network is to: 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model’s parameters 3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)\nIn practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data.\n ### 4.1 - Defining the neural network structure ####\n ### Exercise 2 - layer_sizes\nDefine three variables: - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4, as n_h = 4, but only for this Exercise 2) - n_y: the size of the output layer\nHint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.\n\n# GRADED FUNCTION: layer_sizes\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    #(≈ 3 lines of code)\n    # n_x = ... \n    # n_h = ...\n    # n_y = ... \n    # YOUR CODE STARTS HERE\n    n_x = X.shape[0]\n    n_h = 4\n    n_y = Y.shape[0]\n    # YOUR CODE ENDS HERE\n    return (n_x, n_h, n_y)\n\n\nt_X, t_Y = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nlayer_sizes_test(layer_sizes)\n\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n\n\nExpected output\nThe size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\nAll tests passed!\n ### 4.2 - Initialize the model’s parameters ####\n ### Exercise 3 - initialize_parameters\nImplement the function initialize_parameters().\nInstructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.\n\n# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = np.random.randn(n_h,n_x) * 0.01 \n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01 \n    b2 = np.zeros((n_y, 1))\n    # YOUR CODE ENDS HERE\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nnp.random.seed(2)\nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test(initialize_parameters)\n\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\nAll tests passed!\n ### 4.3 - The Loop\n ### Exercise 4 - forward_propagation\nImplement forward_propagation() using the following equations:\n\\[Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}\\] \\[A^{[1]} = \\tanh(Z^{[1]})\\tag{2}\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}\\] \\[\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}\\]\nInstructions:\n\nCheck the mathematical representation of your classifier in the figure above.\nUse the function sigmoid(). It’s built into (imported) this notebook.\nUse the function np.tanh(). It’s part of the numpy library.\nImplement using these steps:\n\nRetrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() by using parameters[\"..\"].\nImplement Forward Propagation. Compute \\(Z^{[1]}, A^{[1]}, Z^{[2]}\\) and \\(A^{[2]}\\) (the vector of all your predictions on all the examples in the training set).\n\nValues needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.\n\n\n# GRADED FUNCTION:forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]   \n    # YOUR CODE ENDS HERE\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    # (≈ 4 lines of code)\n    # Z1 = ...\n    # A1 = ...\n    # Z2 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    \n    # YOUR CODE ENDS HERE\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n\nt_X, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(t_X, parameters)\nprint(\"A2 = \" + str(A2))\n\nforward_propagation_test(forward_propagation)\n\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n\n\nExpected output\nA2 = [[0.21292656 0.21274673 0.21295976]]\nAll tests passed!\n ### 4.4 - Compute the Cost\nNow that you’ve computed \\(A^{[2]}\\) (in the Python variable “A2”), which contains \\(a^{[2](i)}\\) for all examples, you can compute the cost function as follows:\n\\[J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\\]\n ### Exercise 5 - compute_cost\nImplement compute_cost() to compute the value of the cost \\(J\\).\nInstructions: - There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops: \\(- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})\\):\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)          \n\nUse that to build the whole expression of the cost function.\n\nNotes:\n\nYou can use either np.multiply() and then np.sum() or directly np.dot()).\n\nIf you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array.\n\nYou can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).\nYou can also cast the array as a type float using float().\n\n\n# GRADED FUNCTION: compute_cost\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    # (≈ 2 lines of code)\n    # logprobs = ...\n    # cost = ...\n    # YOUR CODE STARTS HERE\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)\n    cost = -np.sum(logprobs)/m\n    \n    # YOUR CODE ENDS HERE\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    \n    return cost\n\n\nA2, t_Y = compute_cost_test_case()\ncost = compute_cost(A2, t_Y)\nprint(\"cost = \" + str(compute_cost(A2, t_Y)))\n\ncompute_cost_test(compute_cost)\n\ncost = 0.6930587610394646\nAll tests passed!\n\n\nExpected output\ncost = 0.6930587610394646\nAll tests passed!\n ### 4.5 - Implement Backpropagation\nUsing the cache computed during forward propagation, you can now implement backward propagation.\n ### Exercise 6 - backward_propagation\nImplement the function backward_propagation().\nInstructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.\n\n\n\nFigure 1: Backpropagation. Use the six equations on the right.\n\n\n\n\nTips:\n\nTo compute dZ1 you’ll need to compute \\(g^{[1]'}(Z^{[1]})\\). Since \\(g^{[1]}(.)\\) is the tanh activation function, if \\(a = g^{[1]}(z)\\) then \\(g^{[1]'}(z) = 1-a^2\\). So you can compute \\(g^{[1]'}(Z^{[1]})\\) using (1 - np.power(A1, 2)).\n\n\n\n# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    #(≈ 2 lines of code)\n    # W1 = ...\n    # W2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    \n    # YOUR CODE ENDS HERE\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    #(≈ 2 lines of code)\n    # A1 = ...\n    # A2 = ...\n    # YOUR CODE STARTS HERE\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    \n    # YOUR CODE ENDS HERE\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n    # dZ2 = ...\n    # dW2 = ...\n    # db2 = ...\n    # dZ1 = ...\n    # dW1 = ...\n    # db1 = ...\n    # YOUR CODE STARTS HERE\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T)/m\n    db2 = np.sum(dZ2, axis = 1, keepdims=True)/m\n    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1,2))\n    dW1 = np.dot(dZ1, X.T)/m\n    db1 = np.sum(dZ1, axis = 1, keepdims=True)/m\n    # YOUR CODE ENDS HERE\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads\n\n\nparameters, cache, t_X, t_Y = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\nbackward_propagation_test(backward_propagation)\n\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n\n\nExpected output\ndW1 = [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]]\ndb1 = [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]]\ndW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\ndb2 = [[-0.16655712]]\nAll tests passed!\n ### 4.6 - Update Parameters\n ### Exercise 7 - update_parameters\nImplement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\nGeneral gradient descent rule: \\(\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\) where \\(\\alpha\\) is the learning rate and \\(\\theta\\) represents a parameter.\n \n\n\nFigure 2: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n\nHint\n\nUse copy.deepcopy(...) when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n\n\n# GRADED FUNCTION: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = copy.deepcopy(parameters[\"W1\"])\n    b1 = copy.deepcopy(parameters[\"b1\"])\n    W2 = copy.deepcopy(parameters[\"W2\"])\n    b2 = copy.deepcopy(parameters[\"b2\"])\n    # YOUR CODE ENDS HERE\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    #(≈ 4 lines of code)\n    # dW1 = ...\n    # db1 = ...\n    # dW2 = ...\n    # db2 = ...\n    # YOUR CODE STARTS HERE\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    # YOUR CODE ENDS HERE\n    \n    # Update rule for each parameter\n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # YOUR CODE STARTS HERE\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2   \n    # YOUR CODE ENDS HERE\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nupdate_parameters_test(update_parameters)\n\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n\n\nExpected output\nW1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\nAll tests passed!\n ### 4.7 - Integration\nIntegrate your functions in nn_model()\n ### Exercise 8 - nn_model\nBuild your neural network model in nn_model().\nInstructions: The neural network model has to use the previous functions in the right order.\n\n# GRADED FUNCTION: nn_model\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters\n    #(≈ 1 line of code)\n    # parameters = ...\n    # YOUR CODE STARTS HERE\n    parameters = initialize_parameters(X.shape[0], n_h, Y.shape[0])\n    \n    # YOUR CODE ENDS HERE\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        #(≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        # A2, cache = ...\n        \n        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n        # cost = ...\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        # grads = ...\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        # parameters = ...\n        \n        # YOUR CODE STARTS HERE\n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n        \n        # YOUR CODE ENDS HERE\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nnn_model_test(nn_model)\n\nCost after iteration 0: 0.693086\nCost after iteration 1000: 0.000220\nCost after iteration 2000: 0.000108\nCost after iteration 3000: 0.000072\nCost after iteration 4000: 0.000054\nCost after iteration 5000: 0.000043\nCost after iteration 6000: 0.000036\nCost after iteration 7000: 0.000030\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n\n\nExpected output\nCost after iteration 0: 0.693198\nCost after iteration 1000: 0.000219\nCost after iteration 2000: 0.000108\n...\nCost after iteration 8000: 0.000027\nCost after iteration 9000: 0.000024\nW1 = [[ 0.71392202  1.31281102]\n [-0.76411243 -1.41967065]\n [-0.75040545 -1.38857337]\n [ 0.56495575  1.04857776]]\nb1 = [[-0.0073536 ]\n [ 0.01534663]\n [ 0.01262938]\n [ 0.00218135]]\nW2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]\nb2 = [[0.00393452]]\nAll tests passed!\n ## 5 - Test the Model\n ### 5.1 - Predict\n ### Exercise 9 - predict\nPredict with your model by building predict(). Use forward propagation to predict results.\nReminder: predictions = \\(y_{prediction} = \\mathbb 1 \\text{{activation &gt; 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\\)\nAs an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold)\n\n# GRADED FUNCTION: predict\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    #(≈ 2 lines of code)\n    # A2, cache = ...\n    # predictions = ...\n    # YOUR CODE STARTS HERE\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2 &gt; 0.5)\n    \n    # YOUR CODE ENDS HERE\n    \n    return predictions\n\n\nparameters, t_X = predict_test_case()\n\npredictions = predict(parameters, t_X)\nprint(\"Predictions: \" + str(predictions))\n\npredict_test(predict)\n\nPredictions: [[ True False  True]]\nAll tests passed!\n\n\nExpected output\nPredictions: [[ True False  True]]\nAll tests passed!\n ### 5.2 - Test the Model on the Planar Dataset\nIt’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of \\(n_h\\) hidden units!\n\n# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\nCost after iteration 0: 0.693162\nCost after iteration 1000: 0.258625\nCost after iteration 2000: 0.239334\nCost after iteration 3000: 0.230802\nCost after iteration 4000: 0.225528\nCost after iteration 5000: 0.221845\nCost after iteration 6000: 0.219094\nCost after iteration 7000: 0.220638\nCost after iteration 8000: 0.219418\nCost after iteration 9000: 0.218528\n\n\nText(0.5, 1.0, 'Decision Boundary for hidden layer size 4')\n\n\n\n\n\n\n\n\n\n\n# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n\nAccuracy: 90%\n\n\nExpected Output:\n\n\n\nAccuracy\n\n\n90%\n\n\n\nAccuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.\n\nCongrats on finishing this Programming Assignment!\nHere’s a quick recap of all you just accomplished:\n\nBuilt a complete 2-class classification neural network with a hidden layer\nMade good use of a non-linear unit\nComputed the cross entropy loss\nImplemented forward and backward propagation\nSeen the impact of varying the hidden layer size, including overfitting.\n\nYou’ve created a neural network that can learn patterns! Excellent work. Below, there are some optional exercises to try out some other hidden layer sizes, and other datasets.\n ## 6 - Tuning hidden layer size (optional/ungraded exercise)\nRun the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.\n\n# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5]\n\n# you can try with different hidden layer sizes\n# but make sure before you submit the assignment it is set as \"hidden_layer_sizes = [1, 2, 3, 4, 5]\"\n# hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n\nAccuracy for 1 hidden units: 67.5 %\nAccuracy for 2 hidden units: 67.25 %\nAccuracy for 3 hidden units: 90.75 %\nAccuracy for 4 hidden units: 90.5 %\nAccuracy for 5 hidden units: 91.25 %\n\n\n\n\n\n\n\n\n\nInterpretation: - The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. - The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. - Later, you’ll become familiar with regularization, which lets you use very large models (such as n_h = 50) without much overfitting.\nNote: Remember to submit the assignment by clicking the blue “Submit Assignment” button at the upper-right.\nSome optional/ungraded questions that you can explore if you wish: - What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? - Play with the learning_rate. What happens? - What if we change the dataset? (See part 7 below!)\n ## 7- Performance on other datasets\nIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.\n\n# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### START CODE HERE ### (choose your dataset)\ndataset = \"noisy_moons\"\n### END CODE HERE ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\nReferences:\n\nhttp://scs.ryerson.ca/~aharley/neural-networks/\nhttp://cs231n.github.io/neural-networks-case-study/"
  },
  {
    "objectID": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "href": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "",
    "text": "Notation: - Superscript \\([l]\\) denotes a quantity associated with the \\(l^{th}\\) layer. - Example: \\(a^{[L]}\\) is the \\(L^{th}\\) layer activation. \\(W^{[L]}\\) and \\(b^{[L]}\\) are the \\(L^{th}\\) layer parameters. - Superscript \\((i)\\) denotes a quantity associated with the \\(i^{th}\\) example. - Example: \\(x^{(i)}\\) is the \\(i^{th}\\) training example. - Lowerscript \\(i\\) denotes the \\(i^{th}\\) entry of a vector. - Example: \\(a^{[l]}_i\\) denotes the \\(i^{th}\\) entry of the \\(l^{th}\\) layer’s activations).\n## 1 - Packages\nFirst, import all the packages you’ll need during this assignment.\n### v1.1\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom testCases import *\nfrom dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward, load_data\nfrom public_tests import *\n\nimport time\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\nimport copy\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\n## 2 - Outline\nTo build your neural network, you’ll be implementing several “helper functions.” Here’s an outline of the steps:\nNote:\nFor every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.\nIn the backpropagation module, you can then use the cache to calculate the gradients.\n## 3 - Initialization\nYou will write two helper functions to initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one generalizes this initialization process to \\(L\\) layers.\n### 3.1 - 2-layer Neural Network\n### Exercise 1 - initialize_parameters\nCreate and initialize the parameters of the 2-layer neural network.\nInstructions:\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    #(≈ 4 lines of code)\n    # W1 = ...\n    # b1 = ...\n    # W2 = ...\n    # b2 = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters(3,2,1)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_1(initialize_parameters)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters(4,3,2)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_test_2(initialize_parameters)\nExpected output\n### 3.2 - L-layer Neural Network\nThe initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the initialize_parameters_deep function, you should make sure that your dimensions match between each layer. Recall that \\(n^{[l]}\\) is the number of units in layer \\(l\\). For example, if the size of your input \\(X\\) is \\((12288, 209)\\) (with \\(m=209\\) examples) then:\nRemember that when you compute \\(W X + b\\) in python, it carries out broadcasting. For example, if:\n\\[ W = \\begin{bmatrix}\n    w_{00}  & w_{01} & w_{02} \\\\\n    w_{10}  & w_{11} & w_{12} \\\\\n    w_{20}  & w_{21} & w_{22}\n\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n    x_{00}  & x_{01} & x_{02} \\\\\n    x_{10}  & x_{11} & x_{12} \\\\\n    x_{20}  & x_{21} & x_{22}\n\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n    b_0  \\\\\n    b_1  \\\\\n    b_2\n\\end{bmatrix}\\tag{2}\\]\nThen \\(WX + b\\) will be:\n\\[ WX + b = \\begin{bmatrix}\n    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n\\end{bmatrix}\\tag{3}  \\]\n### Exercise 2 - initialize_parameters_deep\nImplement initialization for an L-layer Neural Network.\nInstructions: - The model’s structure is [LINEAR -&gt; RELU] $ $ (L-1) -&gt; LINEAR -&gt; SIGMOID. I.e., it has \\(L-1\\) layers using a ReLU activation function followed by an output layer with a sigmoid activation function. - Use random initialization for the weight matrices. Use np.random.randn(d0, d1, ..., dn) * 0.01. - Use zeros initialization for the biases. Use np.zeros(shape). - You’ll store \\(n^{[l]}\\), the number of units in different layers, in a variable layer_dims. For example, the layer_dims for last week’s Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means W1’s shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to \\(L\\) layers! - Here is the implementation for \\(L=1\\) (one layer neural network). It should inspire you to implement the general case (L-layer neural network).\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        # parameters['W' + str(l)] = ...\n        # parameters['b' + str(l)] = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\nprint(\"Test Case 1:\\n\")\nparameters = initialize_parameters_deep([5,4,3])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ninitialize_parameters_deep_test_1(initialize_parameters_deep)\n\nprint(\"\\033[90m\\nTest Case 2:\\n\")\nparameters = initialize_parameters_deep([4,3,2])\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\ninitialize_parameters_deep_test_2(initialize_parameters_deep)\nExpected output\n## 4 - Forward Propagation Module\n### 4.1 - Linear Forward\nNow that you have initialized your parameters, you can do the forward propagation module. Start by implementing some basic functions that you can use again later when implementing the model. Now, you’ll complete three functions in this order:\nThe linear forward module (vectorized over all the examples) computes the following equations:\n\\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}\\]\nwhere \\(A^{[0]} = X\\).\n### Exercise 3 - linear_forward\nBuild the linear part of forward propagation.\nReminder: The mathematical representation of this unit is \\(Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\). You may also find np.dot() useful. If your dimensions don’t match, printing W.shape may help.\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    # Z = ...\n    # CODE_START\n    \n    # CODE_END\n    cache = (A, W, b)\n    \n    return Z, cache\nt_A, t_W, t_b = linear_forward_test_case()\nt_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\nprint(\"Z = \" + str(t_Z))\n\nlinear_forward_test(linear_forward)\nExpected output\n### 4.2 - Linear-Activation Forward\nIn this notebook, you will use two activation functions:\nFor added convenience, you’re going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you’ll implement a function that does the LINEAR forward step, followed by an ACTIVATION forward step.\n### Exercise 4 - linear_activation_forward\nImplement the forward propagation of the LINEAR-&gt;ACTIVATION layer. Mathematical relation is: \\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\\) where the activation “g” can be sigmoid() or relu(). Use linear_forward() and the correct activation function.\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # CODE_START\n        \n        # CODE_END\n    \n    elif activation == \"relu\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # CODE_START\n        \n        # CODE_END\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\nt_A_prev, t_W, t_b = linear_activation_forward_test_case()\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\nprint(\"With sigmoid: A = \" + str(t_A))\n\nt_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\nprint(\"With ReLU: A = \" + str(t_A))\n\nlinear_activation_forward_test(linear_activation_forward)\nExpected output\nNote: In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.\n### 4.3 - L-Layer Model\nFor even more convenience when implementing the \\(L\\)-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with RELU) \\(L-1\\) times, then follows that with one linear_activation_forward with SIGMOID.\n### Exercise 5 - L_model_forward\nImplement the forward propagation of the above model.\nInstructions: In the code below, the variable AL will denote \\(A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})\\). (This is sometimes also called Yhat, i.e., this is \\(\\hat{Y}\\).)\nHints: - Use the functions you’ve previously written - Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times - Don’t forget to keep track of the caches in the “caches” list. To add a new value c to a list, you can use list.append(c).\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    # The for loop starts at 1 because layer 0 is the input\n    for l in range(1, L):\n        A_prev = A \n        #(≈ 2 lines of code)\n        # A, cache = ...\n        # caches ...\n        # CODE_START\n\n        # CODE_END\n    \n    # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.\n    #(≈ 2 lines of code)\n    # AL, cache = ...\n    # caches ...\n    # CODE_START\n\n    # CODE_END\n          \n    return AL, caches\nt_X, t_parameters = L_model_forward_test_case_2hidden()\nt_AL, t_caches = L_model_forward(t_X, t_parameters)\n\nprint(\"AL = \" + str(t_AL))\n\nL_model_forward_test(L_model_forward)\nExpected output\nAwesome! You’ve implemented a full forward propagation that takes the input X and outputs a row vector \\(A^{[L]}\\) containing your predictions. It also records all intermediate values in “caches”. Using \\(A^{[L]}\\), you can compute the cost of your predictions.\n## 5 - Cost Function\nNow you can implement forward and backward propagation! You need to compute the cost, in order to check whether your model is actually learning.\n### Exercise 6 - compute_cost Compute the cross-entropy cost \\(J\\), using the following formula: \\[-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}\\]\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    # (≈ 1 lines of code)\n    # cost = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n\n    \n    return cost\nt_Y, t_AL = compute_cost_test_case()\nt_cost = compute_cost(t_AL, t_Y)\n\nprint(\"Cost: \" + str(t_cost))\n\ncompute_cost_test(compute_cost)\nExpected Output:\n## 6 - Backward Propagation Module\nJust as you did for the forward propagation, you’ll implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters."
  },
  {
    "objectID": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html#model-architecture",
    "href": "nb/dl_lab3/Building_your_Deep_Neural_Network_Step_by_Step.html#model-architecture",
    "title": "Building your Deep Neural Network: Step by Step",
    "section": "8 - Model architecture",
    "text": "8 - Model architecture\n ### 8.1 - 2-layer Neural Network\nNow that you’re familiar with the dataset, it’s time to build a deep neural network to distinguish cat images from non-cat images!\nYou’re going to build two different models:\n\nA 2-layer neural network\nAn L-layer deep neural network\n\nThen, you’ll compare the performance of these models, and try out some different values for \\(L\\).\nLet’s look at the two architectures:\n\n\n\nFigure 2: 2-layer neural network.  The model can be summarized as: INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.\n\n\nDetailed Architecture of Figure 2: - The input is a (64,64,3) image which is flattened to a vector of size \\((12288,1)\\). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) of size \\((n^{[1]}, 12288)\\). - Then, add a bias term and take its relu to get the following vector: \\([a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T\\). - Multiply the resulting vector by \\(W^{[2]}\\) and add the intercept (bias). - Finally, take the sigmoid of the result. If it’s greater than 0.5, classify it as a cat.\n ### 8.2 - L-layer Deep Neural Network\nIt’s pretty difficult to represent an L-layer deep neural network using the above representation. However, here is a simplified network representation:\n\n\n\nFigure 3: L-layer neural network.  The model can be summarized as: [LINEAR -&gt; RELU] \\(\\times\\) (L-1) -&gt; LINEAR -&gt; SIGMOID\n\n\nDetailed Architecture of Figure 3: - The input is a (64,64,3) image which is flattened to a vector of size (12288,1). - The corresponding vector: \\([x_0,x_1,...,x_{12287}]^T\\) is then multiplied by the weight matrix \\(W^{[1]}\\) and then you add the intercept \\(b^{[1]}\\). The result is called the linear unit. - Next, take the relu of the linear unit. This process could be repeated several times for each \\((W^{[l]}, b^{[l]})\\) depending on the model architecture. - Finally, take the sigmoid of the final linear unit. If it is greater than 0.5, classify it as a cat.\n ### 8.3 - General Methodology\nAs usual, you’ll follow the Deep Learning methodology to build the model:\n\nInitialize parameters / Define hyperparameters\nLoop for num_iterations:\n\nForward propagation\nCompute cost function\nBackward propagation\nUpdate parameters (using parameters, and grads from backprop)\n\nUse trained parameters to predict labels\n\nNow go ahead and implement those two models!\n ## 9 - Two-layer Neural Network\n ### Exercise 11 - two_layer_model\nUse the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)\nlearning_rate = 0.0075\n\n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- If set to True, this will print the cost every 100 iterations \n    \n    Returns:\n    parameters -- a dictionary containing W1, W2, b1, and b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n    \n\n    # CODE_END\n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        #(≈ 2 lines of code)\n        # A1, cache1 = ...\n        # A2, cache2 = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Compute cost\n        #(≈ 1 line of code)\n        # cost = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n        #(≈ 2 lines of code)\n        # dA1, dW2, db2 = ...\n        # dA0, dW1, db1 = ...\n        # CODE_START\n\n        # CODE_END\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n        #(approx. 1 line of code)\n        # parameters = ...\n        # CODE_START\n        \n\n        # CODE_END\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n\n    return parameters, costs\n\ndef plot_costs(costs, learning_rate=0.0075):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\ntwo_layer_model_test(two_layer_model)\n\nExpected output:\ncost after iteration 1 must be around 0.69\n ### 9.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your parameters.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\nplot_costs(costs, learning_rate)\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.6930497356599888\n\n\n\n\nCost after iteration 100\n\n\n0.6464320953428849\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.04421498215868956\n\n\n\nNice! You successfully trained the model. Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.\nNow, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.\n\npredictions_train = predict(train_x, train_y, parameters)\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.9999999999999998\n\n\n\n\npredictions_test = predict(test_x, test_y, parameters)\n\nExpected Output:\n\n\n\nAccuracy\n\n\n0.72\n\n\n\n\nCongratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an \\(L\\)-layer model.\nNote: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping”, later we’ll learn more about it. Early stopping is a way to prevent overfitting.\n ## 10 - L-layer Neural Network\n ### Exercise 12 - L_layer_model\nUse the helper functions you implemented previously to build an \\(L\\)-layer neural network with the following structure: [LINEAR -&gt; RELU]\\(\\times\\)(L-1) -&gt; LINEAR -&gt; SIGMOID. The functions and their inputs are:\ndef initialize_parameters_deep(layers_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\n### CONSTANTS ###\nlayers_dims = [12288, 20, 7, 5, 1] #  4-layer model\n\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization.\n    #(≈ 1 line of code)\n    # parameters = ...\n    # CODE_START\n\n    \n    # CODE_END\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.\n        #(≈ 1 line of code)\n        # AL, caches = ...\n        # CODE_START\n\n        \n        # CODE_END\n        \n        # Compute cost.\n        #(≈ 1 line of code)\n        # cost = ...\n        # CODE_START\n \n        \n        # CODE_END\n    \n        # Backward propagation.\n        #(≈ 1 line of code)\n        # grads = ...    \n        # CODE_START\n\n        \n        # CODE_END\n \n        # Update parameters.\n        #(≈ 1 line of code)\n        # parameters = ...\n        # CODE_START\n\n        \n        # CODE_END\n                \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n\nprint(\"Cost after first iteration: \" + str(costs[0]))\n\nL_layer_model_test(L_layer_model)\n\n ### 10.1 - Train the model\nIf your code passed the previous cell, run the cell below to train your model as a 4-layer neural network.\n\nThe cost should decrease on every iteration.\nIt may take up to 5 minutes to run 2500 iterations.\n\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n\nExpected Output:\n\n\n\nCost after iteration 0\n\n\n0.771749\n\n\n\n\nCost after iteration 100\n\n\n0.672053\n\n\n\n\n…\n\n\n…\n\n\n\n\nCost after iteration 2499\n\n\n0.088439\n\n\n\n\npred_train = predict(train_x, train_y, parameters)\n\nExpected Output:\n\n\n\nTrain Accuracy\n\n\n0.985645933014\n\n\n\n\npred_test = predict(test_x, test_y, parameters)\n\nExpected Output:\n\n\n\nTest Accuracy\n\n\n0.8\n\n\n\n\n\nCongrats! It seems that your 4-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.\nThis is pretty good performance for this task. Nice job!\nLater, we’ll be able to obtain even higher accuracy by systematically searching for better hyperparameters: learning_rate, layers_dims, or num_iterations, for example.\n ## 11 - Results Analysis\nFirst, take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.\n\nprint_mislabeled_images(classes, test_x, test_y, pred_test)\n\nA few types of images the model tends to do poorly on include: - Cat body in an unusual position - Cat appears against a background of a similar color - Unusual cat color and species - Camera Angle - Brightness of the picture - Scale variation (cat is very large or small in image)\n ## 12 - Test with your own image ##\nFrom this point, if you so choose, you can use your own image to test the output of your model. To do that follow these steps:\n\nClick on “File” in the upper bar of this notebook, then click “Open”.\nAdd your image to this Jupyter Notebook’s directory, in the “images” folder\nChange your image’s name in the following code\nRun the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n\n\n## CODE_START ##\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)\n## CODE_END ##\n\nfname = \"images/\" + my_image\nimage = np.array(Image.open(fname).resize((num_px, num_px)))\nplt.imshow(image)\nimage = image / 255.\nimage = image.reshape((1, num_px * num_px * 3)).T\n\nmy_predicted_image = predict(image, my_label_y, parameters)\n\n\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
  },
  {
    "objectID": "nlp_lab4.html",
    "href": "nlp_lab4.html",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "nlp_lab4.html#plan",
    "href": "nlp_lab4.html#plan",
    "title": "NLP: Lab 4 (Naive Bayes classifier)",
    "section": "",
    "text": "Prepare models for the classifier, based on cleaned-up tokens from Lab3.\nRun the Naive Bayes classifier.\n\nUse positive_cleaned_tokens_list and negative_cleaned_tokens_list from Lab3\nWe’ll convert these to a data structure usable for NLTK’s naive Bayes classifier (docs here):\n[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]\ndef get_token_dict(tokens):\n    return dict([token, True] for token in tokens)\n    \ndef get_tweets_for_model(cleaned_tokens_list):   \n    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\nCreate two datasets for positive and negative tweets. Use 7000/3000 split for train and test data.\nimport random\n\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\nFinally we use the nltk’s NaiveBayesClassifier on the training data we’ve just created:\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(10))\nNote the Positive:Negative ratios.\nLet’s check some test phrase. First, download punkt sentence tokenizer (docs here)\nnltk.download('punkt_tab')\nNow we won’t rely on twitter_samples.tokenized, but rather will use a generic tokenization routine - word_tokenize.\nfrom nltk.tokenize import word_tokenize\n\ncustom_tweet = \"the service was so bad\"\n\ncustom_tokens = process_tokens(word_tokenize(custom_tweet))\n\nprint(classifier.classify(get_token_dict(custom_tokens)))\nLet’s package it as a function and test it:\ndef get_sentiment(text):\n    custom_tokens = process_tokens(word_tokenize(text))\n    return classifier.classify(get_token_dict(custom_tokens))\n\ntexts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\nfor t in texts:\n    print(t, \": \", get_sentiment(t))"
  },
  {
    "objectID": "dl_lab1.html",
    "href": "dl_lab1.html",
    "title": "DL: Lab 1",
    "section": "",
    "text": "Lab overview\nWe’ll implement a binary classifier using logistic regression, but via a neural network.\nCode in attached Jupyter notebook."
  },
  {
    "objectID": "dl_lec2.html#neurons",
    "href": "dl_lec2.html#neurons",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nImportant\n\n\nANN \\(\\equiv\\) Artificial Neural Network\nRemember - Neurons, axons, dendrites.\n\n\n\n\n\n\nTip\n\n\nInputs to neurons are scaled with weight.\nWeight is similar to a strength of synaptic connection."
  },
  {
    "objectID": "dl_lec2.html#neurons-1",
    "href": "dl_lec2.html#neurons-1",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons"
  },
  {
    "objectID": "dl_lec2.html#neurons-2",
    "href": "dl_lec2.html#neurons-2",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nComputation\n\n\nANN computes a function of the inputs by propagating the computed values from input neurons to output neurons, using weights as intermediate parameters.\n\n\n\n\n\n\nLearning\n\n\nLearning occurs by changing the weights. External stimuli are required for learning in bio-organisms, in case of ANNs they are provided by the training data.\n\n\n\n\n\n\nTraining\n\n\nTraining data contain input-output pairs. We compare predicted output with annotated output label from training data."
  },
  {
    "objectID": "dl_lec2.html#neurons-3",
    "href": "dl_lec2.html#neurons-3",
    "title": "Deep learning: logistic regression",
    "section": "Neurons",
    "text": "Neurons\n\n\n\nErrors\n\n\nErrors are comparison failures. These are similar to unpleasant feedback modifying synaptic strengths. Goal of changing weights - make predictions better.\n\n\n\n\n\n\nModel generalizations\n\n\nAbility to compute functions of unseen inputs accurately, even though given finite sets of input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#computation-graph",
    "href": "dl_lec2.html#computation-graph",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAlternative view - computation graph.\nWhen used in basic graph, NNs reduce to classical ML models.\n\nLeast-squares regression\nlogistic regression\nlinear regression\n\nNodes compute based on inputs and weights."
  },
  {
    "objectID": "dl_lec2.html#goal",
    "href": "dl_lec2.html#goal",
    "title": "Deep learning: logistic regression",
    "section": "Goal",
    "text": "Goal\nGoal of NN: learn a function that relates inputs to outputs with the use of training examples.\nSettings the edge weights is training."
  },
  {
    "objectID": "dl_lec2.html#structure",
    "href": "dl_lec2.html#structure",
    "title": "Deep learning: logistic regression",
    "section": "Structure",
    "text": "Structure\nConsider a simple case of \\(d\\) inputs and a single binary output. \\[\n(\\overline{X}, y) - \\text{training instance}\n\\]\nFeature variables: \\[\n\\overline{X}=[x_1, \\dots, x_d]\n\\] Observed value: \\(y \\in {0,1}\\), contained in target variable \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#objective",
    "href": "dl_lec2.html#objective",
    "title": "Deep learning: logistic regression",
    "section": "Objective",
    "text": "Objective\n\nlearn the function \\(f(\\cdot)\\), such that \\(y=f_{\\overline{W}}(\\overline{X})\\).\nminimize mismatch between \\(y\\) and \\(f_{\\overline{W}}(\\overline{X})\\). \\(W\\) - weight vector.\n\nIn case of perceptron, we compute a linear function: \\[\\begin{align*}\n  &\\hat{y}=f(\\overline{X}) = sign\\left\\{\\overline{W}^T \\overline{X}^T\\right\\} =  sign\\left\\{\\sum\\limits_{i=1}^d w_i x_i\\right\\}\n\\end{align*}\\] \\(\\hat{y}\\) means value, not observed value \\(y\\)."
  },
  {
    "objectID": "dl_lec2.html#perceptron",
    "href": "dl_lec2.html#perceptron",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nA simplest NN."
  },
  {
    "objectID": "dl_lec2.html#perceptron-1",
    "href": "dl_lec2.html#perceptron-1",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe choose the basic form of the function, but strive to find some parameters.\n\nSign is an activation function\nvalue of the node is also sometimes referred to as an activation.\n\nPerceptron is a single-layer network, as input nodes are not counted."
  },
  {
    "objectID": "dl_lec2.html#perceptron-2",
    "href": "dl_lec2.html#perceptron-2",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nHow does perceptron learn? \\[\n\\overline{W} := \\overline{W} + \\alpha(y-\\hat{y})\\overline{X}^T.\n\\] So, in case when \\(y \\neq \\hat{y}\\), we can write it as \\[\n\\overline{W} := \\overline{W} + \\alpha y \\overline{X}^T.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-3",
    "href": "dl_lec2.html#perceptron-3",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron",
    "text": "Perceptron\nWe can show that perceptron works when data are linearly separable by a hyperplane \\(\\overline{W}^T X = 0\\).\n\n\n\nPerceptron algorithm is not guaranteed to converge when data are not linearly separable."
  },
  {
    "objectID": "dl_lec2.html#bias",
    "href": "dl_lec2.html#bias",
    "title": "Deep learning: logistic regression",
    "section": "Bias",
    "text": "Bias\nBias is needed when binary class distribution is imbalanced: \\[\n\\overline{W}^T \\cdot \\sum_i \\overline{X_i}^T \\neq \\sum_i y_i\n\\] Bias can be incorporated by using a bias neuron.\n\n\n\nProblems\n\n\nIn linearly separable data sets, a nonzero weight vector \\(W\\) exists in which the \\(sign(\\overline{W}^T X) = sign(y_i)\\; \\forall (\\overline{X}_i,y_i)\\).\nHowever, the behavior of the perceptron algorithm for data that are not linearly separable is rather arbitrary."
  },
  {
    "objectID": "dl_lec2.html#loss-function",
    "href": "dl_lec2.html#loss-function",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nML algorithms are loss optimization problems, where gradient descent updates are used to minimize the loss.\nOriginal perceptron did not formally use a loss function.\nRetrospectively we can introduce it as: \\[\nL_i \\equiv \\max\\left\\{-y_i(\\overline{W}^T \\overline{X_i}\\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#loss-function-1",
    "href": "dl_lec2.html#loss-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nWe differentiate: \\[\\begin{align*}\n&\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\left[\\dfrac{\\partial L_i}{\\partial w_1}, \\dots, \\dfrac{\\partial L_i}{\\partial w_d}\\right] = \\\\\n& = \\begin{cases}\n  -y_i \\overline{X_i}, & \\text{if } sign\\{W^T X_i\\} \\neq y_i,\\\\\n  0, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#perceptron-update",
    "href": "dl_lec2.html#perceptron-update",
    "title": "Deep learning: logistic regression",
    "section": "Perceptron update",
    "text": "Perceptron update\nNegative of the vector is the direction of the fastest rate of loss reduction, hence perceptron update: \\[\\begin{align*}\n   &\\overline{W} := \\overline{W} - \\alpha\\dfrac{\\partial L_i}{\\partial \\overline{W}} = \\overline{W} + \\alpha y_i \\overline{X_i}^T.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions",
    "href": "dl_lec2.html#activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nA network with weights \\(\\overline{W}\\) and input \\(\\overline{X}\\) will have a prediction of the form \\[\\begin{align*}\n   &\\hat{y}=\\Phi\\left( \\overline{W}^T \\overline{X}\\right)\n\\end{align*}\\] where \\(\\Phi\\) denotes activation function."
  },
  {
    "objectID": "dl_lec2.html#activation-functions-1",
    "href": "dl_lec2.html#activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nIdentity aka linear activation\n\n\n\\[\n\\Phi(v) = v\n\\]\n\n\n\n\n\n\nSign function\n\n\n\\[\n\\Phi(v) = sign(v)\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-2",
    "href": "dl_lec2.html#activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\n\n\n\nSigmoid function\n\n\n\\[\n\\Phi(v) = \\dfrac{1}{1+e^{-v}}\n\\]\n\n\n\n\n\n\ntanh\n\n\n\\[\n\\Phi(v) = \\dfrac{e^{2v}-1}{e^{2v}+1}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-3",
    "href": "dl_lec2.html#activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nActually, neuron computes two functions:"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-4",
    "href": "dl_lec2.html#activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nWe have pre-activation value and post-activation value.\n\npre-activation: linear transformation\npost-activation: nonlinear transformation"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-5",
    "href": "dl_lec2.html#activation-functions-5",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-6",
    "href": "dl_lec2.html#activation-functions-6",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-7",
    "href": "dl_lec2.html#activation-functions-7",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nTwo more functions that have become popular recently:\n\n\n\nRectified Linear Unit (ReLU)\n\n\n\\[\n\\Phi(v) = \\max\\left\\{v, 0\\right\\}\n\\]\n\n\n\n\n\n\nHard tanh\n\n\n\\[\n\\Phi(v) = \\max\\left\\{\\min\\left[v, 1\\right], -1 \\right\\}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#multiple-activation-fns",
    "href": "dl_lec2.html#multiple-activation-fns",
    "title": "Deep learning: logistic regression",
    "section": "Multiple activation fns",
    "text": "Multiple activation fns"
  },
  {
    "objectID": "dl_lec2.html#activation-functions-8",
    "href": "dl_lec2.html#activation-functions-8",
    "title": "Deep learning: logistic regression",
    "section": "Activation functions",
    "text": "Activation functions\nProperties:\n\nmonotonic\nsaturation at large values\nsquashing"
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function",
    "href": "dl_lec2.html#softmax-activation-function",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function\nUsed for k-way classification problems. Used in the output layer.\n\\[\\begin{align*}\n&\\Phi(v)_i = \\dfrac{\\exp(v_i)}{\\sum\\limits_{i=1}^k \\exp(v_i)}.\n\\end{align*}\\]\nSoftmax layer converts real values to probabilities."
  },
  {
    "objectID": "dl_lec2.html#softmax-activation-function-1",
    "href": "dl_lec2.html#softmax-activation-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Softmax activation function",
    "text": "Softmax activation function"
  },
  {
    "objectID": "dl_lec2.html#loss-functions",
    "href": "dl_lec2.html#loss-functions",
    "title": "Deep learning: logistic regression",
    "section": "Loss functions",
    "text": "Loss functions\n\n\n\nLeast squares regression, numeric targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = (y-\\hat{y})^2\n\\end{align*}\\]\n\n\n\n\n\n\nLogistic regression, binary targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log \\left|y/2 - 1/2 + \\hat{y}\\right|, \\{-1,+1\\}\n\\end{align*}\\]\n\n\n\n\n\n\nMultinomial logistic regression, categorical targets\n\n\n\\[\\begin{align*}\n  &L(\\hat{y}, y) = -\\log (\\hat{y}_r) \\text{ - cross-entropy loss}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-1",
    "href": "dl_lec2.html#multilayer-networks-1",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nSuppose NN contains \\(p_1, \\dots, p_k\\) units in each of its \\(k\\) layers.\nThen column representations of these layers, denoted by \\(\\overline{h}_1, \\dots, \\overline{h}_k\\), have \\(p_1, \\dots, p_k\\) units.\n\nWeights between input layer and first hidden layer: matrix \\(W_1\\), sized \\(p_1 \\times d\\).\nWeights between \\(r\\)-th layer and \\(r+1\\)-th layer: matrix \\(W_r\\) sized \\(p_{r+1}\\times p_r\\)."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-2",
    "href": "dl_lec2.html#multilayer-networks-2",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks"
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-3",
    "href": "dl_lec2.html#multilayer-networks-3",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nTherefore, a \\(d\\)-dimensional input vector \\(\\overline{x}\\) is transformed into the outputs using these equations: \\[\\begin{align*}\n  &\\overline{h}_1 = \\Phi(W_1^T x),\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1}^T \\overline{h}_p), \\forall p \\in \\left\\{1 \\dots k-1 \\right\\} \\\\\n  &\\overline{o} = \\Phi(W_{k+1}^T \\overline{h}_k)\n\\end{align*}\\] Activation functions operate on vectors and are applied element-wise."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-4",
    "href": "dl_lec2.html#multilayer-networks-4",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\n\n\n\nDefinition (Aggarwal)\n\n\nA multilayer network computes a nested composition of parameterized multi-variate functions.\nThe overall function computed from the inputs to the outputs can be controlled very closely by the choice of parameters.\nThe notion of learning refers to the setting of the parameters to make the overall function consistent with observed input-output pairs."
  },
  {
    "objectID": "dl_lec2.html#multilayer-networks-5",
    "href": "dl_lec2.html#multilayer-networks-5",
    "title": "Deep learning: logistic regression",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nInput-output function of NN is difficult to express explicitly. NN can also be called universal function approximators.\n\n\n\nUniversal approximation theorem\n\n\nGiven a family of neural networks, for each function \\(\\displaystyle f\\) from a certain function space, there exists a sequence of neural networks \\(\\phi_1,\\phi_2,\\dots\\) from the family, such that \\(\\phi_{n} \\to f\\) according to some criterion.\n\n\n\nIn other words, the family of neural networks is dense in the function space.\n\n\nK. Hornik, M. Stinchcombe, and H. White. . Neural Networks, 2(5), pp. 359–366, 1989."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions",
    "href": "dl_lec2.html#nonlinear-activation-functions",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nTheorem. A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network.\nProof. Consider a network containing \\(k\\) hidden layers, therefore containing a total of \\((k+1)\\) computational layers (including the output layer).\nThe corresponding \\((k+1)\\) weight matrices between successive layers are denoted by \\(W_1 ...W_{k+1}\\)."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-1",
    "href": "dl_lec2.html#nonlinear-activation-functions-1",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nLet:\n\n\\(\\overline{x}\\) be the \\(d\\)-dimensional column vector corresponding to the input\n\\(\\overline{h_1},\\dots,\\overline{h_k}\\) be the column vectors corresponding to the hidden layers\nand \\(\\overline{o}\\) be the \\(m\\)-dimensional column vector corresponding to the output."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-2",
    "href": "dl_lec2.html#nonlinear-activation-functions-2",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nThen, we have the following recurrence condition for multi-layer networks: \\[\\begin{align*}\n  &\\overline{h_1} = \\Phi(W_1 x) = W_1 x,\\\\\n  &\\overline{h}_{p+1} = \\Phi(W_{p+1} \\overline{h}_p) = W_{p+1}\\overline{h}_p \\;\\; \\forall p \\in \\left\\{1 \\dots k−1\\right\\}, \\\\\n  &\\overline{o} = \\Phi(W_{k+1} \\overline{h}_k) = W_{k+1} \\overline{h}_k.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-3",
    "href": "dl_lec2.html#nonlinear-activation-functions-3",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn all the cases above, the activation function \\(\\Phi(\\cdot)\\) has been set to the identity function. Then, by eliminating the hidden layer variables, we obtain the following: \\[\\begin{align*}\n&\\overline{o} = W_{k+1}W_k \\dots W_1 \\overline{x}\n\\end{align*}\\] Denote \\(W_{xo}=W_{k+1}W_k \\dots W_1\\).\n\n\n\nNote\n\n\nOne can replace the matrix \\(W_{k+1}W_k \\dots W_1\\) with the new \\(d\\times m\\) matrix \\(W_{xo}\\), and learn the coefficients of \\(W_{xo}\\) instead of those of all the matrices \\(W_1, W_2, \\dots W_{k+1}\\), without loss of expressivity."
  },
  {
    "objectID": "dl_lec2.html#nonlinear-activation-functions-4",
    "href": "dl_lec2.html#nonlinear-activation-functions-4",
    "title": "Deep learning: logistic regression",
    "section": "Nonlinear activation functions",
    "text": "Nonlinear activation functions\nIn other words, we have the following: \\[\\begin{align*}\n&\\overline{o} = W_{xo} \\overline{x}\n\\end{align*}\\] However, this condition is exactly identical to that of linear regression with multiple outputs. Therefore, a multilayer neural network with identity activations does not gain over a single-layer network in terms of expressivity.\n\n\n\nLinearity observation\n\n\nThe composition of linear functions is always a linear function. The repeated composition of simple nonlinear functions can be a very complex nonlinear function."
  },
  {
    "objectID": "dl_lec2.html#backpropagation",
    "href": "dl_lec2.html#backpropagation",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nDAG Definition\n\n\nA directed acyclic computational graph is a directed acyclic graph of nodes, where each node contains a variable. Edges might be associated with learnable parameters.\nA variable in a node is either fixed externally (for input nodes with no incoming edges), or it is a computed as a function of the variables in the tail ends of edges incoming into the node and the learnable parameters on the incoming edges.\n\n\n\nDAG is a more general version of NN."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-1",
    "href": "dl_lec2.html#backpropagation-1",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\nA computational graph evaluates compositions of functions.\nA path of length 2 in a computational graph in which the function \\(f(\\cdot)\\) follows \\(g(\\cdot)\\) can be considered a composition function \\(f(g(\\cdot))\\).\nIn case of sigmoid function: \\[\\begin{align*}\n   &f(x) = g(x) = \\dfrac{1}{1+e^{-x}} \\\\\n   &f(g(x)) = \\dfrac{1}{1 + e^{\\left[-\\dfrac{1}{1+e^{-x}}\\right]}}\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#backpropagation-2",
    "href": "dl_lec2.html#backpropagation-2",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nImportant\n\n\nThe inability to easily express the optimization function in closed form in terms of the edge-specific parameters (as is common in all machine learning problems) causes difficulties in computing the derivatives needed for gradient descent.\n\n\n\n\n\n\nExample\n\n\nFor example, if we have a computational graph which has 10 layers, and 2 nodes per layer, the overall composition function would have \\(2^{10}\\) nested “terms”."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-3",
    "href": "dl_lec2.html#backpropagation-3",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nDerivatives of the output with respect to various variables in the computational graph are related to one another with the use of the chain rule of differential calculus.\nTherefore, the chain rule of differential calculus needs to be applied repeatedly to update derivatives of the output with respect to the variables in the computational graph.\nThis approach is referred to as the backpropagation algorithm, because the derivatives of the output with respect to the variables close to the output are simpler to compute (and are therefore computed first while propagating them backwards towards the inputs).\n\nDerivatives are computed numerically, not algebraically."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-4",
    "href": "dl_lec2.html#backpropagation-4",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nForward phase\n\n\n\nUse the attribute values from the input portion of a training data point to fix the values in the input nodes.\nSelect a node for which the values in all incoming nodes have already been computed and apply the node-specific function to also compute its variable.\nRepeat the process until the values in all nodes (including the output nodes) have been computed.\nCompute loss value if the computed and observed values mismatch."
  },
  {
    "objectID": "dl_lec2.html#backpropagation-5",
    "href": "dl_lec2.html#backpropagation-5",
    "title": "Deep learning: logistic regression",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\nBackward phase\n\n\n\nCompute the gradient of the loss with respect to the weights on the edges.\nDerivatives of the loss with respect to weights near the output (where the loss function is computed) are easier to compute and are computed first.\nThe derivatives become increasingly complex as we move towards edge weights away from the output (in the backwards direction) and the chain rule is used repeatedly to compute them.\nUpdate the weights in the negative direction of the gradient.\n\n\n\n\nSingle cycle through all training points is an epoch."
  },
  {
    "objectID": "dl_lec2.html#inputs",
    "href": "dl_lec2.html#inputs",
    "title": "Deep learning: logistic regression",
    "section": "Inputs",
    "text": "Inputs\nLogistic regression is an algorithm for binary classification.\n\\(x \\in \\mathbb{R}^{n_x}, y \\in \\{0,1\\}\\).\n\\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\)- \\(m\\) training examples.\n\n\\(X\\) matrix - \\(m\\) columns and \\(n_x\\) rows.\n\\[\\begin{align*}\n&X = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\end{align*}\\]\n\\[\nY = \\left[y^{(1)}, y^{(2)}, \\dots, y^{(m})\\right]\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression",
    "href": "dl_lec2.html#logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\(X \\in \\mathbb{R}^{n_x,m}\\).\nUsing Numpy syntax:\nX.shape = (n_x,m).\n\\(Y \\in \\mathbb{R}^{1,m}\\).\nUsing Numpy syntax:\nY.shape = (1,m)."
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-1",
    "href": "dl_lec2.html#logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nGoal\n\n\nWe will strive to maximize \\(\\hat{y} = P(y=1 | x)\\), where \\(x \\in \\mathbb{R}^{n_x}\\).\nObviously, \\(0 \\leq \\hat{y} \\leq 1\\).\n\n\n\n\n\n\nImportant\n\n\nIf doing linear regresssion, we can try \\[\n\\hat{y}=w^T x + b.\n\\]\nBut for logistic regression, we do \\[\n\\hat{y}=\\sigma(w^T x + b)$, \\; \\text{where }\\; \\sigma=\\dfrac{1}{1+e^{-z}}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#parameters",
    "href": "dl_lec2.html#parameters",
    "title": "Deep learning: logistic regression",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nInput\n\n\n\\[\nw \\in \\mathbb{R}^{n_x},\\\\\nb \\in \\mathbb{R}.\n\\]\n\n\n\n\n\n\nOutput\n\n\n\\[\n\\hat{y} = \\sigma\\left( w^T x + b\\right),\n\\]\n\\[\nz \\equiv  w^T x + b.\n\\]\n\n\n\n\\(w\\) - weights, \\(b\\) - bias term (intercept)"
  },
  {
    "objectID": "dl_lec2.html#graphs",
    "href": "dl_lec2.html#graphs",
    "title": "Deep learning: logistic regression",
    "section": "Graphs",
    "text": "Graphs\n\n\\(\\sigma=\\dfrac{1}{1+e^{-z}}\\)."
  },
  {
    "objectID": "dl_lec2.html#loss-function-2",
    "href": "dl_lec2.html#loss-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\nFor every \\(\\left\\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\\right\\}\\), we want to find \\(\\hat{y}^{(i)} \\approx y^{(i)}\\). \\[\\begin{align*}\n  &\\hat{y}^{(i)} = \\sigma\\left(w^T x^{(i)} + b\\right)\n\\end{align*}\\] We have to define a loss (error) function - this will estimate our model."
  },
  {
    "objectID": "dl_lec2.html#loss-function-3",
    "href": "dl_lec2.html#loss-function-3",
    "title": "Deep learning: logistic regression",
    "section": "Loss function",
    "text": "Loss function\n\n\n\nQuadratic\n\n\n\\[\nL(\\hat{y}, y) = \\dfrac{1}{2}\\left(\\hat{y}-y)\\right)^2.\n\\]\n\n\n\n\n\n\nLog\n\n\n\\[\nL(\\hat{y}, y) = -\\left((y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}))\\right).\n\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function",
    "href": "dl_lec2.html#cost-function",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\n\n\n\nWhy does it work well?\n\n\nConsider \\(y=0\\) and \\(y=1\\).\n\\[\\begin{align*}\n  &y=1: P(y | x) = \\hat{y},\\\\\n  &y=0: P(y | x) = 1-\\hat{y}\n\\end{align*}\\]\nWe select \\(P(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}\\).\n\\[\n\\log P(y|x) = y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) = -L(\\hat{y}, y).\n\\]\n\\[\\begin{align*}\n  y=1:& L(\\hat{y},y) = -\\log(\\hat{y}),\\\\\n  y=0:& L(\\hat{y},y) = -\\log(1-\\hat{y})\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-1",
    "href": "dl_lec2.html#cost-function-1",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nCost function show how well we’re doing across the whole training set: \\[\\begin{align*}\n&J(w, b) = \\dfrac{1}{m} \\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) = \\\\\n& = -\\dfrac{1}{m} \\sum\\limits_{i=1}^m \\left[y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#cost-function-2",
    "href": "dl_lec2.html#cost-function-2",
    "title": "Deep learning: logistic regression",
    "section": "Cost function",
    "text": "Cost function\nOn \\(m\\) examples: \\[\\begin{align*}\n  &\\log P(m \\dots) = \\log \\prod_{i=1}^m P(y^{(i)} | x^{(i)}) = \\\\\n  & = \\sum\\limits_{i=1}^m \\log P(y^{(i)} | x^{(i)}) = -\\sum\\limits_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent",
    "href": "dl_lec2.html#gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\nProblem\n\n\nMinimization problem: find \\(w,b\\) that minimize \\(J(w,b)\\)."
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-1",
    "href": "dl_lec2.html#gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nWe use \\(J(w,b)\\) because it is convex.\nWe pick an initial point - anything might do, e.g. 0.\nThen we take steps in the direction of steepest descent.\n\n\\[\nw := w - \\alpha \\frac{d J(w,b)}{dw}, \\\\\nb := b - \\alpha \\frac{d J(w,b)}{db}\n\\]\n\\(\\alpha\\) - learning rate"
  },
  {
    "objectID": "dl_lec2.html#gradient-descent-2",
    "href": "dl_lec2.html#gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nforward pass: compute output\nbackward pass: compute derivatives"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent",
    "href": "dl_lec2.html#logistic-regression-gradient-descent",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\n\\[\\begin{align*}\n&z = w^T x + b ,\\\\\n&a \\equiv \\hat{y}  = \\sigma(z),\\\\\n&L(a,y) = -\\left[y\\log(a) + (1 - y)\\log(1 - a)\\right].\n\\end{align*}\\]\nSo, for \\(n_x=2\\) we have a computation graph:\n\\((x_1,x_2,w_1,w_2,b)\\) \\(\\rightarrow\\) \\(z =w_1 x_1+w_2 x_2 + b\\) \\(\\rightarrow\\) \\(\\hat{y}=a=\\sigma(z)\\) \\(\\rightarrow\\) \\(L(a,y)\\)."
  },
  {
    "objectID": "dl_lec2.html#computation-graph-1",
    "href": "dl_lec2.html#computation-graph-1",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nLet’s compute the derivative for \\(L\\) by a: \\[\\begin{align*}\n&\\frac{dL}{da} = -\\dfrac{y}{a} + \\dfrac{1-y}{1-a},\\\\\n&\\frac{da}{dz} = a(1-a).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#computation-graph-2",
    "href": "dl_lec2.html#computation-graph-2",
    "title": "Deep learning: logistic regression",
    "section": "Computation graph",
    "text": "Computation graph\nAfter computing, we’ll have \\[\\begin{align*}\n&dz \\equiv \\dfrac{dL}{dz} = \\dfrac{dL}{da}\\dfrac{da}{dz} = a-y,\\\\\n&dw_1 \\equiv \\frac{dL}{dw_1} = x_1 dz,\\\\\n&dw_2 \\equiv \\frac{dL}{dw_2} = x_2 dz, \\\\\n&db \\equiv \\frac{dL}{db} = dz.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-1",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nGD steps are computed via \\[\\begin{align*}\n&w_1 := w_1 - \\alpha \\frac{dL}{dw_1},\\\\\n&w_2 := w_2 - \\alpha \\frac{dL}{dw_2},\\\\\n&b := b - \\alpha \\frac{dL}{db}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-2",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nConsider now \\(m\\) examples in the training set.\nLet’s recall the definition of the cost function: \\[\\begin{align*}\n&J(w,b) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\\\\n&a^{(i)} = \\hat{y}^{(i)}=\\sigma(w^T x^{(i)} + b).\n\\end{align*}\\] And also \\[\n\\frac{dJ}{dw_1} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{dL(a^{(i)}, y^{(i)})}{dw_1}.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-3",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nLet’s implement the algorithm. First, initialize \\[\nJ=0,\\\\\ndw_1=0,\\\\\ndw_2=0,\\\\\ndb=0\n\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-4",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nfor i=1 to m \\[\\begin{align*}\n  &z^{(i)} = w^T x^{(i)} + b, \\\\\n  &a^{(i)} = \\sigma(z^{(i)}), \\\\\n  &J += -\\left[y^{(i)} \\log a^{(i)} + (1-y^{(i)}) \\log(1-a^{(i)})\\right], \\\\\n  &dz^{(i)} = a^{(i)} - y^{(i)}, \\\\\n  &dw_1 += x_1^{(i)} dz^{(i)},\\\\\n  &dw_2 += x_2^{(i)} dz^{(i)},\\\\\n  &db += dz^{(i)}.\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "href": "dl_lec2.html#logistic-regression-gradient-descent-5",
    "title": "Deep learning: logistic regression",
    "section": "Logistic Regression Gradient Descent",
    "text": "Logistic Regression Gradient Descent\nThen compute averages:\n\\[\nJ = \\dfrac{J}{m}, \\\\\ndw_1 = \\dfrac{dw_1}{m}, \\; dw_2 = \\dfrac{dw_2}{m}, \\\\\ndb = \\dfrac{db}{m}.\n\\]\n\n\nNote that \\(dw_i\\) don’t have a superscript - we use them as accumulators. (In this example feature count \\(n_x=2\\))"
  },
  {
    "objectID": "dl_lec2.html#gd-step",
    "href": "dl_lec2.html#gd-step",
    "title": "Deep learning: logistic regression",
    "section": "GD step",
    "text": "GD step\n\\[\nw_1 := w_1 - \\alpha dw_1,\\\\\nw_2 := w_2 - \\alpha dw_2,\\\\\nb := b - \\alpha db.\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorization",
    "href": "dl_lec2.html#vectorization",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nWe only have 2 features \\(w_1\\) and \\(w_2\\), so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance.\nVectorization techniques exist for this purpose - getting rid of for loops.\n\n\n\nExample\n\n\nWe have to compute \\(z=w^T x + b\\), where \\(w,x \\in \\mathbb{R}^{n_x}\\), and for this we can naturally use a for loop.\nA vectorized Python command is\nz = np.dot(w,x)+b"
  },
  {
    "objectID": "dl_lec2.html#vectorization-1",
    "href": "dl_lec2.html#vectorization-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nProgramming guideline - avoid explicit for loops. \\[\\begin{align*}\n  &u = Av,\\\\\n  &u_i = \\sum_j\\limits A_{ij} v_j\n\\end{align*}\\] To be replaced by\nu = np.dot(A, v)\n\n\nNumpy impl: https://numpy.org/doc/1.21/reference/simd/simd-optimizations.html"
  },
  {
    "objectID": "dl_lec2.html#vectorization-2",
    "href": "dl_lec2.html#vectorization-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorization",
    "text": "Vectorization\nAnother example. Let’s say we have a vector \\[\\begin{align*}\n    &v = \\begin{bmatrix}\n      v_1 \\\\\n      \\vdots \\\\\n      v_n\n    \\end{bmatrix},\n    u = \\begin{bmatrix}\n      e^{v_1},\\\\\n      \\vdots \\\\\n      e^{v_n}\n    \\end{bmatrix}\n\\end{align*}\\] A code listing is\nimport numpy as np\nu = np.exp(v)\nSo we can modify the above code to get rid of for loops (except for the one for \\(m\\))."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression",
    "href": "dl_lec2.html#vectorizing-logistic-regression",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s examine the forward propagation step of LR. \\[\\begin{align*}\n  &z^{(1)} = w^T x^{(1)} + b,\\\\\n  &a^{(1)} = \\sigma(z^{(1)}),\n\\end{align*}\\]\n\\[\\begin{align*}\n  &z^{(2)} = w^T x^{(2)} + b,\\\\\n  &a^{(2)} = \\sigma(z^{(2)}).\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-1",
    "href": "dl_lec2.html#vectorizing-logistic-regression-1",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nLet’s recall what have we defined as our learning matrix: \\[\nX = \\begin{bmatrix}\n  \\vdots & \\vdots & \\dots & \\vdots \\\\\n  x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n  \\vdots & \\vdots & \\dots & \\vdots\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-2",
    "href": "dl_lec2.html#vectorizing-logistic-regression-2",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNext \\[\nZ = [z^{(1)}, \\dots, z^{(m)}] = w^T X + [b, b, \\dots, b] =\\\\\n= [w^T x^{(1)}+b, \\dots, w^T x^{(m)}+b].\n\\]\n\\[\nA = \\left[a^{(1)}, \\dots, a^{(m)}\\right] = \\sigma\\left(Z\\right)\n\\]\n  z = np.dot(w.T, x) + b\n\n\n\\(b\\) is a raw number, Python will automatically take care of expanding it into a vector - this is called broadcasting."
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-3",
    "href": "dl_lec2.html#vectorizing-logistic-regression-3",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nEarlier on, we computed \\[\\begin{align*}\n&dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \\dots\n\\end{align*}\\]\nWe now define \\[\\begin{align*}\n&Y = [y^{(1)}, \\dots, y^{(m)}],\\\\\n&dZ = [dz^{(1)}, \\dots, dz^{(m)}] =\\\\\n&= A-Y = [a^{(1)}-y^{(1)}, \\dots, a^{(m)}-y^{(m)}]\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-4",
    "href": "dl_lec2.html#vectorizing-logistic-regression-4",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nFor \\(db\\) we have \\[\\begin{align*}\n&db = \\frac{1}{m}np.sum(dZ),\\\\\n&dw = \\frac{1}{m}X dZ^T = \\\\\n& \\frac{1}{m}\\begin{bmatrix}\n  \\vdots & & \\vdots \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  \\vdots & & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  dz^{(1)} \\\\\n  \\vdots\\\\\n  dz^{(m)}\n\\end{bmatrix} = \\\\\n& = \\frac{1}{m}\\left[x^{(1)}dz^{(1)} + \\dots +x^{(m)}dz^{(m)}\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "dl_lec2.html#vectorizing-logistic-regression-5",
    "href": "dl_lec2.html#vectorizing-logistic-regression-5",
    "title": "Deep learning: logistic regression",
    "section": "Vectorizing logistic regression",
    "text": "Vectorizing logistic regression\nNow we can go back to the backward propagation algorithm again.\nMultiple iterations of GD will still require a for loop.\nfor it in range(m):\n  Z = np.dot(w.T, X) + B\n  A = sigma(Z)\n  dZ = A-Y\n  dw = 1/m X * dZ.T\n  db = 1/m np.sum(dZ)\n  w := w - alpha * dw\n  b := b - alpha * db"
  }
]