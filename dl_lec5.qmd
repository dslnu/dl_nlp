---
title: "Intro to PyTorch"
author: 
  - name: Vitaly Vlasov
    affiliation: Lviv University
code-fold: false
execute:
  enabled: true
  echo: true
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usepackage{neuralnetwork}
        \usepackage{mathtools}
        \usepackage{amsmath}
        \pgfplotsset{compat=1.16}
        \usepackage{pgfplots}
        \newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{shapes.misc}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    include-in-header: mathjax.html
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '9eea8372bd1d5b26e533c9f62ac3ded8'
      id: 'dfeb146abfa7566d06e3260dc51741a028c92925d9a164cf153da95cae43b4f3'
---
## Imports

```{python}
## Standard libraries
import os
import math
import numpy as np
import time

## Imports for plotting
import matplotlib.pyplot as plt
%matplotlib inline
from IPython.display import set_matplotlib_formats
set_matplotlib_formats('svg', 'pdf') # For export
from matplotlib.colors import to_rgba
import seaborn as sns
sns.set()

## Progress bar
from tqdm.notebook import tqdm
```

## Basic PyTorch import

```{python}
import torch
print("Using torch", torch.__version__)
```

## Set the seed

```{python}
torch.manual_seed(42) # Setting the seed
```

## Tensors

Tensors

Tensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration (more on that later). The name "tensor" is a generalization of concepts you already know. For instance, a vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we will use tensors of various shapes and number of dimensions.

Most common functions you know from numpy can be used on tensors as well. Actually, since numpy arrays are so similar to tensors, we can convert most tensors to numpy arrays (and back) but we don't need it too often.

## Tensors

```{python}
x = torch.Tensor(2, 3, 4)
print(x)
```

## Tensors
The function `torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory. To directly assign values to the tensor during initialization, there are many alternatives including:

* `torch.zeros`: Creates a tensor filled with zeros
* `torch.ones`: Creates a tensor filled with ones
* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1
* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1
* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$
* `torch.Tensor` (input list): Creates a tensor from the list elements you provide



## Tensors
```{python}
# Create a tensor from a (nested) list
x = torch.Tensor([[1, 2], [3, 4]])
print(x)
```

## Tensors
```{python}
# Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]
x = torch.rand(2, 3, 4)
print(x)
```
## Tensors
:::{.callout-tip icon=false}
## Tensor shape
```{python}
shape = x.shape
print("Shape:", x.shape)

size = x.size()
print("Size:", size)

dim1, dim2, dim3 = x.size()
print("Size:", dim1, dim2, dim3)
```
:::

## Tensors

:::{.callout-tip icon=false}
## To Numpy and back again

```{python}
np_arr = np.array([[1, 2], [3, 4]])
tensor = torch.from_numpy(np_arr)

print("Numpy array:", np_arr)
print("PyTorch tensor:", tensor)
```
To transform a PyTorch tensor back to a numpy array, we can use the function .numpy() on tensors:

```{python}
tensor = torch.arange(4)
np_arr = tensor.numpy()

print("PyTorch tensor:", tensor)
print("Numpy array:", np_arr)
```
:::

:::{.callout-note}
The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU (more on GPU support in a later section). In case you have a tensor on GPU, you need to call `.cpu()` on the tensor beforehand. Hence, you get a line like `np_arr = tensor.cpu().numpy()`.
:::

## Operations
Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#), but we will review the most important ones here.

:::{.callout-tip icon=false}
## Addition

```{python}
x1 = torch.rand(2, 3)
x2 = torch.rand(2, 3)
y = x1 + x2

print("X1", x1)
print("X2", x2)
print("Y", y)
```
:::

:::{.callout-important}
Calling x1 + x2 creates a **new** tensor containing the sum of the two inputs.
:::

## Operations
:::{.callout-note icon=false}
## In-place

We can also use in-place operations that are applied directly on the memory of a tensor. We therefore change the values of x2 without the chance to re-accessing the values of x2 before the operation.

```{python}
x1 = torch.rand(2, 3)
x2 = torch.rand(2, 3)
print("X1 (before)", x1)
print("X2 (before)", x2)

x2.add_(x1)
print("X1 (after)", x1)
print("X2 (after)", x2)
```
:::

::: aside
In-place operations are usually marked with a underscore postfix (e.g. "add_" instead of "add").
:::

## Operations
:::{.callout-tip icon=false}
## Shape change
Another common operation aims at changing the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g. a tensor of size (6), or (3,2), ...). In PyTorch, this operation is called `view`:
```{python}
x = torch.arange(6)
print("X", x)
```
```{python}
x = x.view(2, 3)
print("X", x)
```
You can also swap dimensions:
```{python}
x = x.permute(1, 0) # Swapping dimension 0 and 1
print("X", x)
```
:::

## Operations
:::{.callout-tip icon=false}
## Multiplication
Quite often, we have an input vector $\mathbf{x}$, which is transformed using a learned weight matrix $\mathbf{W}$. There are multiple ways and functions to perform matrix multiplication:

* `torch.matmul`: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Can also be written as `a @ b`, similar to numpy. 
* `torch.mm`: Performs the matrix product over two matrices, but doesn't support broadcasting (see [documentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))
:::

## Operations
:::{.callout-tip icon=false}
## Multiplication
* `torch.bmm`: Performs the matrix product with a support batch dimension. If the first tensor $T$ is of shape ($b\times n\times m$), and the second tensor $R$ ($b\times m\times p$), the output $O$ is of shape ($b\times n\times p$), and has been calculated by performing $b$ matrix multiplications of the submatrices of $T$ and $R$: $O_i = T_i @ R_i$
* `torch.einsum`: Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention. Explanation of the Einstein sum can be found in assignment 1.
:::

:::{.callout-note}
Usually, we use `torch.matmul` or `torch.bmm`.
:::

## Operations
:::{.callout-tip icon=false}
## Multiplication
We can try a matrix multiplication with `torch.matmul`:

```{python}
x = torch.arange(6)
x = x.view(2, 3)
print("X", x)
```
```{python}
W = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line
print("W", W)
```
```{python}
h = torch.matmul(x, W) # Verify the result by calculating it by hand too!
print("h", h)
```
:::
