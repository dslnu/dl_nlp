---
title: "Nearest-neighbor search"
author: 
  - name: MSDE
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \pgfplotsset{compat=1.16}
        \usepackage{pgfplots}
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    chalkboard: true
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '19f286c43d34d478f6aaac04c3d1de92'
      id: '2e8cbc3acc2ca7d0b4a4e016fa8804a63f3512b3e00e7872956831f754b3cab5'
---

## ANN

::: {.hidden}
\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\bi}[1]{\textbf{\textit{{#1}}}}
:::

:::{.callout-tip icon=false}
## LSH
Locality sensitive hashing (LSH) is a widely popular technique used in approximate nearest neighbor (**ANN**) search. 

**LSH** is one of the original techniques for producing high quality search, while maintaining lightning fast search speeds. 
:::

:::{.callout-tip icon=false}
## ANN Users

- Google (query-vs-index)
- Amazon (product recommendations)
- Spotify (music recommendations)
- Netflix (movie recommendations)
:::


## Spotify example
:::{.callout-tip icon=false}
Spotify uses **Voyager**.

- modified version of open-source hnswlib 
:::
![](img/spotify_voyager.png)

## Guide to electronic music
![https://music.ishkur.com](img/music_guide.png)

::: aside

:::

## Guide to electronic music
<iframe width="1920" height="1080" src="https://music.ishkur.com" title="Guide"></iframe>

## Search complexity

:::{.callout-warning icon=false}
## Pairwise comparison
Complexity is $O(n^2)$. If a single query against all samples - $O(n)$.

Plus, we compare vectors - sometimes **highly-dimensional** ones.
:::

## Approximate search
:::{.callout-important}
## Problem
Is it even possible to perform a search with sub-linear complexity?
:::

:::{.callout-tip icon=false}
## Solution
The solution is **approximate search**. 

Rather than comparing every vector (**exhaustive search**) — we can approximate and limit our search scope to only the most relevant vectors.
:::

<!--LSH is one algorithm that provides us with those sub-linear search times.-->

## Locality Sensitive Hashing
:::{.callout-tip icon=false}
## Vector comparison complexity
Attempting to find a closest match is linear $O(n)$. 

Pairwise comparison - at best log-linear $O(n \log n)$.
:::

:::{.callout-note}
## Considerations

- we need to reduce the number of comparisons
- we want only to compare vectors that we believe to be potential matches — or candidate pairs
:::

## Locality Sensitive Hashing
:::{.callout-tip icon=false}
## LSH

- several approaches
- we'll consider traditional one first: shingling + MinHashing + banded function
:::

## Locality Sensitive Hashing
:::{.callout-tip icon=false}
## Basic outline

- segment and hash same sample several times
- when we find that a pair of vectors has been hashed to the same value at least once, we tag them as \textit{candidate pairs} — that is, potential matches.
:::
      
:::{.callout-tip icon=false}
## Python dictionaries

- We have a key-value pair which we feed into the dictionary.      
- The key is processed through the dictionary hash function and mapped to a specific bucket. 
- We then connect the respective value to this bucket.
:::

## Locality Sensitive Hashing
![A typical hash function aims to place different values (no matter how similar) into separate buckets.](img/hash.png)

## Locality Sensitive Hashing
:::{.callout-tip icon=false}
## Hash properties

- **determinism**
- **uniformity**
- **non-reversibility**
- **fixed-size output**
- **sensitivity to input changes**
- **collision resistance**
:::

## Locality Sensitive Hashing
:::{.callout-tip icon=false}
## Difference in goals

- Dictionaries: **minimize** collisions.
- LSH: **maximize** collisions (ideally only for similar inputs)
:::

## Locality Sensitive Hashing
![An LSH function aims to place similar values into the same buckets.](img/lsh_hash.png)

::: aside
There is no single approach to hashing in LSH. Indeed, they all share the same ‘bucket similar samples through a hash function’ logic , but they can vary a lot beyond this.
:::
<!--The method we have briefly described and will be covering throughout the remainder of this article could be described as the traditional approach, using shingling, MinHashing, and banding.-->

## Shingling, MinHashing, and LSH
:::{.callout-tip icon=false}
## LSH Steps

- convert text to sparse vectors using *k-shingling* (and *one-hot encoding*)
- use *minhashing* to create `signatures` 
- pass signatures onto LSH process to weed out *candidate pairs*
:::

## Shingling, MinHashing, and LSH
![](img/lsh_process.png)

## k-Shingling
:::{.callout-tip icon=false}
## Definition
*k-Shingling*, or simply *shingling*, is the process of converting a string of text into a set of `shingles`. The process is similar to moving a window of length $k$ down our string of text and taking a picture at each step. We collate all of those pictures to create our **set of shingles**.
:::
![](img/shingling.png)

## k-Shingling
:::{.callout-note}
Shingling removes duplicate items!
:::

:::{.callout-tip icon=false}
## Python example
```python
def shingle(text: str, k: int):
  shingle_set = []
  for i in range(len(text) - k+1):
    shingle_set.append(text[i:i+k])
  return set(shingle_set)
```
:::

## k-Shingling
:::{.callout-note}
## Sparse vectors
Next, create **sparse vectors**. To do this, we first need to union all of our sets to create one big set containing all of the shingles across all of our sets — we call this the **vocabulary** (or **vocab**).
:::
![](img/lsh_vocab.png)

## k-Shingling
:::{.callout-tip icon=false}
## Sparse vectors
We use this vocab to create our sparse vector representations of each set - a **one-hot encoding**. 

All we do is create an empty vector full of zeros and the same length as our vocab — then, we look at which shingles appear in our set.
:::
![](img/lsh_sparse_vector.png)

<!-- For every shingle that appears, we identify the position of that shingle in our vocab and set the respective position in our new zero-vector to 1. Some of you may recognize this as one-hot encoding. -->

## Minhashing
:::{.callout-tip icon=false}
## Minhashing
Minhashing is the next step in our process, allowing us to convert our *sparse vectors* into *dense vectors*. 

**Minhash functions:**

- randomly generate for every position in the signature (e.g. the dense vector)
- for a dense vector/signature of 20 numbers — we would use 20 minhash functions.
- MinHash functions are a randomized order of numbers — and we count from 1 to the final number (which is `len(vocab)`)
:::

## Minhashing
Our signature values are created by first taking a randomly permuted count vector (from 1 to `len(vocab)+1`) and finding the minimum number that aligns with a 1 in our sparse vector.

::: {layout-ncol=2}

![](img/minhash1.png)

![](img/minhash3.png)

![](img/minhash2.png)

![](img/minhash4.png)
:::

## Minhashing
:::{.callout-tip icon=false}
## Process

- Above, we’re using a smaller vocab containing six values so we can easily visualize the process.
- We look at our sparse vector and say, “did this shingle at vocab[1] exist in our set?”. If it did — the sparse vector value will be 1 — in this case, it did not exist (hence the 0 value). So, we move to number 2, identify its position (0) and ask the same question. This time, the answer is yes, and so our minhash output is 2.
- That’s how we produce one value in our minhash signature. But we need to produce 20 (or more) of these values. So, we assign a different minhash function to each signature position — and repeat the process.
:::

## Minhashing
![Here we use four minhash functions/vectors to create a four-digit signature vector. If you count (from one) in each minhash function, and identify the first value that aligns with a one in the sparse vector — you will get 2412.](img/minhash_signature.png)

## Minhashing in Python
We have three steps:

### 1. Generate a randomized MinHash vector.
:::{.callout-tip icon=false}
## Unshuffled
    
```python
  hash_ex = list(range(1, len(vocab)+1))
  print(hash_ex)  # we haven't shuffled yet

  Out: [1, 2, 3, ..., 101]
```
:::

:::{.callout-note icon=false}
## Shuffled
```python
  from random import shuffle

  shuffle(hash_ex)
  print(hash_ex)

  Out: [63, 7, 94, ..., 56]
```
:::

## Minhashing in Python

### 2. Loop through this randomized MinHash vector (starting at 1).

:::{.callout-tip icon=false}
## Matching
Match the index of each value to the equivalent values in the sparse vector `a_1hot`.

If we find a 1 — that index is our signature value.

```python
  print(f"7 -> {hash_ex.index(7)}")

  Out: 7 -> 1
```
We now have a randomized list of integers which we can use in creating our **hashed** signatures. 
:::

## Minhashing in Python

### 2. Loop through this randomized MinHash vector (starting at 1).
:::{.callout-tip icon=false}
## Count
Count up from 1 to len(vocab) + 1 and find if `hash_ex.index(i)` position in our one-hot encoded vectors contains 1 in that position:
```python
for i in range(1, len(vocab)+1):
  idx = hash_ex.index(i)
  signature_val = a_1hot[idx]
  print(f"{i} -> {idx} -> {signature_val}")
  if signature_val == 1:
      print('match!')
      break

Out:  1 -> 58 -> 0
      2 -> 19 -> 0
      3 -> 96 -> 0
      4 -> 92 -> 0
      5 -> 83 -> 0
      6 -> 98 -> 1
      match!
```
:::

## Minhashing in Python
### 3. Build a signature from multiple iterations of 1 and 2.

:::{.callout-tip icon=false}
## Minhash vectors
```python
def create_hash_func(size: int):
  # function for creating the hash vector/function
  hash_ex = list(range(1, len(vocab)+1))
  shuffle(hash_ex)
  return hash_ex

def build_minhash_func(vocab_size: int, nbits: int):
  # function for building multiple minhash vectors
  hashes = []
  for _ in range(nbits):
      hashes.append(create_hash_func(vocab_size))
  return hashes

# we create 20 minhash vectors
minhash_func = build_minhash_func(len(vocab), 20)
```
:::

## Minhashing in Python
:::{.callout-tip icon=false}
## Create hash
```python
def create_hash(vector: list):
  # use this function for creating our signatures (eg the matching)
  signature = []
  for func in minhash_func:
      for i in range(1, len(vocab)+1):
          idx = func.index(i)
          signature_val = vector[idx]
          if signature_val == 1:
              signature.append(idx)
              break
  return signature
```
:::

## Minhashing in Python
:::{.callout-tip icon=false}
## Signatures creation
```python
# now create signatures
a_sig = create_hash(a_1hot)
b_sig = create_hash(b_1hot)
c_sig = create_hash(c_1hot)

print(a_sig)
print(b_sig)
  
Out: 
[44, 21, 73, 14, 2, 13, 62, 70, 17, 5, 12, 86, 21, 18, 10, 10, 86, 47, 17, 78]
[97, 96, 57, 82, 43, 67, 75, 24, 49, 28, 67, 56, 96, 18, 11, 85, 86, 19, 65, 75]
```
:::

## Minhashing
:::{.callout-tip icon=false}
## Outcome
We’ve taken a sparse vector and compressed it into a more densely packed, 20-number signature.
:::

## Information Transfer from Sparse to Signature
:::{.callout-important icon=false}
## Question
Is the information truly maintained between our much larger sparse vector and much smaller dense vector? 
:::

:::{.callout-tip icon=false}
## Answer
We use **Jaccard similarity** to calculate the similarity between our sentences in shingle format — then repeat for the same vectors in signature format.
:::

## Information Transfer from Sparse to Signature
:::{.callout-tip icon=false}
## Jaccard fn
```python
def jaccard(a: set, b: set):
  return len(a.intersection(b)) / len(a.union(b))
```

```python
# Jaccard a and b
jaccard(a, b), jaccard(set(a_sig), set(b_sig))

(0.14814814814814814, 0.10344827586206896)

# Jaccard b and c
jaccard(b, c), jaccard(set(b_sig), set(c_sig))

(0.45652173913043476, 0.34615384615384615)
```
:::

## Band and Hash
:::{.callout-tip icon=false}
## Banding approach to LSH
The final step in identifying similar sentences is the LSH function itself.

- signatures
- hashing segments of each signature
- looking for hash collisions
:::

## Band and Hash
![A high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our vocab, and process it through our minhashing process.](img/band_and_hash.png)

## Band and Hash
:::{.callout-important icon=false}
## Problem
Now, if we were to hash each of these vectors as a whole, we may struggle to build a hashing function that accurately identifies similarity between them — we don’t require that the full vector is equal, only that parts of it are similar.

In most cases, even though parts of two vectors may match perfectly — if the remainder of the vectors are not equal, the hashing function will likely put them into separate buckets.
:::

:::{.callout-tip icon=false}
## Solution
We want signatures that share even some similarity to be hashed into the same bucket, thus being identified as candidate pairs.
:::

## Band and Hash
:::{.callout-tip icon=false}
## How it Works

- The banding method solves this problem by splitting our vectors into sub-parts called \textit{bands} $b$ 
- rather than processing the full vector through our hash function, we pass each band of our vector through a hash function

Imagine we split a 100-dimensionality vector into 20 bands. That gives us 20 opportunities to identify matching sub-vectors between our vectors.
:::

## Band and Hash
![We split our signature into b sub-vectors, each is processed through a hash function (we can use a single hash function, or b hash functions) and mapped to a hash bucket.](img/banding_how.png)

## Band and Hash
:::{.callout-tip icon=false}
## Banding

- If there is a collision between any two sub-vectors, we consider the respective full vectors as candidate pairs.
- We split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all).
:::
![](img/banding_how2.png)

## Band and Hash
:::{.callout-important}
## Problem

- Only part of the two vectors must match for us to consider them. 
- This increases the number of false positives (samples that we mark as candidate matches where they are not similar). 
- However, we try to minimize these as far as possible.
:::

## Band and Hash in Python
:::{.callout-tip icon=false}
## Split fn
```python
def split_vector(signature, b):
  assert len(signature) % b == 0
  r = int(len(signature) / b)
  # code splitting signature in b parts
  subvecs = []
  for i in range(0, len(signature), r):
      subvecs.append(signature[i : i+r])
  return subvecs 
```
:::

## Band and Hash in Python
:::{.callout-tip icon=false}
## Output
```python
band_a = split_vector(a_sig, 10)
band_b = split_vector(b_sig, 10)
band_c = split_vector(c_sig, 10)

Out: [[42, 43],
     [69, 55],
     [29, 96],
     [86, 46],
     [92, 5],
     [72, 65],
     [29, 5],
     [53, 33],
     [40, 94],
     [96, 70]]
```
:::

## Band and Hash in Python
:::{.callout-tip icon=false}
## Looping
Then we loop through the lists to identify any matches between sub-vectors. If we find any matches — we take those vectors as candidate pairs.
      
```python
for b_rows, c_rows in zip(band_b, band_c):
  if b_rows == c_rows:
      print(f"Candidate pair: {b_rows} == {c_rows}")
      # we only need one band to match
      break

Out: Candidate pair: [69, 55] == [69, 55]
```
We find that our two more similar sentences, **b**, and **c** are identified as candidate pairs. 

The less similar of the trio, a — is not identified as a candidate. This is a good result, but if we want to really test LSH, we will need to work with more data.
:::

# Other kinds of ANN

## Tradeoffs
:::{.callout-tip icon=false}
## Tradeoffs

- speed
- memory
- quality
:::

## Flat search
![With flat indexes, we compare our search query $\bb{xq}$ to every other vector in the index.](img/ann_flat_index.png)

## Flat search
:::{.callout-tip icon=false}
## When?

- Search quality is a very high priority.
- Search time does not matter OR when using a small index (<10K).
:::

## Flat search
![Euclidean (L2) and Inner Product (IP) flat index search times using faiss-cpu on an M1 chip. Both using vector dimensionality of 100.](img/ann_flat_search_time.png)

## Flat search
![](img/ann_flat_index_balance.png)

## Speed improvements
:::{.callout-tip icon=false}
## How can we search faster?

- ***Reduce vector size*** - through dimensionality reduction or reducing the number of bits representing our vectors values.
- ***Reduce search scope*** - we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance - and restricting our search to closest clusters or filter through most similar branches.
:::

## Speed improvements
![Using either of these approaches means that we are no longer performing an exhaustive nearest-neighbors search but an approximate nearest-neighbors (ANN) search — as we no longer search the entire, full-resolution dataset.](img/ann_balance.png.png)

## LSH speed
![$nbits \equiv $ `resolution` of the hashed vectors. A higher value means greater accuracy at the cost of more memory and slower search speeds.](img/lsh_nbits.png)

## LSH speed
![So our stored vectors become increasingly larger as our original vector dimensionality d increases. This quickly leads to excessive search times.](img/lsh_search_time.png)

## LSH speed
![Which is mirrored by our index memory size:](img/lsh_mem_size.png)

## LSH speed
:::{.callout-tip icon=false}
## When?

- low-dimensionality vectors (128 is already a bit too large)
- small indexes
:::

## HNSW
:::{.callout-tip icon=false}
## Hierarchical Navigable Small World Graphs

- great search quality
- good search speed
- big index sizes
:::

:::{.callout-note icon=false}
## Facebook example
With 1.59B active users, the average number of steps (or hops) needed to traverse the graph from one user to another was just 3.57.
:::

## HNSW
![Visualization of an NSW graph. Notice that each point is no more than four hops away from another.](img/nsw_graph.png)

## HNSW
![HNSW graphs are built by taking NSW graphs and breaking them apart into multiple layers. With each incremental layer eliminating intermediate connections between vertices. With HNSW, we break networks into several layers, which are traversed during the search.](img/nsw_levels.png)
