<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course â€“ PyTorch: Optimization and Initialization</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">PyTorch: Optimization and Initialization</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Challenges</strong></p>
</div>
<div class="callout-content">
<ul>
<li>ensuring stable gradient flow</li>
<li>avoiding exploding/vanishing gradients</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Plan</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Initialization</strong>: review different techniques, go from simple to complex ones
<ul>
<li>constant/Gaussian</li>
<li>Xavier/Kaiming</li>
</ul></li>
<li><strong>Optimization</strong>:
<ul>
<li>Stochastic Gradient Descent (SGD)</li>
<li>SGD with Momentum</li>
<li>Adam</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="imports" class="slide level2">
<h2>Imports</h2>
<div id="f52f3237" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a></a><span class="im">import</span> os</span>
<span id="cb1-3"><a></a><span class="im">import</span> json</span>
<span id="cb1-4"><a></a><span class="im">import</span> math</span>
<span id="cb1-5"><a></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-6"><a></a><span class="im">import</span> copy</span>
<span id="cb1-7"><a></a></span>
<span id="cb1-8"><a></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-9"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-11"><a></a><span class="op">%</span>matplotlib inline </span>
<span id="cb1-12"><a></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-13"><a></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-14"><a></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-15"><a></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-16"><a></a></span>
<span id="cb1-17"><a></a><span class="co">## Progress bar</span></span>
<span id="cb1-18"><a></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb1-19"><a></a></span>
<span id="cb1-20"><a></a><span class="co">## PyTorch</span></span>
<span id="cb1-21"><a></a><span class="im">import</span> torch</span>
<span id="cb1-22"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-23"><a></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-24"><a></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb1-25"><a></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preparation" class="slide level2">
<h2>Preparation</h2>
<div id="7d67e508" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="co"># Path to the folder where the datasets are/should be downloaded (e.g. MNIST)</span></span>
<span id="cb2-2"><a></a>DATASET_PATH <span class="op">=</span> <span class="st">"../data"</span></span>
<span id="cb2-3"><a></a><span class="co"># Path to the folder where the pretrained models are saved</span></span>
<span id="cb2-4"><a></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../saved_models/tutorial4"</span></span>
<span id="cb2-5"><a></a></span>
<span id="cb2-6"><a></a><span class="co"># Function for setting the seed</span></span>
<span id="cb2-7"><a></a><span class="kw">def</span> set_seed(seed):</span>
<span id="cb2-8"><a></a>    np.random.seed(seed)</span>
<span id="cb2-9"><a></a>    torch.manual_seed(seed)</span>
<span id="cb2-10"><a></a>    <span class="cf">if</span> torch.mps.is_available():</span>
<span id="cb2-11"><a></a>        torch.mps.manual_seed(seed)</span>
<span id="cb2-12"><a></a>        <span class="co">#torch.cuda.manual_seed_all(seed)</span></span>
<span id="cb2-13"><a></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb2-14"><a></a></span>
<span id="cb2-15"><a></a><span class="co"># Ensure that all operations are deterministic on GPU (if used) for reproducibility</span></span>
<span id="cb2-16"><a></a>torch.backends.mps.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-17"><a></a>torch.backends.mps.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-18"><a></a></span>
<span id="cb2-19"><a></a><span class="co"># Fetching the device that will be used throughout this notebook</span></span>
<span id="cb2-20"><a></a>device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>) <span class="cf">if</span> <span class="kw">not</span> torch.mps.is_available() <span class="cf">else</span> torch.device(<span class="st">"mps:0"</span>)</span>
<span id="cb2-21"><a></a><span class="bu">print</span>(<span class="st">"Using device"</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device mps:0</code></pre>
</div>
</div>
</section>
<section id="download" class="slide level2">
<h2>Download</h2>
<div id="52c4186a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> urllib.request</span>
<span id="cb4-2"><a></a><span class="im">from</span> urllib.error <span class="im">import</span> HTTPError</span>
<span id="cb4-3"><a></a><span class="co"># Github URL where saved models are stored for this tutorial</span></span>
<span id="cb4-4"><a></a>base_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/"</span></span>
<span id="cb4-5"><a></a><span class="co"># Files to download</span></span>
<span id="cb4-6"><a></a>pretrained_files <span class="op">=</span> [<span class="st">"FashionMNIST_SGD.config"</span>,    <span class="st">"FashionMNIST_SGD_results.json"</span>,    <span class="st">"FashionMNIST_SGD.tar"</span>, </span>
<span id="cb4-7"><a></a>                    <span class="st">"FashionMNIST_SGDMom.config"</span>, <span class="st">"FashionMNIST_SGDMom_results.json"</span>, <span class="st">"FashionMNIST_SGDMom.tar"</span>, </span>
<span id="cb4-8"><a></a>                    <span class="st">"FashionMNIST_Adam.config"</span>,   <span class="st">"FashionMNIST_Adam_results.json"</span>,   <span class="st">"FashionMNIST_Adam.tar"</span>   ]</span>
<span id="cb4-9"><a></a><span class="co"># Create checkpoint path if it doesn't exist yet</span></span>
<span id="cb4-10"><a></a>os.makedirs(CHECKPOINT_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-11"><a></a></span>
<span id="cb4-12"><a></a><span class="co"># For each file, check whether it already exists. If not, try downloading it.</span></span>
<span id="cb4-13"><a></a><span class="cf">for</span> file_name <span class="kw">in</span> pretrained_files:</span>
<span id="cb4-14"><a></a>    file_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, file_name)</span>
<span id="cb4-15"><a></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(file_path):</span>
<span id="cb4-16"><a></a>        file_url <span class="op">=</span> base_url <span class="op">+</span> file_name</span>
<span id="cb4-17"><a></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>file_url<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb4-18"><a></a>        <span class="cf">try</span>:</span>
<span id="cb4-19"><a></a>            urllib.request.urlretrieve(file_url, file_path)</span>
<span id="cb4-20"><a></a>        <span class="cf">except</span> HTTPError <span class="im">as</span> e:</span>
<span id="cb4-21"><a></a>            <span class="bu">print</span>(<span class="st">"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:</span><span class="ch">\n</span><span class="st">"</span>, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preparation-1" class="slide level2">
<h2>Preparation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Preload FashionMNIST</strong></p>
</div>
<div class="callout-content">
<div id="c678e465" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> FashionMNIST</span>
<span id="cb5-2"><a></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb5-3"><a></a></span>
<span id="cb5-4"><a></a><span class="co"># Transformations applied on each image =&gt; first make them a tensor, then normalize them with mean 0 and std 1</span></span>
<span id="cb5-5"><a></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(),</span>
<span id="cb5-6"><a></a>                                transforms.Normalize((<span class="fl">0.2861</span>,), (<span class="fl">0.3530</span>,))</span>
<span id="cb5-7"><a></a>                               ])</span>
<span id="cb5-8"><a></a></span>
<span id="cb5-9"><a></a><span class="co"># Loading the training dataset. We need to split it into a training and validation part</span></span>
<span id="cb5-10"><a></a>train_dataset <span class="op">=</span> FashionMNIST(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-11"><a></a>train_set, val_set <span class="op">=</span> torch.utils.data.random_split(train_dataset, [<span class="dv">50000</span>, <span class="dv">10000</span>])</span>
<span id="cb5-12"><a></a></span>
<span id="cb5-13"><a></a><span class="co"># Loading the test set</span></span>
<span id="cb5-14"><a></a>test_set <span class="op">=</span> FashionMNIST(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-15"><a></a></span>
<span id="cb5-16"><a></a><span class="co"># We define a set of data loaders that we can use for various purposes later.</span></span>
<span id="cb5-17"><a></a><span class="co"># Note that for actually training a model, we will use different data loaders</span></span>
<span id="cb5-18"><a></a><span class="co"># with a lower batch size.</span></span>
<span id="cb5-19"><a></a>train_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-20"><a></a>val_loader <span class="op">=</span> data.DataLoader(val_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-21"><a></a>test_loader <span class="op">=</span> data.DataLoader(test_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="transformation" class="slide level2">
<h2>Transformation</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>We have changed the parameters of the normalization transformation <code>transforms.Normalize</code>:</p>
<blockquote>
<p>the normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels</p>
</blockquote>
<p>We can calculate the normalization parameters by determining the mean and standard deviation on the original images:</p>
<div id="de422201" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="bu">print</span>(<span class="st">"Mean"</span>, (train_dataset.data.<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span>).mean().item())</span>
<span id="cb6-2"><a></a><span class="bu">print</span>(<span class="st">"Std"</span>, (train_dataset.data.<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span>).std().item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean 0.2860405743122101
Std 0.3530242443084717</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="transformation-1" class="slide level2">
<h2>Transformation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Verification</strong></p>
</div>
<div class="callout-content">
<div id="97440756" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>imgs, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb8-2"><a></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>imgs<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-3"><a></a><span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>imgs<span class="sc">.</span>std()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-4"><a></a><span class="bu">print</span>(<span class="ss">f"Maximum: </span><span class="sc">{</span>imgs<span class="sc">.</span><span class="bu">max</span>()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-5"><a></a><span class="bu">print</span>(<span class="ss">f"Minimum: </span><span class="sc">{</span>imgs<span class="sc">.</span><span class="bu">min</span>()<span class="sc">.</span>item()<span class="sc">:5.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean: 0.020
Standard deviation: 1.011
Maximum: 2.022
Minimum: -0.810</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST.</p>
</div>
</div>
</div>
</section>
<section id="linear-network" class="slide level2">
<h2>Linear network</h2>
<div id="3f24c64c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="kw">class</span> BaseNetwork(nn.Module):</span>
<span id="cb10-2"><a></a>    </span>
<span id="cb10-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, act_fn, input_size<span class="op">=</span><span class="dv">784</span>, num_classes<span class="op">=</span><span class="dv">10</span>, hidden_sizes<span class="op">=</span>[<span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">128</span>]):</span>
<span id="cb10-4"><a></a>        <span class="co">"""</span></span>
<span id="cb10-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb10-6"><a></a><span class="co">            act_fn - Object of the activation function that should be used as non-linearity in the network.</span></span>
<span id="cb10-7"><a></a><span class="co">            input_size - Size of the input images in pixels</span></span>
<span id="cb10-8"><a></a><span class="co">            num_classes - Number of classes we want to predict</span></span>
<span id="cb10-9"><a></a><span class="co">            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN</span></span>
<span id="cb10-10"><a></a><span class="co">        """</span></span>
<span id="cb10-11"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-12"><a></a>        </span>
<span id="cb10-13"><a></a>        <span class="co"># Create the network based on the specified hidden sizes</span></span>
<span id="cb10-14"><a></a>        layers <span class="op">=</span> []</span>
<span id="cb10-15"><a></a>        layer_sizes <span class="op">=</span> [input_size] <span class="op">+</span> hidden_sizes</span>
<span id="cb10-16"><a></a>        <span class="cf">for</span> layer_index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(layer_sizes)):</span>
<span id="cb10-17"><a></a>            layers <span class="op">+=</span> [nn.Linear(layer_sizes[layer_index<span class="op">-</span><span class="dv">1</span>], layer_sizes[layer_index]),</span>
<span id="cb10-18"><a></a>                       act_fn]</span>
<span id="cb10-19"><a></a>        layers <span class="op">+=</span> [nn.Linear(layer_sizes[<span class="op">-</span><span class="dv">1</span>], num_classes)]</span>
<span id="cb10-20"><a></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList(layers) <span class="co"># A module list registers a list of modules as submodules (e.g. for parameters)</span></span>
<span id="cb10-21"><a></a>        </span>
<span id="cb10-22"><a></a>        <span class="va">self</span>.config <span class="op">=</span> {<span class="st">"act_fn"</span>: act_fn.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="st">"input_size"</span>: input_size, <span class="st">"num_classes"</span>: num_classes, <span class="st">"hidden_sizes"</span>: hidden_sizes} </span>
<span id="cb10-23"><a></a>        </span>
<span id="cb10-24"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-25"><a></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-26"><a></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb10-27"><a></a>            x <span class="op">=</span> l(x)</span>
<span id="cb10-28"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="activation" class="slide level2">
<h2>Activation</h2>
<div id="f424c5c9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="kw">class</span> Identity(nn.Module):</span>
<span id="cb11-2"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-3"><a></a>        <span class="cf">return</span> x</span>
<span id="cb11-4"><a></a>    </span>
<span id="cb11-5"><a></a>act_fn_by_name <span class="op">=</span> {</span>
<span id="cb11-6"><a></a>    <span class="st">"tanh"</span>: nn.Tanh,</span>
<span id="cb11-7"><a></a>    <span class="st">"relu"</span>: nn.ReLU,</span>
<span id="cb11-8"><a></a>    <span class="st">"identity"</span>: Identity</span>
<span id="cb11-9"><a></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside><div>
<p>We use the <code>Identity</code> function for simplicity when discussing initialization.</p>
</div></aside></section>
<section id="plotting" class="slide level2">
<h2>Plotting</h2>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Functions</a></li><li><a href="#tabset-1-2">Plots</a></li><li><a href="#tabset-1-3">Weights</a></li><li><a href="#tabset-1-4">Gradients</a></li><li><a href="#tabset-1-5">Activations</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<p>These functions help us to:</p>
<ul>
<li>visualize the weight/parameter distribution inside a network</li>
<li>visualize the gradients that the parameters at different layers receive</li>
<li>visualize the activations, i.e.&nbsp;the output of the linear layers.</li>
</ul>
</div>
<div id="tabset-1-2">
<div id="b84ccb60" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="kw">def</span> plot_dists(val_dict, color<span class="op">=</span><span class="st">"C0"</span>, xlabel<span class="op">=</span><span class="va">None</span>, stat<span class="op">=</span><span class="st">"count"</span>, use_kde<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-2"><a></a>    columns <span class="op">=</span> <span class="bu">len</span>(val_dict)</span>
<span id="cb12-3"><a></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, columns, figsize<span class="op">=</span>(columns<span class="op">*</span><span class="dv">3</span>, <span class="fl">2.5</span>))</span>
<span id="cb12-4"><a></a>    fig_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-5"><a></a>    <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(val_dict.keys()):</span>
<span id="cb12-6"><a></a>        key_ax <span class="op">=</span> ax[fig_index<span class="op">%</span>columns]</span>
<span id="cb12-7"><a></a>        sns.histplot(val_dict[key], ax<span class="op">=</span>key_ax, color<span class="op">=</span>color, bins<span class="op">=</span><span class="dv">50</span>, stat<span class="op">=</span>stat,</span>
<span id="cb12-8"><a></a>                     kde<span class="op">=</span>use_kde <span class="kw">and</span> ((val_dict[key].<span class="bu">max</span>()<span class="op">-</span>val_dict[key].<span class="bu">min</span>())<span class="op">&gt;</span><span class="fl">1e-8</span>)) <span class="co"># Only plot kde if there is variance</span></span>
<span id="cb12-9"><a></a>        key_ax.set_title(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> "</span> <span class="op">+</span> (<span class="vs">r"</span><span class="kw">(</span><span class="vs">%i </span><span class="dv">$</span><span class="ch">\t</span><span class="vs">o</span><span class="dv">$</span><span class="vs"> %i</span><span class="kw">)</span><span class="vs">"</span> <span class="op">%</span> (val_dict[key].shape[<span class="dv">1</span>], val_dict[key].shape[<span class="dv">0</span>]) <span class="cf">if</span> <span class="bu">len</span>(val_dict[key].shape)<span class="op">&gt;</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">""</span>))</span>
<span id="cb12-10"><a></a>        <span class="cf">if</span> xlabel <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-11"><a></a>            key_ax.set_xlabel(xlabel)</span>
<span id="cb12-12"><a></a>        fig_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-13"><a></a>    fig.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb12-14"><a></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-3">
<div id="13c00bba" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a><span class="kw">def</span> visualize_weight_distribution(model, color<span class="op">=</span><span class="st">"C0"</span>):</span>
<span id="cb13-2"><a></a>    weights <span class="op">=</span> {}</span>
<span id="cb13-3"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb13-4"><a></a>        <span class="cf">if</span> name.endswith(<span class="st">".bias"</span>):</span>
<span id="cb13-5"><a></a>            <span class="cf">continue</span></span>
<span id="cb13-6"><a></a>        key_name <span class="op">=</span> <span class="ss">f"Layer </span><span class="sc">{</span>name<span class="sc">.</span>split(<span class="st">'.'</span>)[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb13-7"><a></a>        weights[key_name] <span class="op">=</span> param.detach().view(<span class="op">-</span><span class="dv">1</span>).cpu().numpy()</span>
<span id="cb13-8"><a></a>    </span>
<span id="cb13-9"><a></a>    <span class="co">## Plotting</span></span>
<span id="cb13-10"><a></a>    fig <span class="op">=</span> plot_dists(weights, color<span class="op">=</span>color, xlabel<span class="op">=</span><span class="st">"Weight vals"</span>)</span>
<span id="cb13-11"><a></a>    fig.suptitle(<span class="st">"Weight distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb13-12"><a></a>    plt.show()</span>
<span id="cb13-13"><a></a>    plt.close() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-4">
<div id="4dc8334f" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a><span class="kw">def</span> visualize_gradients(model, color<span class="op">=</span><span class="st">"C0"</span>, print_variance<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb14-2"><a></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a></a><span class="co">    Inputs:</span></span>
<span id="cb14-4"><a></a><span class="co">        net - Object of class BaseNetwork</span></span>
<span id="cb14-5"><a></a><span class="co">        color - Color in which we want to visualize the histogram (for easier separation of activation functions)</span></span>
<span id="cb14-6"><a></a><span class="co">    """</span></span>
<span id="cb14-7"><a></a>    model.<span class="bu">eval</span>()</span>
<span id="cb14-8"><a></a>    small_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-9"><a></a>    imgs, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(small_loader))</span>
<span id="cb14-10"><a></a>    imgs, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb14-11"><a></a>    </span>
<span id="cb14-12"><a></a>    <span class="co"># Pass one batch through the network, and calculate the gradients for the weights</span></span>
<span id="cb14-13"><a></a>    model.zero_grad()</span>
<span id="cb14-14"><a></a>    preds <span class="op">=</span> model(imgs)</span>
<span id="cb14-15"><a></a>    loss <span class="op">=</span> F.cross_entropy(preds, labels) <span class="co"># Same as nn.CrossEntropyLoss, but as a function instead of module</span></span>
<span id="cb14-16"><a></a>    loss.backward()</span>
<span id="cb14-17"><a></a>    <span class="co"># We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots</span></span>
<span id="cb14-18"><a></a>    grads <span class="op">=</span> {name: params.grad.view(<span class="op">-</span><span class="dv">1</span>).cpu().clone().numpy() <span class="cf">for</span> name, params <span class="kw">in</span> model.named_parameters() <span class="cf">if</span> <span class="st">"weight"</span> <span class="kw">in</span> name}</span>
<span id="cb14-19"><a></a>    model.zero_grad()</span>
<span id="cb14-20"><a></a>    </span>
<span id="cb14-21"><a></a>    <span class="co">## Plotting</span></span>
<span id="cb14-22"><a></a>    fig <span class="op">=</span> plot_dists(grads, color<span class="op">=</span>color, xlabel<span class="op">=</span><span class="st">"Grad magnitude"</span>)</span>
<span id="cb14-23"><a></a>    fig.suptitle(<span class="st">"Gradient distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb14-24"><a></a>    plt.show()</span>
<span id="cb14-25"><a></a>    plt.close() </span>
<span id="cb14-26"><a></a>    </span>
<span id="cb14-27"><a></a>    <span class="cf">if</span> print_variance:</span>
<span id="cb14-28"><a></a>        <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(grads.keys()):</span>
<span id="cb14-29"><a></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> - Variance: </span><span class="sc">{</span>np<span class="sc">.</span>var(grads[key])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-5">
<div id="8496c7e2" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="kw">def</span> visualize_activations(model, color<span class="op">=</span><span class="st">"C0"</span>, print_variance<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb15-2"><a></a>    model.<span class="bu">eval</span>()</span>
<span id="cb15-3"><a></a>    small_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">1024</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-4"><a></a>    imgs, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(small_loader))</span>
<span id="cb15-5"><a></a>    imgs, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb15-6"><a></a>    </span>
<span id="cb15-7"><a></a>    <span class="co"># Pass one batch through the network, and calculate the gradients for the weights</span></span>
<span id="cb15-8"><a></a>    feats <span class="op">=</span> imgs.view(imgs.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb15-9"><a></a>    activations <span class="op">=</span> {}</span>
<span id="cb15-10"><a></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-11"><a></a>        <span class="cf">for</span> layer_index, layer <span class="kw">in</span> <span class="bu">enumerate</span>(model.layers):</span>
<span id="cb15-12"><a></a>            feats <span class="op">=</span> layer(feats)</span>
<span id="cb15-13"><a></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(layer, nn.Linear):</span>
<span id="cb15-14"><a></a>                activations[<span class="ss">f"Layer </span><span class="sc">{</span>layer_index<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> feats.view(<span class="op">-</span><span class="dv">1</span>).detach().cpu().numpy()</span>
<span id="cb15-15"><a></a>    </span>
<span id="cb15-16"><a></a>    <span class="co">## Plotting</span></span>
<span id="cb15-17"><a></a>    fig <span class="op">=</span> plot_dists(activations, color<span class="op">=</span>color, stat<span class="op">=</span><span class="st">"density"</span>, xlabel<span class="op">=</span><span class="st">"Activation vals"</span>)</span>
<span id="cb15-18"><a></a>    fig.suptitle(<span class="st">"Activation distribution"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span>
<span id="cb15-19"><a></a>    plt.show()</span>
<span id="cb15-20"><a></a>    plt.close() </span>
<span id="cb15-21"><a></a>    </span>
<span id="cb15-22"><a></a>    <span class="cf">if</span> print_variance:</span>
<span id="cb15-23"><a></a>        <span class="cf">for</span> key <span class="kw">in</span> <span class="bu">sorted</span>(activations.keys()):</span>
<span id="cb15-24"><a></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> - Variance: </span><span class="sc">{</span>np<span class="sc">.</span>var(activations[key])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="initialization" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Properties</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>The variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons.</p>
<ul>
<li>If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value.</li>
<li>Similarly, if the variance increases, it is likely to explode (i.e.&nbsp;head to infinity) the deeper we design our model.</li>
</ul></li>
<li><p>Gradient distribution should have equal variance across layers. If the first layer receives much smaller gradients than the last layer, choosing an appropriate learning rate will be difficult.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="initialization-1" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Initializations depend on the specific activation function used in the network.</p>
</div>
</div>
</div>
<div id="a1a063c6" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a>model <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>Identity()).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="initialization-2" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Constant initialization</strong></p>
</div>
<div class="callout-content">
<div id="1e4235ab" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a><span class="kw">def</span> const_init(model, c<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb17-2"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb17-3"><a></a>        param.data.fill_(c)</span>
<span id="cb17-4"><a></a></span>
<span id="cb17-5"><a></a>const_init(model, c<span class="op">=</span><span class="fl">0.005</span>)</span>
<span id="cb17-6"><a></a>visualize_gradients(model)</span>
<span id="cb17-7"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-15-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-15-output-2.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 2.0582754611968994
Layer 2 - Variance: 13.489115715026855
Layer 4 - Variance: 22.100555419921875
Layer 6 - Variance: 36.209537506103516
Layer 8 - Variance: 14.831426620483398</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="initialization-3" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Constant initialization: analysis</strong></p>
</div>
<div class="callout-content">
<ul>
<li>only the first and the last layer have diverse gradient distributions</li>
<li>the other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<ul>
<li>having the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters.</li>
<li>this would make our layer useless and effectively reduce number of parameters to 1.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="initialization-4" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Constant variance</strong></p>
</div>
<div class="callout-content">
<div id="f517da60" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a><span class="kw">def</span> var_init(model, std<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb19-2"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb19-3"><a></a>        param.data.normal_(std<span class="op">=</span>std)</span>
<span id="cb19-4"><a></a>        </span>
<span id="cb19-5"><a></a>var_init(model, std<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb19-6"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-16-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 0.07771595567464828
Layer 2 - Variance: 0.00406030984595418
Layer 4 - Variance: 0.00020879440126009285
Layer 6 - Variance: 9.545485954731703e-05
Layer 8 - Variance: 3.9654176362091675e-05</code></pre>
</div>
</div>
</div>
</div>
</div>

<aside><div>
<p>The variance of the activation becomes smaller and smaller across layers, and almost <strong>vanishes</strong> in the last layer.</p>
</div></aside></section>
<section id="initialization-5" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Constant variance: higher std</strong></p>
</div>
<div class="callout-content">
<div id="9b30c70f" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>var_init(model, std<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb21-2"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-17-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 7.925612926483154
Layer 2 - Variance: 43.7206916809082
Layer 4 - Variance: 113.83338928222656
Layer 6 - Variance: 312.2013244628906
Layer 8 - Variance: 347.9194641113281</code></pre>
</div>
</div>
</div>
</div>
</div>

<aside><div>
<p>With a higher standard deviation, the activations are likely to <strong>explode.</strong></p>
</div></aside></section>
<section id="initialization-6" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How to find appropriate initialization values</strong></p>
</div>
<div class="callout-content">
<p>Two requirements:</p>
<ol type="1">
<li>The mean of the activations should be zero</li>
<li>The variance of the activations should stay the same across every layer</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Procedure</strong></p>
</div>
<div class="callout-content">
<p>Suppose we want to design an initialization for the following layer: <span class="math display">\[
y=Wx+b, \; y\in\mathbb{R}^{d_y}, \; x\in\mathbb{R}^{d_x}
\]</span> Goal: <span class="math display">\[
\text{Var}(y_i)=\text{Var}(x_i)=\sigma_x^{2},\\
mean(y_i) = 0
\]</span></p>
</div>
</div>
</div>
</section>
<section id="initialization-7" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Procedure</strong></p>
</div>
<div class="callout-content">
<p>We assume <span class="math inline">\(x\)</span> to also have a mean of zero, because, in deep neural networks, <span class="math inline">\(y\)</span> would be the input of another layer. This requires the bias and weight to have an expectation of 0. Actually, as <span class="math inline">\(b\)</span> is a single element per output neuron and is constant across different inputs, we set it to 0 overall.</p>
<p>Next, we need to calculate the variance with which we need to initialize the weight parameters. Along the calculation, we will need the following variance rule: given two independent variables, the variance of their product is <span class="math display">\[
\text{Var}(X\cdot Y) = \mathbb{E}(Y)^2\text{Var}(X) + \mathbb{E}(X)^2\text{Var}(Y) + \text{Var}(X)\text{Var}(Y) = \\
= \mathbb{E}(Y^2)\mathbb{E}(X^2)-\mathbb{E}(Y)^2\mathbb{E}(X)^2
\]</span></p>
<p>(<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not refering to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, but any random variable).</p>
</div>
</div>
</div>
</section>
<section id="initialization-8" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation</strong></p>
</div>
<div class="callout-content">
<!-- The needed variance of the weights, $\text{Var}(w_{ij})$, is calculated as follows: -->
<p><span class="math display">\[
\begin{split}
    &amp;y_i  = \sum_{j} w_{ij}x_{j}\hspace{10mm}\text{Calculation of a single output neuron without bias}\\
    &amp;\text{Var}(y_i) = \sigma_x^{2}   \\
    &amp;= \text{Var}\left(\sum_{j} w_{ij}x_{j}\right)\\
    &amp; = \sum_{j} \text{Var}(w_{ij}x_{j}) \hspace{10mm}\text{Inputs and weights are independent of each other}\\
    &amp; = \sum_{j} \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance rule (see above) with expectations being zero}\\
    &amp; = d_x \cdot \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance equal for all $d_x$ elements}\\
    &amp; = \sigma_x^{2} \cdot d_x \cdot \text{Var}(w_{ij})\\
    &amp;\Rightarrow \text{Var}(w_{ij}) = \sigma_{W}^2  = \frac{1}{d_x}
\end{split}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="initialization-9" class="slide level2 scrollable">
<h2>Initialization</h2>
<p>Thus, we should initialize the weight distribution with a variance of the inverse of the input dimension <span class="math inline">\(d_x\)</span>.</p>
<div id="074519d3" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a><span class="kw">def</span> equal_var_init(model):</span>
<span id="cb23-2"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb23-3"><a></a>        <span class="cf">if</span> name.endswith(<span class="st">".bias"</span>):</span>
<span id="cb23-4"><a></a>            param.data.fill_(<span class="dv">0</span>)</span>
<span id="cb23-5"><a></a>        <span class="cf">else</span>:</span>
<span id="cb23-6"><a></a>            param.data.normal_(std<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span>math.sqrt(param.shape[<span class="dv">1</span>]))</span>
<span id="cb23-7"><a></a>        </span>
<span id="cb23-8"><a></a>equal_var_init(model)</span>
<span id="cb23-9"><a></a>visualize_weight_distribution(model)</span>
<span id="cb23-10"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-18-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-18-output-2.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 0.9882476329803467
Layer 2 - Variance: 0.9890449047088623
Layer 4 - Variance: 1.0196830034255981
Layer 6 - Variance: 1.0159821510314941
Layer 8 - Variance: 0.7536574602127075</code></pre>
</div>
</div>
</section>
<section id="initialization-10" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Our initialization does not restrict us to a normal distribution, but allows any other distribution with:</p>
<ul>
<li>a <strong>mean</strong> of 0</li>
<li>and <strong>variance</strong> of <span class="math inline">\(1/d_x\)</span>.</li>
</ul>
<p>You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.</p>
</div>
</div>
</div>
</section>
<section id="initialization-11" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stabilization of gradient variance: Xavier initialization</strong></p>
</div>
<div class="callout-content">
<p>Ensures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from <span class="math inline">\(\Delta x=W\Delta y\)</span>, and come to the conclusion that we should initialize our layers with <span class="math inline">\(1/d_y\)</span> where <span class="math inline">\(d_y\)</span> is the number of output neurons. As a compromise between both constraints, <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi">Glorot and Bengio (2010)</a> proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:</p>
<p><span class="math display">\[W\sim \mathcal{N}\left(0,\frac{2}{d_x+d_y}\right)\]</span></p>
<p>If we use a uniform distribution, we would initialize the weights with:</p>
<p><span class="math display">\[W\sim U\left[-\frac{\sqrt{6}}{\sqrt{d_x+d_y}}, \frac{\sqrt{6}}{\sqrt{d_x+d_y}}\right]\]</span></p>
</div>
</div>
</div>
</section>
<section id="initialization-12" class="slide level2 scrollable">
<h2>Initialization</h2>
<div id="8961a399" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a><span class="kw">def</span> xavier_init(model):</span>
<span id="cb25-2"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb25-3"><a></a>        <span class="cf">if</span> name.endswith(<span class="st">".bias"</span>):</span>
<span id="cb25-4"><a></a>            param.data.fill_(<span class="dv">0</span>)</span>
<span id="cb25-5"><a></a>        <span class="cf">else</span>:</span>
<span id="cb25-6"><a></a>            bound <span class="op">=</span> math.sqrt(<span class="dv">6</span>)<span class="op">/</span>math.sqrt(param.shape[<span class="dv">0</span>]<span class="op">+</span>param.shape[<span class="dv">1</span>])</span>
<span id="cb25-7"><a></a>            param.data.uniform_(<span class="op">-</span>bound, bound)</span>
<span id="cb25-8"><a></a>        </span>
<span id="cb25-9"><a></a>xavier_init(model)</span>
<span id="cb25-10"><a></a>visualize_gradients(model, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-11"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-19-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>layers.0.weight - Variance: 0.0005975761450827122
layers.2.weight - Variance: 0.0010738210985437036
layers.4.weight - Variance: 0.0013981545343995094
layers.6.weight - Variance: 0.0021510443184524775
layers.8.weight - Variance: 0.020080894231796265</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-19-output-3.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.2443994283676147
Layer 2 - Variance: 1.6894475221633911
Layer 4 - Variance: 1.7133890390396118
Layer 6 - Variance: 2.432518482208252
Layer 8 - Variance: 4.493688106536865</code></pre>
</div>
</div>
</section>
<section id="initialization-13" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Xavier - analysis</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Xavier initialization balances the variance of gradients and activations</li>
<li>that the significantly higher variance for the output layer is due to the large difference of input and output dimension (<span class="math inline">\(128\)</span> vs <span class="math inline">\(10\)</span>)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="initialization-14" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Non-linearity</strong></p>
</div>
<div class="callout-content">
<p>In a <span class="math inline">\(\tanh\)</span>-based network, a common assumption is that for small values during the initial steps in training, the <span class="math inline">\(\tanh\)</span> works as a linear function such that we donâ€™t have to adjust our calculation. We can check if that is the case for us as well:</p>
</div>
</div>
</div>
<div id="1495e14d" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>model <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>nn.Tanh()).to(device)</span>
<span id="cb28-2"><a></a>xavier_init(model)</span>
<span id="cb28-3"><a></a>visualize_gradients(model, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-4"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-20-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>layers.0.weight - Variance: 1.6712088836356997e-05
layers.2.weight - Variance: 2.9089944291627035e-05
layers.4.weight - Variance: 3.600309355533682e-05
layers.6.weight - Variance: 4.9582638894207776e-05
layers.8.weight - Variance: 0.00044954405166208744</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-20-output-3.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.2616938352584839
Layer 2 - Variance: 0.544843316078186
Layer 4 - Variance: 0.2753238379955292
Layer 6 - Variance: 0.26287850737571716
Layer 8 - Variance: 0.2619311511516571</code></pre>
</div>
</div>
</section>
<section id="initialization-15" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Analysis for tanh</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Although the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values.</li>
<li>Therefore, our variance will stabilize around 0.25 if we would go even deeper.</li>
<li>Hence, we can conclude that the Xavier initialization works well for Tanh networks.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="initialization-16" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>ReLU</strong></p>
</div>
<div class="callout-content">
<p>The ReLU activation function sets (in expectation) half of the inputs to 0 so that also the expectation of the input is not zero. However, as long as the expectation of <span class="math inline">\(W\)</span> is zero and <span class="math inline">\(b=0\)</span>, the expectation of the output is zero.</p>
<p>The part where the calculation of the ReLU initialization differs from the identity is when determining <span class="math inline">\(\text{Var}(w_{ij}x_{j})\)</span>:</p>
<p><span class="math display">\[
\text{Var}(w_{ij}x_{j})=\underbrace{\mathbb{E}[w_{ij}^2]}_{=\text{Var}(w_{ij})}\mathbb{E}[x_{j}^2]-\underbrace{\mathbb{E}[w_{ij}]^2}_{=0}\mathbb{E}[x_{j}]^2=\text{Var}(w_{ij})\mathbb{E}[x_{j}^2]
\]</span></p>
</div>
</div>
</div>
</section>
<section id="initialization-17" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>ReLU: expectation calculation</strong></p>
</div>
<div class="callout-content">
<p>If we assume now that <span class="math inline">\(x\)</span> is the output of a ReLU activation (from a previous layer, <span class="math inline">\(x=max(0,\tilde{y})\)</span>), we can calculate the expectation as follows:</p>
<p><span class="math display">\[
\begin{split}
    \mathbb{E}[x^2] &amp; =\mathbb{E}[\max(0,\tilde{y})^2]\\
                    &amp; =\frac{1}{2}\mathbb{E}[{\tilde{y}}^2]\hspace{2cm}\tilde{y}\text{ is zero-centered and symmetric}\\
                    &amp; =\frac{1}{2}\text{Var}(\tilde{y})
\end{split}
\]</span></p>
<p>Thus, we see that we have an additional factor of 1/2 in the equation, so that our desired weight variance becomes <span class="math inline">\(2/d_x\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="initialization-18" class="slide level2 scrollable">
<h2>Initialization</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The Kaiming initialization does not use the harmonic mean between input and output size. In the original paper authors argue that using <span class="math inline">\(d_x\)</span> or <span class="math inline">\(d_y\)</span> both lead to stable gradients throughout the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input <span class="math inline">\(d_x\)</span>:</p>
</div>
</div>
</div>
<div id="1f247e45" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a><span class="kw">def</span> kaiming_init(model):</span>
<span id="cb31-2"><a></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb31-3"><a></a>        <span class="cf">if</span> name.endswith(<span class="st">".bias"</span>):</span>
<span id="cb31-4"><a></a>            param.data.fill_(<span class="dv">0</span>)</span>
<span id="cb31-5"><a></a>        <span class="cf">elif</span> name.startswith(<span class="st">"layers.0"</span>): <span class="co"># The first layer does not have ReLU applied on its input</span></span>
<span id="cb31-6"><a></a>            param.data.normal_(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span>math.sqrt(param.shape[<span class="dv">1</span>]))</span>
<span id="cb31-7"><a></a>        <span class="cf">else</span>:</span>
<span id="cb31-8"><a></a>            param.data.normal_(<span class="dv">0</span>, math.sqrt(<span class="dv">2</span>)<span class="op">/</span>math.sqrt(param.shape[<span class="dv">1</span>]))</span>
<span id="cb31-9"><a></a></span>
<span id="cb31-10"><a></a>model <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>nn.ReLU()).to(device)</span>
<span id="cb31-11"><a></a>kaiming_init(model)</span>
<span id="cb31-12"><a></a>visualize_gradients(model, print_variance<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-13"><a></a>visualize_activations(model, print_variance<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-21-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>layers.0.weight - Variance: 5.593948662863113e-05
layers.2.weight - Variance: 8.165080362232402e-05
layers.4.weight - Variance: 9.53530689002946e-05
layers.6.weight - Variance: 0.0002848389558494091
layers.8.weight - Variance: 0.003961425274610519</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-21-output-3.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0 - Variance: 1.0332646369934082
Layer 2 - Variance: 1.0350595712661743
Layer 4 - Variance: 1.0465645790100098
Layer 6 - Variance: 1.0822820663452148
Layer 8 - Variance: 0.925536036491394</code></pre>
</div>
</div>
</section>
<section id="initialization-19" class="slide level2">
<h2>Initialization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Kaiming - analysis</strong></p>
</div>
<div class="callout-content">
<ul>
<li>The variance stays stable across layers.</li>
<li>Note that for Leaky-ReLU etc., we have to slightly adjust the factor of <span class="math inline">\(2\)</span> in the variance as half of the values are not set to zero anymore.</li>
<li>PyTorch provides a function to calculate this factor for many activation function, see <code>torch.nn.init.calculate_gain</code> (<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain">link</a>).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="initialization-20" class="slide level2">
<h2>Initialization</h2>
<iframe width="1920" height="1080" src="https://www.deeplearning.ai/ai-notes/initialization/index.html" title="Initialization"></iframe>
</section>
<section id="optimization" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Preparation</strong></p>
</div>
<div class="callout-content">
<div id="385e0537" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a><span class="kw">def</span> _get_config_file(model_path, model_name):</span>
<span id="cb34-2"><a></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">".config"</span>)</span>
<span id="cb34-3"><a></a></span>
<span id="cb34-4"><a></a><span class="kw">def</span> _get_model_file(model_path, model_name):</span>
<span id="cb34-5"><a></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">".tar"</span>)</span>
<span id="cb34-6"><a></a></span>
<span id="cb34-7"><a></a><span class="kw">def</span> _get_result_file(model_path, model_name):</span>
<span id="cb34-8"><a></a>    <span class="cf">return</span> os.path.join(model_path, model_name <span class="op">+</span> <span class="st">"_results.json"</span>)</span>
<span id="cb34-9"><a></a></span>
<span id="cb34-10"><a></a><span class="kw">def</span> load_model(model_path, model_name, net<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-11"><a></a>    config_file, model_file <span class="op">=</span> _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)</span>
<span id="cb34-12"><a></a>    <span class="cf">assert</span> os.path.isfile(config_file), <span class="ss">f"Could not find the config file </span><span class="ch">\"</span><span class="sc">{</span>config_file<span class="sc">}</span><span class="ch">\"</span><span class="ss">. Are you sure this is the correct path and you have your model config stored here?"</span></span>
<span id="cb34-13"><a></a>    <span class="cf">assert</span> os.path.isfile(model_file), <span class="ss">f"Could not find the model file </span><span class="ch">\"</span><span class="sc">{</span>model_file<span class="sc">}</span><span class="ch">\"</span><span class="ss">. Are you sure this is the correct path and you have your model stored here?"</span></span>
<span id="cb34-14"><a></a>    <span class="cf">with</span> <span class="bu">open</span>(config_file, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb34-15"><a></a>        config_dict <span class="op">=</span> json.load(f)</span>
<span id="cb34-16"><a></a>    <span class="cf">if</span> net <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-17"><a></a>        act_fn_name <span class="op">=</span> config_dict[<span class="st">"act_fn"</span>].pop(<span class="st">"name"</span>).lower()</span>
<span id="cb34-18"><a></a>        <span class="cf">assert</span> act_fn_name <span class="kw">in</span> act_fn_by_name, <span class="ss">f"Unknown activation function </span><span class="ch">\"</span><span class="sc">{</span>act_fn_name<span class="sc">}</span><span class="ch">\"</span><span class="ss">. Please add it to the </span><span class="ch">\"</span><span class="ss">act_fn_by_name</span><span class="ch">\"</span><span class="ss"> dict."</span></span>
<span id="cb34-19"><a></a>        act_fn <span class="op">=</span> act_fn_by_name[act_fn_name]()</span>
<span id="cb34-20"><a></a>        net <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>act_fn, <span class="op">**</span>config_dict)</span>
<span id="cb34-21"><a></a>    net.load_state_dict(torch.load(model_file))</span>
<span id="cb34-22"><a></a>    <span class="cf">return</span> net</span>
<span id="cb34-23"><a></a>    </span>
<span id="cb34-24"><a></a><span class="kw">def</span> save_model(model, model_path, model_name):</span>
<span id="cb34-25"><a></a>    config_dict <span class="op">=</span> model.config</span>
<span id="cb34-26"><a></a>    os.makedirs(model_path, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-27"><a></a>    config_file, model_file <span class="op">=</span> _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)</span>
<span id="cb34-28"><a></a>    <span class="cf">with</span> <span class="bu">open</span>(config_file, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb34-29"><a></a>        json.dump(config_dict, f)</span>
<span id="cb34-30"><a></a>    torch.save(model.state_dict(), model_file)</span>
<span id="cb34-31"><a></a></span>
<span id="cb34-32"><a></a><span class="kw">def</span> train_model(net, model_name, optim_func, max_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">256</span>, overwrite<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb34-33"><a></a>    <span class="co">"""</span></span>
<span id="cb34-34"><a></a><span class="co">    Train a model on the training set of FashionMNIST</span></span>
<span id="cb34-35"><a></a><span class="co">    </span></span>
<span id="cb34-36"><a></a><span class="co">    Inputs:</span></span>
<span id="cb34-37"><a></a><span class="co">        net - Object of BaseNetwork</span></span>
<span id="cb34-38"><a></a><span class="co">        model_name - (str) Name of the model, used for creating the checkpoint names</span></span>
<span id="cb34-39"><a></a><span class="co">        max_epochs - Number of epochs we want to (maximally) train for</span></span>
<span id="cb34-40"><a></a><span class="co">        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early</span></span>
<span id="cb34-41"><a></a><span class="co">        batch_size - Size of batches used in training</span></span>
<span id="cb34-42"><a></a><span class="co">        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.</span></span>
<span id="cb34-43"><a></a><span class="co">    """</span></span>
<span id="cb34-44"><a></a>    file_exists <span class="op">=</span> os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))</span>
<span id="cb34-45"><a></a>    <span class="cf">if</span> file_exists <span class="kw">and</span> <span class="kw">not</span> overwrite:</span>
<span id="cb34-46"><a></a>        <span class="bu">print</span>(<span class="ss">f"Model file of </span><span class="ch">\"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ch">\"</span><span class="ss"> already exists. Skipping training..."</span>)</span>
<span id="cb34-47"><a></a>        <span class="cf">with</span> <span class="bu">open</span>(_get_result_file(CHECKPOINT_PATH, model_name), <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb34-48"><a></a>            results <span class="op">=</span> json.load(f)</span>
<span id="cb34-49"><a></a>    <span class="cf">else</span>:</span>
<span id="cb34-50"><a></a>        <span class="cf">if</span> file_exists:</span>
<span id="cb34-51"><a></a>            <span class="bu">print</span>(<span class="st">"Model file exists, but will be overwritten..."</span>)</span>
<span id="cb34-52"><a></a>            </span>
<span id="cb34-53"><a></a>        <span class="co"># Defining optimizer, loss and data loader</span></span>
<span id="cb34-54"><a></a>        optimizer <span class="op">=</span>  optim_func(net.parameters())</span>
<span id="cb34-55"><a></a>        loss_module <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-56"><a></a>        train_loader_local <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-57"><a></a>    </span>
<span id="cb34-58"><a></a>        results <span class="op">=</span> <span class="va">None</span></span>
<span id="cb34-59"><a></a>        val_scores <span class="op">=</span> []</span>
<span id="cb34-60"><a></a>        train_losses, train_scores <span class="op">=</span> [], []</span>
<span id="cb34-61"><a></a>        best_val_epoch <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb34-62"><a></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(max_epochs):</span>
<span id="cb34-63"><a></a>            <span class="co">############</span></span>
<span id="cb34-64"><a></a>            <span class="co"># Training #</span></span>
<span id="cb34-65"><a></a>            <span class="co">############</span></span>
<span id="cb34-66"><a></a>            net.train()</span>
<span id="cb34-67"><a></a>            true_preds, count <span class="op">=</span> <span class="fl">0.</span>, <span class="dv">0</span></span>
<span id="cb34-68"><a></a>            t <span class="op">=</span> tqdm(train_loader_local, leave<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-69"><a></a>            <span class="cf">for</span> imgs, labels <span class="kw">in</span> t:</span>
<span id="cb34-70"><a></a>                imgs, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb34-71"><a></a>                optimizer.zero_grad()</span>
<span id="cb34-72"><a></a>                preds <span class="op">=</span> net(imgs)</span>
<span id="cb34-73"><a></a>                loss <span class="op">=</span> loss_module(preds, labels)</span>
<span id="cb34-74"><a></a>                loss.backward()</span>
<span id="cb34-75"><a></a>                optimizer.step()</span>
<span id="cb34-76"><a></a>                <span class="co"># Record statistics during training</span></span>
<span id="cb34-77"><a></a>                true_preds <span class="op">+=</span> (preds.argmax(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb34-78"><a></a>                count <span class="op">+=</span> labels.shape[<span class="dv">0</span>]</span>
<span id="cb34-79"><a></a>                t.set_description(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: loss=</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:4.2f}</span><span class="ss">"</span>)</span>
<span id="cb34-80"><a></a>                train_losses.append(loss.item())</span>
<span id="cb34-81"><a></a>            train_acc <span class="op">=</span> true_preds <span class="op">/</span> count</span>
<span id="cb34-82"><a></a>            train_scores.append(train_acc)</span>
<span id="cb34-83"><a></a></span>
<span id="cb34-84"><a></a>            <span class="co">##############</span></span>
<span id="cb34-85"><a></a>            <span class="co"># Validation #</span></span>
<span id="cb34-86"><a></a>            <span class="co">##############</span></span>
<span id="cb34-87"><a></a>            val_acc <span class="op">=</span> test_model(net, val_loader)</span>
<span id="cb34-88"><a></a>            val_scores.append(val_acc)</span>
<span id="cb34-89"><a></a>            <span class="bu">print</span>(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:2d}</span><span class="ss">] Training accuracy: </span><span class="sc">{</span>train_acc<span class="op">*</span><span class="fl">100.0</span><span class="sc">:05.2f}</span><span class="ss">%, Validation accuracy: </span><span class="sc">{</span>val_acc<span class="op">*</span><span class="fl">100.0</span><span class="sc">:05.2f}</span><span class="ss">%"</span>)</span>
<span id="cb34-90"><a></a></span>
<span id="cb34-91"><a></a>            <span class="cf">if</span> <span class="bu">len</span>(val_scores) <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> val_acc <span class="op">&gt;</span> val_scores[best_val_epoch]:</span>
<span id="cb34-92"><a></a>                <span class="bu">print</span>(<span class="st">"</span><span class="ch">\t</span><span class="st">   (New best performance, saving model...)"</span>)</span>
<span id="cb34-93"><a></a>                save_model(net, CHECKPOINT_PATH, model_name)</span>
<span id="cb34-94"><a></a>                best_val_epoch <span class="op">=</span> epoch</span>
<span id="cb34-95"><a></a>    </span>
<span id="cb34-96"><a></a>    <span class="cf">if</span> results <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-97"><a></a>        load_model(CHECKPOINT_PATH, model_name, net<span class="op">=</span>net)</span>
<span id="cb34-98"><a></a>        test_acc <span class="op">=</span> test_model(net, test_loader)</span>
<span id="cb34-99"><a></a>        results <span class="op">=</span> {<span class="st">"test_acc"</span>: test_acc, <span class="st">"val_scores"</span>: val_scores, <span class="st">"train_losses"</span>: train_losses, <span class="st">"train_scores"</span>: train_scores}</span>
<span id="cb34-100"><a></a>        <span class="cf">with</span> <span class="bu">open</span>(_get_result_file(CHECKPOINT_PATH, model_name), <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb34-101"><a></a>            json.dump(results, f)</span>
<span id="cb34-102"><a></a>            </span>
<span id="cb34-103"><a></a>    <span class="co"># Plot a curve of the validation accuracy</span></span>
<span id="cb34-104"><a></a>    sns.<span class="bu">set</span>()</span>
<span id="cb34-105"><a></a>    plt.plot([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(results[<span class="st">"train_scores"</span>])<span class="op">+</span><span class="dv">1</span>)], results[<span class="st">"train_scores"</span>], label<span class="op">=</span><span class="st">"Train"</span>)</span>
<span id="cb34-106"><a></a>    plt.plot([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(results[<span class="st">"val_scores"</span>])<span class="op">+</span><span class="dv">1</span>)], results[<span class="st">"val_scores"</span>], label<span class="op">=</span><span class="st">"Val"</span>)</span>
<span id="cb34-107"><a></a>    plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb34-108"><a></a>    plt.ylabel(<span class="st">"Validation accuracy"</span>)</span>
<span id="cb34-109"><a></a>    plt.ylim(<span class="bu">min</span>(results[<span class="st">"val_scores"</span>]), <span class="bu">max</span>(results[<span class="st">"train_scores"</span>])<span class="op">*</span><span class="fl">1.01</span>)</span>
<span id="cb34-110"><a></a>    plt.title(<span class="ss">f"Validation performance of </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-111"><a></a>    plt.legend()</span>
<span id="cb34-112"><a></a>    plt.show()</span>
<span id="cb34-113"><a></a>    plt.close()</span>
<span id="cb34-114"><a></a>    </span>
<span id="cb34-115"><a></a>    <span class="bu">print</span>((<span class="ss">f" Test accuracy: </span><span class="sc">{</span>results[<span class="st">'test_acc'</span>]<span class="op">*</span><span class="fl">100.0</span><span class="sc">:4.2f}</span><span class="ss">% "</span>).center(<span class="dv">50</span>, <span class="st">"="</span>)<span class="op">+</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb34-116"><a></a>    <span class="cf">return</span> results</span>
<span id="cb34-117"><a></a>    </span>
<span id="cb34-118"><a></a></span>
<span id="cb34-119"><a></a><span class="kw">def</span> test_model(net, data_loader):</span>
<span id="cb34-120"><a></a>    <span class="co">"""</span></span>
<span id="cb34-121"><a></a><span class="co">    Test a model on a specified dataset.</span></span>
<span id="cb34-122"><a></a><span class="co">    </span></span>
<span id="cb34-123"><a></a><span class="co">    Inputs:</span></span>
<span id="cb34-124"><a></a><span class="co">        net - Trained model of type BaseNetwork</span></span>
<span id="cb34-125"><a></a><span class="co">        data_loader - DataLoader object of the dataset to test on (validation or test)</span></span>
<span id="cb34-126"><a></a><span class="co">    """</span></span>
<span id="cb34-127"><a></a>    net.<span class="bu">eval</span>()</span>
<span id="cb34-128"><a></a>    true_preds, count <span class="op">=</span> <span class="fl">0.</span>, <span class="dv">0</span></span>
<span id="cb34-129"><a></a>    <span class="cf">for</span> imgs, labels <span class="kw">in</span> data_loader:</span>
<span id="cb34-130"><a></a>        imgs, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb34-131"><a></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-132"><a></a>            preds <span class="op">=</span> net(imgs).argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb34-133"><a></a>            true_preds <span class="op">+=</span> (preds <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb34-134"><a></a>            count <span class="op">+=</span> labels.shape[<span class="dv">0</span>]</span>
<span id="cb34-135"><a></a>    test_acc <span class="op">=</span> true_preds <span class="op">/</span> count</span>
<span id="cb34-136"><a></a>    <span class="cf">return</span> test_acc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-1" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>What does optimizer do?</strong></p>
</div>
<div class="callout-content">
<p>It updates the networkâ€™s parameters given the gradients: <span class="math display">\[
\begin{align*}
&amp;w^{t} = f(w^{t-1}, g^{t}, \eta, ...), \\
&amp;w \text{ - parameters}, \\
&amp;g^{t} = \nabla_{w^{(t-1)}} \mathcal{L}^{(t)} \text{ - the gradients at time step } t,\\
&amp;\eta \text{ - learning rate}.
\end{align*}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="optimization-2" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Optimizer template</strong></p>
</div>
<div class="callout-content">
<div id="9e9524e5" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a><span class="kw">class</span> OptimizerTemplate:</span>
<span id="cb35-2"><a></a>    </span>
<span id="cb35-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr):</span>
<span id="cb35-4"><a></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb35-5"><a></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb35-6"><a></a>        </span>
<span id="cb35-7"><a></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb35-8"><a></a>        <span class="co">## Set gradients of all parameters to zero</span></span>
<span id="cb35-9"><a></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb35-10"><a></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb35-11"><a></a>                p.grad.detach_() <span class="co"># For second-order optimizers important</span></span>
<span id="cb35-12"><a></a>                p.grad.zero_()</span>
<span id="cb35-13"><a></a>    </span>
<span id="cb35-14"><a></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb35-15"><a></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb35-16"><a></a>        <span class="co">## Apply update step to all parameters</span></span>
<span id="cb35-17"><a></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb35-18"><a></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>: <span class="co"># We skip parameters without any gradients</span></span>
<span id="cb35-19"><a></a>                <span class="cf">continue</span></span>
<span id="cb35-20"><a></a>            <span class="va">self</span>.update_param(p)</span>
<span id="cb35-21"><a></a>            </span>
<span id="cb35-22"><a></a>    <span class="kw">def</span> update_param(<span class="va">self</span>, p):</span>
<span id="cb35-23"><a></a>        <span class="co"># To be implemented in optimizer-specific classes</span></span>
<span id="cb35-24"><a></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-3" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stochastic Gradient Descent</strong></p>
</div>
<div class="callout-content">
<p>The first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:</p>
<p><span class="math display">\[
\begin{split}
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot g^{(t)}
\end{split}
\]</span></p>
</div>
</div>
</div>
<div id="a78b1040" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a><span class="kw">class</span> SGD(OptimizerTemplate):</span>
<span id="cb36-2"><a></a>    </span>
<span id="cb36-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr):</span>
<span id="cb36-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr)</span>
<span id="cb36-5"><a></a>        </span>
<span id="cb36-6"><a></a>    <span class="kw">def</span> update_param(<span class="va">self</span>, p):</span>
<span id="cb36-7"><a></a>        p_update <span class="op">=</span> <span class="op">-</span><span class="va">self</span>.lr <span class="op">*</span> p.grad</span>
<span id="cb36-8"><a></a>        p.add_(p_update) <span class="co"># In-place update =&gt; saves memory and does not create computation graph</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-4" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Momentum</strong></p>
</div>
<div class="callout-content">
<p>Momentum replaces the gradient in the update by an exponential average of all past gradients including the current one:</p>
<p><span class="math display">\[
\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot m^{(t)}\\
\end{split}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="optimization-5" class="slide level2 scrollable">
<h2>Optimization</h2>
<div id="4ac64bc2" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a><span class="kw">class</span> SGDMomentum(OptimizerTemplate):</span>
<span id="cb37-2"><a></a>    </span>
<span id="cb37-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, momentum<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb37-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr)</span>
<span id="cb37-5"><a></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum <span class="co"># Corresponds to beta_1 in the equation above</span></span>
<span id="cb37-6"><a></a>        <span class="va">self</span>.param_momentum <span class="op">=</span> {p: torch.zeros_like(p.data) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params} <span class="co"># Dict to store m_t</span></span>
<span id="cb37-7"><a></a>        </span>
<span id="cb37-8"><a></a>    <span class="kw">def</span> update_param(<span class="va">self</span>, p):</span>
<span id="cb37-9"><a></a>        <span class="va">self</span>.param_momentum[p] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> p.grad <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> <span class="va">self</span>.param_momentum[p]</span>
<span id="cb37-10"><a></a>        p_update <span class="op">=</span> <span class="op">-</span><span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.param_momentum[p]</span>
<span id="cb37-11"><a></a>        p.add_(p_update)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-6" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Adam</strong></p>
</div>
<div class="callout-content">
<p>Adam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e.&nbsp;the gradients norm. Furthermore, we add a bias correction for the momentum and adaptive learning rate for the first iterations:</p>
<p><span class="math display">\[
\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    v^{(t)} &amp; = \beta_2 v^{(t-1)} + (1 - \beta_2)\cdot \left(g^{(t)}\right)^2\\
    \hat{m}^{(t)} &amp; = \frac{m^{(t)}}{1-\beta^{t}_1}, \hat{v}^{(t)} = \frac{v^{(t)}}{1-\beta^{t}_2}\\
    w^{(t)} &amp; = w^{(t-1)} - \frac{\eta}{\sqrt{\hat{v}^{(t)}} + \epsilon}\circ \hat{m}^{(t)}\\
\end{split}
\]</span></p>
<p>Epsilon is a small constant used to improve numerical stability for very small gradient norms. <!-- Remember that the adaptive learning rate does not replace the learning rate hyperparameter $\eta$, but rather acts as an extra factor and ensures that the gradients of various parameters have a similar norm. --></p>
</div>
</div>
</div>
</section>
<section id="optimization-7" class="slide level2">
<h2>Optimization</h2>
<div id="5d44d2b8" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="kw">class</span> Adam(OptimizerTemplate):</span>
<span id="cb38-2"><a></a>    </span>
<span id="cb38-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.999</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb38-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr)</span>
<span id="cb38-5"><a></a>        <span class="va">self</span>.beta1 <span class="op">=</span> beta1</span>
<span id="cb38-6"><a></a>        <span class="va">self</span>.beta2 <span class="op">=</span> beta2</span>
<span id="cb38-7"><a></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb38-8"><a></a>        <span class="va">self</span>.param_step <span class="op">=</span> {p: <span class="dv">0</span> <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params} <span class="co"># Remembers "t" for each parameter for bias correction</span></span>
<span id="cb38-9"><a></a>        <span class="va">self</span>.param_momentum <span class="op">=</span> {p: torch.zeros_like(p.data) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params}</span>
<span id="cb38-10"><a></a>        <span class="va">self</span>.param_2nd_momentum <span class="op">=</span> {p: torch.zeros_like(p.data) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params}</span>
<span id="cb38-11"><a></a>        </span>
<span id="cb38-12"><a></a>    <span class="kw">def</span> update_param(<span class="va">self</span>, p):</span>
<span id="cb38-13"><a></a>        <span class="va">self</span>.param_step[p] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-14"><a></a>        </span>
<span id="cb38-15"><a></a>        <span class="va">self</span>.param_momentum[p] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1) <span class="op">*</span> p.grad <span class="op">+</span> <span class="va">self</span>.beta1 <span class="op">*</span> <span class="va">self</span>.param_momentum[p]</span>
<span id="cb38-16"><a></a>        <span class="va">self</span>.param_2nd_momentum[p] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2) <span class="op">*</span> (p.grad)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="va">self</span>.beta2 <span class="op">*</span> <span class="va">self</span>.param_2nd_momentum[p]</span>
<span id="cb38-17"><a></a>        </span>
<span id="cb38-18"><a></a>        bias_correction_1 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1 <span class="op">**</span> <span class="va">self</span>.param_step[p]</span>
<span id="cb38-19"><a></a>        bias_correction_2 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2 <span class="op">**</span> <span class="va">self</span>.param_step[p]</span>
<span id="cb38-20"><a></a>        </span>
<span id="cb38-21"><a></a>        p_2nd_mom <span class="op">=</span> <span class="va">self</span>.param_2nd_momentum[p] <span class="op">/</span> bias_correction_2</span>
<span id="cb38-22"><a></a>        p_mom <span class="op">=</span> <span class="va">self</span>.param_momentum[p] <span class="op">/</span> bias_correction_1</span>
<span id="cb38-23"><a></a>        p_lr <span class="op">=</span> <span class="va">self</span>.lr <span class="op">/</span> (torch.sqrt(p_2nd_mom) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb38-24"><a></a>        p_update <span class="op">=</span> <span class="op">-</span>p_lr <span class="op">*</span> p_mom</span>
<span id="cb38-25"><a></a>        </span>
<span id="cb38-26"><a></a>        p.add_(p_update)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-8" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Comparison</strong></p>
</div>
<div class="callout-content">
<div class="panel-tabset">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1">Model creation</a></li><li><a href="#tabset-2-2">SGD</a></li><li><a href="#tabset-2-3">Momentum</a></li><li><a href="#tabset-2-4">Adam</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1">
<div id="2373a9d0" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a>base_model <span class="op">=</span> BaseNetwork(act_fn<span class="op">=</span>nn.ReLU(), hidden_sizes<span class="op">=</span>[<span class="dv">512</span>,<span class="dv">256</span>,<span class="dv">256</span>,<span class="dv">128</span>])</span>
<span id="cb39-2"><a></a>kaiming_init(base_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2">
<div id="4241ac3c" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a>SGD_model <span class="op">=</span> copy.deepcopy(base_model).to(device)</span>
<span id="cb40-2"><a></a>SGD_results <span class="op">=</span> train_model(SGD_model, <span class="st">"FashionMNIST_SGD"</span>, </span>
<span id="cb40-3"><a></a>                          <span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">1e-1</span>), </span>
<span id="cb40-4"><a></a>                          max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file of "FashionMNIST_SGD" already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-28-output-2.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 89.09% ==============
</code></pre>
</div>
</div>
</div>
<div id="tabset-2-3">
<div id="60527f1e" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a></a>SGDMom_model <span class="op">=</span> copy.deepcopy(base_model).to(device)</span>
<span id="cb43-2"><a></a>SGDMom_results <span class="op">=</span> train_model(SGDMom_model, <span class="st">"FashionMNIST_SGDMom"</span>, </span>
<span id="cb43-3"><a></a>                             <span class="kw">lambda</span> params: SGDMomentum(params, lr<span class="op">=</span><span class="fl">1e-1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>), </span>
<span id="cb43-4"><a></a>                             max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file of "FashionMNIST_SGDMom" already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-29-output-2.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 88.83% ==============
</code></pre>
</div>
</div>
</div>
<div id="tabset-2-4">
<div id="a7ac3481" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a></a>Adam_model <span class="op">=</span> copy.deepcopy(base_model).to(device)</span>
<span id="cb46-2"><a></a>Adam_results <span class="op">=</span> train_model(Adam_model, <span class="st">"FashionMNIST_Adam"</span>, </span>
<span id="cb46-3"><a></a>                           <span class="kw">lambda</span> params: Adam(params, lr<span class="op">=</span><span class="fl">1e-3</span>), </span>
<span id="cb46-4"><a></a>                           max_epochs<span class="op">=</span><span class="dv">40</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model file of "FashionMNIST_Adam" already exists. Skipping training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-30-output-2.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>============= Test accuracy: 89.46% ==============
</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-9" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Challenge 1: Pathological curvatures</strong></p>
</div>
<div class="callout-content">
<p>A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization.</p>
<p>In other words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum.</p>
</div>
</div>
</div>
<div id="edba98af" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a></a><span class="kw">def</span> pathological_curve_loss(w1, w2):</span>
<span id="cb49-2"><a></a>    <span class="co"># Example of a pathological curvature. There are many more possible, feel free to experiment here!</span></span>
<span id="cb49-3"><a></a>    x1_loss <span class="op">=</span> torch.tanh(w1)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> torch.<span class="bu">abs</span>(w1)</span>
<span id="cb49-4"><a></a>    x2_loss <span class="op">=</span> torch.sigmoid(w2)</span>
<span id="cb49-5"><a></a>    <span class="cf">return</span> x1_loss <span class="op">+</span> x2_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="46e00319" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a></a><span class="kw">def</span> plot_curve(curve_fn, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), plot_3d<span class="op">=</span><span class="va">False</span>, cmap<span class="op">=</span>cm.viridis, title<span class="op">=</span><span class="st">"Pathological curvature"</span>):</span>
<span id="cb50-2"><a></a>    fig <span class="op">=</span> plt.figure()</span>
<span id="cb50-3"><a></a>    ax <span class="op">=</span> plt.axes(projection<span class="op">=</span><span class="st">'3d'</span>) <span class="cf">if</span> plot_3d <span class="cf">else</span> plt.axes()</span>
<span id="cb50-4"><a></a>    </span>
<span id="cb50-5"><a></a>    x <span class="op">=</span> torch.arange(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], (x_range[<span class="dv">1</span>]<span class="op">-</span>x_range[<span class="dv">0</span>])<span class="op">/</span><span class="fl">100.</span>)</span>
<span id="cb50-6"><a></a>    y <span class="op">=</span> torch.arange(y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>], (y_range[<span class="dv">1</span>]<span class="op">-</span>y_range[<span class="dv">0</span>])<span class="op">/</span><span class="fl">100.</span>)</span>
<span id="cb50-7"><a></a>    x, y <span class="op">=</span> torch.meshgrid(x, y, indexing<span class="op">=</span><span class="st">'xy'</span>)</span>
<span id="cb50-8"><a></a>    z <span class="op">=</span> curve_fn(x, y)</span>
<span id="cb50-9"><a></a>    x, y, z <span class="op">=</span> x.numpy(), y.numpy(), z.numpy()</span>
<span id="cb50-10"><a></a>    </span>
<span id="cb50-11"><a></a>    <span class="cf">if</span> plot_3d:</span>
<span id="cb50-12"><a></a>        ax.plot_surface(x, y, z, cmap<span class="op">=</span>cmap, linewidth<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">"#000"</span>, antialiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-13"><a></a>        ax.set_zlabel(<span class="st">"loss"</span>)</span>
<span id="cb50-14"><a></a>    <span class="cf">else</span>:</span>
<span id="cb50-15"><a></a>        ax.imshow(z[::<span class="op">-</span><span class="dv">1</span>], cmap<span class="op">=</span>cmap, extent<span class="op">=</span>(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>]))</span>
<span id="cb50-16"><a></a>    plt.title(title)</span>
<span id="cb50-17"><a></a>    ax.set_xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">w_1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb50-18"><a></a>    ax.set_ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">w_2</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb50-19"><a></a>    plt.tight_layout()</span>
<span id="cb50-20"><a></a>    <span class="cf">return</span> ax</span>
<span id="cb50-21"><a></a></span>
<span id="cb50-22"><a></a>sns.reset_orig()</span>
<span id="cb50-23"><a></a>_ <span class="op">=</span> plot_curve(pathological_curve_loss, plot_3d<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-24"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-10" class="slide level2">
<h2>Optimization</h2>

<img data-src="dl_lec6_files/figure-revealjs/cell-33-output-1.svg" class="r-stretch"></section>
<section id="optimization-11" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Discussion</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of <span class="math inline">\(w_2\)</span>.</li>
<li>However, if we encounter a point along the ridges, the gradient is much greater in <span class="math inline">\(w_1\)</span> than <span class="math inline">\(w_2\)</span>, and we might end up jumping from one side to the other.</li>
<li>Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="optimization-12" class="slide level2">
<h2>Optimization</h2>
<div id="be7bfd85" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a></a><span class="kw">def</span> train_curve(optimizer_func, curve_func<span class="op">=</span>pathological_curve_loss, num_updates<span class="op">=</span><span class="dv">100</span>, init<span class="op">=</span>[<span class="dv">5</span>,<span class="dv">5</span>]):</span>
<span id="cb51-2"><a></a>    <span class="co">"""</span></span>
<span id="cb51-3"><a></a><span class="co">    Inputs:</span></span>
<span id="cb51-4"><a></a><span class="co">        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list</span></span>
<span id="cb51-5"><a></a><span class="co">        curve_func - Loss function (e.g. pathological curvature)</span></span>
<span id="cb51-6"><a></a><span class="co">        num_updates - Number of updates/steps to take when optimizing </span></span>
<span id="cb51-7"><a></a><span class="co">        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2</span></span>
<span id="cb51-8"><a></a><span class="co">    Outputs:</span></span>
<span id="cb51-9"><a></a><span class="co">        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.</span></span>
<span id="cb51-10"><a></a><span class="co">    """</span></span>
<span id="cb51-11"><a></a>    weights <span class="op">=</span> nn.Parameter(torch.FloatTensor(init), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-12"><a></a>    optimizer <span class="op">=</span> optimizer_func([weights])</span>
<span id="cb51-13"><a></a>    </span>
<span id="cb51-14"><a></a>    list_points <span class="op">=</span> []</span>
<span id="cb51-15"><a></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_updates):</span>
<span id="cb51-16"><a></a>        loss <span class="op">=</span> curve_func(weights[<span class="dv">0</span>], weights[<span class="dv">1</span>])</span>
<span id="cb51-17"><a></a>        list_points.append(torch.cat([weights.data.detach(), loss.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>).detach()], dim<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb51-18"><a></a>        optimizer.zero_grad()</span>
<span id="cb51-19"><a></a>        loss.backward()</span>
<span id="cb51-20"><a></a>        optimizer.step()</span>
<span id="cb51-21"><a></a>    points <span class="op">=</span> torch.stack(list_points, dim<span class="op">=</span><span class="dv">0</span>).numpy()</span>
<span id="cb51-22"><a></a>    <span class="cf">return</span> points</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-13" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Train</strong></p>
</div>
<div class="callout-content">
<div id="0e6248b4" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a></a>SGD_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb52-2"><a></a>SGDMom_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: SGDMomentum(params, lr<span class="op">=</span><span class="dv">10</span>, momentum<span class="op">=</span><span class="fl">0.9</span>))</span>
<span id="cb52-3"><a></a>Adam_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: Adam(params, lr<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-14" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualize</strong></p>
</div>
<div class="callout-content">
<div id="e04bd0de" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a>all_points <span class="op">=</span> np.concatenate([SGD_points, SGDMom_points, Adam_points], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb53-2"><a></a>ax <span class="op">=</span> plot_curve(pathological_curve_loss,</span>
<span id="cb53-3"><a></a>                x_range<span class="op">=</span>(<span class="op">-</span>np.absolute(all_points[:,<span class="dv">0</span>]).<span class="bu">max</span>(), np.absolute(all_points[:,<span class="dv">0</span>]).<span class="bu">max</span>()),</span>
<span id="cb53-4"><a></a>                y_range<span class="op">=</span>(all_points[:,<span class="dv">1</span>].<span class="bu">min</span>(), all_points[:,<span class="dv">1</span>].<span class="bu">max</span>()),</span>
<span id="cb53-5"><a></a>                plot_3d<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb53-6"><a></a>ax.plot(SGD_points[:,<span class="dv">0</span>], SGD_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"SGD"</span>)</span>
<span id="cb53-7"><a></a>ax.plot(SGDMom_points[:,<span class="dv">0</span>], SGDMom_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"SGDMom"</span>)</span>
<span id="cb53-8"><a></a>ax.plot(Adam_points[:,<span class="dv">0</span>], Adam_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"grey"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"Adam"</span>)</span>
<span id="cb53-9"><a></a>plt.legend()</span>
<span id="cb53-10"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec6_files/figure-revealjs/cell-36-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-15" class="slide level2 scrollable">
<h2>Optimization</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Challenge 2: Steep optima</strong></p>
</div>
<div class="callout-content">
<p>A second type of challenging loss surfaces are steep optima. In those, we have a larger part of the surface having very small gradients while around the optimum, we have very large gradients.</p>
<p><strong>An adaptive learning rate becomes crucial.</strong></p>
</div>
</div>
</div>
<div id="ad4ccbd3" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a></a><span class="kw">def</span> bivar_gaussian(w1, w2, x_mean<span class="op">=</span><span class="fl">0.0</span>, y_mean<span class="op">=</span><span class="fl">0.0</span>, x_sig<span class="op">=</span><span class="fl">1.0</span>, y_sig<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb54-2"><a></a>    norm <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> x_sig <span class="op">*</span> y_sig)</span>
<span id="cb54-3"><a></a>    x_exp <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> (w1 <span class="op">-</span> x_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> x_sig<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb54-4"><a></a>    y_exp <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> (w2 <span class="op">-</span> y_mean)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> y_sig<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb54-5"><a></a>    <span class="cf">return</span> norm <span class="op">*</span> torch.exp(x_exp <span class="op">+</span> y_exp)</span>
<span id="cb54-6"><a></a></span>
<span id="cb54-7"><a></a><span class="kw">def</span> comb_func(w1, w2):</span>
<span id="cb54-8"><a></a>    z <span class="op">=</span> <span class="op">-</span>bivar_gaussian(w1, w2, x_mean<span class="op">=</span><span class="fl">1.0</span>, y_mean<span class="op">=-</span><span class="fl">0.5</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb54-9"><a></a>    z <span class="op">-=</span> bivar_gaussian(w1, w2, x_mean<span class="op">=-</span><span class="fl">1.0</span>, y_mean<span class="op">=</span><span class="fl">0.5</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb54-10"><a></a>    z <span class="op">-=</span> bivar_gaussian(w1, w2, x_mean<span class="op">=-</span><span class="fl">0.5</span>, y_mean<span class="op">=-</span><span class="fl">0.8</span>, x_sig<span class="op">=</span><span class="fl">0.2</span>, y_sig<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb54-11"><a></a>    <span class="cf">return</span> z</span>
<span id="cb54-12"><a></a></span>
<span id="cb54-13"><a></a>_ <span class="op">=</span> plot_curve(comb_func, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), plot_3d<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">"Steep optima"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-16" class="slide level2">
<h2>Optimization</h2>

<img data-src="dl_lec6_files/figure-revealjs/cell-38-output-1.svg" class="r-stretch"></section>
<section id="optimization-17" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Train</strong></p>
</div>
<div class="callout-content">
<div id="8243e69e" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a></a>SGD_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">.5</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb55-2"><a></a>SGDMom_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: SGDMomentum(params, lr<span class="op">=</span><span class="dv">1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb55-3"><a></a>Adam_points <span class="op">=</span> train_curve(<span class="kw">lambda</span> params: Adam(params, lr<span class="op">=</span><span class="fl">0.2</span>), comb_func, init<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb55-4"><a></a></span>
<span id="cb55-5"><a></a>all_points <span class="op">=</span> np.concatenate([SGD_points, SGDMom_points, Adam_points], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb55-6"><a></a>ax <span class="op">=</span> plot_curve(comb_func,</span>
<span id="cb55-7"><a></a>                x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb55-8"><a></a>                y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb55-9"><a></a>                plot_3d<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb55-10"><a></a>                title<span class="op">=</span><span class="st">"Steep optima"</span>)</span>
<span id="cb55-11"><a></a>ax.plot(SGD_points[:,<span class="dv">0</span>], SGD_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"SGD"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb55-12"><a></a>ax.plot(SGDMom_points[:,<span class="dv">0</span>], SGDMom_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"SGDMom"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb55-13"><a></a>ax.plot(Adam_points[:,<span class="dv">0</span>], Adam_points[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">"grey"</span>, marker<span class="op">=</span><span class="st">"o"</span>, zorder<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"Adam"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb55-14"><a></a>ax.set_xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb55-15"><a></a>ax.set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb55-16"><a></a>plt.legend()</span>
<span id="cb55-17"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="optimization-18" class="slide level2">
<h2>Optimization</h2>

<img data-src="dl_lec6_files/figure-revealjs/cell-40-output-1.svg" class="r-stretch"></section>
<section id="optimization-19" class="slide level2">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>SGD vs Adam</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/flat_vs_sharp_minima.svg" height="400"></p>
</div>
</div>
</div>
</section>
<section id="optimization-20" class="slide level2 smaller">
<h2>Optimization</h2>
<p><span class="math display">\[
f(x,y) = x^2 + y^2 -ae^{-\frac{(x-1)^2+y^2}{c}} -be^{-\frac{(x+1)^2+y^2}{c}}
\]</span></p>
<iframe width="1920" height="1080" src="./resources/dl_optimization.html" title="Optimization"></iframe>
</section>
<section id="optimization-21" class="slide level2 smaller">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rastrigin function</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
f(x,y) = An + x^2- Acos(2\pi x) + y^2 - Acos(2\pi y), \, A=10, \; x,y \in [-5.12,5.12]
\]</span></p>
</div>
</div>
</div>
<iframe width="1920" height="1080" src="./resources/dl_rastrigin.html" title="Optimization"></iframe>
</section>
<section id="optimization-22" class="slide level2 smaller">
<h2>Optimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rosenbrock function</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
f(x,y) = (a-x)^2 + b(y-x^2)^2
\]</span></p>
</div>
</div>
</div>
<iframe width="1920" height="1080" src="./resources/dl_rosenbrock.html" title="Optimization"></iframe>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"06347abe4efa760e6afd7b47aca27942191e25a3998dfd45bc03d4e1f2d5e4df"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>