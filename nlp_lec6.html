<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="MSDE">
  <title>Deep Learning/NLP course – CNNs for NLP</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CNNs for NLP</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
MSDE 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="cnns-for-nlp" class="slide level2">
<h2>CNNs for NLP</h2>
<div class="hidden">

</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Comparison to image processing</strong></p>
</div>
<div class="callout-content">
<ul>
<li>instead of image pixels - sentences or documents represented as a matrix</li>
<li>these vectors can either be word embeddings like word2vec or GloVe</li>
<li>or one-hot vectors</li>
</ul>
<p>For a 10 word sentence using a 100-dimensional embedding we would have a 10×100 matrix as our input - this would be the “image”.</p>
</div>
</div>
</div>
</section>
<section id="comparison-to-image-processing-1" class="slide level2">
<h2>Comparison to image processing</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Filters</strong></p>
</div>
<div class="callout-content">
<ul>
<li>In vision, our filters slide over local patches of an image, but in NLP we typically use filters that slide over full rows of the matrix (words).<br>
</li>
<li>Thus, the “width” of our filters is usually the same as the width of the input matrix.</li>
<li>The height, or region size, may vary, but sliding windows over 2-5 words at a time is typical.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="diagram" class="slide level2">
<h2>Diagram</h2>

<img data-src="img/cnn_nlp_diagram.png" class="r-stretch"></section>
<section id="diagram-1" class="slide level2">
<h2>Diagram</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<ul>
<li>three filter region sizes: 2, 3 and 4, each of which has 2 filters.</li>
<li>each filter generates variable-length feature maps</li>
<li>Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded.</li>
<li>Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer.</li>
<li>The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="comparison-to-image-processing-2" class="slide level2">
<h2>Comparison to image processing</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Intuitions broken ?</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Location invariance</li>
<li>Compositionality</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Benefits</strong></p>
</div>
<div class="callout-content">
<p>A big argument for CNNs is that they are fast. <strong>Very fast</strong>.</p>
</div>
</div>
</div>
</section>
<section id="applications" class="slide level2">
<h2>Applications</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Classification tasks</strong></p>
</div>
<div class="callout-content">
<p><strong><em>Good idea:</em></strong></p>
<ul>
<li>Sentiment Analysis</li>
<li>Spam Detection</li>
<li>Topic Categorization</li>
</ul>
</div>
</div>
</div>
</section>
<section id="applications-1" class="slide level2">
<h2>Applications</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Order of words lost</strong></p>
</div>
<div class="callout-content">
<p><strong><em>Bad idea (unless you do it right):</em></strong></p>
<ul>
<li>Sequence Tagging</li>
<li>PoS Tagging</li>
<li>Entity Extraction (with a caveat)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Comparison with transformers</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>PaLM</strong>: 540B parameters</li>
<li><strong>GPT-3</strong>: 175B parameters</li>
<li><strong>T5-11B</strong>: 11B parameters (FOSS, outperforms GPT-3)</li>
<li><strong>GPT-J</strong>: 6B parameters (FOSS, outperforms GPT-3)</li>
<li><strong>CNNs</strong>: less than 200k parameters</li>
</ul>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-1" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>CNN benefits</strong></p>
</div>
<div class="callout-content">
<p>The main advantage of CNNs over previous NLP algorithms is that</p>
<ul>
<li>they can recognize patterns in text no matter where those patterns occur in the text (<strong>translation invariance</strong>)</li>
<li>and how spread out they are (<strong>scale invariance</strong>).<br>
</li>
</ul>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-2" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Comparison to other NLP methods</strong></p>
</div>
<div class="callout-content">
<ul>
<li>TF-IDF: don’t recognize and generalize from text patterns</li>
<li>fully-connected NNs: over-generalize from particular patterns at particular locations</li>
<li>RNNs:
<ul>
<li>Cognitively plausible</li>
<li>not best for classification (if just use last state),</li>
<li>much slower than CNNs</li>
<li>good for sequence tagging and classification</li>
<li>great for language models</li>
<li>can be amazing with attention mechanisms</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-3" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stencil example</strong></p>
</div>
<div class="callout-content">
<p><img data-src="img/stencil_example.png" height="500"></p>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-4" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Considerations</strong></p>
</div>
<div class="callout-content">
<ul>
<li>We don’t need to program the kernels - just decide their width.</li>
<li>CNN optimizer will calculate weights within the kernel</li>
<li>by matching the patterns that are most predictive of the target variable.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-5" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>What does CNN do with a kernel?</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li>Measure similarity between kernel and text (<strong>dot product</strong>)</li>
<li>Find the max value of kernel match by <strong>sliding</strong> through textl</li>
<li>Convert max value to a probability using activation function (<strong>max pooling</strong>)</li>
</ol>
<p>Convolution is steps 1 and 2.</p>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-6" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>convolve() in Python</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">def</span> convolve(inpt, kernel):</span>
<span id="cb1-2"><a></a>  output <span class="op">=</span> []</span>
<span id="cb1-3"><a></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(inpt) <span class="op">-</span> <span class="bu">len</span>(kernel) <span class="op">+</span> <span class="dv">1</span>):  <span class="co"># #1</span></span>
<span id="cb1-4"><a></a>      output.append(</span>
<span id="cb1-5"><a></a>          <span class="bu">sum</span>(</span>
<span id="cb1-6"><a></a>              [</span>
<span id="cb1-7"><a></a>                  inpt[i <span class="op">+</span> k] <span class="op">*</span> kernel[k]</span>
<span id="cb1-8"><a></a>                  <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(kernel))  <span class="co"># #2</span></span>
<span id="cb1-9"><a></a>              ]))</span>
<span id="cb1-10"><a></a>  <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-7" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %d2l.ai --> <img data-src="img/cnn_1dim_cross_correlation.png"></p>
</section>
<section id="cnns-in-nlp-8" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/1d_conv_for_text.png" height="600"></p>
</section>
<section id="cnns-in-nlp-9" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/1d_conv_for_text_with_padding.png" height="600" alt="With padding"></p>
</section>
<section id="cnns-in-nlp-10" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/3chan_conv_for_text.png" alt="3 channels"></p>
</section>
<section id="cnns-in-nlp-11" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/1d_conv_for_text_max_pooling.png" alt="Max pooling"></p>
</section>
<section id="cnns-in-nlp-12" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>PyTorch example</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb2-2"><a></a>word_embed_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-3"><a></a>seq_len <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb2-4"><a></a><span class="bu">input</span> <span class="op">=</span> torch.randn(batch_size, word_embed_size, seq_len)</span>
<span id="cb2-5"><a></a>conv1 <span class="op">=</span> Conv1d(in_channels<span class="op">=</span>word_embed_size, out_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">3</span>) <span class="co"># can add: padding=1</span></span>
<span id="cb2-6"><a></a>hidden1 <span class="op">=</span> conv1(<span class="bu">input</span>)</span>
<span id="cb2-7"><a></a>hidden2 <span class="op">=</span> torch.<span class="bu">max</span>(hidden1, dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># max pool</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-13" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/1d_conv_stride.png" alt="Stride"></p>
</section>
<section id="cnns-in-nlp-14" class="slide level2">
<h2>CNNs in NLP</h2>
<p><!-- %Stanford lecture --> <img data-src="img/1d_conv_dilation.png" alt="Dilation = 2"></p>
</section>
<section>
<section id="pytorch-1-d-cnn-on-4-d-embedding-vectors" class="title-slide slide level1 center">
<h1>PyTorch 1-D CNN on 4-D embedding vectors</h1>

</section>
<section id="cnns-in-nlp-15" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Manual kernel</strong></p>
</div>
<div class="callout-content">
<p>Let’s start with a manual kernel first.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb3-2"><a></a><span class="im">from</span> nltk.tag <span class="im">import</span> pos_tag</span>
<span id="cb3-3"><a></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a></a></span>
<span id="cb3-5"><a></a>tags <span class="op">=</span> <span class="st">'ADV ADJ VERB NOUN'</span>.split()</span>
<span id="cb3-6"><a></a>quote <span class="op">=</span> <span class="st">'The right word may be effective, but no word was ever as effective as a rightly timed pause.'</span></span>
<span id="cb3-7"><a></a>tokens <span class="op">=</span> pos_tag(word_tokenize(quote), tagset<span class="op">=</span><span class="st">'universal'</span>)</span>
<span id="cb3-8"><a></a>tagged_words <span class="op">=</span> [[word] <span class="op">+</span> [<span class="bu">int</span>(tag <span class="op">==</span> t) <span class="cf">for</span> t <span class="kw">in</span> tags] <span class="cf">for</span> word, tag <span class="kw">in</span> tokens]</span>
<span id="cb3-9"><a></a></span>
<span id="cb3-10"><a></a>df <span class="op">=</span> pd.DataFrame(tagged_words, columns<span class="op">=</span>[<span class="st">'token'</span>] <span class="op">+</span> tags).T</span>
<span id="cb3-11"><a></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-16" class="slide level2 smaller">
<h2>CNNs in NLP</h2>
<pre><code>            0      1     2    3   4          5  6    7   8     9    10    11  12  \
    token  The  right  word  may  be  effective  ,  but  no  word  was  ever  as   
    ADV      0      0     0    0   0          0  0    0   0     0    0     1   1   
    ADJ      0      1     0    0   0          1  0    0   0     0    0     0   0   
    VERB     0      0     0    1   1          0  0    0   0     0    1     0   0   
    NOUN     0      0     1    0   0          0  0    0   0     1    0     0   0   

                  13  14 15       16     17     18 19  
    token  effective  as  a  rightly  timed  pause  .  
    ADV            0   0  0        1      0      0  0  
    ADJ            1   0  0        0      0      0  0  
    VERB           0   0  0        0      1      0  0  
    NOUN           0   0  0        0      0      1  0  </code></pre>
</section>
<section id="cnns-in-nlp-17" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tensor</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a></a>x <span class="op">=</span> torch.tensor(</span>
<span id="cb5-3"><a></a>      df.iloc[<span class="dv">1</span>:].astype(<span class="bu">float</span>).values,</span>
<span id="cb5-4"><a></a>      dtype<span class="op">=</span>torch.float32)  <span class="co"># #1</span></span>
<span id="cb5-5"><a></a>x <span class="op">=</span> x.unsqueeze(<span class="dv">0</span>) <span class="co"># #2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pattern</strong></p>
</div>
<div class="callout-content">
<p>Now you construct that pattern that we want to search for in the text: adverb, verb, then noun.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>kernel <span class="op">=</span> pd.DataFrame(</span>
<span id="cb6-2"><a></a>[[<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.</span>],</span>
<span id="cb6-3"><a></a> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.</span>],</span>
<span id="cb6-4"><a></a> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.</span>],</span>
<span id="cb6-5"><a></a> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">1.</span>]], index<span class="op">=</span>tags)</span>
<span id="cb6-6"><a></a><span class="bu">print</span>(kernel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-18" class="slide level2">
<h2>CNNs in NLP</h2>

<img data-src="img/1d_cnn_example.png" class="r-stretch"></section>
<section id="cnns-in-nlp-19" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>kernel <span class="op">=</span> torch.tensor(kernel.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb7-2"><a></a>kernel <span class="op">=</span> kernel.unsqueeze(<span class="dv">0</span>)  <span class="co"># #1</span></span>
<span id="cb7-3"><a></a>conv <span class="op">=</span> torch.nn.Conv1d(in_channels<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb7-4"><a></a>                     out_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-5"><a></a>                     kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb7-6"><a></a>                     bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-7"><a></a>conv.load_state_dict({<span class="st">'weight'</span>: kernel})</span>
<span id="cb7-8"><a></a><span class="bu">print</span>(conv.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="cnns-in-nlp-20" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a></a></span>
<span id="cb8-3"><a></a>y <span class="op">=</span> np.array(conv.forward(x).detach()).squeeze()</span>
<span id="cb8-4"><a></a>df.loc[<span class="st">'y'</span>] <span class="op">=</span> pd.Series(y)</span>
<span id="cb8-5"><a></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="cnns-in-nlp-21" class="slide level2">
<h2>CNNs in NLP</h2>

<img data-src="img/1d_cnn_example_output.png" class="r-stretch quarto-figure-center"><p class="caption">The y value reaches a maximum value of 3 where all 3 values of 1 in the kernel line up perfectly with the three 1’s forming the same pattern within the part-of-speech tags for the sentence.</p></section></section>
<section>
<section id="embeddings-a-recap" class="title-slide slide level1 center">
<h1>Embeddings: a recap</h1>

</section>
<section id="embeddings-in-pytorch" class="slide level2">
<h2>Embeddings in PyTorch</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>nn.Embedding</strong></p>
</div>
<div class="callout-content">
<p>The nn.Embedding layer is a simple lookup table that maps an index value to a weight matrix of a certain dimension.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<ul>
<li>During the training the parameters of the nn.Embedding layer in a neural network are adjusted in order to optimize the performance of the model.</li>
<li>Vectors are optimised to represent the meaning or context of the input tokens in relation to the task the model is trained for (e.g.&nbsp;text generation, language translation).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="embeddings" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameters</strong></p>
</div>
<div class="callout-content">
<p>The nn.Embedding layer takes in two arguments:</p>
<ul>
<li>vocabulary size</li>
<li>size of encoded representation</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-3"><a></a></span>
<span id="cb9-4"><a></a><span class="co"># Define the embedding layer with 10 vocab size and 50 vector embeddings.</span></span>
<span id="cb9-5"><a></a>embedding <span class="op">=</span> nn.Embedding(<span class="dv">10</span>, <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<ul>
<li>embedding lookup table shape is <span class="math inline">\((10,50)\)</span>.</li>
<li>each row is initialized with <code>torch.nn.init.uniform_()</code></li>
<li>weights are initialized with random values between -1 and 1.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="embeddings-1" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How to check a particular embedding</strong></p>
</div>
<div class="callout-content">
<p>To examine the embeddings for a given word (eg. first word in the table), run:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>embedding(torch.LongTensor([<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Output is a vector of size 50: <img data-src="img/embedding_sample.png"></p>
</div>
</div>
</div>
</section>
<section id="embeddings-2" class="slide level2">
<h2>Embeddings</h2>

<img data-src="img/cnn_embedding_dims.png" class="r-stretch"></section>
<section id="embeddings-3" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Initialization</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Normal</strong>: initializes the weights with random values drawn from a normal distribution with a mean of 0 and a standard deviation of 1. It is also known as Gaussian initialization.</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>nn.init.normal_(embedding.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Constant</strong>: initializes the weights with a specific constant value.</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>nn.init.constant_(embedding.weight, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="embeddings-4" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Initialization</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Xavier</strong>: based on the work of Xavier Glorot and Yoshua Bengio, and they are designed to work well with sigmoid and tanh activation functions. They initialize the weights to values that are close to zero, but not too small.</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>nn.init.xavier_uniform_(embedding.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Kaiming</strong>: based on the work of He et al., and they are designed to work well with ReLU and its variants (LeakyReLU, PReLU, RReLU, etc.). They also initialize the weights to values that are close to zero, but not too small.</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>nn.init.kaiming_normal_(embedding.weight, nonlinearity<span class="op">=</span><span class="st">'leaky_relu'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Pre-trained</strong>: pre-trained word vectors such as GloVe or word2vec, which have been trained on large corpora and have been shown to be useful for many natural language processing tasks. The process of using a pre-trained word vectors is called <strong>fine-tuning</strong>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="embeddings-5" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pre-trained embeddings: advantages</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Improve model performance</strong>: provide the model with a good set of initial weights that capture the meaning of words.</li>
<li><strong>Save computation time and resources</strong>: embeddings have already been learned on a large corpus.</li>
<li><strong>Transfer learning</strong>: pre-trained word embeddings can be used for transfer learning, which means that you can use the embeddings learned on one task as a starting point for a different but related task.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="embeddings-6" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pre-trained embeddings example</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-3"><a></a></span>
<span id="cb15-4"><a></a><span class="co"># Load a pre-trained embedding model</span></span>
<span id="cb15-5"><a></a>pretrained_embeddings <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">50</span>) <span class="co"># Example only, not actual pre-trained embeddings</span></span>
<span id="cb15-6"><a></a></span>
<span id="cb15-7"><a></a><span class="co"># Initialize the embedding layer with the pre-trained embeddings</span></span>
<span id="cb15-8"><a></a>embedding.weight.data.copy_(pretrained_embeddings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>OR:</strong></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a>embedding_layer <span class="op">=</span> nn.Embedding.from_pretrained(pretrained_embeddings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="embeddings-7" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Use pre-trained embeddings from popular libraries like GloVe or fastText:</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a><span class="im">import</span> torchtext</span>
<span id="cb17-2"><a></a></span>
<span id="cb17-3"><a></a><span class="co"># Load pre-trained GloVe embeddings</span></span>
<span id="cb17-4"><a></a>glove <span class="op">=</span> torchtext.vocab.GloVe(name<span class="op">=</span><span class="st">'6B'</span>, dim<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb17-5"><a></a>embedding_layer <span class="op">=</span> nn.Embedding.from_pretrained(glove.vectors)   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="embeddings-8" class="slide level2">
<h2>Embeddings</h2>

<img data-src="img/pytorch_pretrained.png" class="r-stretch"></section>
<section id="embeddings-9" class="slide level2">
<h2>Embeddings</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Freezing</strong></p>
</div>
<div class="callout-content">
<p>In some cases when performing transfer learning, you may need to freeze the pre-trained embeddings during training process, so that they are not updated during the backpropagation step and only the last dense layer is updated. To do this:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a>embedding_layer.weight.requiresGrad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="cnns-in-nlp-22" class="slide level2">
<h2>CNNs in NLP</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cnn_word_embeddings.png" height="550"></p>
<figcaption>torcn.nn.Embedding example</figcaption>
</figure>
</div>
</section></section>
<section>
<section id="kim-paper" class="title-slide slide level1 center">
<h1>Kim paper</h1>

</section>
<section id="kim-paper-1" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Abstract</strong></p>
</div>
<div class="callout-content">
<p>We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.</p>
</div>
</div>
</div>
<!--
% \begin{frame}{Kim paper}
%   Within natural language process- ing, much of the work with deep learning meth- ods has involved learning word vector representa- tions through neural language models (Bengio et al., 2003; Yih et al., 2011; Mikolov et al., 2013) and performing composition over the learned word vectors for classification (Collobert et al., 2011). Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimen- sions. In such dense representations, semantically close words are likewise close—in euclidean or cosine distance—in the lower dimensional vector space.
% \end{frame}

% \begin{frame}{Kim paper}
%   In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. These vectors were trained by Mikolov et al. (2013) on 100 billion words of Google News, and are publicly available.1 We initially keep the word vectors static and learn only the other param- eters of the model. Despite little tuning of hyper- parameters, this simple model achieves excellent results on multiple benchmarks, suggesting that the pre-trained vectors are ‘universal’ feature ex- tractors that can be utilized for various classifica- tion tasks. Learning task-specific vectors through fine-tuning results in further improvements. We finally describe a simple modification to the archi- tecture to allow for the use of both pre-trained and task-specific vectors by having multiple channels.
% \end{frame}

-->
</section>
<section id="kim-paper-2" class="slide level2">
<h2>Kim paper</h2>
<!-- %Evaluates a CNN architecture on various classification datasets, mostly comprised of Sentiment Analysis and Topic Categorization tasks. -->

<img data-src="img/kim_paper_diagram.png" class="r-stretch quarto-figure-center"><p class="caption">The input layer is a sentence comprised of concatenated word2vec word embeddings. That’s followed by a convolutional layer with multiple filters, then a max-pooling layer, and finally a softmax classifier.</p></section>
<section id="kim-paper-3" class="slide level2">
<h2>Kim paper</h2>
<p>Let <span class="math inline">\(\boldsymbol{x}_i \in \textrm{R}^k\)</span> be the <span class="math inline">\(k\)</span>-dimensional word vector corresponding to the <span class="math inline">\(i\)</span>-th word in a sentence.</p>
<p>A sentence of length <span class="math inline">\(n\)</span> (padded when necessary) is represented as <span class="math display">\[
\boldsymbol{x}_{1:n} = \boldsymbol{x}_1 \oplus \boldsymbol{x}_2 \oplus \dots \boldsymbol{x}_n,
\]</span> where <span class="math inline">\(\oplus\)</span> is the concatenation operator.</p>
<p>In general, <span class="math inline">\(\boldsymbol{x}_{i:i+j}\)</span> will refer to the concatenation of words <span class="math inline">\(\boldsymbol{x}_i, \boldsymbol{x}_{i+1}, \dots, \boldsymbol{x}_{i+j}\)</span>.</p>
</section>
<section id="kim-paper-4" class="slide level2">
<h2>Kim paper</h2>
<p>A <strong>convolution</strong> operation involves a <strong>filter</strong> <span class="math inline">\(\boldsymbol{w} \in \textrm{R}^{hk}\)</span>, which is applied to a window of <span class="math inline">\(h\)</span> words to produce a <strong>feature</strong>.</p>

<img data-src="img/cnn_kim_conv1.png" class="r-stretch"></section>
<section id="kim-paper-5" class="slide level2">
<h2>Kim paper</h2>
<p>For example, a feature <span class="math inline">\(c_i\)</span> is generated from a window of words <span class="math inline">\(\boldsymbol{x}_{i:i+h-1}\)</span> by <span class="math display">\[
c_i = f(\boldsymbol{w} \cdot \boldsymbol{x}_{i:i+h-1} + b),
\]</span> where <span class="math inline">\(b \in \textrm{R}\)</span> is a bias term and <span class="math inline">\(f\)</span> is an activation function.</p>
</section>
<section id="kim-paper-6" class="slide level2">
<h2>Kim paper</h2>
<p>This filter is applied to each possible window of words in the sentence <span class="math display">\[
\left\{\boldsymbol{x}_{1:h}, \boldsymbol{x}_{2:h+1}, \dots, \boldsymbol{x}_{n-h+1:n}\right\}
\]</span> to produce a <strong>feature map</strong>: <span class="math display">\[
\boldsymbol{c} = \left[c_1,c_2,\dots,c_{n-h+1}\right], \; c \in \textrm{R}^{n-h+1}.
\]</span> <img data-src="img/cnn_kim_conv2.png" height="200"></p>
</section>
<section id="kim-paper-7" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pooling</strong></p>
</div>
<div class="callout-content">
<p>Apply a max-overtime pooling operation (Collobert et al., 2011) over the feature map and take the maximum value <span class="math display">\[
\hat{\boldsymbol{c}} = \max{\boldsymbol{c}}
\]</span> as the feature corresponding to this particular filter.</p>
<ul>
<li>multiple filters</li>
<li>softmax layer whose output is the probability distribution over labels.</li>
<li>multiple channels: one is kept static, another is fine-tuned via backprop.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="kim-paper-8" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hyperparameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Nonlinearity: ReLU</li>
<li>Window filter sizes h = 3, 4, 5</li>
<li>Each filter size has 100 feature maps</li>
<li>Dropout p = 0.5 (2-4% accuracy improvement)</li>
<li><span class="math inline">\(L_2\)</span> constraint <span class="math inline">\(s=3\)</span> for rows of softmax</li>
<li>Mini batch size for SGD training: 50</li>
<li>Word vectors: pre-trained with word2vec, k = 30</li>
</ul>
</div>
</div>
</div>
</section>
<section id="kim-paper-9" class="slide level2">
<h2>Kim paper</h2>

<img data-src="img/kim_paper_dataset_stats.png" class="r-stretch"></section>
<section id="kim-paper-10" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset descriptions</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>MR</strong>: Movie reviews with one sentence per review. Classification involves detecting positive/negative reviews (Pang and Lee, 2005).</li>
<li><strong>SST-1</strong>: Stanford Sentiment Treebank - an extension of MR but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by Socher et al.&nbsp;(2013).</li>
<li><strong>SST-2</strong>: Same as SST-1 but with neutral reviews removed and binary labels.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="kim-paper-11" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset descriptions</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Subj</strong>: Subjectivity dataset where the task is to classify a sentence as being subjective or objective (Pang and Lee, 2004).</li>
<li><strong>TREC</strong>: TREC question dataset - task involves classifying a question into 6 question types (whether the question is about person, location, numeric information, etc.) (Li and Roth, 2002).</li>
<li><strong>CR</strong>: Customer reviews of various products (cameras, MP3s etc.). Task is to predict positive/negative reviews (Hu and Liu, 2004).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="kim-paper-12" class="slide level2">
<h2>Kim paper</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Variants</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>CNN-rand</strong>: Our baseline model where all words are randomly initialized and then mod- ified during training.</li>
<li><strong>CNN-static</strong>: A model with pre-trained vectors from word2vec. All words— including the unknown ones that are ran- domly initialized—are kept static and only the other parameters of the model are learned.</li>
<li><strong>CNN-non-static</strong>: Same as above but the pre- trained vectors are fine-tuned for each task.</li>
<li><strong>CNN-multichannel</strong>: A model with two sets of word vectors. Each set of vectors is treated as a ‘channel’ and each filter is applied to both channels, but gradients are back-propagated only through one of the channels. Both channels are initialized with word2vec.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="kim-paper-13" class="slide level2">
<h2>Kim paper</h2>

<img data-src="img/kim_paper_results.png" class="r-stretch"></section></section>
<section>
<section id="character-level-networks" class="title-slide slide level1 center">
<h1>Character-level networks</h1>

</section>
<section id="character-level-networks-1" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Zhang, Zhao, Lecun (2015)</strong></p>
</div>
<div class="callout-content">
<ul>
<li>explore text as a kind of raw signal at character level</li>
<li>no knowledge about semantics/syntax required</li>
<li>no knowledge of words required</li>
<li>can work for different languages</li>
<li>misspellings or emoticons may be naturally learnt</li>
</ul>
</div>
</div>
</div>
</section>
<section id="character-level-networks-2" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Design</strong></p>
</div>
<div class="callout-content">
<p>Main component is the temporal convolutional module. Suppose we have a discrete input function <span class="math display">\[
g(x) \in [1,l]\rightarrow \mathrm{R},
\]</span> and a discrete kernel function <span class="math display">\[
f(x) \in [1,k]\rightarrow \textrm{R}.
\]</span></p>
<p>The convolution <span class="math inline">\(h(y) \in [1, \lfloor(l-k+1)/d\rfloor] \rightarrow \mathrm{R}\)</span> between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> with stride <span class="math inline">\(d\)</span> is defined as <span class="math display">\[
h(y) = \sum\limits_{x=1}^k f(x) \cdot g(y\cdot d - x + c),
\]</span> where <span class="math inline">\(c=k-d+1\)</span> is a offset constant.</p>
</div>
</div>
</div>
</section>
<section id="character-level-networks-3" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parametrization</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>Module is parameterized by a set of such kernel functions <span class="math inline">\(f_{ij}(x)\)</span>, where <span class="math inline">\(i=1,2,\dots,m\)</span>, and <span class="math inline">\(j=1,2,\dots,n\)</span> which we call weights, on a set of inputs <span class="math inline">\(g_i(x)\)</span> and outputs <span class="math inline">\(h_j(y)\)</span>.</p></li>
<li><p>We call each <span class="math inline">\(g_i\)</span> (or <span class="math inline">\(h_j\)</span>) input (or output) features, and <span class="math inline">\(m\)</span> (or <span class="math inline">\(n\)</span>) input (or output) feature size.</p></li>
<li><p>Output <span class="math inline">\(h_j(y)\)</span> is obtained by a sum of the convolutions between <span class="math inline">\(g_i(x)\)</span> and <span class="math inline">\(f_{ij}(x)\)</span>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="character-level-networks-4" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Temporal max-pooling</strong></p>
</div>
<div class="callout-content">
<p>A 1-D version of max-pooling used in computer vision.</p>
<p>Given a discrete input function <span class="math inline">\(g(x) \in [1,l]\rightarrow \mathrm{R}\)</span>, the max-pooling function <span class="math inline">\(h(y) \in [1, \lfloor(l-k+1)/d\rfloor] \rightarrow \mathrm{R}\)</span> of <span class="math inline">\(g(x)\)</span> is defined as <span class="math display">\[
h(y) = \max\limits_{x=1}^k g(y\cdot d - x + c).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="character-level-networks-5" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Activation fn: <span class="math inline">\(h(x) = \max\left\{0,x\right\}\)</span></li>
<li>SGD with minibatch size=128</li>
<li>Momentum=0.9</li>
</ul>
</div>
</div>
</div>
</section>
<section id="character-level-networks-6" class="slide level2">
<h2>Character-level networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Quantization</strong></p>
</div>
<div class="callout-content">
<ul>
<li>use alphabet of size <span class="math inline">\(m\)</span></li>
<li>quantize each character using 1-of-m (or one-hot) encoding</li>
<li>then the sequence of characters is transformed to a sequence of <span class="math inline">\(m\)</span>-sized vectors with fixed length <span class="math inline">\(l_0\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="character-level-networks-7" class="slide level2">
<h2>Character-level networks</h2>

<img data-src="img/char_level_cnn_alphabet.png" class="r-stretch quarto-figure-center"><p class="caption">Alphabet size: 70</p></section>
<section id="character-level-networks-8" class="slide level2">
<h2>Character-level networks</h2>

<img data-src="img/char_level_cnn_diagram.png" class="r-stretch quarto-figure-center"><p class="caption">Model design. Number of features: 70. Input feature length: 1014.</p></section>
<section id="character-level-networks-9" class="slide level2">
<h2>Character-level networks</h2>

<img data-src="img/char_level_cnn_relative_errors.png" class="r-stretch quarto-figure-center"><p class="caption">Number of features: 70. Input feature length: 1014.</p></section>
<section id="character-level-networks-10" class="slide level2">
<h2>Character-level networks</h2>



<img data-src="img/cnn_character_aware_nlms.png" class="r-stretch quarto-figure-center"><p class="caption">Character-aware neural language models (2015)</p></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"6d196d5078d3ce4d28b7dca4742923cef0fd2294378f9ae1769913fa5e8e6f8b"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>