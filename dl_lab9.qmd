---
title: "DL: Lab 9 (Annotated Encoder-Decoder with Attention)"
execute:
  enabled: true
  echo: true
  cache: true
format:
  html:
    code-fold: false
jupyter: python3
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
filters:
  - diagram
---

# Annotated Encoder-Decoder with Attention

Jupyter notebook attached (dl_lab9.zip)

Please do the following:

1. In `Part1_EncoderDecoder.ipynb`, complete notebook. Modify the code to use NLTK's tokenizer instead of spacy.
2. In `Part2_SelfAttention.ipynb`, complete computation of weights/scores/final context vector for multi-head attention.
