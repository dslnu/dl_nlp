<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.29">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Deep learning: multi-layer NNs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Deep learning: multi-layer NNs</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section>
<section id="neural-network-representation" class="title-slide slide level1 center">
<h1>Neural Network Representation</h1>

</section>
<section id="deep-networks" class="slide level2">
<h2>Deep networks</h2>
<p>Suppose we have this network:</p>

<img data-src="img/deep_nns1.png" class="r-stretch"></section>
<section id="deep-networks-1" class="slide level2 smaller">
<h2>Deep Networks</h2>
<p>What happens in Hidden layer 1 and Output layer?</p>
<p>We will use this notation: <span class="math display">\[\begin{align*}
  &amp; z^{[1]} = W^{[1]}x+b^{[1]} \\
  &amp; a^{[1]} = \sigma(z^{[1]})\\
  &amp; z^{[2]} = W^{[2]}a^{[1]}+b^{[2]} \\
  &amp; a^{[2]} = \sigma(z^{[2]})
\end{align*}\]</span> And then we compute <span class="math inline">\(L(a^{[2]}, y)\)</span>.</p>
<p>And then, similarly, for backpropagation, we will compute <span class="math inline">\(da^{[2]}, dz^{[2]}, dW^{[2]}\)</span>.</p>

<aside><div>
<p>Do not confuse round and square brackets.</p>
</div></aside></section>
<section id="deep-networks-2" class="slide level2">
<h2>Deep Networks</h2>
<p>Alternative notation: <span class="math inline">\(a^{[0]} = x\)</span>. So in the picture <span class="math display">\[\begin{align*}
  a = \begin{bmatrix}
    a_1^{[1]} \\
    a_2^{[1]}\\
    a_3^{[1]}\\
    a_4^{[1]}
  \end{bmatrix}
\end{align*}\]</span> 2-layer network (input layer not counted). Input layer is layer 0.</p>
</section>
<section id="deep-networks-3" class="slide level2">
<h2>Deep Networks</h2>
<!-- ```tikz{height=600} -->
<!-- \begin{neuralnetwork}[height=4,layertitleheight=4em] -->
<!--         \inputlayer[count=3, bias=false] -->
<!--         \hiddenlayer[count=4, bias=false] \linklayers -->
<!--         \outputlayer[count=1] \linklayers -->
<!-- \end{neuralnetwork} -->
<!-- ``` -->

<img data-src="img/deep_nns2.png" class="r-stretch"><div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>First hidden layer params</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(W^{[1]}\)</span> ((4,3) matrix)</li>
<li><span class="math inline">\(b^{[1]}\)</span> ((4,1) matrix).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Second hidden layer params</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(W^{[2]}\)</span> is a (1,4) matrix</li>
<li><span class="math inline">\(b^{[2]}\)</span> is a (1,1) matrix.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="deep-networks-4" class="slide level2">
<h2>Deep Networks</h2>
<p>For all nodes: <span class="math display">\[\begin{align*}
  &amp; z_1^{[1]} = W_1^{[1]T}x + b_1^{[1]},\; &amp; a_1^{[1]} = \sigma(z_1^{[1]}), \\
  &amp; z_2^{[1]} = W_2^{[1]T}x + b_2^{[1]},\; &amp; a_2^{[1]} = \sigma(z_2^{[1]}), \\
  &amp; z_3^{[1]} = W_3^{[1]T}x + b_3^{[1]},\; &amp; a_3^{[1]} = \sigma(z_3^{[1]}), \\
  &amp; z_4^{[1]} = W_4^{[1]T}x + b_4^{[1]},\; &amp; a_4^{[1]} = \sigma(z_4^{[1]})
\end{align*}\]</span> So, if we have <span class="math inline">\(a_i^{[l]}\)</span>, then <span class="math inline">\(l\)</span> means layer, and <span class="math inline">\(i\)</span> means node number in a layer.</p>
</section>
<section id="deep-networks-5" class="slide level2">
<h2>Deep Networks</h2>

<img data-src="img/ff_nn_examples.png" class="r-stretch"></section>
<section id="deep-networks-6" class="slide level2">
<h2>Deep Networks</h2>
<p>For <span class="math inline">\(m\)</span> training examples, we compute <span class="math inline">\(x^{(m)} \rightarrow a^{[2](m)} = \hat{y}^{(m)}\)</span>.</p>
<p>The block of code:</p>
<p><code>for i = 1 to m:</code></p>
<p><span class="math display">\[\begin{align*}
  &amp;z^{[1](i)} = W^{[1]}x^{(i)}+b^{[1]},\\
  &amp;a^{[1](i)} = \sigma(z^{[1](i)}),\\
  &amp;z^{[2](i)} = W^{[2]}a^{[1](i)}+b^{[2]},\\
  &amp;a^{[2](i)} = \sigma(z^{[2](i)})
\end{align*}\]</span></p>
</section>
<section id="deep-networks-7" class="slide level2 smaller">
<h2>Deep Networks</h2>
<p>How do we vectorize it across multiple training examples? We compute by going to matrices: <span class="math display">\[\begin{align*}
  &amp;Z^{[1]} = W^{[1]}X+b^{[1]},\; &amp;A^{[1]} = \sigma(Z^{[1]}),\\
  &amp;Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]},\; &amp;A^{[2]} = \sigma(Z^{[2]})
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
&amp;Z^{[1]} =\begin{bmatrix}
  \vdots &amp; \vdots &amp; \dots &amp; \vdots \\
  z^{[1](1)} &amp; z^{[1](2)} &amp; \dots &amp; z^{[1](m)} \\
  \vdots &amp; \vdots &amp; \dots &amp; \vdots
\end{bmatrix},\\
&amp;A^{[1]} = \begin{bmatrix}
  \vdots &amp; \vdots &amp; \dots &amp; \vdots \\
  a^{[1](1)} &amp; a^{[1](2)} &amp; \dots &amp; a^{[1](m)} \\
  \vdots &amp; \vdots &amp; \dots &amp; \vdots
\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="activation-functions" class="slide level2">
<h2>Activation Functions</h2>
<!-- % Using tanh has the effecting of "centering" the data. -->
<!-- % Sigmoid function might be used for the output layer in case of binary classification problems. -->
<!-- % Activation fns might be different for different layers. -->
<!-- % tanh has a very small slope on very large or very small values of z. -->
<!-- % ReLU function: $a=\max(0,z)$. -->
<!-- % Leaky ReLU function: $a=\max(0.01*z,z)$. -->
<!-- % Was proven in Aggarwal's book too. -->
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Sigmoid derivative</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\dfrac{d\sigma}{dz} = \sigma(z)(1-\sigma(z))
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tanh derivative</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\dfrac{d\tanh}{dz} = 1-(\tanh(z))^2
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>ReLU derivative</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;g'(z) = \begin{cases}
    0 , &amp; \text{if } z &lt; 0,\\
    1, &amp; \text{if } z &gt; 0, \\
    \text{undefined}, &amp; \text{if } z = 0
  \end{cases}
\end{align*}\]</span></p>
</div>
</div>
</div>
</section></section>
<section>
<section id="gradient-descent-for-neural-networks" class="title-slide slide level1 center">
<h1>Gradient Descent for Neural Networks</h1>

</section>
<section id="deep-networks-8" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameters</strong></p>
</div>
<div class="callout-content">
<p>Review:</p>
<p><span class="math inline">\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\)</span>.</p>
<p><span class="math inline">\(n_x = n^{[0]}\)</span>, <span class="math inline">\(n^{[2]} = 1\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Cost function</strong></p>
</div>
<div class="callout-content">
<p>In our case will be <span class="math display">\[\begin{align*}
  &amp;J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})= \frac{1}{m}\sum\limits_{i=1}^m L(\hat{y},y).
\end{align*}\]</span></p>
</div>
</div>
</div>
<p>Dimensions for <span class="math inline">\(W^{[1]}\)</span> are <span class="math inline">\((n^{[1]}, n^{[0]})\)</span>.</p>
</section>
<section id="deep-networks-9" class="slide level2">
<h2>Deep Networks</h2>
<p>For gradient descent we compute</p>
<ul>
<li><span class="math inline">\(\hat{y}^{(i)}\)</span></li>
<li><span class="math inline">\(dW^{[1]} \equiv \dfrac{dJ}{dW^{[1]}}, dB^{[1]} \equiv \dfrac{dJ}{dB^{[1]}}\)</span></li>
<li><span class="math inline">\(W^{[1]} = W^{[1]} - \alpha dW^{[1]}\)</span></li>
<li><span class="math inline">\(B^{[1]} = B^{[1]} - \alpha dB^{[1]}\)</span></li>
</ul>
</section>
<section id="deep-networks-10" class="slide level2">
<h2>Deep Networks</h2>
<p>How do we compute the derivatives? <span class="math display">\[\begin{align*}
  &amp;dZ^{[2]} = A^{[2]} - Y,\\
  &amp;dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T},\\
  &amp;dB^{[2]} = \frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)
\end{align*}\]</span></p>
<!-- %TODO derivative computation -->
</section>
<section id="deep-networks-11" class="slide level2">
<h2>Deep Networks</h2>
<p>Next step: <span class="math display">\[\begin{align*}
  &amp;dZ^{[1]} = W^{[2]T}dZ^{[2]} \odot g^{[1]'}(Z^{[1]}),\\
  &amp;dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T},\\
  &amp;dB^{[1]} = \frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)
\end{align*}\]</span></p>
</section>
<section id="deep-networks-12" class="slide level2">
<h2>Deep Networks</h2>
<p>Forward propagation:</p>

<img data-src="img/deep_nns3.png" class="r-stretch"></section>
<section id="deep-networks-13" class="slide level2">
<h2>Deep Networks</h2>
<p>Backward propagation:</p>

<img data-src="img/deep_nns4.png" class="r-stretch"></section>
<section id="deep-networks-14" class="slide level2 smaller">
<h2>Deep Networks</h2>
<p>Vectorized versions: <span class="math display">\[\begin{align*}
  &amp;d\vec{Z}^{[2]} = A^{[2]} - Y,\\
  &amp;d\vec{W}^{[2]} = \frac{1}{m}d\vec{Z}^{[2]}\vec{A}^{[1]T},\\
  &amp;d\vec{b}^{[2]} = \frac{1}{m}np.sum(d\vec{Z}^{[2]}, axis=1,keepdims=True),\\
  &amp;d\vec{Z}^{[1]} = W^{[2]T}d\vec{Z}^{[2]} \odot g^{[1]'}(\vec{Z}^{[1]}),\\
  &amp;d\vec{W}^{[1]} = \frac{1}{m}d\vec{Z}^{[1]}\vec{X}^{T},\\
  &amp;d\vec{b}^{[1]} = \frac{1}{m}np.sum(d\vec{Z}^{[1]}, axis=1,keepdims=True)
\end{align*}\]</span></p>
</section>
<section id="deep-networks-random-initialization" class="slide level2">
<h2>Deep Networks: Random Initialization</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Symmetry-breaking problem</strong></p>
</div>
<div class="callout-content">
<p>If we initialize weights to zero, the hidden units will be symmetric. <span class="math display">\[
W^{[1]} = np.random.randn((2,2))*0.01,\\
b^{[1]} = np.zero((2,1))
\]</span></p>
</div>
</div>
</div>

<aside><div>
<p>The 0.01 multiplier is because we don’t want to end up at flat parts of the activation function.</p>
</div></aside></section></section>
<section>
<section id="forward-propagation-in-a-deep-network" class="title-slide slide level1 center">
<h1>Forward Propagation in a Deep Network</h1>

</section>
<section id="deep-networks-15" class="slide level2">
<h2>Deep Networks</h2>
<p><span class="math inline">\(a^{[l]}\)</span> - activations in layer <span class="math inline">\(l\)</span>. <img data-src="img/deep_nns5.png"></p>
</section>
<section id="deep-networks-16" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>General rule</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\)</span>, <span class="math inline">\(a^{[l]} = g^{[l]}(z^{[l]})\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Vectorized versions</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
&amp;Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}, \\
&amp;A^{[l]} = g^{[l]}(Z^{[l]})
\end{align*}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>For loop is necessary for multiple layers.</p>
</div>
</div>
</div>
</section>
<section id="deep-networks-17" class="slide level2">
<h2>Deep Networks</h2>

<img data-src="img/feature_engineering.png" class="r-stretch"></section>
<section id="deep-networks-18" class="slide level2">
<h2>Deep Networks</h2>

<img data-src="img/feature_engineering2.png" class="r-stretch"></section>
<section id="deep-networks-19" class="slide level2">
<h2>Deep Networks</h2>

<img data-src="img/deep_nns6.png" class="r-stretch"><div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Getting your Matrix Dimensions Right</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li>Dimensions of <span class="math inline">\(W^{[l]}\)</span> are <span class="math inline">\((n^{[l]}, n^{[l-1]})\)</span>.</li>
<li>Dimensions of <span class="math inline">\(b^{[l]}\)</span> should be <span class="math inline">\((n^{[l]}, 1)\)</span>.</li>
<li>Dimensions of dW and db should be identical to the ones for W and b.</li>
<li>Dimension of <span class="math inline">\(Z^{[1]}\)</span> is <span class="math inline">\((n^{[1]}, m)\)</span>.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="deep-networks-20" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Intuition from <strong>circuit theory</strong></strong></p>
</div>
<div class="callout-content">
<p>Small L-layer network requires exponentially less hidden units than shallower networks.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>To compute XOR, we’ll need <span class="math inline">\(O(\log \, n)\)</span> layers.</p>
<p>With a single hidden layer, we’ll need <span class="math inline">\(2^{n-1}\)</span> hidden units.</p>
</div>
</div>
</div>
</section>
<section id="deep-networks-21" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Forward propagation:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Inputs: <span class="math inline">\(a^{[l-1]}\)</span></li>
<li>Parameters: <span class="math inline">\(W^{[l]}\)</span>, <span class="math inline">\(b^{[l]}\)</span>.</li>
<li>Outputs: <span class="math inline">\(a^{[l]}\)</span></li>
<li>Cache: <span class="math inline">\(z^{[l]}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backward propagation:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Inputs: <span class="math inline">\(da^{[l]}\)</span></li>
<li>Parameters: <span class="math inline">\(W^{[l]}\)</span>, <span class="math inline">\(b^{[l]}\)</span>.</li>
<li>Outputs: <span class="math inline">\(da^{[l-1]}\)</span></li>
<li>Cache: <span class="math inline">\(dz^{[l]}\)</span>, <span class="math inline">\(dW^{[l]}\)</span>, <span class="math inline">\(db^{[l]}\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="deep-networks-22" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backward propagation steps:</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;dz^{[l]} = da^{[l]}\odot g^{[l]'}(z^{[l]})\\
  &amp;dW^{[l]} = dz^{[l]} \cdot (a^{[l-1]T})\\
  &amp;db^{[l]} = dz^{[l]} \\
  &amp;da^{[l-1]} = W^{[l]T} \cdot dz^{[l]}
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section id="deep-networks-23" class="slide level2">
<h2>Deep Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Vectorized versions:</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;dZ^{[l]} = dA^{[l]}\odot g^{[l]'}(Z^{[l]})\\
  &amp;dW^{[l]} = \frac{1}{m} dZ^{[l]} \cdot (A^{[l-1]T})\\
  &amp;db^{[l]} = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\
  &amp;dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]}
\end{align*}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Final layer</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;da^{[l]} = -\frac{y}{a} + \frac{1-y}{1-a}.
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section id="parameters-vs-hyperparameters" class="slide level2">
<h2>Parameters vs Hyperparameters</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(W^{[i]}\)</span></li>
<li><span class="math inline">\(b^{[i]}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hyperparameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li>learning rate</li>
<li>n of iterations</li>
<li>n of hidden layers (<span class="math inline">\(L\)</span>)</li>
<li>n of hidden units (<span class="math inline">\(n^{[i]}\)</span>)</li>
<li>choice of activation functions</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="backpropagation-in-detail" class="title-slide slide level1 center">
<h1>Backpropagation in detail</h1>

</section>
<section id="backpropagation" class="slide level2">
<h2>Backpropagation</h2>
<p><span class="math display">\[\begin{align}
z_{j, i}^{[l]} &amp;= \sum_k w_{j, k}^{[l]} a_{k, i}^{[l - 1]} + b_j^{[l]}, \label{eq:z_scalar} \\
a_{j, i}^{[l]} &amp;= g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}). \label{eq:a_scalar}
\end{align}\]</span></p>
</section>
<section id="backpropagation-1" class="slide level2 smaller">
<h2>Backpropagation</h2>
<table class="caption-top">
<colgroup>
<col style="width: 40%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Entity</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(l\)</span></td>
<td>The current layer <span class="math inline">\(l = 1, \dots, L\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n^{[l]}\)</span></td>
<td>The number of nodes in the current layer</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n^{[l - 1]}\)</span></td>
<td>The number of nodes in the previous layer</td>
</tr>
<tr class="even">
<td><span class="math inline">\(j\)</span></td>
<td>The <span class="math inline">\(j\)</span>-th node of the current layer, <span class="math inline">\(j = 1, \dots, n^{[l]}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k\)</span></td>
<td>The <span class="math inline">\(k\)</span>-th node of the previous layer,<span class="math inline">\(k = 1, \dots, n^{[l - 1]}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(i\)</span></td>
<td>The current training example <span class="math inline">\(i = 1, \dots, m\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(z_{j, i}^{[l]}\)</span></td>
<td>A weighted sum of the activations of the previous layer</td>
</tr>
</tbody>
</table>
</section>
<section id="backpropagation-2" class="slide level2 smaller">
<h2>Backpropagation</h2>
<table class="caption-top">
<colgroup>
<col style="width: 15%">
<col style="width: 85%">
</colgroup>
<thead>
<tr class="header">
<th>Entity</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(w_{j, k}^{[l]}\)</span></td>
<td>A weight that scales the <span class="math inline">\(k\)</span>-th activation of the previous layer</td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_j^{[l]}\)</span></td>
<td>A bias in the current layer</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(a_{j, i}^{[l]}\)</span></td>
<td>An activation in the current layer</td>
</tr>
<tr class="even">
<td><span class="math inline">\(a_{k, i}^{[l - 1]}\)</span></td>
<td>An activation in the previous layer</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(g_j^{[l]}\)</span></td>
<td>An activation function <span class="math inline">\(g_j^{[l]} \colon \mathbb{R}^{n^{[l]}} \to \mathbb{R}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="backpropagation-3" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>We vectorize the nodes:</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
z_{1, i}^{[l]} \\
\vdots \\
z_{j, i}^{[l]} \\
\vdots \\
z_{n^{[l]}, i}^{[l]}
\end{bmatrix} &amp;=
\begin{bmatrix}
w_{1, 1}^{[l]} &amp; \dots &amp; w_{1, k}^{[l]} &amp; \dots &amp; w_{1, n^{[l - 1]}}^{[l]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{j, 1}^{[l]} &amp; \dots &amp; w_{j, k}^{[l]} &amp; \dots &amp; w_{j, n^{[l - 1]}}^{[l]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{n^{[l]}, 1}^{[l]} &amp; \dots &amp; w_{n^{[l]}, k}^{[l]} &amp; \dots &amp; w_{n^{[l]}, n^{[l - 1]}}^{[l]}
\end{bmatrix}

\begin{bmatrix}
a_{1, i}^{[l - 1]} \\
\vdots \\
a_{k, i}^{[l - 1]} \\
\vdots \\
a_{n^{[l - 1]}, i}^{[l - 1]}
\end{bmatrix} +
\begin{bmatrix}
b_1^{[l]} \\
\vdots \\
b_j^{[l]} \\
\vdots \\
b_{n^{[l]}}^{[l]}
\end{bmatrix},
\end{align*}\]</span></p>
</section>
<section id="backpropagation-4" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
a_{1, i}^{[l]} \\
\vdots \\
a_{j, i}^{[l]} \\
\vdots \\
a_{n^{[l]}, i}^{[l]}
\end{bmatrix} &amp;=
\begin{bmatrix}
g_1^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\vdots \\
g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\vdots \\
g_{n^{[l]}}^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\end{bmatrix},
\end{align*}\]</span></p>
</section>
<section id="backpropagation-5" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>which we can write as</p>
<p><span class="math display">\[\begin{align}
\vec{z}_{:, i}^{[l]} &amp;= \vec{W}^{[l]} \vec{a}_{:, i}^{[l - 1]} + \vec{b}^{[l]}, \label{eq:z} \\
\vec{a}_{:, i}^{[l]} &amp;= \vec{g}^{[l]}(\vec{z}_{:, i}^{[l]}), \label{eq:a}
\end{align}\]</span></p>
<p>where <span class="math display">\[\begin{align*}
  &amp;\vec{z}_{:, i}^{[l]} \in \mathbb{R}^{n^{[l]}}, \,
  &amp;\vec{W}^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l - 1]}}, \,
  &amp;\vec{b}^{[l]} \in \mathbb{R}^{n^{[l]}}, \\
  &amp;\vec{a}_{:, i}^{[l]} \in \mathbb{R}^{n^{[l]}}, \;
  &amp;\vec{a}_{:, i}^{[l - 1]} \in \mathbb{R}^{n^{[l - 1]}}, \;
  &amp;\vec{g}^{[l]} \colon \mathbb{R}^{n^{[l]}} \to \mathbb{R}^{n^{[l]}}.
\end{align*}\]</span></p>

<aside><div>
<p>We have used a colon to clarify that <span class="math inline">\(\vec{z}_{:, i}^{[l]}\)</span> is the <span class="math inline">\(i\)</span>-th column of <span class="math inline">\(\vec{Z}^{[l]}\)</span>, and so on.</p>
</div></aside></section>
<section id="backpropagation-6" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>Next, we vectorize the training examples:</p>
<p><span class="math display">\[\begin{align}
\vec{Z}^{[l]} &amp;=
\begin{bmatrix}
\vec{z}_{:, 1}^{[l]} &amp; \dots &amp; \vec{z}_{:, i}^{[l]} &amp; \dots &amp; \vec{z}_{:, m}^{[l]}
\end{bmatrix} \label{eq:Z} \\
&amp;= \vec{W}^{[l]}
\begin{bmatrix}
\vec{a}_{:, 1}^{[l - 1]} &amp; \dots &amp; \vec{a}_{:, i}^{[l - 1]} &amp; \dots &amp; \vec{a}_{:, m}^{[l - 1]}
\end{bmatrix} +
\begin{bmatrix}
\vec{b}^{[l]} &amp; \dots &amp; \vec{b}^{[l]} &amp; \dots &amp; \vec{b}^{[l]}
\end{bmatrix} \notag \\
&amp;= \vec{W}^{[l]} \vec{A}^{[l - 1]} + \text{broadcast}(\vec{b}^{[l]}), \notag \\
\vec{A}^{[l]} &amp;=
\begin{bmatrix}
\vec{a}_{:, 1}^{[l]} &amp; \dots &amp; \vec{a}_{:, i}^{[l]} &amp; \dots &amp; \vec{a}_{:, m}^{[l]}
\end{bmatrix}, \label{eq:A}
\end{align}\]</span></p>
<p>where <span class="math display">\[\begin{align*}
  &amp;\vec{Z}^{[l]} \in \mathbb{R}^{n^{[l]} \times m}, \\
  &amp;\vec{A}^{[l]} \in \mathbb{R}^{n^{[l]} \times m}, \\
  &amp;\vec{A}^{[l - 1]} \in \mathbb{R}^{n^{[l - 1]} \times m}.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-7" class="slide level2 smaller">
<h2>Backpropagation</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Numpy broadcasting</strong></p>
</div>
<div class="callout-content">
<p>Smaller array is “broadcast” across the larger array so that they have compatible shapes.</p>
<p>Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python.</p>
</div>
</div>
</div>

<img data-src="img/numpy_broadcasting.png" class="r-stretch"></section>
<section id="backpropagation-8" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>We would also like to establish two additional notations:</p>
<p><span class="math display">\[\begin{align}
\vec{A}^{[0]} &amp;= \vec{X}, \label{eq:A_zero} \\
\vec{A}^{[L]} &amp;= \vec{\hat{Y}}, \label{eq:A_L}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\vec{X} \in \mathbb{R}^{n^{[0]} \times m}\)</span> denotes the inputs and <span class="math inline">\(\vec{\hat{Y}} \in \mathbb{R}^{n^{[L]} \times m}\)</span> denotes the predictions/outputs.</p>
<p>Finally, we are ready to define the cost function:</p>
<p><span class="math display">\[\begin{equation}
J = f(\vec{\hat{Y}}, \vec{Y}) = f(\vec{A}^{[L]}, \vec{Y}), \label{eq:J}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\vec{Y} \in \mathbb{R}^{n^{[L]} \times m}\)</span> denotes the targets and <span class="math inline">\(f \colon \mathbb{R}^{2 n^{[L]}} \to \mathbb{R}\)</span> can be tailored to our needs.</p>
</section>
<section id="backpropagation-9" class="slide level2">
<h2>Backpropagation</h2>
<p><span class="math display">\[
\def\pdv#1#2{\frac{\partial #1}{\partial #2}}
\def\dpdv#1#2{\frac{\partial #1}{\partial #2}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\peq}{\phantom{=}}
\]</span></p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Chain rule</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align}
u_i &amp;= g_i(x_1, \dots, x_j, \dots, x_n), \label{eq:example_u_scalar} \\
y_k &amp;= f_k(u_1, \dots, u_i, \dots, u_m). \label{eq:example_y_scalar}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{equation}
\pdv{y_k}{x_j} = \sum_i \pdv{y_k}{u_i} \pdv{u_i}{x_j}. \label{eq:chain_rule}
\end{equation}\]</span></p>
</div>
</div>
</div>
</section>
<section id="backpropagation-10" class="slide level2">
<h2>Backpropagation</h2>
<p>Let’s write out first derivatives of <span class="math inline">\(J\)</span> with respect to parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[\begin{align}
\pdv{J}{w_{j, k}^{[l]}} &amp;= \sum_i \pdv{J}{z_{j, i}^{[l]}} \pdv{z_{j, i}^{[l]}}{w_{j, k}^{[l]}} = \sum_i \pdv{J}{z_{j, i}^{[l]}} a_{k, i}^{[l - 1]}, \label{eq:dw_scalar} \\
\pdv{J}{b_j^{[l]}} &amp;= \sum_i \pdv{J}{z_{j, i}^{[l]}} \pdv{z_{j, i}^{[l]}}{b_j^{[l]}} = \sum_i \pdv{J}{z_{j, i}^{[l]}}. \label{eq:db_scalar}
\end{align}\]</span></p>
</section>
<section id="backpropagation-11" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>Vectorization results in</p>
<p><span class="math display">\[\begin{align*}
&amp;
\begin{bmatrix}
\dpdv{J}{w_{1, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{1, k}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{1, n^{[l - 1]}}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{w_{j, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{j, k}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{j, n^{[l - 1]}}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{w_{n^{[l]}, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{n^{[l]}, k}^{[l]}} &amp; \dots &amp; \dpdv{J}{w_{n^{[l]}, n^{[l - 1]}}^{[l]}}
\end{bmatrix} =
\begin{bmatrix}
\dpdv{J}{z_{1, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{1, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{1, m}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{z_{j, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{j, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{j, m}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{n^{[l]}, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{n^{[l]}, m}^{[l]}}
\end{bmatrix} \notag \\
&amp;\peq{} \cdot
\begin{bmatrix}
a_{1, 1}^{[l - 1]} &amp; \dots &amp; a_{k, 1}^{[l - 1]} &amp; \dots &amp; a_{n^{[l - 1]}, 1}^{[l - 1]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1, i}^{[l - 1]} &amp; \dots &amp; a_{k, i}^{[l - 1]} &amp; \dots &amp; a_{n^{[l - 1]}, i}^{[l - 1]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1, m}^{[l - 1]} &amp; \dots &amp; a_{k, m}^{[l - 1]} &amp; \dots &amp; a_{n^{[l - 1]}, m}^{[l - 1]}
\end{bmatrix}, \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-12" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
\dpdv{J}{b_1^{[l]}} \\
\vdots \\
\dpdv{J}{b_j^{[l]}} \\
\vdots \\
\dpdv{J}{b_{n^{[l]}}^{[l]}}
\end{bmatrix} =
\begin{bmatrix}
\dpdv{J}{z_{1, 1}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{j, 1}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{n^{[l]}, 1}^{[l]}}
\end{bmatrix} + \dots +
\begin{bmatrix}
\dpdv{J}{z_{1, i}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{j, i}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{n^{[l]}, i}^{[l]}}
\end{bmatrix} + \dots +
\begin{bmatrix}
\dpdv{J}{z_{1, m}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{j, m}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{n^{[l]}, m}^{[l]}}
\end{bmatrix},
\end{align*}\]</span></p>
</section>
<section id="backpropagation-13" class="slide level2">
<h2>Backpropagation</h2>
<p><span class="math display">\[\begin{align}
\pdv{J}{\vec{W}^{[l]}} &amp;= \sum_i \pdv{J}{\vec{z}_{:, i}^{[l]}} \vec{a}_{:, i}^{[l - 1]T} = \pdv{J}{\vec{Z}^{[l]}} \vec{A}^{[l - 1]^T}, \label{eq:dW} \\
\pdv{J}{\vec{b}^{[l]}} &amp;= \sum_i \pdv{J}{\vec{z}_{:, i}^{[l]}} = \underbrace{\sum_{\text{axis} = 1} \pdv{J}{\vec{Z}^{[l]}}}_\text{column vector}, \label{eq:db}
\end{align}\]</span> where <span class="math inline">\(\pdv{J}{\vec{z}_{:, i}^{[l]}} \in \R^{n^{[l]}}\)</span>, <span class="math inline">\(\pdv{J}{\vec{Z}^{[l]}} \in \R^{n^{[l]} \times m}\)</span>, <span class="math inline">\(\pdv{J}{\vec{W}^{[l]}} \in \R^{n^{[l]} \times n^{[l - 1]}}\)</span>, and <span class="math inline">\(\pdv{J}{\vec{b}^{[l]}} \in \R^{n^{[l]}}\)</span>.</p>
</section>
<section id="backpropagation-14" class="slide level2">
<h2>Backpropagation</h2>
<p>Looking back at <span class="math inline">\(\eqref{eq:dw_scalar}\)</span> and <span class="math inline">\(\eqref{eq:db_scalar}\)</span>, we see that the only unknown entity is <span class="math inline">\(\pdv{J}{z_{j, i}^{[l]}}\)</span>. By applying the chain rule once again, we get</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{z_{j, i}^{[l]}} = \sum_p \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}}, \label{eq:dz_scalar}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p = 1, \dots, n^{[l]}\)</span>.</p>
</section>
<section id="backpropagation-15" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>Next, we present the vectorized version of <span class="math inline">\(\eqref{eq:dz_scalar}\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
\begin{bmatrix}
\dpdv{J}{z_{1, i}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{j, i}^{[l]}} \\
\vdots \\
\dpdv{J}{z_{n^{[l]}, i}^{[l]}}
\end{bmatrix} =
\begin{bmatrix}
\dpdv{a_{1, i}^{[l]}}{z_{1, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{j, i}^{[l]}}{z_{1, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{n^{[l]}, i}^{[l]}}{z_{1, i}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{a_{1, i}^{[l]}}{z_{j, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{n^{[l]}, i}^{[l]}}{z_{j, i}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{a_{1, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{j, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}} &amp; \dots &amp; \dpdv{a_{n^{[l]}, i}^{[l]}}{z_{n^{[l]}, i}^{[l]}}
\end{bmatrix}
\begin{bmatrix}
\dpdv{J}{a_{1, i}^{[l]}} \\
\vdots \\
\dpdv{J}{a_{j, i}^{[l]}} \\
\vdots \\
\dpdv{J}{a_{n^{[l]}, i}^{[l]}}
\end{bmatrix},
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-16" class="slide level2">
<h2>Backpropagation</h2>
<p>Which can be converted into</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{z}_{:, i}^{[l]}} = \pdv{\vec{a}_{:, i}^{[l]}}{\vec{z}_{:, i}^{[l]}} \pdv{J}{\vec{a}_{:, i}^{[l]}}, \label{eq:dz}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\pdv{J}{\vec{a}_{:, i}^{[l]}} \in \R^{n^{[l]}}\)</span> and <span class="math inline">\(\pdv{\vec{a}_{:, i}^{[l]}}{\vec{z}_{:, i}^{[l]}} \in \R^{n^{[l]} \times n^{[l]}}\)</span>.</p>
</section>
<section id="backpropagation-17" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>We have already encountered</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[l]}} =
\begin{bmatrix}
\dpdv{J}{\vec{z}_{:, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{\vec{z}_{:, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{\vec{z}_{:, m}^{[l]}}
\end{bmatrix}, \label{eq:dZ}
\end{equation}\]</span></p>
<p>and for the sake of completeness, we also clarify that</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{A}^{[l]}} =
\begin{bmatrix}
\dpdv{J}{\vec{a}_{:, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{\vec{a}_{:, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{\vec{a}_{:, m}^{[l]}}
\end{bmatrix}, \label{eq:dA}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\pdv{J}{\vec{A}^{[l]}} \in \R^{n^{[l]} \times m}\)</span>.</p>
<p>On purpose, we have omitted the details of <span class="math inline">\(g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]})\)</span>; consequently, we cannot derive an analytic expression for <span class="math inline">\(\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\)</span>, which we depend on in <span class="math inline">\(\eqref{eq:dz_scalar}\)</span>.</p>
</section>
<section id="backpropagation-18" class="slide level2 smaller">
<h2>Backpropagation</h2>
<p>Furthermore, according to <span class="math inline">\(\eqref{eq:dz_scalar}\)</span>, we see that <span class="math inline">\(\pdv{J}{z_{j, i}^{[l]}}\)</span> also depends on <span class="math inline">\(\pdv{J}{a_{j, i}^{[l]}}\)</span>.</p>
<p><span class="math inline">\(\pdv{J}{a_{j, i}^{[l]}}\)</span> has already been computed when we reach the <span class="math inline">\(l\)</span>-th layer during backward propagation.</p>
<p>How? Each layer paves the way for the previous layer by also computing <span class="math inline">\(\pdv{J}{a_{k, i}^{[l - 1]}}\)</span>, which we shall do now:</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{a_{k, i}^{[l - 1]}} = \sum_j \pdv{J}{z_{j, i}^{[l]}} \pdv{z_{j, i}^{[l]}}{a_{k, i}^{[l - 1]}} = \sum_j \pdv{J}{z_{j, i}^{[l]}} w_{j, k}^{[l]}. \label{eq:da_prev_scalar}
\end{equation}\]</span></p>
</section>
<section id="backpropagation-19" class="slide level2 smaller">
<h2>Backpropagation</h2>
<!-- As usual, our next step is vectorization: -->
<p><span class="math display">\[\begin{equation*}
\begin{split}
&amp;
\begin{bmatrix}
\dpdv{J}{a_{1, 1}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{1, i}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{1, m}^{[l - 1]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{a_{k, 1}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{k, i}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{k, m}^{[l - 1]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{a_{n^{[l - 1]}, 1}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{n^{[l - 1]}, i}^{[l - 1]}} &amp; \dots &amp; \dpdv{J}{a_{n^{[l - 1]}, m}^{[l - 1]}}
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
w_{1, 1}^{[l]} &amp; \dots &amp; w_{j, 1}^{[l]} &amp; \dots &amp; w_{n^{[l]}, 1}^{[l]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1, k}^{[l]} &amp; \dots &amp; w_{j, k}^{[l]} &amp; \dots &amp; w_{n^{[l]}, k}^{[l]} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1, n^{[l - 1]}}^{[l]} &amp; \dots &amp; w_{j, n^{[l - 1]}}^{[l]} &amp; \dots &amp; w_{n^{[l]}, n^{[l - 1]}}^{[l]}
\end{bmatrix} \cdot
\begin{bmatrix}
\dpdv{J}{z_{1, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{1, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{1, m}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{z_{j, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{j, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{j, m}^{[l]}} \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dpdv{J}{z_{n^{[l]}, 1}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{n^{[l]}, i}^{[l]}} &amp; \dots &amp; \dpdv{J}{z_{n^{[l]}, m}^{[l]}}
\end{bmatrix},
\end{split}
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-20" class="slide level2">
<h2>Backpropagation</h2>
<p>which we can write as <span class="math display">\[\begin{align}
&amp;\pdv{J}{\vec{A}^{[l - 1]}} =\vec{W}^{[l]T} \pdv{J}{\vec{Z}^{[l]}}, \label{eq:dA_prev}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\pdv{J}{\vec{A}^{[l - 1]}} \in \R^{n^{[l - 1]} \times m}\)</span>.</p>
</section>
<section id="backpropagation-21" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Initial values</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Forward: <span class="math inline">\(\vec{A}^{[0]} = \vec{X}\)</span></li>
<li>Backward: <span class="math inline">\(\pdv{J}{\vec{A}^{[L]}} = \pdv{J}{\vec{\hat{Y}}}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computations</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Forward: <span class="math inline">\(\vec{A}^{[0]} = \vec{X}, \vec{A}^{[L]} = \vec{\hat{Y}}, J = f(\vec{\hat{Y}}, \vec{Y}) = f(\vec{A}^{[L]}, \vec{Y})\)</span></li>
<li>Backward: <span class="math inline">\(\dfrac{\partial J}{\partial \vec{A}^{[L]}} = \dfrac{\partial J}{\partial \vec{\hat{Y}}}, \dfrac{\partial J}{\partial \vec{W}^{[l]}}, \dfrac{\partial J}{\partial \vec{b}^{[l]}}\)</span></li>
</ul>
</div>
</div>
</div>
<!-- % If not stopped prematurely, it eventually computes -->
<!-- % \(\pdv{J}{\vec{A}^{[0]}} = \pdv{J}{\vec{X}}\), a partial derivative we -->
<!-- % usually ignore. -->
</section>
<section id="backpropagation-22" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Backpropagation seed</strong></p>
</div>
<div class="callout-content">
<p>We have yet to derive an analytic expression for the backpropagation seed <span class="math inline">\(\dpdv{J}{\vec{A}^{[L]}} = \dpdv{J}{\vec{\hat{Y}}}\)</span>.</p>
</div>
</div>
</div>
<p>Let’s derive an analytic expression for <span class="math inline">\(\dpdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}}\)</span> or, by extension, <span class="math inline">\(\dpdv{J}{z_{j, i}^{[l]}}\)</span>.</p>
</section>
<section id="backpropagation-relu" class="slide level2">
<h2>Backpropagation: ReLU</h2>
<p>The rectified linear unit, or ReLU for short, is given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
a_{j, i}^{[l]} &amp;= g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
&amp;= \max(0, z_{j, i}^{[l]}) = \\
&amp;=
\begin{cases}
z_{j, i}^{[l]} &amp;\text{if } z_{j, i}^{[l]} &gt; 0, \\
0 &amp;\text{otherwise.}
\end{cases}
\end{split}
\end{equation*}\]</span></p>
<p>In other words,</p>
<p><span class="math display">\[\begin{equation}
\vec{A}^{[l]} = \max(0, \vec{Z}^{[l]}).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-relu-1" class="slide level2 smaller">
<h2>Backpropagation: ReLU</h2>
<p>Compute the partial derivatives of the activations in the current layer:</p>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &amp;:=
\begin{cases}
1 &amp;\text{if } z_{j, i}^{[l]} &gt; 0, \\
0 &amp;\text{otherwise,}
\end{cases} \\
&amp;= I(z_{j, i}^{[l]} &gt; 0), \notag \\
\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &amp;= 0, \quad \forall p \ne j.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-relu-2" class="slide level2 smaller">
<h2>Backpropagation: ReLU</h2>
<p>It follows that</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[l]}} &amp;= \sum_p \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \pdv{J}{a_{j, i}^{[l]}} \pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\
&amp;= \pdv{J}{a_{j, i}^{[l]}} I(z_{j, i}^{[l]} &gt; 0),
\end{split}
\end{equation*}\]</span></p>
<p>which we can vectorize as</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[l]}} = \pdv{J}{\vec{A}^{[l]}} \odot I(\vec{Z}^{[l]} &gt; 0),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes element-wise multiplication (<strong>Hadamard product</strong>).</p>
</section>
<section id="backpropagation-sigmoid" class="slide level2">
<h2>Backpropagation: Sigmoid</h2>
<p>The sigmoid activation function is given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
a_{j, i}^{[l]} &amp;= g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
&amp;= \sigma(z_{j, i}^{[l]}) = \frac{1}{1 + \exp(-z_{j, i}^{[l]})}.
\end{split}
\end{equation*}\]</span></p>
<p>Vectorization yields</p>
<p><span class="math display">\[\begin{equation}
\vec{A}^{[l]} = \frac{1}{1 + \exp(-\vec{Z}^{[l]})}.
\end{equation}\]</span></p>
</section>
<section id="backpropagation-sigmoid-1" class="slide level2">
<h2>Backpropagation: Sigmoid</h2>
<p>To practice backward propagation, first, we construct a computation graph:</p>
<p><img data-src="img/deep_nns7.png" height="500"></p>
</section>
<section id="backpropagation-sigmoid-2" class="slide level2 smaller">
<h2>Backpropagation: Sigmoid</h2>
<p>Compute, starting from outside:</p>
<p><span class="math display">\[\begin{align*}
&amp;\pdv{a_{j, i}^{[l]}}{u_4} = 1, \; \pdv{a_{j, i}^{[l]}}{u_3} = \pdv{a_{j, i}^{[l]}}{u_4} \pdv{u_4}{u_3} = -\frac{1}{u_3^2} = -\frac{1}{(1 + \exp(-z_{j, i}^{[l]}))^2}, \\
&amp;\pdv{a_{j, i}^{[l]}}{u_2} = \pdv{a_{j, i}^{[l]}}{u_3} \pdv{u_3}{u_2} = -\frac{1}{u_3^2} = -\frac{1}{(1 + \exp(-z_{j, i}^{[l]}))^2}, \\
&amp;\pdv{a_{j, i}^{[l]}}{u_1} = \pdv{a_{j, i}^{[l]}}{u_2} \pdv{u_2}{u_1} = -\frac{1}{u_3^2} \exp(u_1) = -\frac{\exp(-z_{j, i}^{[l]})}{(1 + \exp(-z_{j, i}^{[l]}))^2}, \\
&amp;\pdv{a_{j, i}^{[l]}}{u_0} = \pdv{a_{j, i}^{[l]}}{u_1} \pdv{u_1}{u_0} = \frac{1}{u_3^2} \exp(u_1) = \frac{\exp(-z_{j, i}^{[l]})}{(1 + \exp(-z_{j, i}^{[l]}))^2}.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-sigmoid-3" class="slide level2 smaller">
<h2>Backpropagation: Sigmoid</h2>
<p>Let us simplify:</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &amp;= \frac{\exp(-z_{j, i}^{[l]})}{(1 + \exp(-z_{j, i}^{[l]}))^2} \\
&amp;= \frac{1 + \exp(-z_{j, i}^{[l]}) - 1}{(1 + \exp(-z_{j, i}^{[l]}))^2} \notag \\
&amp;= \frac{1}{1 + \exp(-z_{j, i}^{[l]})} - \frac{1}{(1 + \exp(-z_{j, i}^{[l]}))^2} \notag \\
&amp;= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).
\end{split}
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-sigmoid-4" class="slide level2 smaller">
<h2>Backpropagation: Sigmoid</h2>
<p>We also note that</p>
<p><span class="math display">\[\begin{equation*}
\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \quad \forall p \ne j.
\end{equation*}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[l]}} &amp;= \sum_p \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\
&amp;= \pdv{J}{a_{j, i}^{[l]}} \pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\
&amp;= \pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}).
\end{split}
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-sigmoid-5" class="slide level2">
<h2>Backpropagation: Sigmoid</h2>
<p>Lastly, no summations mean trivial vectorization:</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[l]}} = \pdv{J}{\vec{A}^{[l]}} \odot \vec{A}^{[l]} \odot (1 - \vec{A}^{[l]}).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-tanh" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p>The hyperbolic tangent function, i.e., the tanh activation function, is given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
a_{j, i}^{[l]} &amp;= g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
&amp;= \tanh(z_{j, i}^{[l]}) \\
&amp;= \frac{\exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]})}{\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})}.
\end{split}
\end{equation*}\]</span></p>
<p>By utilizing element-wise multiplication, we get</p>
<p><span class="math display">\[\begin{equation}
\vec{A}^{[l]} = \frac{1}{\exp(\vec{Z}^{[l]}) + \exp(-\vec{Z}^{[l]})} \odot (\exp(\vec{Z}^{[l]}) - \exp(-\vec{Z}^{[l]})).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-tanh-1" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p>Computation graph:</p>

<img data-src="img/deep_nns8.png" class="r-stretch"></section>
<section id="backpropagation-tanh-2" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p>We compute the partial derivatives:</p>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{u_7} &amp;= 1, \\
\pdv{a_{j, i}^{[l]}}{u_6} &amp;= \pdv{a_{j, i}^{[l]}}{u_7} \pdv{u_7}{u_6} = u_4 = \exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]}), \\
\pdv{a_{j, i}^{[l]}}{u_5} &amp;= \pdv{a_{j, i}^{[l]}}{u_6} \pdv{u_6}{u_5} = -u_4 \frac{1}{u_5^2} = -\frac{\exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2}, \\
\pdv{a_{j, i}^{[l]}}{u_4} &amp;= \pdv{a_{j, i}^{[l]}}{u_7} \pdv{u_7}{u_4} = u_6 = \frac{1}{\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})},
\end{align*}\]</span></p>
</section>
<section id="backpropagation-tanh-3" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{u_3} &amp;= \pdv{a_{j, i}^{[l]}}{u_4} \pdv{u_4}{u_3} + \pdv{a_{j, i}^{[l]}}{u_5} \pdv{u_5}{u_3} \\
&amp;= -u_6 - u_4 \frac{1}{u_5^2} \notag \\
&amp;= -\frac{1}{\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})} - \frac{\exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \notag \\
&amp;= -\frac{2 \exp(z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2}, \notag \\
\pdv{a_{j, i}^{[l]}}{u_2} &amp;= \pdv{a_{j, i}^{[l]}}{u_4} \pdv{u_4}{u_2} + \pdv{a_{j, i}^{[l]}}{u_5} \pdv{u_5}{u_2} = u_6 - u_4 \frac{1}{u_5^2} =\notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-tanh-4" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p><span class="math display">\[\begin{align*}
&amp;= \frac{1}{\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})} - \frac{\exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \notag \\
&amp;= \frac{2 \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2}, \notag \\
\pdv{a_{j, i}^{[l]}}{u_1} &amp;= \pdv{a_{j, i}^{[l]}}{u_3} \pdv{u_3}{u_1} \\
&amp;= \Bigl(-u_6 - u_4 \frac{1}{u_5^2}\Bigr) \exp(u_1) \notag \\
&amp;= -\frac{2 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2}, \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-tanh-5" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{u_0} &amp;= \pdv{a_{j, i}^{[l]}}{u_1} \pdv{u_1}{u_0} + \pdv{a_{j, i}^{[l]}}{u_2} \pdv{u_2}{u_0} \\
&amp;= -\Bigl(-u_6 - u_4 \frac{1}{u_5^2}\Bigr) \exp(u_1) + \Bigl(u_6 - u_4 \frac{1}{u_5^2}\Bigr) \exp(u_0) \notag \\
&amp;= \frac{2 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} + \frac{2 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \notag \\
&amp;= \frac{4 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2}. \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-tanh-6" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p>It follows that</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &amp;= \frac{4 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]})}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \\
&amp;= \frac{\exp(z_{j, i}^{[l]})^2 + 2 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})^2}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \\
&amp;\peq\negmedspace{} - \frac{\exp(z_{j, i}^{[l]})^2 - 2 \exp(z_{j, i}^{[l]}) \exp(-z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]})^2}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} \\
&amp;= 1 - \frac{(\exp(z_{j, i}^{[l]}) - \exp(-z_{j, i}^{[l]}))^2}{(\exp(z_{j, i}^{[l]}) + \exp(-z_{j, i}^{[l]}))^2} = 1 - a_{j, i}^{[l]} a_{j, i}^{[l]}.
\end{split}
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-tanh-7" class="slide level2 smaller">
<h2>Backpropagation: tanh</h2>
<p>Similarly to the sigmoid activation function: <span class="math inline">\(\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = 0, \quad \forall p \ne j.\)</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[l]}} &amp;= \sum_p \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \pdv{J}{a_{j, i}^{[l]}} \pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \\
&amp;= \pdv{J}{a_{j, i}^{[l]}} \left(1 - a_{j, i}^{[l]} a_{j, i}^{[l]}\right),
\end{split}
\end{equation*}\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[l]}} = \pdv{J}{\vec{A}^{[l]}} \odot (1 - \vec{A}^{[l]} \odot \vec{A}^{[l]}).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-softmax" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p>The softmax activation function is given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
a_{j, i}^{[l]} &amp;= g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
&amp;= \frac{\exp(z_{j, i}^{[l]})}{\sum_p \exp(z_{p, i}^{[l]})}.
\end{split}
\end{equation*}\]</span></p>
<p>Vectorization results in</p>
<p><span class="math display">\[\begin{equation}
\vec{A}^{[l]} = \frac{1}{\text{broadcast}(\underbrace{\sum_{\text{axis} = 0} \exp(\vec{Z}^{[l]})}_\text{row vector})} \odot \exp(\vec{Z}^{[l]}).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-softmax-1" class="slide level2">
<h2>Backpropagation: softmax</h2>
<p>To begin with, we construct a computation graph for the <span class="math inline">\(j\)</span>-th activation of the current layer:</p>

<img data-src="img/deep_nns9.png" class="r-stretch"></section>
<section id="backpropagation-softmax-2" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p>By applying the chain rule, we get</p>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{u_5} &amp;= 1, \\
\pdv{a_{j, i}^{[l]}}{u_4} &amp;= \pdv{a_{j, i}^{[l]}}{u_5} \pdv{u_5}{u_4} = u_1 = \exp(z_{j, i}^{[l]}), \\
\pdv{a_{j, i}^{[l]}}{u_3} &amp;= \pdv{a_{j, i}^{[l]}}{u_4} \pdv{u_4}{u_3} = -u_1 \frac{1}{u_3^2} = -\frac{\exp(z_{j, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2}, \\
\pdv{a_{j, i}^{[l]}}{u_1} &amp;= \pdv{a_{j, i}^{[l]}}{u_3} \pdv{u_3}{u_1} + \pdv{a_{j, i}^{[l]}}{u_5} \pdv{u_5}{u_1} \\
&amp;= -u_1 \frac{1}{u_3^2} + u_4 = -\frac{\exp(z_{j, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2} + \frac{1}{\sum_p \exp(z_{p, i}^{[l]})}, \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-softmax-3" class="slide level2">
<h2>Backpropagation: softmax</h2>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{u_{-1}} &amp;= \pdv{a_{j, i}^{[l]}}{u_1} \pdv{u_1}{u_{-1}} = \Bigl(-u_1 \frac{1}{u_3^2} + u_4\Bigr) \exp(u_{-1}) \notag \\
&amp;= -\frac{\exp(z_{j, i}^{[l]})^2}{(\sum_p \exp(z_{p, i}^{[l]}))^2} + \frac{\exp(z_{j, i}^{[l]})}{\sum_p \exp(z_{p, i}^{[l]})}. \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-softmax-4" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p>Next, we need to take into account that (z_{j, i}^{[l]}) also affects other activations in the same layer:</p>
<p><span class="math display">\[\begin{align*}
u_{-1} &amp;= z_{j, i}^{[l]}, \\
u_{0, p} &amp;= z_{p, i}^{[l]}, &amp;&amp;\forall p \ne j, \\
u_1 &amp;= \exp(u_{-1}), \\
u_{2, p} &amp;= \exp(u_{0, p}), &amp;&amp;\forall p \ne j, \\
u_3 &amp;= u_1 + \sum_{p \ne j} u_{2, p}, \\
u_4 &amp;= \frac{1}{u_3}, \\
u_5 &amp;= u_{2, p} u_4 = a_{p, i}^{[l]}, &amp;&amp;\forall p \ne j.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-softmax-5" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p><span class="math display">\[\begin{align*}
\pdv{a_{p, i}^{[l]}}{u_5} &amp;= 1, \\
\pdv{a_{p, i}^{[l]}}{u_4} &amp;= \pdv{a_{p, i}^{[l]}}{u_5} \pdv{u_5}{u_4} = u_{2, p} = \exp(z_{p, i}^{[l]}), \\
\pdv{a_{p, i}^{[l]}}{u_3} &amp;= \pdv{a_{p, i}^{[l]}}{u_4} \pdv{u_4}{u_3} = -u_{2, p} \frac{1}{u_3^2} = -\frac{\exp(z_{p, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2}, \\
\pdv{a_{p, i}^{[l]}}{u_1} &amp;= \pdv{a_{p, i}^{[l]}}{u_3} \pdv{u_3}{u_1} = -u_{2, p} \frac{1}{u_3^2} = -\frac{\exp(z_{p, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2}, \\
\pdv{a_{p, i}^{[l]}}{u_{-1}} &amp;= \pdv{a_{p, i}^{[l]}}{u_1} \pdv{u_1}{u_{-1}} = -u_{2, p} \frac{1}{u_3^2} \exp(u_{-1}) = -\frac{\exp(z_{p, i}^{[l]}) \exp(z_{j, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2}.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-softmax-6" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p>We now know that</p>
<p><span class="math display">\[\begin{align*}
\pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} &amp;= -\frac{\exp(z_{j, i}^{[l]})^2}{(\sum_p \exp(z_{p, i}^{[l]}))^2} + \frac{\exp(z_{j, i}^{[l]})}{\sum_p \exp(z_{p, i}^{[l]})} \\
&amp;= a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}), \notag \\
\pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} &amp;= -\frac{\exp(z_{p, i}^{[l]}) \exp(z_{j, i}^{[l]})}{(\sum_p \exp(z_{p, i}^{[l]}))^2} \\
&amp;= -a_{p, i}^{[l]} a_{j, i}^{[l]}, \quad \forall p \ne j. \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-softmax-7" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[l]}} &amp;= \sum_p \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} = \pdv{J}{a_{j, i}^{[l]}} \pdv{a_{j, i}^{[l]}}{z_{j, i}^{[l]}} + \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} \pdv{a_{p, i}^{[l]}}{z_{j, i}^{[l]}} \\
&amp;= \pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]} (1 - a_{j, i}^{[l]}) - \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} a_{j, i}^{[l]} \\
&amp;= a_{j, i}^{[l]} \Bigl(\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \sum_{p \ne j} \pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\Bigr) \\
&amp;= a_{j, i}^{[l]} \Bigl(\pdv{J}{a_{j, i}^{[l]}} (1 - a_{j, i}^{[l]}) - \sum_p \pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]} + \pdv{J}{a_{j, i}^{[l]}} a_{j, i}^{[l]}\Bigr) \\
&amp;= a_{j, i}^{[l]} \Bigl(\pdv{J}{a_{j, i}^{[l]}} - \sum_p \pdv{J}{a_{p, i}^{[l]}} a_{p, i}^{[l]}\Bigr),
\end{split}
\end{equation*}\]</span></p>
</section>
<section id="backpropagation-softmax-8" class="slide level2 smaller">
<h2>Backpropagation: softmax</h2>
<p>Vectorized version: <span class="math display">\[\begin{equation*}
\pdv{J}{\vec{z}_{:, i}^{[l]}} = \vec{a}_{:, i}^{[l]} \odot \Bigl(\pdv{J}{\vec{a}_{:, i}^{[l]}} - \underbrace{{\vec{a}_{:, i}^{[l]}}^T \pdv{J}{\vec{a}_{:, i}^{[l]}}}_{\text{scalar}}\Bigr).
\end{equation*}\]</span></p>
<p>Let us not stop with the vectorization just yet:</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[l]}} = \vec{A}^{[l]} \odot \Bigl(\pdv{J}{\vec{A}^{[l]}} - \text{broadcast}\bigl(\underbrace{\sum_{\text{axis} = 0} \pdv{J}{\vec{A}^{[l]}} \odot \vec{A}^{[l]}}_\text{row vector}\bigr)\Bigr).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-binary-cost" class="slide level2 smaller">
<h2>Backpropagation: binary cost</h2>
<p>In binary classification, the cost function is given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
J &amp;= f(\vec{\hat{Y}}, \vec{Y}) = f(\vec{A}^{[L]}, \vec{Y}) \\
&amp;= -\frac{1}{m} \sum_i (y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)) \\
&amp;= -\frac{1}{m} \sum_i (y_i \log(a_i^{[L]}) + (1 - y_i) \log(1 - a_i^{[L]})),
\end{split}
\end{equation*}\]</span></p>
<p>which we can write as</p>
<p><span class="math display">\[\begin{equation}
J = -\frac{1}{m} \underbrace{\sum_{\text{axis} = 1} (\vec{Y} \odot \log(\vec{A}^{[L]}) + (1 - \vec{Y}) \odot \log(1 - \vec{A}^{[L]}))}_\text{scalar}.
\end{equation}\]</span></p>
</section>
<section id="backpropagation-binary-cost-1" class="slide level2">
<h2>Backpropagation: binary cost</h2>
<p>Next, we construct a computation graph:</p>

<img data-src="img/deep_nns10.png" class="r-stretch"></section>
<section id="backpropagation-binary-cost-2" class="slide level2 smaller">
<h2>Backpropagation: binary cost</h2>
<p>Let’s compute derivatives:</p>
<p><span class="math display">\[\begin{align*}
\pdv{J}{u_5} &amp;= 1, \; \pdv{J}{u_{4, i}} = \pdv{J}{u_5} \pdv{u_5}{u_{4, i}} = -\frac{1}{m}, \\
\pdv{J}{u_{3, i}} &amp;= \pdv{J}{u_{4, i}} \pdv{u_{4, i}}{u_{3, i}} = -\frac{1}{m} (1 - y_i), \\
\pdv{J}{u_{2, i}} &amp;= \pdv{J}{u_{4, i}} \pdv{u_{4, i}}{u_{2, i}} = -\frac{1}{m} y_i, \\
\pdv{J}{u_{1, i}} &amp;= \pdv{J}{u_{3, i}} \pdv{u_{3, i}}{u_{1, i}} = -\frac{1}{m} (1 - y_i) \frac{1}{u_{1, i}} = -\frac{1}{m} \frac{1 - y_i}{1 - a_i^{[L]}}, \\
\pdv{J}{u_{0, i}} &amp;= \pdv{J}{u_{1, i}} \pdv{u_{1, i}}{u_{0, i}} + \pdv{J}{u_{2, i}} \pdv{u_{2, i}}{u_{0, i}} = \frac{1}{m} (1 - y_i) \frac{1}{u_{1, i}} - \frac{1}{m} y_i \frac{1}{u_{0, i}} = \\
&amp;=\frac{1}{m} \Bigl(\frac{1 - y_i}{1 - a_i^{[L]}} - \frac{y_i}{a_i^{[L]}}\Bigr). \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-binary-cost-3" class="slide level2 smaller">
<h2>Backpropagation: binary cost</h2>
<p>Thus,</p>
<p><span class="math display">\[\begin{equation*}
\pdv{J}{a_i^{[L]}} = \frac{1}{m} \Bigl(\frac{1 - y_i}{1 - a_i^{[L]}} - \frac{y_i}{a_i^{[L]}}\Bigr),
\end{equation*}\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{A}^{[L]}} = \frac{1}{m} \Bigl(\frac{1}{1 - \vec{A}^{[L]}} \odot (1 - \vec{Y}) - \frac{1}{\vec{A}^{[L]}} \odot \vec{Y}\Bigr).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-binary-cost-4" class="slide level2 smaller">
<h2>Backpropagation: binary cost</h2>
<p>In addition, since the sigmoid activation function is used in the output layer, we get</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_i^{[L]}} &amp;= \pdv{J}{a_i^{[L]}} a_i^{[L]} (1 - a_i^{[L]}) \\
&amp;= \frac{1}{m} \Bigl(\frac{1 - y_i}{1 - a_i^{[L]}} - \frac{y_i}{a_i^{[L]}}\Bigr) a_i^{[L]} (1 - a_i^{[L]}) \\
&amp;= \frac{1}{m} ((1 - y_i) a_i^{[L]} - y_i (1 - a_i^{[L]})) \\
&amp;= \frac{1}{m} (a_i^{[L]} - y_i).
\end{split}
\end{equation*}\]</span></p>
<p>In other words,</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[L]}} = \frac{1}{m} (\vec{A}^{[L]} - \vec{Y}).
\end{equation}\]</span></p>
<p>Note that both <span class="math inline">\(\pdv{J}{\vec{Z}^{[L]}} \in \R^{1 \times m}\)</span> and <span class="math inline">\(\pdv{J}{\vec{A}^{[L]}} \in \R^{1 \times m}\)</span>, because <span class="math inline">\(n^{[L]} = 1\)</span> in this case.</p>
</section>
<section id="backpropagation-multiclass-cost" class="slide level2 smaller">
<h2>Backpropagation: multiclass cost</h2>
<p>In multiclass classification, the cost function is instead given by</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
J &amp;= f(\vec{\hat{Y}}, \vec{Y}) = f(\vec{A}^{[L]}, \vec{Y}) \\
&amp;= -\frac{1}{m} \sum_i \sum_j y_{j, i} \log(\hat{y}_{j, i}) \\
&amp;= -\frac{1}{m} \sum_i \sum_j y_{j, i} \log(a_{j, i}^{[L]}),
\end{split}
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(j = 1, \dots, n^{[L]}\)</span>. We can vectorize the cost expression:</p>
<p><span class="math display">\[\begin{equation}
J = -\frac{1}{m} \underbrace{\sum_{\substack{\text{axis} = 0 \\ \text{axis} = 1}} \vec{Y} \odot \log(\vec{A}^{[L]})}_\text{scalar}.
\end{equation}\]</span></p>
</section>
<section id="backpropagation-multiclass-cost-1" class="slide level2">
<h2>Backpropagation: multiclass cost</h2>

<img data-src="img/deep_nns11.png" class="r-stretch"></section>
<section id="backpropagation-cost" class="slide level2 smaller">
<h2>Backpropagation: cost</h2>
<p>With the computation graph in place, we can perform backward propagation:</p>
<p><span class="math display">\[\begin{align*}
\pdv{J}{u_4} &amp;= 1, \\
\pdv{J}{u_{3, i}} &amp;= \pdv{J}{u_4} \pdv{u_4}{u_{3, i}} = -\frac{1}{m}, \\
\pdv{J}{u_{2, j, i}} &amp;= \pdv{J}{u_{3, i}} \pdv{u_{3, i}}{u_{2, j, i}} = -\frac{1}{m}, \\
\pdv{J}{u_{1, j, i}} &amp;= \pdv{J}{u_{2, j, i}} \pdv{u_{2, j, i}}{u_{1, j, i}} = -\frac{1}{m} y_{j, i}, \\
\pdv{J}{u_{0, j, i}} &amp;= \pdv{J}{u_{1, j, i}} \pdv{u_{1, j, i}}{u_{0, j, i}} = -\frac{1}{m} y_{j, i} \frac{1}{u_{0, j, i}} = -\frac{1}{m} \frac{y_{j, i}}{a_{j, i}^{[L]}}.
\end{align*}\]</span></p>
</section>
<section id="backpropagation-multiclass-cost-2" class="slide level2 smaller">
<h2>Backpropagation: multiclass cost</h2>
<p>Hence,</p>
<p><span class="math display">\[\begin{equation*}
\pdv{J}{a_{j, i}^{[L]}} = -\frac{1}{m} \frac{y_{j, i}}{a_{j, i}^{[L]}}.
\end{equation*}\]</span></p>
<p>Vectorization is trivial:</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{A}^{[L]}} = -\frac{1}{m} \frac{1}{\vec{A}^{[L]}} \odot \vec{Y}.
\end{equation}\]</span></p>
</section>
<section id="backpropagation-multiclass-cost-3" class="slide level2 smaller">
<h2>Backpropagation: multiclass cost</h2>
<p>Furthermore, since the output layer uses the softmax activation function, we get</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[L]}} &amp;= a_{j, i}^{[L]} \Bigl(\pdv{J}{a_{j, i}^{[L]}} - \sum_p \pdv{J}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\Bigr) = a_{j, i}^{[L]} \Bigl(-\frac{1}{m} \frac{y_{j, i}}{a_{j, i}^{[L]}} + \sum_p \frac{1}{m} \frac{y_{p, i}}{a_{p, i}^{[L]}} a_{p, i}^{[L]}\Bigr) \\
&amp;= \frac{1}{m} \Bigl(-y_{j, i} + a_{j, i}^{[L]} \underbrace{\sum_p y_{p, i}}_{\sum \text{probabilities} = 1}\Bigr) = \frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}).
\end{split}
\end{equation*}\]</span></p>
<p>Note that <span class="math inline">\(p = 1, \dots, n^{[L]}\)</span>. To conclude,</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[L]}} = \frac{1}{m} (\vec{A}^{[L]} - \vec{Y}).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-multilabel-cost" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>
<p>We can view multi-label classification as <span class="math inline">\(j\)</span> binary classification problems:</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
J &amp;= f(\vec{\hat{Y}}, \vec{Y}) = f(\vec{A}^{[L]}, \vec{Y}) \\
&amp;= \sum_j \Bigl(-\frac{1}{m} \sum_i (y_{j, i} \log(\hat{y}_{j, i}) + (1 - y_{j, i}) \log(1 - \hat{y}_{j, i}))\Bigr) \\
&amp;= \sum_j \Bigl(-\frac{1}{m} \sum_i (y_{j, i} \log(a_{j, i}^{[L]}) + (1 - y_{j, i}) \log(1 - a_{j, i}^{[L]}))\Bigr),
\end{split}
\end{equation*}\]</span></p>
<p>where once again <span class="math inline">\(j = 1, \dots, n^{[L]}\)</span>.</p>
</section>
<section id="backpropagation-multilabel-cost-1" class="slide level2">
<h2>Backpropagation: multilabel cost</h2>
<p>Vectorization gives</p>
<p><span class="math display">\[\begin{equation}
J = -\frac{1}{m} \underbrace{\sum_{\substack{\text{axis} = 1 \\ \text{axis} = 0}} (\vec{Y} \odot \log(\vec{A}^{[L]}) + (1 - \vec{Y}) \odot \log(1 - \vec{A}^{[L]}))}_\text{scalar}.
\end{equation}\]</span></p>
</section>
<section id="backpropagation-multilabel-cost-2" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>

<img data-src="img/deep_nns12.png" class="r-stretch"></section>
<section id="backpropagation-multilabel-cost-3" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>
<p>Next, we compute the partial derivatives:</p>
<p><span class="math display">\[\begin{align*}
\pdv{J}{u_6} &amp;= 1, \\
\pdv{J}{u_{5, j}} &amp;= \pdv{J}{u_6} \pdv{u_6}{u_{5, j}} = 1, \\
\pdv{J}{u_{4, j, i}} &amp;= \pdv{J}{u_{5, j}} \pdv{u_{5, j}}{u_{4, j, i}} = -\frac{1}{m}, \\
\pdv{J}{u_{3, j, i}} &amp;= \pdv{J}{u_{4, j, i}} \pdv{u_{4, j, i}}{u_{3, j, i}} = -\frac{1}{m} (1 - y_{j, i}), \\
\pdv{J}{u_{2, j, i}} &amp;= \pdv{J}{u_{4, j, i}} \pdv{u_{4, j, i}}{u_{2, j, i}} = -\frac{1}{m} y_{j, i}, \\
\pdv{J}{u_{1, j, i}} &amp;= \pdv{J}{u_{3, j, i}} \pdv{u_{3, j, i}}{u_{1, j, i}} = -\frac{1}{m} (1 - y_{j, i}) \frac{1}{u_{1, j, i}} = -\frac{1}{m} \frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}},
\end{align*}\]</span></p>
</section>
<section id="backpropagation-multilabel-cost-4" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>
<p><span class="math display">\[\begin{align*}
\pdv{J}{u_{0, j, i}} &amp;= \pdv{J}{u_{1, j, i}} \pdv{u_{1, j, i}}{u_{0, j, i}} + \pdv{J}{u_{2, j, i}} \pdv{u_{2, j, i}}{u_{0, j, i}} \\
&amp;= \frac{1}{m} (1 - y_{j, i}) \frac{1}{u_{1, j, i}} - \frac{1}{m} y_{j, i} \frac{1}{u_{0, j, i}} \notag \\
&amp;= \frac{1}{m} \Bigl(\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \frac{y_{j, i}}{a_{j, i}^{[L]}}\Bigr). \notag
\end{align*}\]</span></p>
</section>
<section id="backpropagation-multilabel-cost-5" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>
<p>Simply put, we have</p>
<p><span class="math display">\[\begin{equation*}
\pdv{J}{a_{j, i}^{[L]}} = \frac{1}{m} \Bigl(\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \frac{y_{j, i}}{a_{j, i}^{[L]}}\Bigr),
\end{equation*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{A}^{[L]}} = \frac{1}{m} \Bigl(\frac{1}{1 - \vec{A}^{[L]}} \odot (1 - \vec{Y}) - \frac{1}{\vec{A}^{[L]}} \odot \vec{Y}\Bigr).
\end{equation}\]</span></p>
</section>
<section id="backpropagation-multilabel-cost-6" class="slide level2 smaller">
<h2>Backpropagation: multilabel cost</h2>
<p>Bearing in mind that we view multi-label classification as <span class="math inline">\(j\)</span> binary classification problems, we also know that the output layer uses the sigmoid activation function.</p>
<p>As a result,</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
\pdv{J}{z_{j, i}^{[L]}} &amp;= \pdv{J}{a_{j, i}^{[L]}} a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\
&amp;= \frac{1}{m} \Bigl(\frac{1 - y_{j, i}}{1 - a_{j, i}^{[L]}} - \frac{y_{j, i}}{a_{j, i}^{[L]}}\Bigr) a_{j, i}^{[L]} (1 - a_{j, i}^{[L]}) \\
&amp;= \frac{1}{m} ((1 - y_{j, i}) a_{j, i}^{[L]} - y_{j, i} (1 - a_{j, i}^{[L]})) = \frac{1}{m} (a_{j, i}^{[L]} - y_{j, i}),
\end{split}
\end{equation*}\]</span></p>
<p>which we can vectorize as</p>
<p><span class="math display">\[\begin{equation}
\pdv{J}{\vec{Z}^{[L]}} = \frac{1}{m} (\vec{A}^{[L]} - \vec{Y}).
\end{equation}\]</span></p>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"11e673604a90f9305f7da7d0313a2862688c0eb7568c4eadb64868141de66d23"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>