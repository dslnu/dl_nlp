<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.29">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Recurrent neural networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Recurrent neural networks</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="recurrent-neural-networks" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="hidden">

</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Uses</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Language Modelling and Generating Text</li>
<li>Speech Recognition</li>
<li>Generating Image Descriptions</li>
<li>Video Tagging</li>
</ul>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-1" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn_sequences.jpeg" class="r-stretch quarto-figure-center"><p class="caption">Recurrent networks operate on sequences of vectors</p></section>
<section id="recurrent-neural-networks-2" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector.</p></li>
<li><p>This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables.</p></li>
<li><p>Viewed this way, RNNs essentially describe programs and are Turing-complete.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>RNNs as programs</strong></p>
</div>
<div class="callout-content">
<p>If training vanilla neural nets is optimization over <strong>functions</strong>, training recurrent nets is optimization over <strong>programs</strong>.</p>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-3" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Cycles</strong></p>
</div>
<div class="callout-content">
<ul>
<li>feedforward networks – no cycles</li>
<li>RNN has cycles and transmits information back into itself.</li>
<li>cycles are <strong>recurrent connections</strong></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Connection types</strong></p>
</div>
<div class="callout-content">
<ul>
<li>standard connections: applied synchronously to propagate each layer’s activations to the subsequent layer at the same time step</li>
<li>recurrent connections: dynamic, passing information across adjacent time steps</li>
</ul>
<p>While Feedforward Networks pass information through the network without cycles, recurrent networks have cycles. This enables them to extend the functionality of Feedforward Networks to also take into account previous inputs <span class="math inline">\(\boldsymbol{X}_{0:t-1}\)</span> and not only the current input <span class="math inline">\(\boldsymbol{X}_t\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-4" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn_simple.png" class="r-stretch"></section>
<section id="recurrent-neural-networks-5" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn_simple2.png" class="r-stretch"></section>
<section id="recurrent-neural-networks-6" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Notation</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(n\)</span> – number of samples</li>
<li><span class="math inline">\(d\)</span> – number of inputs</li>
<li><span class="math inline">\(h\)</span> – number of hidden units</li>
<li><span class="math inline">\(\boldsymbol{H}_t \in \mathbb{R}^{n \times h}\)</span> – hidden state at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)</span> – input at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)</span> – weight matrix</li>
<li><span class="math inline">\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)</span> – hidden-state-to-hidden-state matrix</li>
<li><span class="math inline">\(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)</span> – bias parameter</li>
<li><span class="math inline">\(\phi\)</span> – activation function (usually sigmoid or tanh)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-7" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/ff_vs_rn.png" class="r-stretch quarto-figure-center"><p class="caption">Note, that here the option of having multiple hidden layers is aggregated to one Hidden Layer block <span class="math inline">\(\boldsymbol{H}\)</span>. This block can obviously be extended to multiple hidden layers.</p></section>
<section id="recurrent-neural-networks-8" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Equation for hidden variable</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{hidden_var}
\boldsymbol{H}_t = \phi_h \left(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh} + \boldsymbol{b}_h\right)
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Equation for output variable</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{output_var}
\boldsymbol{O}_t = \phi_o \left(\boldsymbol{H}_t \boldsymbol{W}_{ho} + \boldsymbol{b}_o\right)
\]</span> Since <span class="math inline">\(\boldsymbol{H}_t\)</span> recursively includes <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> and this process occurs for every time step the RNN includes traces of all hidden states that preceded <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> as well as <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> itself.</p>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-9" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation for hidden variable</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{hidden_var_computation}
\boldsymbol{H} = \phi_h \left(\boldsymbol{X} \boldsymbol{W}_{xh} + \boldsymbol{b}_h\right)
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation for output variable</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{output_var_computation}
\boldsymbol{O} = \phi_o \left(\boldsymbol{H} \boldsymbol{W}_{ho} + \boldsymbol{b}_o\right)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-10" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hidden state computation</strong></p>
</div>
<div class="callout-content">
<p>At any time step <span class="math inline">\(t\)</span>:</p>
<ul>
<li>concatenate the input <span class="math inline">\(\boldsymbol{X}_t\)</span> at the current time step <span class="math inline">\(t\)</span> and the hidden state <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> at the previous time step <span class="math inline">\(t\)</span></li>
<li>feed the concatenation result into a fully connected layer with the activation function <span class="math inline">\(\phi\)</span></li>
<li>The output of such a fully connected layer is the hidden state <span class="math inline">\(\boldsymbol{H}_t\)</span> of the current time step <span class="math inline">\(t\)</span></li>
<li>the model parameters are the concatenation of <span class="math inline">\(\boldsymbol{W}_{xh}\)</span> and <span class="math inline">\(\boldsymbol{W}_{hh}\)</span> and a bias <span class="math inline">\(\boldsymbol{b}_h\)</span></li>
<li><span class="math inline">\(\boldsymbol{H}_t\)</span> will participate in computing the hidden state <span class="math inline">\(\boldsymbol{H}_{t+1}\)</span> of the next time step <span class="math inline">\(t+1\)</span></li>
<li><span class="math inline">\(\boldsymbol{H}_t\)</span> will also be fed into the fully connected output layer to compute the output <span class="math inline">\(\boldsymbol{O}_t\)</span> of the current time step <span class="math inline">\(t\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="recurrent-neural-networks-11" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn_hidden_state.png" class="r-stretch"></section>
<section id="recurrent-neural-networks-12" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Language modeling</strong></p>
</div>
<div class="callout-content">
<p>We aim to predict the next token based on the current and past tokens; thus we shift the original sequence by one token as the targets (labels).</p>
<p><strong>Bengio</strong> et al.&nbsp;(2003) first proposed to use a neural network for language modeling.</p>
</div>
</div>
</div>
<!-- Let the minibatch size be one, and the sequence of the text be “machine”. To simplify training in subsequent sections, we tokenize text into characters rather than words and consider a character-level language model. The next figure demonstrates how to predict the next character based on the current and previous characters via an RNN for character-level language modeling. -->
</section>
<section id="recurrent-neural-networks-13" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn_char_based_lang_model.png" class="r-stretch quarto-figure-center"><p class="caption">A character-level language model based on the RNN. The input and target sequences are “machin” and “achine”, respectively.</p></section>
<section id="recurrent-neural-networks-14" class="slide level2">
<h2>Recurrent neural networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Process</strong></p>
</div>
<div class="callout-content">
<p>During the training process, we run a softmax operation on the output from the output layer for each time step, and then use the cross-entropy loss to compute the error between the model output and the target.</p>
<p>Because of the recurrent computation of the hidden state in the hidden layer, the output <span class="math inline">\(\boldsymbol{O}_3\)</span> of time step 3 is determined by the text sequence “m”, “a”, and “c”. Since the next character of the sequence in the training data is “h”, the loss of time step 3 will depend on the probability distribution of the next character generated based on the feature sequence “m”, “a”, “c” and the target “h” of this time step.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>In practice, each token is represented by a <span class="math inline">\(d\)</span>-dimensional vector, and we use a batch size <span class="math inline">\(n\)</span>. Therefore, the input <span class="math inline">\(\boldsymbol{X}_t\)</span> at time step <span class="math inline">\(t\)</span> will be an <span class="math inline">\(n \times d\)</span> matrix.</p>
</div>
</div>
</div>
</section>
<section>
<section id="backpropagation-in-rnns" class="title-slide slide level1 center">
<h1>Backpropagation in RNNs</h1>

</section>
<section id="backpropagation-in-rnns-1" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backpropagation Through Time (BPTT)</strong></p>
</div>
<div class="callout-content">
<ul>
<li>BPTT expands (or unrolls) the computational graph of an RNN one time step at a time.</li>
<li>unrolled RNN is a FF neural network with same parameters appearing at each time step</li>
<li>The gradient with respect to each parameter must be summed across all places that the parameter occurs in the unrolled net.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="backpropagation-in-rnns-2" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backpropagation Through Time (BPTT)</strong></p>
</div>
<div class="callout-content">
<p>When we forward pass our input <span class="math inline">\(\boldsymbol{X}_t\)</span> through the network, we compute the hidden state <span class="math inline">\(\boldsymbol{H}_t\)</span> and the output state <span class="math inline">\(\boldsymbol{O}_t\)</span> one step at at time.</p>
<p>We can then define a loss function <span class="math inline">\(L(\boldsymbol{O}, \boldsymbol{Y})\)</span> to describe the difference between all outputs <span class="math inline">\(\boldsymbol{O}_t\)</span> and target values <span class="math inline">\(\boldsymbol{Y}_t\)</span>, summing up loss terms <span class="math inline">\(l_t\)</span>.</p>
<p><span class="math display">\[
\label{loss_function}
L(\boldsymbol{O}, \boldsymbol{Y}) = \sum\limits_{t=1}^T l_t (\boldsymbol{O}_t, \boldsymbol{Y}_t)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="backpropagation-in-rnns-3" class="slide level2">
<h2>Backpropagation in RNNs</h2>

<img data-src="img/bptt_graph.png" class="r-stretch quarto-figure-center"><p class="caption">Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.</p></section>
<section id="backpropagation-in-rnns-4" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<p><span class="math display">\[
\notag
\frac{\partial L}{\partial \boldsymbol{W}_{ho}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \frac{\partial \phi_o}{\partial \boldsymbol{W}_{ho}} = \\
\label{loss_function_derivative1}
= \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t} \cdot \frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \boldsymbol{H}_t.
\]</span></p>
</section>
<section id="backpropagation-in-rnns-5" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<p><span class="math display">\[
\label{loss_function_derivative2}
\frac{\partial L}{\partial \boldsymbol{W}_{hh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \frac{\partial \phi_o}{\boldsymbol{H}_t} \cdot \frac{\partial \boldsymbol{H}_t}{\partial \phi_h} \cdot \frac{\partial \phi_h}{\partial \boldsymbol{W}_{hh}}= \\
= \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t} \cdot \frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \boldsymbol{W}_{ho} \cdot \frac{\partial \boldsymbol{H}_t}{\partial \phi_h} \cdot \frac{\partial \phi_h}{\partial \boldsymbol{W}_hh}.
\]</span></p>
</section>
<section id="backpropagation-in-rnns-6" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<p><span class="math display">\[
\label{loss_function_derivative3}
\frac{\partial L}{\partial \boldsymbol{W}_{xh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \frac{\partial \phi_o}{\boldsymbol{H}_t} \cdot \frac{\partial \boldsymbol{H}_t}{\partial \phi_h} \cdot \frac{\partial \phi_h}{\partial \boldsymbol{W}_{xh}}= \\
= \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t} \cdot \frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \boldsymbol{W}_{ho} \cdot \frac{\partial \boldsymbol{H}_t}{\partial \phi_h} \cdot \frac{\partial \phi_h}{\partial \boldsymbol{W}_xh}.
\]</span></p>
</section>
<section id="backpropagation-in-rnns-7" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<p>Since each <span class="math inline">\(\boldsymbol{H}_t\)</span> depends on the previous time step, we can substitute the last part from above equations to obtain <span class="math display">\[
\label{loss_function_derivative4}
\frac{\partial L}{\partial \boldsymbol{W}_{hh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \boldsymbol{W}_{ho} \sum\limits_{k=1}^t \frac{\partial \boldsymbol{H}_t}{\partial \boldsymbol{H}_k}\frac{\partial \boldsymbol{H}_t}{\partial \boldsymbol{W}_{hh}}.
\]</span></p>
<p><span class="math display">\[
\label{loss_function_derivative5}
\frac{\partial L}{\partial \boldsymbol{W}_{xh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \boldsymbol{W}_{ho} \sum\limits_{k=1}^t \frac{\partial \boldsymbol{H}_t}{\partial \boldsymbol{H}_k}\frac{\partial \boldsymbol{H}_t}{\partial \boldsymbol{W}_{xh}}.
\]</span></p>
</section>
<section id="backpropagation-in-rnns-8" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>OR:</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{loss_function_derivative6}
\frac{\partial L}{\partial \boldsymbol{W}_{hh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \boldsymbol{W}_{ho} \sum\limits_{k=1}^t \left(\boldsymbol{W}^T_{hh}\right)^{t-k} \cdot \boldsymbol{H}_k.
\]</span></p>
<p><span class="math display">\[
\label{loss_function_derivative7}
\frac{\partial L}{\partial \boldsymbol{W}_{xh}} = \sum\limits_{t=1}^T \frac{\partial l_t}{\partial \boldsymbol{O}_t}\cdot\frac{\partial \boldsymbol{O}_t}{\partial \phi_o} \cdot \boldsymbol{W}_{ho} \sum\limits_{k=1}^t \left(\boldsymbol{W}^T_{hh}\right)^{t-k} \cdot \boldsymbol{X}_k.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="backpropagation-in-rnns-9" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>A problem</strong></p>
</div>
<div class="callout-content">
<p>We can see that we need to store powers of <span class="math inline">\(\boldsymbol{W}_{hh}^k\)</span> as we proceed through each loss term <span class="math inline">\(l_t\)</span> of the overall loss function <span class="math inline">\(L\)</span> that can become very large.</p>
<p>For these large values this method becomes numerically unstable since eigenvalues smaller than 1 vanish and eigenvalues larger than 1 diverge.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>Truncate a sum at computationally convenient size - <strong>Truncated BPTT</strong>.</p>
</div>
</div>
</div>
</section>
<section id="backpropagation-in-rnns-10" class="slide level2">
<h2>Backpropagation in RNNs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Truncated BPTT</strong></p>
</div>
<div class="callout-content">
<p>This establishes an upper bound for the number of time steps the gradient can flow back to.</p>
<p><strong>Interpretation:</strong> a moving window of past time steps which the RNN considers. Anything before the cut-off time step doesn’t get taken into account. Since BPTT basically unfolds the RNN to create a new layer for each time step we can also think of this procedure as limiting the number of hidden layers.</p>
<p><strong>Result:</strong> model focuses primarily on short-term influence rather than long-term consequences.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="lstms" class="title-slide slide level1 center">
<h1>LSTMs</h1>

</section>
<section id="lstms-1" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>In <span class="math inline">\(\eqref{loss_function_derivative4}\)</span> and <span class="math inline">\(\eqref{loss_function_derivative5}\)</span> we see <span class="math inline">\(\frac{\partial \boldsymbol{H}_t}{\partial \boldsymbol{H}_k}\)</span> that introduces a matrix multiplication over a potentially very long sequence.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>LSTMs were designed to handle a vanishing gradient problem.</p>
<p>Since they use a more constant error, they allow RNNs to learn over a lot more time steps (way over 1000).</p>
</div>
</div>
</div>
</section>
<section id="lstms-2" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gates</strong></p>
</div>
<div class="callout-content">
<p>To achieve that, LSTMs store more information outside of the traditional neural network flow in structures called <strong>gated cells</strong>.</p>
<h3 id="gate-types">Gate types</h3>
<ul>
<li><strong>output gate</strong> <span class="math inline">\(\boldsymbol{O}_t\)</span> – reads entries of the cell</li>
<li><strong>input gate</strong> <span class="math inline">\(\boldsymbol{I}_t\)</span> – reads data into the cell</li>
<li><strong>forget gate</strong> <span class="math inline">\(\boldsymbol{F}_t\)</span> – resets the content of the cell.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="lstms-3" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computations</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{lstm_computations}
\boldsymbol{O}_t = \sigma\left(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o\right), \\
\boldsymbol{I}_t = \sigma\left(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i\right), \\
\boldsymbol{F}_t = \sigma\left(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f\right).
\]</span></p>
<p>Here <span class="math inline">\(\boldsymbol{W}_{xo}, \boldsymbol{W}_{xi}\boldsymbol{W}_{xf} \in \mathbb{R}^{d \times h}\)</span>, and <span class="math inline">\(\boldsymbol{W}_{ho}, \boldsymbol{W}_{hi}\boldsymbol{W}_{hf} \in \mathbb{R}^{h \times h}\)</span> are weight matrices and <span class="math inline">\(\boldsymbol{b}_o, \boldsymbol{b}_i, \boldsymbol{b}_f \in \mathbb{R}^{1 \times h}\)</span> are their respective biases.</p>
<p>Further, they use the sigmoid activation function <span class="math inline">\(\sigma\)</span> to transform the output <span class="math inline">\(\in (0, 1)\)</span> which each results in a vector with entries <span class="math inline">\(\in (0, 1)\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="lstms-4" class="slide level2">
<h2>LSTMs</h2>

<img data-src="img/lec12_lstm1.png" class="r-stretch quarto-figure-center"><p class="caption">Calculation of input, forget, and output gates in an LSTM</p></section>
<section id="lstms-5" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Candidate memory cell</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(\tilde{\boldsymbol{C}}_t \in \mathbb{R}^{n \times h}\)</span> – similar computation as previously mentioned gates but uses tanh to have output <span class="math inline">\(\in (-1, 1)\)</span>.</p>
<p>Has its own weights <span class="math inline">\(\boldsymbol{W}_{xc} \in \mathbb{R}^{d \times h},\boldsymbol{W}_{hc} \in \mathbb{R}^{h \times h}\)</span> and biases <span class="math inline">\(\boldsymbol{b}_c \in \mathbb{R}^{1 \times h}\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{candidate_memory_cell_computation}
\tilde{\boldsymbol{C}}_t = tanh \left(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c\right)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="lstms-6" class="slide level2">
<h2>LSTMs</h2>

<img data-src="img/lec12_lstm2.png" class="r-stretch quarto-figure-center"><p class="caption">Computation of candidate memory cells in LSTM.</p></section>
<section id="lstms-7" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Old memory content</strong></p>
</div>
<div class="callout-content">
<p>To put things together we introduce old memory content <span class="math inline">\(\boldsymbol{C}_{t−1} \in \mathbb{R}^{n \times h}\)</span> which together with the introduced gates controls how much of the old memory content we want to preserve to get to the new memory content <span class="math inline">\(\boldsymbol{C}_t\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{candidate_memory_cell_computation2}
\boldsymbol{C}_t = \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="lstms-8" class="slide level2">
<h2>LSTMs</h2>

<img data-src="img/lec12_lstm3.png" class="r-stretch quarto-figure-center"><p class="caption">Computation of memory cells in an LSTM.</p></section>
<section id="lstms-9" class="slide level2">
<h2>LSTMs</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hidden states computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{hidden_states_computation}
\boldsymbol{H}_t = \boldsymbol{O}_t \odot tanh\left(\boldsymbol{C}_t\right), \; \boldsymbol{H}_t \in \mathbb{R}^{n \times h}.
\]</span></p>
<ul>
<li>the output gate is close to 1 – we allow the memory cell internal state to impact the subsequent layers uninhibited,</li>
<li>the output gate is close to – we prevent the current memory from impacting other layers of the network at the current time step.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>A memory cell can accrue information across many time steps without impacting the rest of the network (as long as the output gate takes values close to 0), and then suddenly impact the network at a subsequent time step as soon as the output gate flips from values close to 0 to values close to 1.</p>
</div>
</div>
</div>
</section>
<section id="lstms-10" class="slide level2">
<h2>LSTMs</h2>

<img data-src="img/lec12_lstm4.png" class="r-stretch quarto-figure-center"><p class="caption">Computation of the hidden state in an LSTM.</p></section></section>
<section>
<section id="gated-recurrent-units" class="title-slide slide level1 center">
<h1>Gated Recurrent Units</h1>

</section>
<section id="gated-recurrent-units-1" class="slide level2">
<h2>Gated Recurrent Units</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rationale</strong></p>
</div>
<div class="callout-content">
<ul>
<li>simpler architecture</li>
<li>retain internal state and gating mechanisms</li>
<li>but speed up computation</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Outline</strong></p>
</div>
<div class="callout-content">
<ul>
<li>instead of LSTM’s 3 gates, use 2: <strong>reset gate</strong> and <strong>update gate</strong></li>
<li>reset gate controls how much of the previous state we want to remember</li>
<li>update gate controls how much of the new state is just a copy of the old one</li>
</ul>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-units-2" class="slide level2">
<h2>Gated Recurrent Units</h2>

<img data-src="img/gru_gate_computation.png" class="r-stretch quarto-figure-center"><p class="caption">Computing the reset gate and the update gate in a GRU model.</p></section>
<section id="gated-recurrent-units-3" class="slide level2">
<h2>Gated Recurrent Units</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gate computation</strong></p>
</div>
<div class="callout-content">
<p>Suppose that for a given time step <span class="math inline">\(t\)</span> we have:</p>
<ul>
<li>a minibatch <span class="math inline">\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)</span></li>
<li>hidden state of the previous time step <span class="math inline">\(\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}\)</span></li>
<li><span class="math inline">\(\boldsymbol{W}_{xr}, \boldsymbol{W}_{xz} \in \mathbb{R}^{d \times h}, \; \boldsymbol{W}_{hr}, \boldsymbol{W}_{hz} \in \mathbb{R}^{h \times h}\)</span> are weights</li>
<li><span class="math inline">\(\boldsymbol{b}_r, \boldsymbol{b}_z \in \mathbb{R}^{1 \times h}\)</span> are bias parameters</li>
</ul>
<p>We compute: <span class="math display">\[
\boldsymbol{R}_t = \sigma\left(\boldsymbol{X}_t \boldsymbol{W}_{xr} + \boldsymbol{H}_{t-1}\boldsymbol{W}_{hr} + \boldsymbol{b}_r\right),\\
\boldsymbol{Z}_t = \sigma\left(\boldsymbol{X}_t \boldsymbol{W}_{xz} + \boldsymbol{H}_{t-1}\boldsymbol{W}_{hz} + \boldsymbol{b}_z\right).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-units-4" class="slide level2">
<h2>Gated Recurrent Units</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Candidate hidden state at time step <span class="math inline">\(t\)</span></strong></p>
</div>
<div class="callout-content">
<p>Next, we integrate reset gate <span class="math inline">\(\boldsymbol{R}_t\)</span> with the regular updating mechanism and obtain: <span class="math display">\[
\tilde{\boldsymbol{H}}_t = tanh\left(\boldsymbol{X}_t \boldsymbol{W}_{xh} + (\boldsymbol{R}_t \odot \boldsymbol{H}_{t-1}) \boldsymbol{W}_{hh} + \boldsymbol{b}_h\right), \; \tilde{\boldsymbol{H}}_t \in \mathbb{R}^{n\times h}
\]</span></p>
<ul>
<li>influence of previous states is reduced with the Hadamard product of <span class="math inline">\(\boldsymbol{R}_t\)</span> and <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span></li>
<li>entries in <span class="math inline">\(\boldsymbol{R}_t\)</span> close to 1 – vanilla RNN</li>
<li>entries in <span class="math inline">\(\boldsymbol{R}_t\)</span> close to 0 – candidate hidden state is the result of MLP with <span class="math inline">\(\boldsymbol{X}_t\)</span> as an input</li>
<li>any pre-existing hidden state is thus reset to defaults.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-units-5" class="slide level2">
<h2>Gated Recurrent Units</h2>

<img data-src="img/gru_candidate_hidden_state_computation.png" class="r-stretch quarto-figure-center"><p class="caption">Computing the candidate hidden state in a GRU model.</p></section>
<section id="gated-recurrent-units-6" class="slide level2">
<h2>Gated Recurrent Units</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Update gate</strong></p>
</div>
<div class="callout-content">
<p>Finally, we need to incorporate the effect of the update gate <span class="math inline">\(\boldsymbol{Z}_t\)</span>.</p>
<p>This determines the extent to which the new hidden state <span class="math inline">\(\boldsymbol{H}_t \in \mathbb{R}^{n \times h}\)</span> matches the old state <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> compared with how much it resembles the new candidate state <span class="math inline">\(\tilde{\boldsymbol{H}}_t\)</span>.</p>
<p>The update gate <span class="math inline">\(\boldsymbol{Z}_t\)</span> can be used for this purpose, simply by taking elementwise convex combinations of <span class="math inline">\(\boldsymbol{H}_{t-1}\)</span> and <span class="math inline">\(\tilde{\boldsymbol{H}}_t\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{H}_t = \boldsymbol{Z}_t \odot \boldsymbol{H}_{t-1} + (1-\boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t.
\]</span></p>
<ul>
<li><span class="math inline">\(\boldsymbol{Z}_t\)</span> close to 1 – retain the old state. In this case the information from <span class="math inline">\(\boldsymbol{X}_t\)</span> is ignored%, effectively skipping time step <span class="math inline">\(t\)</span> in the dependency chain.</li>
<li><span class="math inline">\(\boldsymbol{Z}_t\)</span> is close to 0 – the new latent state <span class="math inline">\(\boldsymbol{H}_t\)</span> approaches the candidate latent state <span class="math inline">\(\tilde{\boldsymbol{H}}_t\)</span>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-units-7" class="slide level2">
<h2>Gated Recurrent Units</h2>

<img data-src="img/gru_hidden_state_computation.png" class="r-stretch quarto-figure-center"><p class="caption">Computing the hidden state in a GRU model.</p></section>
<section id="gated-recurrent-units-8" class="slide level2">
<h2>Gated Recurrent Units</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Summary</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Reset gates help capture <strong>short-term</strong> dependencies in sequences.</li>
<li>Update gates help capture <strong>long-term</strong> dependencies in sequences.</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="deep-recurrent-neural-networks" class="title-slide slide level1 center">
<h1>Deep Recurrent Neural Networks</h1>

</section>
<section id="deep-recurrent-neural-networks-1" class="slide level2">
<h2>Deep Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stacking</strong></p>
</div>
<div class="callout-content">
<p>To construct a deep RNN with <span class="math inline">\(L\)</span> hidden layers we simply stack ordinary RNNs of any type on top of each other.</p>
<p>Each hidden state <span class="math inline">\(\boldsymbol{H}^{(l)}_t \in \mathbb{R}^{n\times h}\)</span> is passed to the next time step of the current layer <span class="math inline">\(\boldsymbol{H}^{(l)}_{t+1}\)</span> as well as the current time step of the next layer <span class="math inline">\(\boldsymbol{H}^{(l+1)}_t\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>State computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{drnn_subsequent_state}
\boldsymbol{H}^{(l)}_t = \phi_l \left(\boldsymbol{H}^{(l-1)}_t \boldsymbol{W}_{xh}^{(l)} + \boldsymbol{H}_{t-1}^{(l)} \boldsymbol{W}_{hh}^{(l)} + \boldsymbol{b}_h^{(l)}\right),
\]</span> where <span class="math inline">\(\boldsymbol{H}_t^{(0)} = \boldsymbol{X}_t\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="deep-recurrent-neural-networks-2" class="slide level2">
<h2>Deep Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Output computation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{drnn_output_computation}
\boldsymbol{O}_t = \phi_o \left(\boldsymbol{H}_t^{(L)} \boldsymbol{W}_{ho} + \boldsymbol{b}_o\right), \; \boldsymbol{O}_t \in \mathbb{R}^{n \times o}.
\]</span></p>
<p>Note that we only use the hidden state of layer <span class="math inline">\(L\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="deep-recurrent-neural-networks-3" class="slide level2">
<h2>Deep Recurrent Neural Networks</h2>

<img data-src="img/deep_rnn_arch.png" class="r-stretch quarto-figure-center"><p class="caption">Architecture of a deep recurrent neural network.</p></section></section>
<section>
<section id="bidirectional-recurrent-neural-networks" class="title-slide slide level1 center">
<h1>Bidirectional Recurrent Neural Networks</h1>

</section>
<section id="bidirectional-recurrent-neural-networks-1" class="slide level2">
<h2>Bidirectional Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Language modeling example</strong></p>
</div>
<div class="callout-content">
<p>Based on our current models we are able to reliably predict the next sequence element (i.e.&nbsp;the next word) based on what we have seen so far. However, there scenarios where we might want to fill in a gap in a sentence and the part of the sentence after the gap conveys significant information. This information is necessary to take into account to perform well on this kind of task. On a more generalised level we want to incorporate a <strong>look-ahead property</strong> for sequences.</p>
<ol type="1">
<li>I am <code>___</code>.</li>
<li>I am <code>___</code> hungry.</li>
<li>I am <code>___</code> hungry, and I can eat half a pig.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="bidirectional-recurrent-neural-networks-2" class="slide level2">
<h2>Bidirectional Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>To achieve this look-ahead property Bidirectional Recurrent Neural Networks (BRNNs) got introduced which basically add another hidden layer which run the sequence backwards starting from the last element.</p>
<p>We simply implement two unidirectional RNN layers chained together in opposite directions and acting on the same input. For the first RNN layer, the first input is <span class="math inline">\(\boldsymbol{X}_1\)</span> and the last input is <span class="math inline">\(\boldsymbol{X}_T\)</span>, but for the second RNN layer, the first input is <span class="math inline">\(\boldsymbol{X}_T\)</span> and the last input is <span class="math inline">\(\boldsymbol{X}_1\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="bidirectional-recurrent-neural-networks-3" class="slide level2">
<h2>Bidirectional Recurrent Neural Networks</h2>

<img data-src="img/brnn_architecture.png" class="r-stretch quarto-figure-center"><p class="caption">Architecture of a bidirectional recurrent neural network.</p></section>
<section id="bidirectional-recurrent-neural-networks-4" class="slide level2">
<h2>Bidirectional Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Forward/Backward Hidden States</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\label{brnn_forward}
\overrightarrow{\boldsymbol{H}}_t = \phi\left(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(f)} + \overrightarrow{\boldsymbol{H}}_{t-1}\boldsymbol{W}_{hh}^{(f)} + \boldsymbol{b}_h^{(f)}\right),\; \overrightarrow{\boldsymbol{H}}_t \in \mathbb{R}^{n \times h},
\]</span> <span class="math display">\[
\label{brnn_backward}
\overleftarrow{\boldsymbol{H}}_t = \phi\left(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(b)} + \overleftarrow{\boldsymbol{H}}_{t+1}\boldsymbol{W}_{hh}^{(b)} + \boldsymbol{b}_h^{(b)}\right), \; \overleftarrow{\boldsymbol{H}}_t \in \mathbb{R}^{n \times h}.
\]</span></p>
<p>Note two sets of hidden matrices and biases: <span class="math display">\[
\boldsymbol{W}_{xh}^{(f)},\boldsymbol{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \; \boldsymbol{W}_{hh}^{(f)},\boldsymbol{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h},\\
\boldsymbol{b}_h^{(f)}, \boldsymbol{b}_h^{(b)} \in \mathbb{R}^{1 \times h}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="bidirectional-recurrent-neural-networks-5" class="slide level2">
<h2>Bidirectional Recurrent Neural Networks</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>BRNN output</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\boldsymbol{O}_t = \phi\left(\left[\overrightarrow{\boldsymbol{H}}_t \frown \overleftarrow{\boldsymbol{H}}_t\right]\boldsymbol{W}_{ho} + \boldsymbol{b}_o\right),
\]</span> where <span class="math inline">\(\frown\)</span> denotes matrix concatenation (stacking them on top of each other).</p>
<p>Weight matrices <span class="math inline">\(\boldsymbol{W}_{ho} \in \mathbb{R}^{2h\times o}\)</span>, bias parameters <span class="math inline">\(\boldsymbol{b}_o \in \mathbb{R}^{1 \times o}\)</span>.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="encoder-decoder-architecture" class="title-slide slide level1 center">
<h1>Encoder-Decoder Architecture</h1>

</section>
<section id="encoder-decoder-architecture-1" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<ul>
<li>network is twofold</li>
<li><strong>encoder network</strong> – encode the (variable-length) input into a state</li>
<li><strong>decoder network</strong> – decode the state into an output</li>
</ul>
</div>
</div>
</div>

<img data-src="img/enc_dec_arch.png" class="r-stretch"></section>
<section id="encoder-decoder-architecture-2" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>seq2seq</strong></p>
</div>
<div class="callout-content">
<p>Based on this Encoder-Decoder architecture a model called Sequence to Sequence (seq2seq) got proposed for generating a sequence output based on a sequence input. This model uses RNNs for the encoder as well as the decoder where the hidden state of the encoder gets passed to the hidden state of the decoder.</p>
<p>It mainly focuses on mapping a fixed length input sequence of size <span class="math inline">\(n\)</span> to an fixed length output sequence of size <span class="math inline">\(m\)</span> where <span class="math inline">\(n \neq m\)</span> can be true but isn’t a necessity.</p>
</div>
</div>
</div>
</section>
<section id="encoder-decoder-architecture-3" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>

<img data-src="img/seq2seq.png" class="r-stretch quarto-figure-center"><p class="caption">Seq2seq model</p></section>
<section id="encoder-decoder-architecture-4" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Encoder</strong></p>
</div>
<div class="callout-content">
<ul>
<li>RNN accepts a single element of the sequence <span class="math inline">\(\boldsymbol{X}_t\)</span>, <span class="math inline">\(t\)</span> being order of the sequence element</li>
<li>these RNNs can be LSTMs or GRUs for performance</li>
<li>hidden states <span class="math inline">\(\boldsymbol{H}_t\)</span> are computed according to the definition of hidden states in the used RNN type (LSTM or GRU)</li>
<li>The Encoder Vector (context) is a representation of the last hidden state of the encoder network which aims to aggregate all information from all previous input elements.</li>
<li>This functions as initial hidden state of the decoder network of the model and enables the decoder to make accurate predictions.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="encoder-decoder-architecture-5" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder</strong></p>
</div>
<div class="callout-content">
<ul>
<li>RNN which predicts an output <span class="math inline">\(\boldsymbol{Y}_t\)</span> at a time step <span class="math inline">\(t\)</span></li>
<li>The produced output is again a sequence where each <span class="math inline">\(\boldsymbol{Y}_t\)</span> is a sequence element with order <span class="math inline">\(t\)</span></li>
<li>At each time step the RNN accepts a hidden state from the previous unit and itself produces an output as well as a new hidden state.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="encoder-decoder-architecture-6" class="slide level2">
<h2>Encoder-Decoder Architecture</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Encoder computation</strong></p>
</div>
<div class="callout-content">
<p>Encoder transforms the hidden states at all time steps into a context variable <span class="math inline">\(\boldsymbol{C}\)</span> through a customized function <span class="math inline">\(q\)</span>: <span class="math display">\[
\boldsymbol{C} = q\left(\boldsymbol{H}_1, \dots, \boldsymbol{H}_T\right)
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Decoder computation</strong></p>
</div>
<div class="callout-content">
<p>Decoder assigns a predicted probability to each possible token occurring at step <span class="math inline">\(t'+1\)</span> conditioned upon the previous tokens in the target <span class="math inline">\(y_1, \dots, y_{t'}\)</span> and the context variable <span class="math inline">\(\boldsymbol{C}\)</span>, i.e.&nbsp;<span class="math inline">\(P(y_{t'+1} | y_1, \dots, y_{t'}, \boldsymbol{C})\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{S}_{t'} = g\left(y_{t'-1}, \boldsymbol{C}, \boldsymbol{S}_{t'-1}\right)
\]</span></p>
</div>
</div>
</div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"422433a0ba05fc61fae86fba5472bf329e67b28592de698028ca670214f8a8b2"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>