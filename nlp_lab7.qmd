---
title: "NLP: Lab 7 (TF-IDF)"
execute:
  enabled: true
  echo: true
  cache: true
format:
  html:
    code-fold: false
jupyter: python3
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
filters:
  - diagram
---
# Terms
Let's agree on terminology:

- **corpus** (plural: **corpora**) is a collection of documents
- **document** is a sequence of sentences. Say, a paragraph, or a chapter
- **sentence** is a sequence of words
- **word** is synonymous to **term**

## TF-IDF vectorizer

In this model,  the meaning of a word is defined by a simple function of the counts of distinct words.

Usually used for **term-document** matrices.

Denote a term by $t$, a document by $d$, and the corpus by $D$. 
$$
count(t,d) \equiv n \text{ of times that } t \text{ appears in } d
$$

By **term frequency** we denote
$$
  TF(t,d) \equiv \begin{cases}
   1 + log(count(t,d)), \; \text{if } count(t,d) > 0,\\
   0, \; otherwise
 \end{cases}
$$
By **document frequency** we denote
$$
DF(t,D) \equiv n \text{ of documents that contain } t
$$

**Inverse document frequency** is defined to counter-balance the impact of often-encountered terms. IDF is a numerical measure of how much information a term provides:
$$
 IDF(t,D)=log\dfrac{|D|+1}{DF(t,D)+1},
$$

where $|D|$ is the total number of documents in the corpus. 

The TF-IDF measure is simply the product of TF and IDF:
$$
TFIDF(t,d,D)=TF(t,d)â‹…IDF(t,D).
$$

# Exercises
**Task 0**. Using https://www.nltk.org/howto/corpus.html#overview, implement TF-IDF vectorizer for e.g. Treebank corpus

# Recommended reading 

- Chapter 6.5 from [Jurafsky's book](https://web.stanford.edu/~jurafsky/slp3/).
