<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Principal Component Analysis</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Principal Component Analysis</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="plan" class="slide level2">
<h2>Plan</h2>
<div class="hidden">

</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Notions to discuss</strong></p>
</div>
<div class="callout-content">
<ul>
<li>PCA</li>
<li>SVD</li>
<li>LSA</li>
<li>SNE</li>
<li><del>NSA</del></li>
<li><del>FBI</del></li>
</ul>
</div>
</div>
</div>
</section>
<section id="dimensionality" class="slide level2">
<h2>Dimensionality</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Curse of Dimensionality</strong></p>
</div>
<div class="callout-content">
<ul>
<li>When the dimensionality increases, the volume of the space increases so fast that the available data become sparse.</li>
<li>In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Motivation</strong></p>
</div>
<div class="callout-content">
<p>Principal component analysis (PCA):</p>
<ul>
<li>a standard tool in modern data analysis (in diverse fields from neuroscience to computer graphics)</li>
<li>a simple, non-parametric method for extracting relevant information from confusing data sets</li>
<li>with minimal effort PCA provides a roadmap for how to reduce a complex data set to a lower dimension to reveal the sometimes hidden, simplified structures that often underlie it.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-1" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Uses</strong></p>
</div>
<div class="callout-content">
<ul>
<li>data visualization</li>
<li>feature selection</li>
<li>noise reduction</li>
<li>machine learning</li>
<li>data mining</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-example" class="slide level2">
<h2>PCA: example</h2>
<p><img data-src="img/spring.png"></p>

<aside><div>
<p>Pretend we are studying the motion of the physicist’s ideal spring.</p>
</div></aside></section>
<section id="pca-2" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Experiment setup</strong></p>
</div>
<div class="callout-content">
<ul>
<li>We choose three camera positions <span class="math inline">\(\vec{a}\)</span>, <span class="math inline">\(\vec{b}\)</span>, <span class="math inline">\(\vec{c}\)</span> at some arbitrary angles with respect to the system.</li>
<li>The angles between our measurements might not even be 90 degrees!</li>
<li>Now, we record with the cameras for several minutes.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Question</strong></p>
</div>
<div class="callout-content">
<p>The <strong>big question</strong> remains: <em>how do we get from this data set to a simple equation of <span class="math inline">\(x\)</span>?</em></p>
</div>
</div>
</div>
</section>
<section id="pca-3" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Issues</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Which measurements to perform? (what is <strong>important</strong>?)</li>
<li><strong>What is noise?</strong></li>
<li>How many dimensions to measure? (what is <strong>redundant</strong>?)</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>Identify a most meaningful basis to re-express a data set.</p>
<p>In case of spring example, goal of PCA is to determine that <strong><span class="math inline">\(x\)</span> axis is the one that matters</strong>.</p>
</div>
</div>
</div>
</section>
<section id="pca-4" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Measurement definition</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\vec{X} = \begin{bmatrix}
  x_A \\
  y_A \\
  x_B \\
  y_B \\
  x_C \\
  y_C
\end{bmatrix}
\]</span></p>
</div>
</div>
</div>

<aside><div>
<p>If we record the ball’s position for 10 minutes at 120 Hz, then we have recorded 10 × 60 × 120 = 72000 of these vectors.</p>
</div></aside></section>
<section id="pca-5" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Naive basis</strong></p>
</div>
<div class="callout-content">
<p><strong>Naive basis</strong>: reflects the methods we used to measure the data. <span class="math display">\[
\boldsymbol{B} = \begin{bmatrix}
  \boldsymbol{b_1} \\
  \vdots \\
  \boldsymbol{b_m}
\end{bmatrix} = \begin{bmatrix}
1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; 1
\end{bmatrix} = \boldsymbol{I}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-6" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Question</strong></p>
</div>
<div class="callout-content">
<p>Is there another linear basis that best re-expresses our data set?</p>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Note the linearity assumption!</p>
</div>
</div>
</div>
</section>
<section id="pca-7" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Basis change: definition</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(\boldsymbol{X}\)</span> be the original data set (<span class="math inline">\(m\times n\)</span> matrix with <span class="math inline">\(m=6\)</span> and <span class="math inline">\(n=72000\)</span>).</p>
<p>Let <span class="math inline">\(\boldsymbol{Y}\)</span> be another <span class="math inline">\(m\times n\)</span> matrix such that: <span class="math display">\[\begin{align}
\label{basis}
&amp;\boldsymbol{P}\boldsymbol{X} = \boldsymbol{Y}
\end{align}\]</span></p>
<p><span class="math inline">\(p_i\)</span>: rows of <span class="math inline">\(P\)</span>, <span class="math inline">\(x_i\)</span>: columns of <span class="math inline">\(X\)</span>, <span class="math inline">\(y_i\)</span>: columns of <span class="math inline">\(Y\)</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(\boldsymbol{P}\)</span> transforms <span class="math inline">\(\boldsymbol{X}\)</span> into <span class="math inline">\(\boldsymbol{Y}\)</span></li>
<li><span class="math inline">\(\boldsymbol{P}\)</span> is a rotation and stretch geometrically</li>
<li>Rows of <span class="math inline">\(\boldsymbol{P}\)</span> are a new set of basis vectors</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-8" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Basis change</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
   &amp;\boldsymbol{P}\boldsymbol{X} = \begin{bmatrix}
     \boldsymbol{p_1} \\
     \vdots \\
     \boldsymbol{p_m}
\end{bmatrix} \begin{bmatrix}
\boldsymbol{x_1} &amp; \dots &amp; \boldsymbol{x_n}
\end{bmatrix} = \\
   &amp; = \begin{bmatrix}
     \boldsymbol{p_1} \cdot \boldsymbol{x_1} &amp; \dots &amp; \boldsymbol{p_1}\cdot\boldsymbol{x_n} \\
         \vdots &amp; \ddots &amp; \vdots\\
         \boldsymbol{p_m}\cdot\boldsymbol{x_1} &amp; \dots &amp; \boldsymbol{p_m}\cdot\boldsymbol{x_n}
   \end{bmatrix}
\end{align*}\]</span> <span class="math inline">\(j\)</span>th coefficient of <span class="math inline">\(\boldsymbol{y_i}\)</span> is a projection on the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(\boldsymbol{P}\)</span>, therefore, <span class="math inline">\(\boldsymbol{p_i}\)</span> are a new set of basis vectors for columns of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
<p><span class="math inline">\(\boldsymbol{p_i}\)</span> will become <strong>principal components</strong> of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-9" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Questions on <span class="math inline">\(\boldsymbol{P}\)</span></strong></p>
</div>
<div class="callout-content">
<ul>
<li>what is the best way to re-express <span class="math inline">\(\boldsymbol{X}\)</span>?</li>
<li>what is a good choice of <span class="math inline">\(\boldsymbol{P}\)</span>?</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How to express best?</strong></p>
</div>
<div class="callout-content">
<p>Use signal-to-noise ratio. <span class="math display">\[
SNR = \dfrac{\sigma^2_{signal}}{\sigma^2_{noise}}
\]</span> High SNR - precision measurement, low - noisy data.</p>
</div>
</div>
</div>
</section>
<section id="pca-10" class="slide level2">
<h2>PCA</h2>
<p><img data-src="img/snr.png"></p>

<aside><div>
<p>What does <strong><em>maximizing the variance</em></strong> mean: finding the appropriate <strong>rotation</strong>.</p>
</div></aside></section>
<section id="pca-11" class="slide level2">
<h2>PCA</h2>
<p><img data-src="img/redundancy.png"></p>

<aside><div>
<ul>
<li><span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> are distinct measurements.</li>
<li>High redundancy allows to drop measurements.</li>
<li>This is the central idea behind dimensional reduction.</li>
</ul>
</div></aside></section>
<section id="pca-12" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How do we generalize to higher dimensions?</strong></p>
</div>
<div class="callout-content">
<p>Consider two sets of measurements with zero means <span class="math display">\[\begin{align*}
   &amp;A=\left\{a_1, a_2, \dots, a_n\right\},  B=\left\{b_1, b_2, \dots, b_n\right\}.  
\end{align*}\]</span></p>
<p>Variances are <span class="math display">\[\begin{align*}
&amp;\sigma_A^2 = \dfrac{1}{n}\sum\limits_i a_i^2, \, \sigma_B^2 = \dfrac{1}{n}\sum\limits_i b_i^2,\\
\end{align*}\]</span></p>
<p><strong>Covariance</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span class="math display">\[
\sigma_{AB}^2 = \dfrac{1}{n}\sum\limits_i a_i b_i
\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-13" class="slide level2">
<h2>PCA</h2>
<p>Absolute value of covariance measures the degree of redundancy.</p>
<ul>
<li><span class="math inline">\(\sigma_{AB} = 0 \Leftrightarrow A \text{ and } B\)</span> are uncorrelated</li>
<li><span class="math inline">\(\sigma_{AB}^2 =  \sigma_A^2 \text{ if } A=B\)</span></li>
</ul>
</section>
<section id="pca-14" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Matrix form</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
   &amp;\boldsymbol{a} = \left[a_1 a_2 \dots a_n\right] \\
   &amp;\boldsymbol{b} = \left[b_1 b_2 \dots b_n\right] \\
   &amp;\sigma_{\boldsymbol{a}\boldsymbol{b}}^2 \equiv \dfrac{1}{n} \boldsymbol{a}\boldsymbol{b}^T
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-15" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Generalization</strong></p>
</div>
<div class="callout-content">
<p>Let’s generalize to a multiple number of vectors.</p>
<ul>
<li>Rename <span class="math inline">\(\boldsymbol{a}\)</span> and <span class="math inline">\(\boldsymbol{b}\)</span> to <span class="math inline">\(\boldsymbol{x_1}\)</span> and <span class="math inline">\(\boldsymbol{x_2}\)</span></li>
<li>Introduce additional <strong>measurement types</strong> <span class="math inline">\(\boldsymbol{x_i},\,i=\overline{3,m}\)</span>.</li>
<li>Define a new matrix: <span class="math display">\[
\boldsymbol{X} = \begin{bmatrix}
\boldsymbol{x_1}\\
\vdots\\
\boldsymbol{x_m}
\end{bmatrix}
\]</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-16" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Covariance matrix</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\boldsymbol{C_X} \equiv \dfrac{1}{n} \boldsymbol{X} \boldsymbol{X}^T
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Covariance matrix properties</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(\boldsymbol{C_X}\)</span> is a square symmetric <span class="math inline">\(m\times m\)</span> matrix</li>
<li>diagonal terms of <span class="math inline">\(\boldsymbol{C_X}\)</span> are the <strong>variance</strong> of particular measurement types</li>
<li>off-diagonal terms of <span class="math inline">\(\boldsymbol{C_X}\)</span> are the <strong>covariance</strong> between particular measurement types</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-17" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Proof of symmetricity</strong></p>
</div>
<div class="callout-content">
<p><strong>Theorem 1</strong>. Inverse of orthogonal matrix is its transpose.</p>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> orthogonal matrix where <span class="math inline">\(a_i\)</span> is the <span class="math inline">\(i\)</span>th column vector. We have <span class="math display">\[
(A^T A)_{ij} = a_i^T a_j = \begin{cases} 1, \; \text{ if } i=j,\\ 0 \; \text{ otherwise} \end{cases}
\]</span> Therefore, <span class="math inline">\(A^T A = I \Rightarrow A^{-1} = A^T\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-18" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Proof of symmetricity</strong></p>
</div>
<div class="callout-content">
<p><strong>Theorem 2</strong>. For any matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(A^T A\)</span> and <span class="math inline">\(A A^T\)</span> are symmetric. <span class="math display">\[\begin{align*}
  &amp; (A A^T)^T = A^{TT} A^T = A A^T,\\
  &amp; (A^T A)^T = A^T A^{TT} = A^T A.
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-19" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Covariance</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(\boldsymbol{C_X}\)</span> captures the covariance between all possible pairs of measurements. The covariance values reflect the noise and redundancy in our measurements.</p>
<ul>
<li>In the diagonal terms, by assumption, large values correspond to <strong>interesting structure</strong>.</li>
<li>In the off-diagonal terms large magnitudes correspond to high <strong>redundancy</strong>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-20" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>New goals</strong></p>
</div>
<div class="callout-content">
<ul>
<li>minimize redundancy, measured by the magnitude of the covariance</li>
<li>maximize the signal, measured by the variance.</li>
</ul>
<p>Let’s transform <span class="math inline">\(\boldsymbol{C_X}\)</span> into some optimized matrix <span class="math inline">\(\boldsymbol{C_Y}\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Optimized matrix <span class="math inline">\(\boldsymbol{C_Y}\)</span></strong></p>
</div>
<div class="callout-content">
<ul>
<li>all off-diagonal terms should be zero (this means that <span class="math inline">\(\boldsymbol{Y}\)</span> is decorrelated);</li>
<li>each successive dimension in <span class="math inline">\(\boldsymbol{Y}\)</span> should be rank-ordered according to variance.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-21" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Diagonalizing</strong></p>
</div>
<div class="callout-content">
<p>What are the methods for diagonalizing <span class="math inline">\(\boldsymbol{C_Y}\)</span>?</p>
<p>We assume that all basis vectors <span class="math inline">\(\left\{\boldsymbol{p_1}, \dots, \boldsymbol{p_m}\right\}\)</span> are orthonormal, that is, <span class="math inline">\(\boldsymbol{P}\)</span> is an <strong>orthonormal matrix</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How does PCA work?</strong></p>
</div>
<div class="callout-content">
<p>Looking at figure, it aligns the basis with the axis of maximal variance.</p>
</div>
</div>
</div>
</section>
<section id="pca-22" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Algorithm</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li>Select a normalized direction in <span class="math inline">\(m\)</span>-dimensional space along which the variance in <span class="math inline">\(X\)</span> is maximized. Save this vector as <span class="math inline">\(\boldsymbol{p_1}\)</span>.</li>
<li>Find another direction along which variance is maximized, however, because of the orthonormality condition, restrict the search to all directions orthogonal to all previous selected directions. Save this vector as <span class="math inline">\(\boldsymbol{p_i}\)</span>.</li>
<li>Repeat this procedure until <span class="math inline">\(m\)</span> vectors are selected.</li>
</ol>
<p>Resulting ordered set of <span class="math inline">\(\boldsymbol{p}\)</span> are called <strong>principal components</strong>.</p>
</div>
</div>
</div>
</section>
<section id="pca-23" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Assumptions review</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Linearity</strong>. We assume the problem can be solved by change of basis.</li>
<li><strong>Large variances have important structure</strong>. This is sometimes incorrect.</li>
<li><strong>Principal components are orthogonal</strong>. This makes PCA soluble with linear algebra techniques.</li>
</ul>
</div>
</div>
</div>
</section>
<section>
<section id="solving-using-eigenvector-decomposition" class="title-slide slide level1 center">
<h1>Solving using eigenvector decomposition</h1>

</section>
<section id="pca-first-algebraic-solution." class="slide level2">
<h2>PCA: First algebraic solution.</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Setup</strong></p>
</div>
<div class="callout-content">
<p>We have a dataset <span class="math inline">\(\boldsymbol{X}\)</span> which is a <span class="math inline">\(m \times n\)</span> matrix:</p>
<ul>
<li><span class="math inline">\(m\)</span> being number of dimensions (measurement types)</li>
<li><span class="math inline">\(n\)</span> - number of samples.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>Find some orthonormal matrix <span class="math inline">\(\boldsymbol{P}\)</span> in <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{P}\boldsymbol{X}\)</span> such that <span class="math inline">\(\boldsymbol{C_Y} \equiv \dfrac{1}{n} \boldsymbol{Y}\boldsymbol{Y}^T\)</span> is a diagonal matrix.</p>
<p>The rows of <span class="math inline">\(\boldsymbol{P}\)</span> are the <strong>principal components</strong> of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-24" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong><span class="math inline">\(\boldsymbol{C_Y}\)</span> rewrite</strong></p>
</div>
<div class="callout-content">
<p>Let’s rewrite <span class="math inline">\(\boldsymbol{C_Y}\)</span> in terms of unknown variable <span class="math inline">\(\boldsymbol{P}\)</span>: <span class="math display">\[\begin{align*}
   &amp;\boldsymbol{C_Y} = \dfrac{1}{n}\boldsymbol{Y}\boldsymbol{Y}^T =  \dfrac{1}{n}(\boldsymbol{P}\boldsymbol{X})(\boldsymbol{P}\boldsymbol{X})^T  = \\
   &amp; = \dfrac{1}{n} \boldsymbol{P} \boldsymbol{X} \boldsymbol{X}^T \boldsymbol{P}^T = \boldsymbol{P}(\dfrac{1}{n} \boldsymbol{X} \boldsymbol{X}^T) \boldsymbol{P}^T = \\
   &amp;= \boldsymbol{P} \boldsymbol{C_X} \boldsymbol{P}^T,
\end{align*}\]</span> where <span class="math inline">\(\boldsymbol{C_X}\)</span> is the covariance matrix of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-25" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>Any symmetrix matrix <span class="math inline">\(A\)</span> is diagonalized by an orthogonal matrix of its eigenvectors.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p><strong>Theorem 3</strong>. A matrix is symmetric <span class="math inline">\(\Leftrightarrow\)</span> it is orthogonally diagonalizable.</p>
<p><span class="math inline">\((\Rightarrow)\)</span> If <span class="math inline">\(A\)</span> is orthogonally diagonalizable, then <span class="math inline">\(A\)</span> is symmetric.</p>
<p>Orthogonally diagonalizable means that <span class="math inline">\(\exists E: A = E D E^T\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix and <span class="math inline">\(E\)</span> is a matrix that diagonalizes <span class="math inline">\(A\)</span>. Let’s compute <span class="math inline">\(A^T\)</span>: <span class="math display">\[
A^T = (E D E^T)^T = E^{TT}D^T E^T = E D E^T = A
\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-26" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p><strong>Theorem 4</strong>. A symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors.</p>
<p>Let <span class="math inline">\(A\)</span> be a square $n n $ symmetric matrix with eigenvectors <span class="math inline">\(\left\{e_1, \dots, e_n\right\}\)</span>. Let <span class="math inline">\(E=\left[e_1 \dots e_n\right]\)</span>. This theorem asserts that <span class="math inline">\(\exists \text{ diagonal matrix } D: A = E D E^T\)</span>.</p>
<p>First, let’s prove that any matrix can be orthogonally diagonalized if and only if it that matrix’s eigenvectors are all linearly independent.</p>
<p>Let <span class="math inline">\(A\)</span> be some matrix with independent eigenvectors (not degenerate). Let <span class="math inline">\(D\)</span> be a diagonal matrix where <span class="math inline">\(i\)</span>th eigenvalue is placed in <span class="math inline">\(ii\)</span>th position. We will show that <span class="math inline">\(AE=ED\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-27" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
   &amp; AE = \left[Ae_1 \dots Ae_n\right],\\
   &amp; ED = \left[\lambda_1 e_1 \dots \lambda_n e_n\right].
\end{align*}\]</span> Evidently, if <span class="math inline">\(AE=ED\)</span> then <span class="math inline">\(Ae_i = \lambda_i e_i \; \forall i\)</span>. This is the definition of the eigenvalue equation. Therefore, <span class="math inline">\(A = E D E^{-1}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-28" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>Now let’s prove that a symmetric matrix always has orthogonal eigenvectors. Suppose that <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_1\)</span> are distinct eigenvalues for eigenvectors <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span>. <span class="math display">\[\begin{align*}
  &amp;\lambda_1 e_1 \cdot e_2 = (\lambda_1 e_1)^T e_2 = (A e_1)^T e_2 =\\
  &amp; = e_1^T A^T e_2 = e_1^T A e_2 = e_1^T(\lambda_2 e_2) = \lambda_2 e_1 \cdot e_2.
\end{align*}\]</span> As <span class="math inline">\(\lambda_1 \neq \lambda_2\)</span>, then <span class="math inline">\(e_1 \cdot e_2 = 0\)</span>.</p>
<p>So, <span class="math inline">\(E\)</span> is an orthogonal matrix, and by theorem 1 <span class="math inline">\(E^T = E^{-1}\)</span> and <span class="math inline">\(A = E D E^T\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-29" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>For symmetric matrix <span class="math inline">\(\boldsymbol{A}\)</span> we have <span class="math inline">\(\boldsymbol{A} = \boldsymbol{E} \boldsymbol{D} \boldsymbol{E}^T\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix and <span class="math inline">\(E\)</span> is a matrix of eigenvectors of <span class="math inline">\(A\)</span> arranged as columns.</p>
<p>Note that <span class="math inline">\(\boldsymbol{A}\)</span> might have <span class="math inline">\(r \leq m\)</span> orthonormal eigenvectors where <span class="math inline">\(r\)</span> is the rank. This will mean that <span class="math inline">\(\boldsymbol{A}\)</span> is . Therefore, we’ll need to select additional <span class="math inline">\((m-r)\)</span> additional orthogonal vectors to fill matrix <span class="math inline">\(\boldsymbol{E}\)</span>.</p>
<p>These vectors do not affect the final solution because variances associated with these directions are <span class="math inline">\(0\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-30" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>We select the matrix <span class="math inline">\(\boldsymbol{P}\)</span> to be a matrix where each row <span class="math inline">\(\boldsymbol{p_i}\)</span> is an eigenvector of <span class="math inline">\(\dfrac{1}{n}\boldsymbol{X} \boldsymbol{X}^T\)</span>. By this selection, <span class="math inline">\(\boldsymbol{P} \equiv \boldsymbol{E}^T\)</span>. Keeping in mind that <span class="math inline">\(\boldsymbol{P}^{-1} = \boldsymbol{P}^T\)</span> (Theorem 1), we have: <span class="math display">\[\begin{align*}
   &amp; \boldsymbol{C_Y} = \boldsymbol{P} \boldsymbol{C_X} \boldsymbol{P}^T \\
   &amp;= \boldsymbol{P}(\boldsymbol{E}^T \boldsymbol{D} \boldsymbol{E})\boldsymbol{P}^T  \\
   &amp;= \boldsymbol{P}(\boldsymbol{P}^T \boldsymbol{D} \boldsymbol{P})\boldsymbol{P}^T = \\
   &amp; = (\boldsymbol{P} \boldsymbol{P}^T) \boldsymbol{D} (\boldsymbol{P} \boldsymbol{P}^T)  \\
   &amp; = (\boldsymbol{P} \boldsymbol{P}^{-1})\boldsymbol{D}(\boldsymbol{P} \boldsymbol{P}^{-1}).
\end{align*}\]</span> Therefore, <span class="math inline">\(\boldsymbol{C_Y} = \boldsymbol{D}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-31" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Result</strong></p>
</div>
<div class="callout-content">
<ul>
<li>choice of <span class="math inline">\(\boldsymbol{P}\)</span> diagonalizes <span class="math inline">\(\boldsymbol{C_Y}\)</span></li>
<li>principal components of <span class="math inline">\(\boldsymbol{X}\)</span> are the eigenvectors of <span class="math inline">\(\boldsymbol{C_X} = \dfrac{1}{n}\boldsymbol{X} \boldsymbol{X}^T\)</span></li>
<li>the <span class="math inline">\(i\)</span>th diagonal value of <span class="math inline">\(\boldsymbol{C_Y}\)</span> is the variance of <span class="math inline">\(\boldsymbol{X}\)</span> along <span class="math inline">\(\boldsymbol{p_i}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Practical computation</strong></p>
</div>
<div class="callout-content">
<ul>
<li>subtract the mean off each measurement type</li>
<li>compute eigenvectors of <span class="math inline">\(\boldsymbol{C_X}\)</span></li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="singular-value-decomposition" class="title-slide slide level1 center">
<h1>Singular value decomposition</h1>

</section>
<section id="pca-another-algebraic-solution" class="slide level2">
<h2>PCA: Another algebraic solution</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Setup</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(\boldsymbol{X}\)</span> be an arbitrary <span class="math inline">\(n \times m\)</span> matrix (!) and <span class="math inline">\(\boldsymbol{X}^T \boldsymbol{X}\)</span> be a rank <span class="math inline">\(r\)</span>, square, symmetric <span class="math inline">\(m \times m\)</span> matrix.</p>
<ul>
<li><span class="math inline">\(\left\{\hat{\boldsymbol{v}_1}, \dots, \hat{\boldsymbol{v}_r}\right\}\)</span> is the set of orthonormal <span class="math inline">\(m \times 1\)</span> eigenvectors with associated eigenvalues <span class="math inline">\(\left\{\lambda_1, \dots, \lambda_r \right\}\)</span> for the symmetric matrix <span class="math inline">\(\boldsymbol{X}^T \boldsymbol{X}\)</span>: <span class="math display">\[
(\boldsymbol{X}^T \boldsymbol{X})\hat{\boldsymbol{v}_i} = \lambda_i \hat{\boldsymbol{v}_i}
\]</span></li>
<li><span class="math inline">\(\sigma_i \equiv \sqrt{\lambda_i}\)</span> are positive real and termed the <strong>singular values</strong></li>
<li><span class="math inline">\(\left\{\hat{\boldsymbol{u}_1}, \dots, \hat{\boldsymbol{u}_r}\right\}\)</span> is the set of <span class="math inline">\(n \times 1\)</span> vectors defined by <span class="math inline">\(\hat{\boldsymbol{u}_i} \equiv \dfrac{1}{\sigma_i}\boldsymbol{X} \hat{\boldsymbol{v}_i}\)</span>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-32" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Properties</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{u}_i} \cdot \hat{\boldsymbol{u}_j} = \begin{cases} 1, \; \text{ if } i=j;\\ 0, \; \text{otherwise}\end{cases}\)</span></li>
<li><span class="math inline">\(\|\boldsymbol{X} \hat{\boldsymbol{v}_i}\| = \sigma_i\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-33" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p><strong>Theorem 5.</strong> For any arbitrary <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(X\)</span>, the symmetric matrix <span class="math inline">\(X^T X\)</span> has a set of orthonormal eigenvectors of <span class="math inline">\(\left\{\hat{\boldsymbol{v}_1}, \dots, \hat{\boldsymbol{v}_n}\right\}\)</span> and a set of associated eigenvalues <span class="math inline">\(\left\{\lambda_1, \dots, \lambda_n\right\}\)</span>. The set of vectors <span class="math inline">\(\left\{\boldsymbol{X}\hat{\boldsymbol{v_1}}, \dots, \boldsymbol{X}\hat{\boldsymbol{v_n}}\right\}\)</span> then forms an orthogonal basis, where each vector <span class="math inline">\(X\hat{\boldsymbol{v_i}}\)</span> is of length <span class="math inline">\(\sqrt{\lambda_i}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
    &amp;(X \hat{v_i}) \cdot (X \hat{v_j}) = (X \hat{v_i})^T \cdot (X \hat{v_j}) = \\
    &amp; = \hat{v_i} X^T X \hat{v_j} = \hat{v_i}^T(\lambda_j v_j) = \lambda_j \hat{v_i} \cdot \hat{v_j} =\\
    &amp; = \lambda_j \delta_{ij}.
\end{align*}\]</span> And so we have <span class="math display">\[
\|X \hat{v_i}\|^2 = (X \hat{v_i})\cdot(X \hat{v_i}) = \lambda_i.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-34" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The scalar version of SVD</strong></p>
</div>
<div class="callout-content">
<p>Is a restatement of the third definition: <span class="math display">\[\begin{align}
\label{scalar_svd}
&amp;\boldsymbol{X} \hat{\boldsymbol{v}_i} = \sigma_i \hat{\boldsymbol{u}_i}
\end{align}\]</span></p>
<p>The set of eigenvectors <span class="math inline">\(\left\{\hat{\boldsymbol{v}_1}, \dots, \hat{\boldsymbol{v}_r}\right\}\)</span> and the set of vectors <span class="math inline">\(\left\{\hat{\boldsymbol{u}_1}, \dots, \hat{\boldsymbol{u}_r}\right\}\)</span> are both bases in <span class="math inline">\(r\)</span>-dimensional space.</p>
</div>
</div>
</div>
</section>
<section id="pca-35" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The matrix version of SVD.</strong></p>
</div>
<div class="callout-content">
<p>Construct a new diagonal matrix <span class="math inline">\(\Sigma\)</span>: <span class="math display">\[
\Sigma \equiv \begin{bmatrix}
  \sigma_{\tilde{1}} \\
  &amp; \ddots &amp; &amp; \text{0} \\
  &amp; &amp; \sigma_{\tilde{r}} \\
  \text{0}  &amp; &amp; &amp; \ddots \\
                   &amp; &amp; &amp; &amp;  0
\end{bmatrix}
\]</span> where <span class="math inline">\(\sigma_{\tilde{1}} \geq \dots \geq \sigma_{\tilde{r}}\)</span> are the rank-ordered set of singular values.</p>
</div>
</div>
</div>
</section>
<section id="pca-36" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The matrix version of SVD.</strong></p>
</div>
<div class="callout-content">
<p>Likewise we construct accompanying orthogonal matrices <span class="math display">\[\begin{align*}
   &amp; \boldsymbol{V} = \left[\hat{\boldsymbol{v_1}}, \dots, \hat{\boldsymbol{v_m}}\right],\; \boldsymbol{U} = \left[\hat{\boldsymbol{u_1}}, \dots, \hat{\boldsymbol{u_n}}\right],
\end{align*}\]</span> where we appended additional <span class="math inline">\((m-r)\)</span> and <span class="math inline">\((n-r)\)</span> orthonormal vectors to fill up the matrices for <span class="math inline">\(\boldsymbol{V}\)</span> and <span class="math inline">\(\boldsymbol{U}\)</span> respectively, in order to deal with degeneracy issues.</p>
<p><span class="math display">\[
\boldsymbol{X}\boldsymbol{V} = \boldsymbol{U} \boldsymbol{\Sigma},
\]</span> where each column of <span class="math inline">\(\boldsymbol{V}\)</span> and <span class="math inline">\(\boldsymbol{U}\)</span> perform the scalar version of decomposition <span class="math inline">\(\eqref{scalar_svd}\)</span>.</p>
<p>As <span class="math inline">\(\boldsymbol{V}\)</span> is orthogonal, we multiply both sides by <span class="math inline">\(\boldsymbol{V}^{-1}=\boldsymbol{V}^T\)</span> and obtain <span class="math display">\[\begin{align}
\label{svd}
&amp;\boldsymbol{X} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^T.
\end{align}\]</span></p>
</div>
</div>
</div>
</section>
<section id="pca-37" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p>Equation <span class="math inline">\(\eqref{svd}\)</span> states that any arbitrary matrix <span class="math inline">\(\boldsymbol{X}\)</span> can be converted into an orthogonal matrix, diagonal matrix, and another orthogonal matrix (or a rotation, a stretch, and a second rotation).</p>
<p>Reinterpret equation <span class="math inline">\(\eqref{scalar_svd}\)</span> as <span class="math inline">\(\boldsymbol{X}\boldsymbol{a} = k\boldsymbol{b}\)</span>, where <span class="math inline">\(\boldsymbol{a}\)</span> and <span class="math inline">\(\boldsymbol{b}\)</span> are column vectors and <span class="math inline">\(k\)</span> is a scalar constant.</p>
<p>The set <span class="math inline">\(\left\{\hat{\boldsymbol{v_1}}, \dots, \hat{\boldsymbol{v_m}}\right\}\)</span> is analogous to <span class="math inline">\(\boldsymbol{a}\)</span> and <span class="math inline">\(\left\{\hat{\boldsymbol{u_1}}, \dots, \hat{\boldsymbol{u_n}}\right\}\)</span> is analogous to <span class="math inline">\(\boldsymbol{b}\)</span>.</p>
<p><span class="math inline">\(\left\{\hat{\boldsymbol{v_1}}, \dots, \hat{\boldsymbol{v_m}}\right\}\)</span> and <span class="math inline">\(\left\{\hat{\boldsymbol{u_1}}, \dots, \hat{\boldsymbol{u_n}}\right\}\)</span> are orthonormal sets of vectors which span an <span class="math inline">\(m\)</span> or <span class="math inline">\(n\)</span> dimensional space, respectively.</p>
<p>Inputs are <span class="math inline">\(\boldsymbol{a}\)</span> and outputs are <span class="math inline">\(\boldsymbol{b}\)</span>. Can we formalize the view that <span class="math inline">\(\left\{\hat{\boldsymbol{v_1}}, \dots, \hat{\boldsymbol{v_m}}\right\}\)</span> and <span class="math inline">\(\left\{\hat{\boldsymbol{u_1}}, \dots, \hat{\boldsymbol{u_n}}\right\}\)</span> span all possible inputs and outputs?</p>
</div>
</div>
</div>
</section>
<section id="pca-38" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p>From <span class="math inline">\(\eqref{svd}\)</span> we have <span class="math display">\[\begin{align*}
   &amp; \boldsymbol{X} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^T \Rightarrow \\
   &amp; \boldsymbol{U}^T \boldsymbol{X} = \boldsymbol{\Sigma} \boldsymbol{V}^T \Rightarrow \\
   &amp; \boldsymbol{U}^T \boldsymbol{X} = \boldsymbol{Z},
\end{align*}\]</span> where <span class="math inline">\(\boldsymbol{Z} \equiv \boldsymbol{\Sigma} \boldsymbol{V}^T\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-39" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p>Comparing to <span class="math inline">\(\eqref{basis}\)</span>, <span class="math inline">\(\left\{\hat{\boldsymbol{u_1}}, \dots, \hat{\boldsymbol{u_n}}\right\}\)</span> perform the same role as <span class="math inline">\(\left\{\hat{\boldsymbol{p_1}}, \dots, \hat{\boldsymbol{p_m}}\right\}\)</span>. Hence, <span class="math inline">\(\boldsymbol{U}^T\)</span> is a <strong>change of basis</strong> from <span class="math inline">\(\boldsymbol{X}\)</span> to <span class="math inline">\(\boldsymbol{Z}\)</span>.</p>
<p>Therefore, from the fact that the orthonormal basis <span class="math inline">\(\boldsymbol{U}^T\)</span> (or <span class="math inline">\(\boldsymbol{P}\)</span>) transforms column vectors it follows that <span class="math inline">\(\boldsymbol{U}^T\)</span> is a basis that spans the columns of <span class="math inline">\(\boldsymbol{X}\)</span>. Bases that span the columns are termed <strong>column spaces</strong>.</p>
<p>Column spaces are equivalent to matrix <strong>outputs</strong>.</p>
<p>Row spaces are equivalent to matrix <strong>inputs</strong>.</p>
</div>
</div>
</div>
</section>
<section id="pca-40" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>How are PCA and SVD related?</strong></p>
</div>
<div class="callout-content">
<p>Consider original <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\boldsymbol{X}\)</span>. Define <span class="math display">\[
\boldsymbol{Y} \equiv \dfrac{1}{\sqrt{n}} \boldsymbol{X}^T,
\]</span> where each column of <span class="math inline">\(\boldsymbol{Y}\)</span> has zero mean. Consider: <span class="math display">\[\begin{align*}
  &amp;\boldsymbol{Y}^T \boldsymbol{Y} = \left(\dfrac{1}{\sqrt{n}}\boldsymbol{X}^T\right)^T\left(\dfrac{1}{\sqrt{n}}\boldsymbol{X}^T\right) = \dfrac{1}{n}\boldsymbol{X} \boldsymbol{X}^T =  \boldsymbol{C_X}.
\end{align*}\]</span> By construction <span class="math inline">\(\boldsymbol{Y}^T \boldsymbol{Y}\)</span> equals the covariance matrix of <span class="math inline">\(\boldsymbol{X}\)</span>. Principal components of <span class="math inline">\(\boldsymbol{X}\)</span> are the eigenvectors of <span class="math inline">\(\boldsymbol{C_X}\)</span>. If we calculate the SVD of <span class="math inline">\(\boldsymbol{Y}\)</span>, the columns of matrix <span class="math inline">\(\boldsymbol{V}\)</span> contain the eigenvectors of <span class="math inline">\(\boldsymbol{Y}^T \boldsymbol{Y} = \boldsymbol{C_X}\)</span>.</p>
<p>Therefore, columns of <span class="math inline">\(\boldsymbol{V}\)</span> are the principal components of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-41" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(\boldsymbol{V}\)</span> spans the row space of <span class="math inline">\(\boldsymbol{Y} \equiv \dfrac{1}{\sqrt{n}} \boldsymbol{X}^T\)</span>. Therefore, <span class="math inline">\(\boldsymbol{V}\)</span> must also span the column space of <span class="math inline">\(\dfrac{1}{\sqrt{n}} \boldsymbol{X}\)</span>.</p>
<p>We conclude that finding the principal components amounts to finding an orthonormal basis that spans the <strong>column space</strong> of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="pca-42" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Summary of PCA</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Organize data as <span class="math inline">\(m \times n\)</span> matrix, where <span class="math inline">\(m\)</span> is the number of measurement types and <span class="math inline">\(n\)</span> is the number of samples</li>
<li>Subtract off the mean for each measurement type</li>
<li>Calculate the SVD or the eigenvectors of the covariance</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-43" class="slide level2">
<h2>PCA</h2>
<!-- PCA is non-parametric! -->
<p><img data-src="img/ferris_wheel.png" height="430"></p>

<aside><div>
<p>Example of when PCA fails (red lines). (a) Tracking a person on a ferris wheel (black dots). All dynamics can be described by the phase of the wheel θ, a non-linear combination of the naive basis. (b) Non-Gaussian distributed data and non-orthogonal axes causes PCA to fail. The axes with the largest variance do not correspond to the appropriate answer.</p>
</div></aside></section>
<section id="pca-44" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Loss function</strong></p>
</div>
<div class="callout-content">
<p>It can be proved that under a common loss function, mean squared error (<span class="math inline">\(L_2\)</span> norm), PCA provides the optimal reduced representation of the data.</p>
<p>This means that selecting orthogonal directions for principal components is the best solution to predicting the original data.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Kernel PCA</strong></p>
</div>
<div class="callout-content">
<p>Decorrelation is removing second-order dependencies.</p>
<p>Higher-order dependencies: if prior knowledge is known about the problem, then a nonlinearity (i.e.&nbsp;<strong>kernel</strong>) might be applied to the data to transform the data to a more appropriate naive basis. This parametric approach is often termed <strong>kernel PCA</strong>.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="latent-semantic-analysis" class="title-slide slide level1 center">
<h1>Latent semantic analysis</h1>

</section>
<section id="lsa" class="slide level2">
<h2>LSA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Basic concepts: LSA (latent semantic analysis)</strong></p>
</div>
<div class="callout-content">
<p>SVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first e.g.&nbsp;300 dimensions are used as the LSA embedding.</p>
<p>Alternatively, this is <strong>PCA applied to NLP data</strong>.</p>
</div>
</div>
</div>
</section>
<section id="lsa-1" class="slide level2">
<h2>LSA</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Issues with TF-IDF</strong></p>
</div>
<div class="callout-content">
<ul>
<li>focus on spelling and word usage</li>
<li>lemmatization might group some words, but synonyms will be handled separately</li>
<li>TF-IDF assumes that frequency is the only thing that matters</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>Use <strong>topics</strong> - aggregated words.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="pca-in-python" class="title-slide slide level1 center">
<h1>PCA in Python</h1>
<!-- %Python guide (beginners guide to PCA) -->
</section>
<section id="pca-45" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Basic concepts</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Data preprocessing</strong>: Before applying PCA, it is important to preprocess the data by removing any missing values and scaling the features to have zero mean and unit variance.</li>
<li><strong>Covariance matrix</strong>: PCA starts by computing a covariance matrix, which is a matrix that contains the pairwise covariances between all the features in the dataset.</li>
<li><strong>Eigenvectors and eigenvalues</strong>: The eigenvectors of the covariance matrix are the principal components (PCs), and the corresponding eigenvalues are the variance explained by each PC.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-46" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Basic concepts</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Dimensionality reduction</strong>: The data can then be projected onto the chosen PCs by multiplying the original data matrix with the matrix of PCs. This results in a lower-dimensional representation of the data.</li>
<li><strong>Reconstruction</strong>: If needed, the reduced data can be reconstructed back to the original space by multiplying the reduced data matrix with the transpose of the matrix of PCs.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-47" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>What is a principal component: recap</strong></p>
</div>
<div class="callout-content">
<p>In principal component analysis, a principal component is a new feature that is constructed from a linear combination of the original features in a dataset.</p>
<ul>
<li>The principal components are ordered such that the first principal component has the highest possible variance (i.e., the greatest amount of spread or dispersion in the data)</li>
<li>each subsequent component in turn has the highest variance possible under the constraint that it is orthogonal (i.e., uncorrelated) to the previous components.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Idea behind PCA</strong></p>
</div>
<div class="callout-content">
<p>Reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space, while still preserving as much of the variance in the data as possible.</p>
</div>
</div>
</div>
</section>
<section id="pca-48" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Making sense of principal components</strong></p>
</div>
<div class="callout-content">
<p>PCs are the directions in which the data varies the most. To make sense of the PCs, you can consider the following:</p>
<ul>
<li><strong>Variance explained</strong>: The PCs are ranked in order of the variance they explain. The first PC explains the most variance, the second PC explains the second most variance, and so on. You can check the percentage of variance explained by each PC to understand how much information each PC captures.</li>
<li><strong>Loadings</strong>: Loadings are the coefficients that describe the relationship between the original features and the PCs. You can check the loadings of each feature to see which features contribute the most to each PC.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-49" class="slide level2">
<h2>PCA</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>PCA items</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Components</strong>: The PCs are linear combinations of the original features. The components of each PC tell you which features contribute the most to each PC.</li>
<li><strong>Data visualization</strong>: You can also use data visualization like scatter plots or biplots to understand the PCs. Scatter plots can show you how the data is distributed along each PC, and biplots can show you the relationship between the original features and the PCs.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-math-summary" class="slide level2">
<h2>PCA math summary</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Preliminary</strong></p>
</div>
<div class="callout-content">
<p>Before applying PCA, missing values need to be removed from the dataset and each variable should be scaled to have zero mean and unit variance.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Compute</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><span class="math inline">\(\text{Covariance matrix} = (1/n) \cdot X^T \cdot X\)</span>, where <span class="math inline">\(X\)</span> is the data matrix with <span class="math inline">\(n\)</span> samples and <span class="math inline">\(p\)</span> features.</p></li>
<li><p>Then eigenvectors and eigenvalues are obtained by solving the following equation: <span class="math display">\[
\text{Covariance matrix} \cdot v = \lambda \cdot v
\]</span> where <span class="math inline">\(v\)</span> is the eigenvector and <span class="math inline">\(\lambda\)</span> is the corresponding eigenvalue.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-math-summary-1" class="slide level2">
<h2>PCA math summary</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Compute</strong></p>
</div>
<div class="callout-content">
<ul>
<li>To complete the dimensionality reduction, data can then be projected onto the chosen PCs by multiplying the original data matrix with the matrix of PCs: <span class="math display">\[
   X_{reduced} = X \cdot PCs
   \]</span></li>
<li>To reconstruct the reduced data back to the original space, we multiply the reduced data matrix with the transpose of the matrix of PCs: <span class="math display">\[
   X_{reconstructed} = X_{reduced} * {PCs}^T
   \]</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-in-python-1" class="slide level2">
<h2>PCA in Python</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a> <span class="co"># Import libraries </span></span>
<span id="cb1-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a><span class="co"># Load data</span></span>
<span id="cb1-7"><a></a>data <span class="op">=</span> np.loadtxt(<span class="st">'data.txt'</span>)</span>
<span id="cb1-8"><a></a></span>
<span id="cb1-9"><a></a><span class="co"># Normalize data</span></span>
<span id="cb1-10"><a></a>mean <span class="op">=</span> np.mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-11"><a></a>std <span class="op">=</span> np.std(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-12"><a></a>data <span class="op">=</span> (data <span class="op">-</span> mean) <span class="op">/</span> std</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pca-in-python-2" class="slide level2">
<h2>PCA in Python</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>  <span class="co"># Compute covariance matrix</span></span>
<span id="cb2-2"><a></a>  cov_matrix <span class="op">=</span> np.cov(data, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-3"><a></a></span>
<span id="cb2-4"><a></a>  <span class="co"># Compute eigenvectors and eigenvalues</span></span>
<span id="cb2-5"><a></a>  eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(cov_matrix)</span>
<span id="cb2-6"><a></a></span>
<span id="cb2-7"><a></a>  <span class="co"># Sort in descending order</span></span>
<span id="cb2-8"><a></a>  idx <span class="op">=</span> np.argsort(<span class="op">-</span>eigenvalues)</span>
<span id="cb2-9"><a></a>  eigenvalues <span class="op">=</span> eigenvalues[idx]</span>
<span id="cb2-10"><a></a>  eigenvectors <span class="op">=</span> eigenvectors[:,idx]</span>
<span id="cb2-11"><a></a></span>
<span id="cb2-12"><a></a>  <span class="co"># Select top principal components</span></span>
<span id="cb2-13"><a></a>  k <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-14"><a></a>  eigenvectors <span class="op">=</span> eigenvectors[:,:k]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pca-in-python-3" class="slide level2">
<h2>PCA in Python</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>  <span class="co"># Project the data onto the principal components</span></span>
<span id="cb3-2"><a></a>  projected_data <span class="op">=</span> np.dot(data, eigenvectors.T)</span>
<span id="cb3-3"><a></a>  <span class="co"># Visualize the results using scatter from matplotlib</span></span>
<span id="cb3-4"><a></a>  plt.scatter(projected_data[:,<span class="dv">0</span>], projected)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section></section>
<section>
<section id="non-linear-reduction-methods" class="title-slide slide level1 center">
<h1>Non-linear reduction methods</h1>
<!-- % Visualizing your embeddings article -->
</section>
<section id="neighbor-graph-algorithms" class="slide level2">
<h2>Neighbor graph algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Common steps</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Compute high dimensional probabilities <span class="math inline">\(p\)</span>.</li>
<li>Compute low dimensional probabilities <span class="math inline">\(q\)</span>.</li>
<li>Calculate the difference between the probabilities by a given cost function <span class="math inline">\(C(p,q)\)</span>.</li>
<li>Minimize the cost function.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="sne" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p><strong>Stochastic Neighbor Embedding (SNE)</strong> tries to place the objects in a low-dimensional space so as to optimally preserve neighborhood identity, and can be naturally extended to allow multiple different low-d images of each object.</p>
</div>
</div>
</div>
</section>
<section id="sne-1" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Step 1.</strong></p>
</div>
<div class="callout-content">
<p><strong>High-dimensional probabilities</strong>.</p>
<p>Compute probability that that object <span class="math inline">\(i\)</span> would pick <span class="math inline">\(j\)</span> as neighbor: <span class="math display">\[
p_{ij} = \dfrac{exp\left(-d_{ij}^2\right)}{\sum\limits_{k \neq i} exp\left(-d_{ik}^2\right)}
\]</span> Values <span class="math inline">\(d_{ij}\)</span> represent between points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>: <span class="math display">\[
d_{ij}^2 = \dfrac{\|\boldsymbol{x}_i-\boldsymbol{x}_j\|^2}{2\sigma_i^2}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sne-2" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Step 1.</strong></p>
</div>
<div class="callout-content">
<p>What is the meaning of <span class="math inline">\(\sigma_i\)</span>? It is found by a binary search that makes the Shannon entropy of the distribution over neighbors equal to <span class="math inline">\(log_2 k\)</span>, where <span class="math inline">\(k\)</span> is the effective number of local neighbors or .</p>
<p>We compute entropy via: <span class="math display">\[\begin{align*}
&amp;H = -\sum\limits_j p_{ij} log_2 p_{ij} = log_2 k,\\
&amp;k = 2^{-\sum\limits_j p_{ij} log_2 p_{ij}}
\end{align*}\]</span> By tuning <span class="math inline">\(\sigma_i\)</span> we try to match the value of <span class="math inline">\(k\)</span> set by the user.</p>
</div>
</div>
</div>
</section>
<section id="sne-3" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Step 1.</strong></p>
</div>
<div class="callout-content">
<p>The higher the effective number of local neighbors (perplexity), the higher <span class="math inline">\(\sigma_i\)</span> and the wider the Gaussian function used in the dissimilarities.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Intuition</strong></p>
</div>
<div class="callout-content">
<p>Mathematical intuition: The higher the perplexity, the more likely it is to consider points that are far away as neighbors.</p>
</div>
</div>
</div>

<aside><div>
<p><strong>Advice</strong>: The authors of SNE and t-SNE use perplexity values between 5 and 50.</p>
</div></aside></section>
<section id="sne-4" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Step 2.</strong></p>
</div>
<div class="callout-content">
<p><strong>Low-dimensional probabilities</strong>.</p>
<p>Now that we have the high-dimensional probabilities, we move on to calculate the low dimensional ones, which depend on where the data points are mapped in the low dimensional space.</p>
<!-- %Fortunately, these are easier to compute since SNE also uses Gaussian neighborhoods but with fixed variance (no perplexity parameter), -->
<p>In the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance (which we set without loss of generality to be <span class="math inline">\(\dfrac{1}{2}\)</span>) so the <strong>induced probability</strong> <span class="math inline">\(q_{ij}\)</span> that point <span class="math inline">\(i\)</span> picks point <span class="math inline">\(j\)</span> as its neighbor is a function of the low-dimensional images <span class="math inline">\(y_i\)</span> of all the objects and is given by expression: <span class="math display">\[
q_{ij} = \dfrac{exp\left(-\|\boldsymbol{y}_i-\boldsymbol{y}_j\|^2\right)}{\sum\limits_{k \neq i} exp\left(-\|\boldsymbol{y}_i-\boldsymbol{y}_k\|^2\right)}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sne-5" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Step 3.</strong></p>
</div>
<div class="callout-content">
<p><strong>Choice of cost function</strong>.</p>
<p>If the points <span class="math inline">\(Y_i\)</span> are placed correctly in the low-dimensional space, the conditional probabilities <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> will be very similar. To measure the mismatch between both probabilities, SNE uses the as a loss function for each point. Each point in both high and low dimensional space has a conditional probability to call another point its neighbor. Hence, we have as many loss functions as we have data points. We define the cost function as the sum of the KL divergences over all data points, <span class="math display">\[
C = \sum\limits_i \sum\limits_j p_{ij} log\dfrac{p_{ij}}{q_{ij}} = \sum\limits_i KL(P_i \| Q_I).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sne-6" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Kullback-Leibler Divergence</strong></p>
</div>
<div class="callout-content">
<p>The KL divergence, which is closely related to relative entropy, information divergence, and information for discrimination, is a non-symmetric measure of the difference between two probability distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>.</p>
<p>Specifically, the KL divergence of <span class="math inline">\(q(x)\)</span> from <span class="math inline">\(p(x)\)</span>, denoted DKL(p(x),q(x)), is a measure of the information lost when <span class="math inline">\(q(x)\)</span> is used to approximate <span class="math inline">\(p(x)\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="sne-7" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Kullback-Leibler Divergence</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> are two probability distributions of a discrete random variable <span class="math inline">\(x\)</span>. That is, both <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> sum up to <span class="math inline">\(1\)</span>, and <span class="math inline">\(p(x) &gt; 0\)</span> and <span class="math inline">\(q(x) &gt; 0\)</span> <span class="math inline">\(\forall x \in X\)</span>.</p>
<p>KL divergence is defined as: <span class="math display">\[
KL(p(x) \| q(x)) = \sum\limits_{x \in X} p(x) ln \dfrac{p(x)}{q(x)}.
\]</span> Continuous version: <span class="math display">\[
KL(p(x) \| q(x)) = \int\limits_{-\infty}^{\infty} p(x) ln \dfrac{p(x)}{q(x)}dx.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sne-8" class="slide level2">
<h2>SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Differentiation of the cost function</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\dfrac{\partial C}{\partial \boldsymbol{y}_i} = 2\sum\limits_j (\boldsymbol{y}_i-\boldsymbol{y}_j)(p_{ij}-q_{ij}+p_{ji}-q_{ij}).
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p>A sum of forces pulling toward or pushing it away depending on whether is observed to be a neighbor more or less often than desired.</p>
</div>
</div>
</div>
</section>
<section id="sne-9" class="slide level2">
<h2>SNE</h2>
<!-- %It’s worth developing some intuition about the loss function used. The algorithm’s creators state that “while SNE emphasizes local distances, its cost function cleanly enforces both keeping the images of nearby objects nearby and keeping the images of widely separated objects relatively far apart.” Let’s see if this is true using the next figure where we see the KL divergence for the high and low dimensional probabilities p and q, respectively. -->
<p><img data-src="img/divergence.png" height="500"></p>

<!-- ## SNE -->
<!-- If two points are close together in high dimensional space, their dissimilarities are low and the probability $p$ should be high ($p ~ 1$). Then, if they were mapped far away, the low dimensional probability would be low ($q ~ 0$). In this scenario we can see that the loss function takes very high values, severely penalizing that mistake. -->
<!-- On the other hand, if two points are far from each other in high dimensional space, their dissimilarities are high and the probability $p$ should be low ($p ~ 0$). Then, if they were mapped near each other, the low dimensional probability would be high ($q ~ 1$). We can see that the KL divergence is not penalizing this mistake as much as we would want. %This is a key issue that will be solved by UMAP. -->
<aside><div>
<p>The algorithm starts by placing all the <span class="math inline">\(y_i\)</span> in random locations very close to the origin, and then is trained minimizing the cost function <span class="math inline">\(C\)</span> using gradient descent.</p>
</div></aside></section>
<section id="sne-10" class="slide level2">
<h2>SNE</h2>
<p><img data-src="img/sne.png" height="500"></p>

<aside><div>
<p>The result of running the SNE algorithm on 3000 256-dimensional grayscale images of handwritten digits.</p>
</div></aside></section>
<section id="sne-11" class="slide level2">
<h2>SNE</h2>
<p><img data-src="img/nips_sne.png" height="550"></p>

<aside><div>
<p>Embedding of NIPS authors in 2D (red dots are authors who published 6+ papers).</p>
</div></aside></section>
<section id="t-sne" class="slide level2">
<h2>t-SNE</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problems</strong></p>
</div>
<div class="callout-content">
<ul>
<li>cost function is very difficult to optimize</li>
<li>a crowding problem</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solutions</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Symmetrization</li>
<li>use of t-distributions for the low-dimensional probabilities</li>
</ul>
</div>
</div>
</div>
</section>
<section id="t-sne-1" class="slide level2">
<h2>t-SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Symmetric SNE.</strong></p>
</div>
<div class="callout-content">
<p>The probability of point <span class="math inline">\(x_i\)</span> to consider point <span class="math inline">\(x_j\)</span> as its neighbor is not the same probability that point <span class="math inline">\(x_j\)</span> would consider point <span class="math inline">\(x_i\)</span> as a neighbor. We symmetrize pairwise probabilities in high dimensional space by defining <span class="math display">\[
\tilde{p}_{ij} = \dfrac{p_{ij}+p_{ji}}{2n}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="t-sne-2" class="slide level2">
<h2>t-SNE</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>The Crowding Problem</strong></p>
</div>
<div class="callout-content">
<p>If we want to correctly project close distances between points, the moderate distances are distorted and appear as huge distances in the low dimensional space. <!-- %According to the authors of t-SNE, this is because “the area of the two-dimensional map that is available to accommodate moderately distant data points will not be nearly large enough compared with the area available to accommodate nearby data points.” --></p>
<p>To solve this, use the Student t-Distribution (which is what gives the ‘t’ to t-SNE) with one degree of freedom for the low-dimensional probabilities: <span class="math display">\[
q_{ij} = \dfrac{\left(1+\|y_i-y_j\|^2\right)^{-1}}{\sum\limits_{k \neq l} \left(1+\|y_k-y_l\|^2\right)^{-1}}
\]</span></p>
<p>Now <span class="math inline">\(p_{ij}=p_{ji}\)</span> and <span class="math inline">\(q_{ij}=q_{ji}\)</span>, and the gradient of the cost function <span class="math display">\[
C = \sum\limits_i \sum\limits_j \tilde{p}_{ij} log\dfrac{\tilde{p}_{ij}}{q_{ij}}
\]</span> is easier to compute.</p>
</div>
</div>
</div>
</section>
<section id="t-sne-3" class="slide level2">
<h2>t-SNE</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Local structure</strong></p>
</div>
<div class="callout-content">
<p>Since t-SNE also uses KL divergence as its loss function, it also carries the problems discussed in the previous section. This is not to say that it is completely ignored, but the main takeaway is that t-SNE severely prioritizes the conservation of the local structure.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Global structure</strong></p>
</div>
<div class="callout-content">
<p>Since the KL divergence function does not penalize the misplacement in low dimensional space of points that are far away in high dimensional space, we can conclude that the global structure is not well preserved. t-SNE will group similar data points together into clusters, but distances between clusters might not mean anything.</p>
</div>
</div>
</div>
</section>
<section id="t-sne-4" class="slide level2">
<h2>t-SNE</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb4-2"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a></a></span>
<span id="cb4-4"><a></a><span class="kw">def</span> read_glove(file_path):</span>
<span id="cb4-5"><a></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path) <span class="im">as</span> f:</span>
<span id="cb4-6"><a></a>        <span class="cf">for</span> i, line <span class="kw">in</span> <span class="bu">enumerate</span>(f):</span>
<span id="cb4-7"><a></a>            fields <span class="op">=</span> line.rstrip().split(<span class="st">' '</span>)</span>
<span id="cb4-8"><a></a>            vec <span class="op">=</span> [<span class="bu">float</span>(x) <span class="cf">for</span> x <span class="kw">in</span> fields[<span class="dv">1</span>:]]</span>
<span id="cb4-9"><a></a>            word <span class="op">=</span> fields[<span class="dv">0</span>]</span>
<span id="cb4-10"><a></a>            <span class="cf">yield</span> (word, vec)</span>
<span id="cb4-11"><a></a>words <span class="op">=</span> []</span>
<span id="cb4-12"><a></a>vectors <span class="op">=</span> []</span>
<span id="cb4-13"><a></a><span class="cf">for</span> word, vec <span class="kw">in</span> read_glove(<span class="st">'data/glove/glove.42B.300d.txt'</span>):</span>
<span id="cb4-14"><a></a>    words.append(word)</span>
<span id="cb4-15"><a></a>    vectors.append(vec)</span>
<span id="cb4-16"><a></a>model <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, init<span class="op">=</span><span class="st">'pca'</span>, random_state<span class="op">=</span><span class="dv">0</span>) coordinates <span class="op">=</span> model.fit_transform(vectors)</span>
<span id="cb4-17"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb4-18"><a></a><span class="cf">for</span> word, xy <span class="kw">in</span> <span class="bu">zip</span>(words, coordinates):</span>
<span id="cb4-19"><a></a>    plt.scatter(xy[<span class="dv">0</span>], xy[<span class="dv">1</span>])</span>
<span id="cb4-20"><a></a>    plt.annotate(word,</span>
<span id="cb4-21"><a></a>plt.xlim(<span class="dv">25</span>, <span class="dv">55</span>)</span>
<span id="cb4-22"><a></a>plt.ylim(<span class="op">-</span><span class="dv">15</span>, <span class="dv">15</span>)</span>
<span id="cb4-23"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="t-sne-5" class="slide level2">
<h2>t-SNE</h2>

<img data-src="img/glove_sne.png" class="r-stretch quarto-figure-center"><p class="caption">GloVe embeddings visualized by t-SNE</p></section></section>
<section>
<section id="python-pcat-sne-visualization" class="title-slide slide level1 center">
<h1>Python PCA/t-SNE Visualization</h1>
<!-- % https://builtin.com/data-science/tsne-python -->
</section>
<section id="python-pca-visualization" class="slide level2">
<h2>Python PCA Visualization</h2>
<h3 id="use-mnist-data-set.">Use MNIST data set.</h3>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Get libraries</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>  <span class="im">from</span> __future__ <span class="im">import</span> print_function</span>
<span id="cb5-2"><a></a>  <span class="im">import</span> time</span>
<span id="cb5-3"><a></a></span>
<span id="cb5-4"><a></a>  <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-5"><a></a>  <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-6"><a></a></span>
<span id="cb5-7"><a></a>  <span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_mldata</span>
<span id="cb5-8"><a></a>  <span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb5-9"><a></a>  <span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb5-10"><a></a></span>
<span id="cb5-11"><a></a>  <span class="op">%</span>matplotlib inline</span>
<span id="cb5-12"><a></a>  <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-13"><a></a>  <span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb5-14"><a></a></span>
<span id="cb5-15"><a></a>  <span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-1" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Load data</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>   mnist <span class="op">=</span> fetch_mldata(<span class="st">"MNIST original"</span>)</span>
<span id="cb6-2"><a></a>   X <span class="op">=</span> mnist.data <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb6-3"><a></a>   y <span class="op">=</span> mnist.target</span>
<span id="cb6-4"><a></a></span>
<span id="cb6-5"><a></a>   <span class="bu">print</span>(X.shape, y.shape)</span>
<span id="cb6-6"><a></a></span>
<span id="cb6-7"><a></a>   [out] (<span class="dv">70000</span>, <span class="dv">784</span>) (<span class="dv">70000</span>,) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-2" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Convert to Pandas and randomize</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>    feat_cols <span class="op">=</span> [ <span class="st">'pixel'</span><span class="op">+</span><span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]) ]</span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>    df <span class="op">=</span> pd.DataFrame(X,columns<span class="op">=</span>feat_cols)</span>
<span id="cb7-4"><a></a>    df[<span class="st">'y'</span>] <span class="op">=</span> y</span>
<span id="cb7-5"><a></a>    df[<span class="st">'label'</span>] <span class="op">=</span> df[<span class="st">'y'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> i: <span class="bu">str</span>(i))</span>
<span id="cb7-6"><a></a></span>
<span id="cb7-7"><a></a>    X, y <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb7-8"><a></a></span>
<span id="cb7-9"><a></a>    <span class="bu">print</span>(<span class="st">'Size of the dataframe: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(df.shape))</span>
<span id="cb7-10"><a></a></span>
<span id="cb7-11"><a></a>    [out] Size of the dataframe: (<span class="dv">70000</span>, <span class="dv">785</span>)</span>
<span id="cb7-12"><a></a></span>
<span id="cb7-13"><a></a>    <span class="co"># For reproducibility of the results</span></span>
<span id="cb7-14"><a></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-15"><a></a></span>
<span id="cb7-16"><a></a>    rndperm <span class="op">=</span> np.random.permutation(df.shape[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-3" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Check some numbers</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>    plt.gray()</span>
<span id="cb8-2"><a></a>    fig <span class="op">=</span> plt.figure( figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">7</span>) )</span>
<span id="cb8-3"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">15</span>):</span>
<span id="cb8-4"><a></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">3</span>,<span class="dv">5</span>,i<span class="op">+</span><span class="dv">1</span>, title<span class="op">=</span><span class="st">"Digit: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="bu">str</span>(df.loc[rndperm[i],<span class="st">'label'</span>])) )</span>
<span id="cb8-5"><a></a>    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((<span class="dv">28</span>,<span class="dv">28</span>)).astype(<span class="bu">float</span>))</span>
<span id="cb8-6"><a></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>

<img data-src="img/digits_mnist.png" class="r-stretch"></section>
<section id="python-pca-visualization-4" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Use Scikit-learn for PCA</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb9-2"><a></a>    pca_result <span class="op">=</span> pca.fit_transform(df[feat_cols].values)</span>
<span id="cb9-3"><a></a></span>
<span id="cb9-4"><a></a>    df[<span class="st">'pca-one'</span>] <span class="op">=</span> pca_result[:,<span class="dv">0</span>]</span>
<span id="cb9-5"><a></a>    df[<span class="st">'pca-two'</span>] <span class="op">=</span> pca_result[:,<span class="dv">1</span>] </span>
<span id="cb9-6"><a></a>    df[<span class="st">'pca-three'</span>] <span class="op">=</span> pca_result[:,<span class="dv">2</span>]</span>
<span id="cb9-7"><a></a></span>
<span id="cb9-8"><a></a>    <span class="bu">print</span>(<span class="st">'Explained variation per principal component: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(pca.explained_variance_ratio_))</span>
<span id="cb9-9"><a></a></span>
<span id="cb9-10"><a></a>    Explained variation per principal component: [<span class="fl">0.09746116</span> <span class="fl">0.07155445</span> <span class="fl">0.06149531</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-5" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Scatterplot of first 2 PCs</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">10</span>))</span>
<span id="cb10-2"><a></a>    sns.scatterplot(</span>
<span id="cb10-3"><a></a>        x<span class="op">=</span><span class="st">"pca-one"</span>, y<span class="op">=</span><span class="st">"pca-two"</span>,</span>
<span id="cb10-4"><a></a>        hue<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb10-5"><a></a>        palette<span class="op">=</span>sns.color_palette(<span class="st">"hls"</span>, <span class="dv">10</span>),</span>
<span id="cb10-6"><a></a>        data<span class="op">=</span>df.loc[rndperm,:],</span>
<span id="cb10-7"><a></a>        legend<span class="op">=</span><span class="st">"full"</span>,</span>
<span id="cb10-8"><a></a>        alpha<span class="op">=</span><span class="fl">0.3</span></span>
<span id="cb10-9"><a></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-6" class="slide level2">
<h2>Python PCA Visualization</h2>

<img data-src="img/2d_pca_plot.jpg" class="r-stretch"></section>
<section id="python-pca-visualization-7" class="slide level2">
<h2>Python PCA Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Scatterplot of first 2 PCs in 3D</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>    ax <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">10</span>)).gca(projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb11-2"><a></a>    ax.scatter(</span>
<span id="cb11-3"><a></a>        xs<span class="op">=</span>df.loc[rndperm,:][<span class="st">"pca-one"</span>], </span>
<span id="cb11-4"><a></a>        ys<span class="op">=</span>df.loc[rndperm,:][<span class="st">"pca-two"</span>], </span>
<span id="cb11-5"><a></a>        zs<span class="op">=</span>df.loc[rndperm,:][<span class="st">"pca-three"</span>], </span>
<span id="cb11-6"><a></a>        c<span class="op">=</span>df.loc[rndperm,:][<span class="st">"y"</span>], </span>
<span id="cb11-7"><a></a>        cmap<span class="op">=</span><span class="st">'tab10'</span></span>
<span id="cb11-8"><a></a>    )</span>
<span id="cb11-9"><a></a>    ax.set_xlabel(<span class="st">'pca-one'</span>)</span>
<span id="cb11-10"><a></a>    ax.set_ylabel(<span class="st">'pca-two'</span>)</span>
<span id="cb11-11"><a></a>    ax.set_zlabel(<span class="st">'pca-three'</span>)</span>
<span id="cb11-12"><a></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-pca-visualization-8" class="slide level2">
<h2>Python PCA Visualization</h2>

<img data-src="img/3d_pca_plot.jpg" class="r-stretch"></section>
<section id="python-t-sne-visualization" class="slide level2">
<h2>Python t-SNE Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Run PCA</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>    N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb12-2"><a></a>    df_subset <span class="op">=</span> df.loc[rndperm[:N],:].copy()</span>
<span id="cb12-3"><a></a>    data_subset <span class="op">=</span> df_subset[feat_cols].values</span>
<span id="cb12-4"><a></a></span>
<span id="cb12-5"><a></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-6"><a></a>    pca_result <span class="op">=</span> pca.fit_transform(data_subset)</span>
<span id="cb12-7"><a></a></span>
<span id="cb12-8"><a></a>    df_subset[<span class="st">'pca-one'</span>] <span class="op">=</span> pca_result[:,<span class="dv">0</span>]</span>
<span id="cb12-9"><a></a>    df_subset[<span class="st">'pca-two'</span>] <span class="op">=</span> pca_result[:,<span class="dv">1</span>] </span>
<span id="cb12-10"><a></a>    df_subset[<span class="st">'pca-three'</span>] <span class="op">=</span> pca_result[:,<span class="dv">2</span>]</span>
<span id="cb12-11"><a></a></span>
<span id="cb12-12"><a></a>    <span class="bu">print</span>(<span class="st">'Explained variation per principal component: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(pca.explained_variance_ratio_))</span>
<span id="cb12-13"><a></a></span>
<span id="cb12-14"><a></a>    [out] Explained variation per principal component: [<span class="fl">0.09730166</span> <span class="fl">0.07135901</span> <span class="fl">0.06183721</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-t-sne-visualization-1" class="slide level2">
<h2>Python t-SNE Visualization</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Run t-SNE</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>    time_start <span class="op">=</span> time.time()</span>
<span id="cb13-2"><a></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, verbose<span class="op">=</span><span class="dv">1</span>, perplexity<span class="op">=</span><span class="dv">40</span>, n_iter<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb13-3"><a></a>    tsne_results <span class="op">=</span> tsne.fit_transform(data_subset)</span>
<span id="cb13-4"><a></a>    <span class="bu">print</span>(<span class="st">'t-SNE done! Time elapsed: </span><span class="sc">{}</span><span class="st"> seconds'</span>.<span class="bu">format</span>(time.time()<span class="op">-</span>time_start))</span>
<span id="cb13-5"><a></a></span>
<span id="cb13-6"><a></a>    [out] [t<span class="op">-</span>SNE] Computing <span class="dv">121</span> nearest neighbors...</span>
<span id="cb13-7"><a></a>    [t<span class="op">-</span>SNE] Indexed <span class="dv">10000</span> samples <span class="kw">in</span> <span class="fl">0.564</span><span class="er">s</span>...</span>
<span id="cb13-8"><a></a>    [t<span class="op">-</span>SNE] Computed neighbors <span class="cf">for</span> <span class="dv">10000</span> samples <span class="kw">in</span> <span class="fl">121.191</span><span class="er">s</span>...</span>
<span id="cb13-9"><a></a>    [t<span class="op">-</span>SNE] Computed conditional probabilities <span class="cf">for</span> sample <span class="dv">1000</span> <span class="op">/</span> <span class="dv">10000</span></span>
<span id="cb13-10"><a></a>    ...</span>
<span id="cb13-11"><a></a>    [t<span class="op">-</span>SNE] Mean sigma: <span class="fl">2.129023</span></span>
<span id="cb13-12"><a></a>    [t<span class="op">-</span>SNE] KL divergence after <span class="dv">300</span> iterations: <span class="fl">2.823509</span></span>
<span id="cb13-13"><a></a>    t<span class="op">-</span>SNE done<span class="op">!</span> Time elapsed: <span class="fl">157.3975932598114</span> seconds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-t-sne-visualization-2" class="slide level2">
<h2>Python t-SNE Visualization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Plot t-SNE</strong></p>
</div>
<div class="callout-content">
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>    df_subset[<span class="st">'tsne-2d-one'</span>] <span class="op">=</span> tsne_results[:,<span class="dv">0</span>]</span>
<span id="cb14-2"><a></a>    df_subset[<span class="st">'tsne-2d-two'</span>] <span class="op">=</span> tsne_results[:,<span class="dv">1</span>]</span>
<span id="cb14-3"><a></a></span>
<span id="cb14-4"><a></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">10</span>))</span>
<span id="cb14-5"><a></a>    sns.scatterplot(</span>
<span id="cb14-6"><a></a>        x<span class="op">=</span><span class="st">"tsne-2d-one"</span>, y<span class="op">=</span><span class="st">"tsne-2d-two"</span>,</span>
<span id="cb14-7"><a></a>        hue<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb14-8"><a></a>        palette<span class="op">=</span>sns.color_palette(<span class="st">"hls"</span>, <span class="dv">10</span>),</span>
<span id="cb14-9"><a></a>        data<span class="op">=</span>df_subset,</span>
<span id="cb14-10"><a></a>        legend<span class="op">=</span><span class="st">"full"</span>,</span>
<span id="cb14-11"><a></a>        alpha<span class="op">=</span><span class="fl">0.3</span></span>
<span id="cb14-12"><a></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="python-t-sne-visualization-3" class="slide level2">
<h2>Python t-SNE Visualization</h2>



<img data-src="img/tsne_plot.jpg" class="r-stretch quarto-figure-center"><p class="caption">Plot t-SNE</p></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":"19f286c43d34d478f6aaac04c3d1de92","id":"2e8cbc3acc2ca7d0b4a4e016fa8804a63f3512b3e00e7872956831f754b3cab5"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>