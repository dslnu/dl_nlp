<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="MSDE">
  <title>Deep Learning/NLP course – Transformers 2</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
  <script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Transformers 2</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
MSDE 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="preliminary-imports" class="slide level2">
<h2>Preliminary imports</h2>
<div id="ffb57923" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a></a><span class="im">import</span> os</span>
<span id="cb1-3"><a></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-4"><a></a><span class="im">import</span> random</span>
<span id="cb1-5"><a></a><span class="im">import</span> math</span>
<span id="cb1-6"><a></a><span class="im">import</span> json</span>
<span id="cb1-7"><a></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-8"><a></a></span>
<span id="cb1-9"><a></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-10"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-11"><a></a>plt.set_cmap(<span class="st">'cividis'</span>)</span>
<span id="cb1-12"><a></a><span class="op">%</span>matplotlib inline </span>
<span id="cb1-13"><a></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-14"><a></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-15"><a></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> to_rgb</span>
<span id="cb1-16"><a></a><span class="im">import</span> matplotlib</span>
<span id="cb1-17"><a></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-18"><a></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-19"><a></a>sns.reset_orig()</span>
<span id="cb1-20"><a></a></span>
<span id="cb1-21"><a></a><span class="co">## tqdm for loading bars</span></span>
<span id="cb1-22"><a></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb1-23"><a></a></span>
<span id="cb1-24"><a></a><span class="co">## PyTorch</span></span>
<span id="cb1-25"><a></a><span class="im">import</span> torch</span>
<span id="cb1-26"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-27"><a></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-28"><a></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb1-29"><a></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-30"><a></a></span>
<span id="cb1-31"><a></a><span class="co">## Torchvision</span></span>
<span id="cb1-32"><a></a><span class="im">import</span> torchvision</span>
<span id="cb1-33"><a></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> CIFAR100</span>
<span id="cb1-34"><a></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb1-35"><a></a></span>
<span id="cb1-36"><a></a><span class="co"># PyTorch Lightning</span></span>
<span id="cb1-37"><a></a><span class="cf">try</span>:</span>
<span id="cb1-38"><a></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb1-39"><a></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary</span></span>
<span id="cb1-40"><a></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet pytorch<span class="op">-</span>lightning<span class="op">&gt;=</span><span class="fl">1.4</span></span>
<span id="cb1-41"><a></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb1-42"><a></a><span class="im">from</span> pytorch_lightning.callbacks <span class="im">import</span> LearningRateMonitor, ModelCheckpoint</span>
<span id="cb1-43"><a></a></span>
<span id="cb1-44"><a></a><span class="co"># Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)</span></span>
<span id="cb1-45"><a></a>DATASET_PATH <span class="op">=</span> <span class="st">"../data"</span></span>
<span id="cb1-46"><a></a><span class="co"># Path to the folder where the pretrained models are saved</span></span>
<span id="cb1-47"><a></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../saved_models/tutorial6"</span></span>
<span id="cb1-48"><a></a></span>
<span id="cb1-49"><a></a><span class="co"># Setting the seed</span></span>
<span id="cb1-50"><a></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb1-51"><a></a></span>
<span id="cb1-52"><a></a><span class="co"># Ensure that all operations are deterministic on GPU (if used) for reproducibility</span></span>
<span id="cb1-53"><a></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-54"><a></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-55"><a></a></span>
<span id="cb1-56"><a></a>device <span class="op">=</span> torch.device(<span class="st">"mps:0"</span>) <span class="cf">if</span> torch.mps.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb1-57"><a></a><span class="bu">print</span>(<span class="st">"Device:"</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Device: mps:0</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 960x480 with 0 Axes&gt;</code></pre>
</div>
</div>
</section>
<section id="pre-trained-models" class="slide level2">
<h2>Pre-trained models</h2>
<div id="9228d260" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> urllib.request</span>
<span id="cb4-2"><a></a><span class="im">from</span> urllib.error <span class="im">import</span> HTTPError</span>
<span id="cb4-3"><a></a><span class="co"># Github URL where saved models are stored for this tutorial</span></span>
<span id="cb4-4"><a></a>base_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/"</span></span>
<span id="cb4-5"><a></a><span class="co"># Files to download</span></span>
<span id="cb4-6"><a></a>pretrained_files <span class="op">=</span> [<span class="st">"ReverseTask.ckpt"</span>, <span class="st">"SetAnomalyTask.ckpt"</span>]</span>
<span id="cb4-7"><a></a></span>
<span id="cb4-8"><a></a><span class="co"># Create checkpoint path if it doesn't exist yet</span></span>
<span id="cb4-9"><a></a>os.makedirs(CHECKPOINT_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-10"><a></a></span>
<span id="cb4-11"><a></a><span class="co"># For each file, check whether it already exists. If not, try downloading it.</span></span>
<span id="cb4-12"><a></a><span class="cf">for</span> file_name <span class="kw">in</span> pretrained_files:</span>
<span id="cb4-13"><a></a>    file_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, file_name)</span>
<span id="cb4-14"><a></a>    <span class="cf">if</span> <span class="st">"/"</span> <span class="kw">in</span> file_name:</span>
<span id="cb4-15"><a></a>        os.makedirs(file_path.rsplit(<span class="st">"/"</span>,<span class="dv">1</span>)[<span class="dv">0</span>], exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-16"><a></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(file_path):</span>
<span id="cb4-17"><a></a>        file_url <span class="op">=</span> base_url <span class="op">+</span> file_name</span>
<span id="cb4-18"><a></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>file_url<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb4-19"><a></a>        <span class="cf">try</span>:</span>
<span id="cb4-20"><a></a>            urllib.request.urlretrieve(file_url, file_path)</span>
<span id="cb4-21"><a></a>        <span class="cf">except</span> HTTPError <span class="im">as</span> e:</span>
<span id="cb4-22"><a></a>            <span class="bu">print</span>(<span class="st">"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:</span><span class="ch">\n</span><span class="st">"</span>, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="what-is-attention" class="slide level2">
<h2>What is Attention?</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Attention - recap</strong></p>
</div>
<div class="callout-content">
<p>The attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal of attention mechanism</strong></p>
</div>
<div class="callout-content">
<p>Take an average over the features of multiple elements. But instead of weighting each element equally, we want to weight them depending on their actual values.</p>
<p>In other words, we want to dynamically decide on which inputs we want to <strong>attend</strong> more than others.</p>
</div>
</div>
</div>
</section>
<section id="what-is-attention-1" class="slide level2">
<h2>What is Attention?</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Structure of attention mechanism</strong></p>
</div>
<div class="callout-content">
<p>Attention mechanism has usually four parts we need to specify:</p>
<ul>
<li><strong>Query</strong>: The query is a feature vector that describes what we are looking for in the sequence, i.e.&nbsp;what would we maybe want to pay attention to.</li>
<li><strong>Keys</strong>: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.</li>
<li><strong>Values</strong>: For each input element, we also have a value vector. This feature vector is the one we want to average over.</li>
<li><strong>Score function</strong>: To rate which elements we want to pay attention to, we need to specify a score function <span class="math inline">\(f_{attn}\)</span>. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-attention-2" class="slide level2">
<h2>What is Attention?</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Softmax</strong></p>
</div>
<div class="callout-content">
<p>The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query:</p>
<p><span class="math display">\[
\alpha_i = \frac{\exp\left(f_{attn}\left(\text{key}_i, \text{query}\right)\right)}{\sum_j \exp\left(f_{attn}\left(\text{key}_j, \text{query}\right)\right)}, \hspace{5mm} \text{out} = \sum_i \alpha_i \cdot \text{value}_i
\]</span></p>
</div>
</div>
</div>
<!-- For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights. -->

<img data-src="img/attention_example.svg" class="r-stretch"></section>
<section id="what-is-attention-3" class="slide level2">
<h2>What is Attention?</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Self-attention</strong></p>
</div>
<div class="callout-content">
<p>The attention applied inside the Transformer architecture is called <strong>self-attention</strong>.</p>
<ul>
<li>In self-attention, each sequence element provides a key, value, and query.</li>
<li>For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element.</li>
<li>In Transformer architecture, we use <strong>scaled dot product attention</strong>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention" class="slide level2">
<h2>Scaled Dot Product Attention</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>Have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dot product attention</strong></p>
</div>
<div class="callout-content">
<ul>
<li>a set of queries <span class="math inline">\(Q\in\mathbb{R}^{T\times d_k}\)</span></li>
<li>keys <span class="math inline">\(K\in\mathbb{R}^{T\times d_k}\)</span></li>
<li>values <span class="math inline">\(V\in\mathbb{R}^{T\times d_v}\)</span></li>
</ul>
<p>where <span class="math inline">\(T\)</span> is the sequence length, and <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> are the hidden dimensionalities.</p>
<p><span class="math display">\[
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention-1" class="slide level2">
<h2>Scaled Dot Product Attention</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Intuition</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>The attention value from element <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is based on its similarity of the query <span class="math inline">\(Q_i\)</span> and key <span class="math inline">\(K_j\)</span>, using the dot product as the similarity metric.</p></li>
<li><p>The matrix multiplication <span class="math inline">\(QK^T\)</span> performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape <span class="math inline">\(T\times T\)</span>.</p></li>
<li><p>Each row represents the attention logits for a specific element <span class="math inline">\(i\)</span> to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention).</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention-2" class="slide level2 smaller">
<h2>Scaled Dot Product Attention</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="img/scaled_dot_product_attn.svg" width="400"></p>
</div><div class="column" style="width:40%;">
<p>The block <code>Mask (opt.)</code> in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch.</p>
</div></div>
</section>
<section id="scaled-dot-product-attention-3" class="slide level2">
<h2>Scaled Dot Product Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Scaling factor</strong></p>
</div>
<div class="callout-content">
<p>This scaling factor <span class="math inline">\(1/\sqrt{d_k}\)</span> is crucial to maintain an appropriate variance of attention values after initialization. Remember that we initialize our layers with the intention of having equal variance throughout the model, and hence, <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> might also have a variance close to <span class="math inline">\(1\)</span>. However, performing a dot product over two vectors with a variance <span class="math inline">\(\sigma^2\)</span> results in a scalar having <span class="math inline">\(d_k\)</span>-times higher variance:</p>
<p><span class="math display">\[q_i \sim \mathcal{N}(0,\sigma^2), k_i \sim \mathcal{N}(0,\sigma^2) \to \text{Var}\left(\sum_{i=1}^{d_k} q_i\cdot k_i\right) = \sigma^4\cdot d_k\]</span></p>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention-4" class="slide level2">
<h2>Scaled Dot Product Attention</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Scaling factor: problem</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>If we do not scale down the variance back to <span class="math inline">\(\sim\sigma^2\)</span>, the softmax over the logits will already saturate to <span class="math inline">\(1\)</span> for one random element and <span class="math inline">\(0\)</span> for all others.</p></li>
<li><p>The gradients through the softmax will be close to zero so that we can’t learn the parameters appropriately. Note that the extra factor of <span class="math inline">\(\sigma^2\)</span>, i.e., having <span class="math inline">\(\sigma^4\)</span> instead of <span class="math inline">\(\sigma^2\)</span>, is usually not an issue, since we keep the original variance <span class="math inline">\(\sigma^2\)</span> close to <span class="math inline">\(1\)</span> anyways.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention-5" class="slide level2">
<h2>Scaled Dot Product Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Python function</strong></p>
</div>
<div class="callout-content">
<p>The function below computes the output features given the triple of queries, keys, and values:</p>
<div id="5a9e04ae" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="kw">def</span> scaled_dot_product(q, k, v, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-2"><a></a>    d_k <span class="op">=</span> q.size()[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-3"><a></a>    attn_logits <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-4"><a></a>    attn_logits <span class="op">=</span> attn_logits <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb5-5"><a></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-6"><a></a>        attn_logits <span class="op">=</span> attn_logits.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">9e15</span>)</span>
<span id="cb5-7"><a></a>    attention <span class="op">=</span> F.softmax(attn_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-8"><a></a>    values <span class="op">=</span> torch.matmul(attention, v)</span>
<span id="cb5-9"><a></a>    <span class="cf">return</span> values, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention-6" class="slide level2 smaller">
<h2>Scaled Dot Product Attention</h2>
<div id="f66efbcc" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>seq_len, d_k <span class="op">=</span> <span class="dv">3</span>, <span class="dv">2</span></span>
<span id="cb6-2"><a></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb6-3"><a></a>q <span class="op">=</span> torch.randn(seq_len, d_k)</span>
<span id="cb6-4"><a></a>k <span class="op">=</span> torch.randn(seq_len, d_k)</span>
<span id="cb6-5"><a></a>v <span class="op">=</span> torch.randn(seq_len, d_k)</span>
<span id="cb6-6"><a></a>values, attention <span class="op">=</span> scaled_dot_product(q, k, v)</span>
<span id="cb6-7"><a></a><span class="bu">print</span>(<span class="st">"Q</span><span class="ch">\n</span><span class="st">"</span>, q)</span>
<span id="cb6-8"><a></a><span class="bu">print</span>(<span class="st">"K</span><span class="ch">\n</span><span class="st">"</span>, k)</span>
<span id="cb6-9"><a></a><span class="bu">print</span>(<span class="st">"V</span><span class="ch">\n</span><span class="st">"</span>, v)</span>
<span id="cb6-10"><a></a><span class="bu">print</span>(<span class="st">"Values</span><span class="ch">\n</span><span class="st">"</span>, values)</span>
<span id="cb6-11"><a></a><span class="bu">print</span>(<span class="st">"Attention</span><span class="ch">\n</span><span class="st">"</span>, attention)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Q
 tensor([[ 0.3367,  0.1288],
        [ 0.2345,  0.2303],
        [-1.1229, -0.1863]])
K
 tensor([[ 2.2082, -0.6380],
        [ 0.4617,  0.2674],
        [ 0.5349,  0.8094]])
V
 tensor([[ 1.1103, -1.6898],
        [-0.9890,  0.9580],
        [ 1.3221,  0.8172]])
Values
 tensor([[ 0.5698, -0.1520],
        [ 0.5379, -0.0265],
        [ 0.2246,  0.5556]])
Attention
 tensor([[0.4028, 0.2886, 0.3086],
        [0.3538, 0.3069, 0.3393],
        [0.1303, 0.4630, 0.4067]])</code></pre>
</div>
</div>
</section>
<section id="multi-head-attention" class="slide level2">
<h2>Multi-Head Attention</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>We extend the attention mechanisms to multiple heads, i.e.&nbsp;multiple different query-key-value triplets on the same features.</p>
<p>We transform query, key, and value matrix into <span class="math inline">\(h\)</span> sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix.</p>
<p><span class="math display">\[
\begin{split}
    \text{Multihead}(Q,K,V) &amp; = \text{Concat}(\text{head}_1,...,\text{head}_h)W^{O}\\
    \text{where } \text{head}_i &amp; = \text{Attention}(QW_i^Q,KW_i^K, VW_i^V)
\end{split}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="multi-head-attention-1" class="slide level2">
<h2>Multi-Head Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multi-Head attention parameters</strong></p>
</div>
<div class="callout-content">
<p>We refer to this as Multi-Head Attention layer with the learnable parameters</p>
<ul>
<li><span class="math inline">\(W_{1...h}^{Q}\in\mathbb{R}^{D\times d_k}\)</span></li>
<li><span class="math inline">\(W_{1...h}^{K}\in\mathbb{R}^{D\times d_k}\)</span></li>
<li><span class="math inline">\(W_{1...h}^{V}\in\mathbb{R}^{D\times d_v}\)</span></li>
<li><span class="math inline">\(W^{O}\in\mathbb{R}^{h\cdot d_v\times d_{out}}\)</span> (<span class="math inline">\(D\)</span> being the input dimensionality).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="multi-head-attention-2" class="slide level2">
<h2>Multi-Head Attention</h2>

<img data-src="img/multihead_attention.svg" class="r-stretch quarto-figure-center"><p class="caption">Multi-Head Attention as a computational graph</p></section>
<section id="multi-head-attention-3" class="slide level2">
<h2>Multi-Head Attention</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Application to NN</strong></p>
</div>
<div class="callout-content">
<p>How are we applying a Multi-Head Attention layer in a neural network, where we don’t have an arbitrary query, key, and value vector as input?</p>
<p>Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, <span class="math inline">\(X\in\mathbb{R}^{B\times T\times d_{\text{model}}}\)</span>, as <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> (<span class="math inline">\(B\)</span> being the batch size, <span class="math inline">\(T\)</span> the sequence length, <span class="math inline">\(d_{\text{model}}\)</span> the hidden dimensionality of <span class="math inline">\(X\)</span>).</p>
<p>The consecutive weight matrices <span class="math inline">\(W^{Q}\)</span>, <span class="math inline">\(W^{K}\)</span>, and <span class="math inline">\(W^{V}\)</span> can transform <span class="math inline">\(X\)</span> to the corresponding feature vectors that represent the queries, keys, and values of the input.</p>
</div>
</div>
</div>
</section>
<section id="multi-head-attention-4" class="slide level2">
<h2>Multi-Head Attention</h2>
<div id="dce6e216" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="co"># Helper function to support different mask shapes.</span></span>
<span id="cb8-2"><a></a><span class="co"># Output shape supports (batch_size, number of heads, seq length, seq length)</span></span>
<span id="cb8-3"><a></a><span class="co"># If 2D: broadcasted over batch size and number of heads</span></span>
<span id="cb8-4"><a></a><span class="co"># If 3D: broadcasted over number of heads</span></span>
<span id="cb8-5"><a></a><span class="co"># If 4D: leave as is</span></span>
<span id="cb8-6"><a></a><span class="kw">def</span> expand_mask(mask):</span>
<span id="cb8-7"><a></a>    <span class="cf">assert</span> mask.ndim <span class="op">&gt;=</span> <span class="dv">2</span>, <span class="st">"Mask must be at least 2-dimensional with seq_length x seq_length"</span></span>
<span id="cb8-8"><a></a>    <span class="cf">if</span> mask.ndim <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb8-9"><a></a>        mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb8-10"><a></a>    <span class="cf">while</span> mask.ndim <span class="op">&lt;</span> <span class="dv">4</span>:</span>
<span id="cb8-11"><a></a>        mask <span class="op">=</span> mask.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb8-12"><a></a>    <span class="cf">return</span> mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multi-head-attention-5" class="slide level2">
<h2>Multi-Head Attention</h2>
<div id="472de021" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="kw">class</span> MultiheadAttention(nn.Module):</span>
<span id="cb9-2"><a></a>    </span>
<span id="cb9-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, embed_dim, num_heads):</span>
<span id="cb9-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-5"><a></a>        <span class="cf">assert</span> embed_dim <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"Embedding dimension must be 0 modulo number of heads."</span></span>
<span id="cb9-6"><a></a>        </span>
<span id="cb9-7"><a></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb9-8"><a></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb9-9"><a></a>        <span class="va">self</span>.head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</span>
<span id="cb9-10"><a></a>        </span>
<span id="cb9-11"><a></a>        <span class="co"># Stack all weight matrices 1...h together for efficiency</span></span>
<span id="cb9-12"><a></a>        <span class="co"># Note that in many implementations you see "bias=False" which is optional</span></span>
<span id="cb9-13"><a></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(input_dim, <span class="dv">3</span><span class="op">*</span>embed_dim)</span>
<span id="cb9-14"><a></a>        <span class="va">self</span>.o_proj <span class="op">=</span> nn.Linear(embed_dim, input_dim)</span>
<span id="cb9-15"><a></a>        </span>
<span id="cb9-16"><a></a>        <span class="va">self</span>._reset_parameters()</span>
<span id="cb9-17"><a></a></span>
<span id="cb9-18"><a></a>    <span class="kw">def</span> _reset_parameters(<span class="va">self</span>):</span>
<span id="cb9-19"><a></a>        <span class="co"># Original Transformer initialization, see PyTorch documentation</span></span>
<span id="cb9-20"><a></a>        nn.init.xavier_uniform_(<span class="va">self</span>.qkv_proj.weight)</span>
<span id="cb9-21"><a></a>        <span class="va">self</span>.qkv_proj.bias.data.fill_(<span class="dv">0</span>)</span>
<span id="cb9-22"><a></a>        nn.init.xavier_uniform_(<span class="va">self</span>.o_proj.weight)</span>
<span id="cb9-23"><a></a>        <span class="va">self</span>.o_proj.bias.data.fill_(<span class="dv">0</span>)</span>
<span id="cb9-24"><a></a></span>
<span id="cb9-25"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>, return_attention<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-26"><a></a>        batch_size, seq_length, _ <span class="op">=</span> x.size()</span>
<span id="cb9-27"><a></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-28"><a></a>            mask <span class="op">=</span> expand_mask(mask)</span>
<span id="cb9-29"><a></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv_proj(x)</span>
<span id="cb9-30"><a></a>        </span>
<span id="cb9-31"><a></a>        <span class="co"># Separate Q, K, V from linear output</span></span>
<span id="cb9-32"><a></a>        qkv <span class="op">=</span> qkv.reshape(batch_size, seq_length, <span class="va">self</span>.num_heads, <span class="dv">3</span><span class="op">*</span><span class="va">self</span>.head_dim)</span>
<span id="cb9-33"><a></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>) <span class="co"># [Batch, Head, SeqLen, Dims]</span></span>
<span id="cb9-34"><a></a>        q, k, v <span class="op">=</span> qkv.chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-35"><a></a>        </span>
<span id="cb9-36"><a></a>        <span class="co"># Determine value outputs</span></span>
<span id="cb9-37"><a></a>        values, attention <span class="op">=</span> scaled_dot_product(q, k, v, mask<span class="op">=</span>mask)</span>
<span id="cb9-38"><a></a>        values <span class="op">=</span> values.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>) <span class="co"># [Batch, SeqLen, Head, Dims]</span></span>
<span id="cb9-39"><a></a>        values <span class="op">=</span> values.reshape(batch_size, seq_length, <span class="va">self</span>.embed_dim)</span>
<span id="cb9-40"><a></a>        o <span class="op">=</span> <span class="va">self</span>.o_proj(values)</span>
<span id="cb9-41"><a></a>        </span>
<span id="cb9-42"><a></a>        <span class="cf">if</span> return_attention:</span>
<span id="cb9-43"><a></a>            <span class="cf">return</span> o, attention</span>
<span id="cb9-44"><a></a>        <span class="cf">else</span>:</span>
<span id="cb9-45"><a></a>            <span class="cf">return</span> o</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multi-head-attention-6" class="slide level2">
<h2>Multi-Head Attention</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Permutation-equivariance</strong></p>
</div>
<div class="callout-content">
<p>One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g.&nbsp;<span class="math inline">\(X_1\leftrightarrow X_2\)</span> (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched.</p>
<p>Hence, the multi-head attention is actually looking at the input not as a sequence, but as <strong>a set of elements</strong>.</p>
<p>This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable!</p>
</div>
</div>
</div>

<aside><div>
<p>But what if the order of the input is actually important for solving the task, like language modeling? The answer is <strong>positional encoding</strong>.</p>
</div></aside></section>
<section id="comparison-with-cnns-and-rnns" class="slide level2">
<h2>Comparison with CNNs and RNNs</h2>

<img data-src="img/comparison_conv_rnn.svg" class="r-stretch quarto-figure-center"><p class="caption"><span class="math inline">\(n\)</span> is the sequence length, <span class="math inline">\(d\)</span> is the representation dimension and <span class="math inline">\(k\)</span> is the kernel size of convolutions.</p></section>
<section id="transformer-encoder" class="slide level2">
<h2>Transformer Encoder</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>History reminder</strong></p>
</div>
<div class="callout-content">
<p>Originally, the Transformer model was designed for machine translation:</p>
<ul>
<li>it got an <strong>encoder-decoder</strong> structure where the encoder takes as input the sentence in the original language and generates an attention-based representation.</li>
<li>on the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN.</li>
</ul>
<p>This structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding.</p>
</div>
</div>
</div>
</section>
<section id="full-architecture" class="slide level2">
<h2>Full architecture</h2>

<img data-src="img/transformer_architecture.svg" class="r-stretch"></section>
<section id="transformer-encoder-1" class="slide level2">
<h2>Transformer Encoder</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Encoder</strong></p>
</div>
<div class="callout-content">
<ul>
<li>The encoder consists of <span class="math inline">\(N\)</span> identical blocks that are applied in sequence.</li>
<li>Taking as input <span class="math inline">\(x\)</span>, it is first passed through a Multi-Head Attention block as we have implemented above.</li>
<li>The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum.</li>
<li>Overall, it calculates <span class="math inline">\(\text{LayerNorm}(x+\text{Multihead}(x,x,x))\)</span> (<span class="math inline">\(x\)</span> being <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> input to the attention layer).<br>
</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformer-encoder-2" class="slide level2">
<h2>Transformer Encoder</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Residual connections</strong></p>
</div>
<div class="callout-content">
<p>The residual connection is crucial in the Transformer architecture for two reasons:</p>
<ol type="1">
<li>Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.</li>
<li>Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position <span class="math inline">\(i\)</span> has no relation to its original input.</li>
</ol>
<!-- All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow. -->
</div>
</div>
</div>
</section>
<section id="layer-normalization" class="slide level2">
<h2>Layer Normalization</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence.</p>
</div>
</div>
</div>
</section>
<section id="additional-feed-forward-nn" class="slide level2">
<h2>Additional Feed-Forward NN</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear<span class="math inline">\(\to\)</span>ReLU<span class="math inline">\(\to\)</span>Linear MLP. The full transformation including the residual connection can be expressed as:</p>
<p><span class="math display">\[
\begin{split}
    \text{FFN}(x) &amp; = \max(0, xW_1+b_1)W_2 + b_2\\
    x &amp; = \text{LayerNorm}(x + \text{FFN}(x))
\end{split}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p>This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to “post-process” the new information added by the previous Multi-Head Attention, and prepare it for the next attention block.</p>
</div>
</div>
</div>
</section>
<section id="additional-feed-forward-nn-1" class="slide level2">
<h2>Additional Feed-Forward NN</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dimensionality</strong></p>
</div>
<div class="callout-content">
<p>Usually, the inner dimensionality of the MLP is 2-8<span class="math inline">\(\times\)</span> larger than <span class="math inline">\(d_{\text{model}}\)</span>, i.e.&nbsp;the dimensionality of the original input <span class="math inline">\(x\)</span>.</p>
<p>The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.</p>
</div>
</div>
</div>
</section>
<section id="encoder-implementation" class="slide level2">
<h2>Encoder implementation</h2>
<div id="b4b43956" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb10-2"><a></a>    </span>
<span id="cb10-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, num_heads, dim_feedforward, dropout<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb10-4"><a></a>        <span class="co">"""</span></span>
<span id="cb10-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb10-6"><a></a><span class="co">            input_dim - Dimensionality of the input</span></span>
<span id="cb10-7"><a></a><span class="co">            num_heads - Number of heads to use in the attention block</span></span>
<span id="cb10-8"><a></a><span class="co">            dim_feedforward - Dimensionality of the hidden layer in the MLP</span></span>
<span id="cb10-9"><a></a><span class="co">            dropout - Dropout probability to use in the dropout layers</span></span>
<span id="cb10-10"><a></a><span class="co">        """</span></span>
<span id="cb10-11"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-12"><a></a>        </span>
<span id="cb10-13"><a></a>        <span class="co"># Attention layer</span></span>
<span id="cb10-14"><a></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiheadAttention(input_dim, input_dim, num_heads)</span>
<span id="cb10-15"><a></a>        </span>
<span id="cb10-16"><a></a>        <span class="co"># Two-layer MLP</span></span>
<span id="cb10-17"><a></a>        <span class="va">self</span>.linear_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb10-18"><a></a>            nn.Linear(input_dim, dim_feedforward),</span>
<span id="cb10-19"><a></a>            nn.Dropout(dropout),</span>
<span id="cb10-20"><a></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb10-21"><a></a>            nn.Linear(dim_feedforward, input_dim)</span>
<span id="cb10-22"><a></a>        )</span>
<span id="cb10-23"><a></a>        </span>
<span id="cb10-24"><a></a>        <span class="co"># Layers to apply in between the main layers</span></span>
<span id="cb10-25"><a></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(input_dim)</span>
<span id="cb10-26"><a></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(input_dim)</span>
<span id="cb10-27"><a></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb10-28"><a></a></span>
<span id="cb10-29"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-30"><a></a>        <span class="co"># Attention part</span></span>
<span id="cb10-31"><a></a>        attn_out <span class="op">=</span> <span class="va">self</span>.self_attn(x, mask<span class="op">=</span>mask)</span>
<span id="cb10-32"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(attn_out)</span>
<span id="cb10-33"><a></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb10-34"><a></a>        </span>
<span id="cb10-35"><a></a>        <span class="co"># MLP part</span></span>
<span id="cb10-36"><a></a>        linear_out <span class="op">=</span> <span class="va">self</span>.linear_net(x)</span>
<span id="cb10-37"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(linear_out)</span>
<span id="cb10-38"><a></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb10-39"><a></a>        </span>
<span id="cb10-40"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transformer-encoder-3" class="slide level2">
<h2>Transformer Encoder</h2>
<div id="75efa316" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="kw">class</span> TransformerEncoder(nn.Module):</span>
<span id="cb11-2"><a></a>    </span>
<span id="cb11-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_layers, <span class="op">**</span>block_args):</span>
<span id="cb11-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-5"><a></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderBlock(<span class="op">**</span>block_args) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb11-6"><a></a></span>
<span id="cb11-7"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb11-8"><a></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb11-9"><a></a>            x <span class="op">=</span> l(x, mask<span class="op">=</span>mask)</span>
<span id="cb11-10"><a></a>        <span class="cf">return</span> x</span>
<span id="cb11-11"><a></a></span>
<span id="cb11-12"><a></a>    <span class="kw">def</span> get_attention_maps(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb11-13"><a></a>        attention_maps <span class="op">=</span> []</span>
<span id="cb11-14"><a></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb11-15"><a></a>            _, attn_map <span class="op">=</span> l.self_attn(x, mask<span class="op">=</span>mask, return_attention<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-16"><a></a>            attention_maps.append(attn_map)</span>
<span id="cb11-17"><a></a>            x <span class="op">=</span> l(x)</span>
<span id="cb11-18"><a></a>        <span class="cf">return</span> attention_maps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<!-- `get_attention_maps` returns the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. -->
</section>
<section id="positional-encoding" class="slide level2">
<h2>Positional encoding</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rationale</strong></p>
</div>
<div class="callout-content">
<p>Instead of learning an embedding for every possible position, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al.&nbsp;are sine and cosine functions of different frequencies, as follows:</p>
<p><span class="math display">\[
PE_{(pos,i)} = \begin{cases}
    \sin\left(\frac{pos}{10000^{i/d_{\text{model}}}}\right) &amp; \text{if}\hspace{3mm} i \text{ mod } 2=0\\
    \cos\left(\frac{pos}{10000^{(i-1)/d_{\text{model}}}}\right) &amp; \text{otherwise}\\
\end{cases}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="positional-encoding-1" class="slide level2">
<h2>Positional encoding</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(PE_{(pos,i)}\)</span> represents the position encoding at position <span class="math inline">\(pos\)</span> in the sequence, and hidden dimensionality <span class="math inline">\(i\)</span>. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see “Positional encoding”), and constitute the position information. We distinguish between even (<span class="math inline">\(i \text{ mod } 2=0\)</span>) and uneven (<span class="math inline">\(i \text{ mod } 2=1\)</span>) hidden dimensionalities where we apply a sine/cosine respectively.</p>
<p>The intuition behind this encoding is that you can represent <span class="math inline">\(PE_{(pos+k,:)}\)</span> as a linear function of <span class="math inline">\(PE_{(pos,:)}\)</span>, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from <span class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000\cdot 2\pi\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="positional-encoding-implementation" class="slide level2">
<h2>Positional encoding implementation</h2>
<div id="a782c3ce" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb12-2"><a></a></span>
<span id="cb12-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb12-4"><a></a>        <span class="co">"""</span></span>
<span id="cb12-5"><a></a><span class="co">        Inputs</span></span>
<span id="cb12-6"><a></a><span class="co">            d_model - Hidden dimensionality of the input.</span></span>
<span id="cb12-7"><a></a><span class="co">            max_len - Maximum length of a sequence to expect.</span></span>
<span id="cb12-8"><a></a><span class="co">        """</span></span>
<span id="cb12-9"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-10"><a></a></span>
<span id="cb12-11"><a></a>        <span class="co"># Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs</span></span>
<span id="cb12-12"><a></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb12-13"><a></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-14"><a></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb12-15"><a></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb12-16"><a></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb12-17"><a></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb12-18"><a></a>        </span>
<span id="cb12-19"><a></a>        <span class="co"># register_buffer =&gt; Tensor which is not a parameter, but should be part of the modules state.</span></span>
<span id="cb12-20"><a></a>        <span class="co"># Used for tensors that need to be on the same device as the module.</span></span>
<span id="cb12-21"><a></a>        <span class="co"># persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) </span></span>
<span id="cb12-22"><a></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe, persistent<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-23"><a></a></span>
<span id="cb12-24"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-25"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>)]</span>
<span id="cb12-26"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="positional-encoding-visualization" class="slide level2">
<h2>Positional encoding visualization</h2>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Code</a></li><li><a href="#tabset-1-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div id="4a988948" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>encod_block <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="dv">48</span>, max_len<span class="op">=</span><span class="dv">96</span>)</span>
<span id="cb13-2"><a></a>pe <span class="op">=</span> encod_block.pe.squeeze().T.cpu().numpy()</span>
<span id="cb13-3"><a></a></span>
<span id="cb13-4"><a></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb13-5"><a></a>pos <span class="op">=</span> ax.imshow(pe, cmap<span class="op">=</span><span class="st">"RdGy"</span>, extent<span class="op">=</span>(<span class="dv">1</span>,pe.shape[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>,pe.shape[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb13-6"><a></a>fig.colorbar(pos, ax<span class="op">=</span>ax)</span>
<span id="cb13-7"><a></a>ax.set_xlabel(<span class="st">"Position in sequence"</span>)</span>
<span id="cb13-8"><a></a>ax.set_ylabel(<span class="st">"Hidden dimension"</span>)</span>
<span id="cb13-9"><a></a>ax.set_title(<span class="st">"Positional encoding over hidden dimensions"</span>)</span>
<span id="cb13-10"><a></a>ax.set_xticks([<span class="dv">1</span>]<span class="op">+</span>[i<span class="op">*</span><span class="dv">10</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">1</span><span class="op">+</span>pe.shape[<span class="dv">1</span>]<span class="op">//</span><span class="dv">10</span>)])</span>
<span id="cb13-11"><a></a>ax.set_yticks([<span class="dv">1</span>]<span class="op">+</span>[i<span class="op">*</span><span class="dv">10</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">1</span><span class="op">+</span>pe.shape[<span class="dv">0</span>]<span class="op">//</span><span class="dv">10</span>)])</span>
<span id="cb13-12"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2">
<div id="a8cbbe5c" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-12-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="positional-encoding-visualization-1" class="slide level2">
<h2>Positional encoding visualization</h2>
<div class="panel-tabset">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1">Code</a></li><li><a href="#tabset-2-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1">
<div id="5ae4d8fc" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>sns.set_theme()</span>
<span id="cb14-2"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">4</span>))</span>
<span id="cb14-3"><a></a>ax <span class="op">=</span> [a <span class="cf">for</span> a_list <span class="kw">in</span> ax <span class="cf">for</span> a <span class="kw">in</span> a_list]</span>
<span id="cb14-4"><a></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ax)):</span>
<span id="cb14-5"><a></a>    ax[i].plot(np.arange(<span class="dv">1</span>,<span class="dv">17</span>), pe[i,:<span class="dv">16</span>], color<span class="op">=</span><span class="ss">f'C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>, marker<span class="op">=</span><span class="st">"o"</span>, markersize<span class="op">=</span><span class="dv">6</span>, markeredgecolor<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb14-6"><a></a>    ax[i].set_title(<span class="ss">f"Encoding in hidden dimension </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-7"><a></a>    ax[i].set_xlabel(<span class="st">"Position in sequence"</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-8"><a></a>    ax[i].set_ylabel(<span class="st">"Positional encoding"</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-9"><a></a>    ax[i].set_xticks(np.arange(<span class="dv">1</span>,<span class="dv">17</span>))</span>
<span id="cb14-10"><a></a>    ax[i].tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-11"><a></a>    ax[i].tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'minor'</span>, labelsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb14-12"><a></a>    ax[i].set_ylim(<span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1.2</span>)</span>
<span id="cb14-13"><a></a>fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb14-14"><a></a>sns.reset_orig()</span>
<span id="cb14-15"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2">
<div id="a2a6155a" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-14-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="learning-rate-warm-up" class="slide level2">
<h2>Learning rate warm-up</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>We gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations.</p>
<p>Training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing.</p>
</div>
</div>
</div>

<img data-src="img/warmup_loss_plot.svg" class="r-stretch"></section>
<section id="learning-rate-warm-up-1" class="slide level2">
<h2>Learning rate warm-up</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Intuition</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like <a href="https://arxiv.org/abs/1908.03265">RAdam</a> have been shown to overcome this issue, not requiring warm-up for training Transformers.</li>
<li>The iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Pre-Layer Normalization</a> (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques (<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Adaptive Normalization</a>, <a href="https://arxiv.org/abs/2003.07845">Power Normalization</a>).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="learning-rate-warm-up-2" class="slide level2">
<h2>Learning rate warm-up</h2>
<div id="947f192d" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="kw">class</span> CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):</span>
<span id="cb15-2"><a></a>    </span>
<span id="cb15-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, optimizer, warmup, max_iters):</span>
<span id="cb15-4"><a></a>        <span class="va">self</span>.warmup <span class="op">=</span> warmup</span>
<span id="cb15-5"><a></a>        <span class="va">self</span>.max_num_iters <span class="op">=</span> max_iters</span>
<span id="cb15-6"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(optimizer)</span>
<span id="cb15-7"><a></a>        </span>
<span id="cb15-8"><a></a>    <span class="kw">def</span> get_lr(<span class="va">self</span>):</span>
<span id="cb15-9"><a></a>        lr_factor <span class="op">=</span> <span class="va">self</span>.get_lr_factor(epoch<span class="op">=</span><span class="va">self</span>.last_epoch)</span>
<span id="cb15-10"><a></a>        <span class="cf">return</span> [base_lr <span class="op">*</span> lr_factor <span class="cf">for</span> base_lr <span class="kw">in</span> <span class="va">self</span>.base_lrs]</span>
<span id="cb15-11"><a></a>    </span>
<span id="cb15-12"><a></a>    <span class="kw">def</span> get_lr_factor(<span class="va">self</span>, epoch):</span>
<span id="cb15-13"><a></a>        lr_factor <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.cos(np.pi <span class="op">*</span> epoch <span class="op">/</span> <span class="va">self</span>.max_num_iters))</span>
<span id="cb15-14"><a></a>        <span class="cf">if</span> epoch <span class="op">&lt;=</span> <span class="va">self</span>.warmup:</span>
<span id="cb15-15"><a></a>            lr_factor <span class="op">*=</span> epoch <span class="op">*</span> <span class="fl">1.0</span> <span class="op">/</span> <span class="va">self</span>.warmup</span>
<span id="cb15-16"><a></a>        <span class="cf">return</span> lr_factor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="learning-rate-warm-up-3" class="slide level2">
<h2>Learning rate warm-up</h2>
<div class="panel-tabset">
<ul id="tabset-3" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-3-1">Code</a></li><li><a href="#tabset-3-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1">
<div id="599f4a0b" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="co"># Needed for initializing the lr scheduler</span></span>
<span id="cb16-2"><a></a>p <span class="op">=</span> nn.Parameter(torch.empty(<span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb16-3"><a></a>optimizer <span class="op">=</span> optim.Adam([p], lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb16-4"><a></a>lr_scheduler <span class="op">=</span> CosineWarmupScheduler(optimizer<span class="op">=</span>optimizer, warmup<span class="op">=</span><span class="dv">100</span>, max_iters<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb16-5"><a></a></span>
<span id="cb16-6"><a></a><span class="co"># Plotting</span></span>
<span id="cb16-7"><a></a>epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">2000</span>))</span>
<span id="cb16-8"><a></a>sns.<span class="bu">set</span>()</span>
<span id="cb16-9"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb16-10"><a></a>plt.plot(epochs, [lr_scheduler.get_lr_factor(e) <span class="cf">for</span> e <span class="kw">in</span> epochs])</span>
<span id="cb16-11"><a></a>plt.ylabel(<span class="st">"Learning rate factor"</span>)</span>
<span id="cb16-12"><a></a>plt.xlabel(<span class="st">"Iterations (in batches)"</span>)</span>
<span id="cb16-13"><a></a>plt.title(<span class="st">"Cosine Warm-up Learning Rate Scheduler"</span>)</span>
<span id="cb16-14"><a></a>plt.show()</span>
<span id="cb16-15"><a></a>sns.reset_orig()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-3-2">
<div id="f9c0fc39" class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-17-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="pytorch-lightning-module" class="slide level2">
<h2>PyTorch Lightning Module</h2>
<div id="8e1d7c4b" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a><span class="kw">class</span> TransformerPredictor(pl.LightningModule):</span>
<span id="cb17-2"><a></a></span>
<span id="cb17-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout<span class="op">=</span><span class="fl">0.0</span>, input_dropout<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb17-4"><a></a>        <span class="co">"""</span></span>
<span id="cb17-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb17-6"><a></a><span class="co">            input_dim - Hidden dimensionality of the input</span></span>
<span id="cb17-7"><a></a><span class="co">            model_dim - Hidden dimensionality to use inside the Transformer</span></span>
<span id="cb17-8"><a></a><span class="co">            num_classes - Number of classes to predict per sequence element</span></span>
<span id="cb17-9"><a></a><span class="co">            num_heads - Number of heads to use in the Multi-Head Attention blocks</span></span>
<span id="cb17-10"><a></a><span class="co">            num_layers - Number of encoder blocks to use.</span></span>
<span id="cb17-11"><a></a><span class="co">            lr - Learning rate in the optimizer</span></span>
<span id="cb17-12"><a></a><span class="co">            warmup - Number of warmup steps. Usually between 50 and 500</span></span>
<span id="cb17-13"><a></a><span class="co">            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler</span></span>
<span id="cb17-14"><a></a><span class="co">            dropout - Dropout to apply inside the model</span></span>
<span id="cb17-15"><a></a><span class="co">            input_dropout - Dropout to apply on the input features</span></span>
<span id="cb17-16"><a></a><span class="co">        """</span></span>
<span id="cb17-17"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-18"><a></a>        <span class="va">self</span>.save_hyperparameters()</span>
<span id="cb17-19"><a></a>        <span class="va">self</span>._create_model()</span>
<span id="cb17-20"><a></a></span>
<span id="cb17-21"><a></a>    <span class="kw">def</span> _create_model(<span class="va">self</span>):</span>
<span id="cb17-22"><a></a>        <span class="co"># Input dim -&gt; Model dim</span></span>
<span id="cb17-23"><a></a>        <span class="va">self</span>.input_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb17-24"><a></a>            nn.Dropout(<span class="va">self</span>.hparams.input_dropout),</span>
<span id="cb17-25"><a></a>            nn.Linear(<span class="va">self</span>.hparams.input_dim, <span class="va">self</span>.hparams.model_dim)</span>
<span id="cb17-26"><a></a>        )</span>
<span id="cb17-27"><a></a>        <span class="co"># Positional encoding for sequences</span></span>
<span id="cb17-28"><a></a>        <span class="va">self</span>.positional_encoding <span class="op">=</span> PositionalEncoding(d_model<span class="op">=</span><span class="va">self</span>.hparams.model_dim)</span>
<span id="cb17-29"><a></a>        <span class="co"># Transformer</span></span>
<span id="cb17-30"><a></a>        <span class="va">self</span>.transformer <span class="op">=</span> TransformerEncoder(num_layers<span class="op">=</span><span class="va">self</span>.hparams.num_layers,</span>
<span id="cb17-31"><a></a>                                              input_dim<span class="op">=</span><span class="va">self</span>.hparams.model_dim,</span>
<span id="cb17-32"><a></a>                                              dim_feedforward<span class="op">=</span><span class="dv">2</span><span class="op">*</span><span class="va">self</span>.hparams.model_dim,</span>
<span id="cb17-33"><a></a>                                              num_heads<span class="op">=</span><span class="va">self</span>.hparams.num_heads,</span>
<span id="cb17-34"><a></a>                                              dropout<span class="op">=</span><span class="va">self</span>.hparams.dropout)</span>
<span id="cb17-35"><a></a>        <span class="co"># Output classifier per sequence lement</span></span>
<span id="cb17-36"><a></a>        <span class="va">self</span>.output_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb17-37"><a></a>            nn.Linear(<span class="va">self</span>.hparams.model_dim, <span class="va">self</span>.hparams.model_dim),</span>
<span id="cb17-38"><a></a>            nn.LayerNorm(<span class="va">self</span>.hparams.model_dim),</span>
<span id="cb17-39"><a></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-40"><a></a>            nn.Dropout(<span class="va">self</span>.hparams.dropout),</span>
<span id="cb17-41"><a></a>            nn.Linear(<span class="va">self</span>.hparams.model_dim, <span class="va">self</span>.hparams.num_classes)</span>
<span id="cb17-42"><a></a>        ) </span>
<span id="cb17-43"><a></a></span>
<span id="cb17-44"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>, add_positional_encoding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb17-45"><a></a>        <span class="co">"""</span></span>
<span id="cb17-46"><a></a><span class="co">        Inputs:</span></span>
<span id="cb17-47"><a></a><span class="co">            x - Input features of shape [Batch, SeqLen, input_dim]</span></span>
<span id="cb17-48"><a></a><span class="co">            mask - Mask to apply on the attention outputs (optional)</span></span>
<span id="cb17-49"><a></a><span class="co">            add_positional_encoding - If True, we add the positional encoding to the input.</span></span>
<span id="cb17-50"><a></a><span class="co">                                      Might not be desired for some tasks.</span></span>
<span id="cb17-51"><a></a><span class="co">        """</span></span>
<span id="cb17-52"><a></a>        x <span class="op">=</span> <span class="va">self</span>.input_net(x)</span>
<span id="cb17-53"><a></a>        <span class="cf">if</span> add_positional_encoding:</span>
<span id="cb17-54"><a></a>            x <span class="op">=</span> <span class="va">self</span>.positional_encoding(x)</span>
<span id="cb17-55"><a></a>        x <span class="op">=</span> <span class="va">self</span>.transformer(x, mask<span class="op">=</span>mask)</span>
<span id="cb17-56"><a></a>        x <span class="op">=</span> <span class="va">self</span>.output_net(x)</span>
<span id="cb17-57"><a></a>        <span class="cf">return</span> x</span>
<span id="cb17-58"><a></a></span>
<span id="cb17-59"><a></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb17-60"><a></a>    <span class="kw">def</span> get_attention_maps(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>, add_positional_encoding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb17-61"><a></a>        <span class="co">"""</span></span>
<span id="cb17-62"><a></a><span class="co">        Function for extracting the attention matrices of the whole Transformer for a single batch.</span></span>
<span id="cb17-63"><a></a><span class="co">        Input arguments same as the forward pass.</span></span>
<span id="cb17-64"><a></a><span class="co">        """</span></span>
<span id="cb17-65"><a></a>        x <span class="op">=</span> <span class="va">self</span>.input_net(x)</span>
<span id="cb17-66"><a></a>        <span class="cf">if</span> add_positional_encoding:</span>
<span id="cb17-67"><a></a>            x <span class="op">=</span> <span class="va">self</span>.positional_encoding(x)</span>
<span id="cb17-68"><a></a>        attention_maps <span class="op">=</span> <span class="va">self</span>.transformer.get_attention_maps(x, mask<span class="op">=</span>mask)</span>
<span id="cb17-69"><a></a>        <span class="cf">return</span> attention_maps</span>
<span id="cb17-70"><a></a></span>
<span id="cb17-71"><a></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>):</span>
<span id="cb17-72"><a></a>        optimizer <span class="op">=</span> optim.Adam(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="va">self</span>.hparams.lr)</span>
<span id="cb17-73"><a></a>        </span>
<span id="cb17-74"><a></a>        <span class="co"># Apply lr scheduler per step</span></span>
<span id="cb17-75"><a></a>        lr_scheduler <span class="op">=</span> CosineWarmupScheduler(optimizer, </span>
<span id="cb17-76"><a></a>                                             warmup<span class="op">=</span><span class="va">self</span>.hparams.warmup, </span>
<span id="cb17-77"><a></a>                                             max_iters<span class="op">=</span><span class="va">self</span>.hparams.max_iters)</span>
<span id="cb17-78"><a></a>        <span class="cf">return</span> [optimizer], [{<span class="st">'scheduler'</span>: lr_scheduler, <span class="st">'interval'</span>: <span class="st">'step'</span>}]</span>
<span id="cb17-79"><a></a></span>
<span id="cb17-80"><a></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb17-81"><a></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb17-82"><a></a></span>
<span id="cb17-83"><a></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb17-84"><a></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>    </span>
<span id="cb17-85"><a></a></span>
<span id="cb17-86"><a></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb17-87"><a></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section>
<section id="applications" class="title-slide slide level1 center">
<h1>Applications</h1>

</section>
<section id="sequence-to-sequence" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>A Sequence-to-Sequence task represents a task where the input <em>and</em> the output is a sequence, not necessarily of the same length. Popular tasks in this domain include:</p>
<ul>
<li>machine translation</li>
<li>summarization</li>
</ul>
<p>For this, we usually have a Transformer encoder for <strong>interpreting</strong> the input sequence, and a decoder for <strong>generating</strong> the output in an autoregressive manner.</p>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-1" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Task definition</strong></p>
</div>
<div class="callout-content">
<p>Here, however, we will go back to a much simpler example task and use only the encoder.</p>
<p>Given a sequence of <span class="math inline">\(N\)</span> numbers between <span class="math inline">\(0\)</span> and <span class="math inline">\(M\)</span>, the task is to reverse the input sequence. In Numpy notation, if our input is <span class="math inline">\(x\)</span>, the output should be <span class="math inline">\(x\)</span>[::-1].</p>
<p>Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well.</p>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-2" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset</strong></p>
</div>
<div class="callout-content">
<div id="784c8a23" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a><span class="kw">class</span> ReverseDataset(data.Dataset):</span>
<span id="cb18-2"><a></a></span>
<span id="cb18-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_categories, seq_len, size):</span>
<span id="cb18-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a></a>        <span class="va">self</span>.num_categories <span class="op">=</span> num_categories</span>
<span id="cb18-6"><a></a>        <span class="va">self</span>.seq_len <span class="op">=</span> seq_len</span>
<span id="cb18-7"><a></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb18-8"><a></a>        </span>
<span id="cb18-9"><a></a>        <span class="va">self</span>.data <span class="op">=</span> torch.randint(<span class="va">self</span>.num_categories, size<span class="op">=</span>(<span class="va">self</span>.size, <span class="va">self</span>.seq_len))</span>
<span id="cb18-10"><a></a>  </span>
<span id="cb18-11"><a></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-12"><a></a>        <span class="cf">return</span> <span class="va">self</span>.size</span>
<span id="cb18-13"><a></a></span>
<span id="cb18-14"><a></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb18-15"><a></a>        inp_data <span class="op">=</span> <span class="va">self</span>.data[idx]</span>
<span id="cb18-16"><a></a>        labels <span class="op">=</span> torch.flip(inp_data, dims<span class="op">=</span>(<span class="dv">0</span>,))</span>
<span id="cb18-17"><a></a>        <span class="cf">return</span> inp_data, labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-3" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Data loaders</strong></p>
</div>
<div class="callout-content">
<div id="26eb50d1" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>dataset <span class="op">=</span> partial(ReverseDataset, <span class="dv">10</span>, <span class="dv">16</span>)</span>
<span id="cb19-2"><a></a>train_loader <span class="op">=</span> data.DataLoader(dataset(<span class="dv">50000</span>), batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-3"><a></a>val_loader   <span class="op">=</span> data.DataLoader(dataset(<span class="dv">1000</span>), batch_size<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb19-4"><a></a>test_loader  <span class="op">=</span> data.DataLoader(dataset(<span class="dv">10000</span>), batch_size<span class="op">=</span><span class="dv">128</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Data examples</strong></p>
</div>
<div class="callout-content">
<div id="dd9ed4ff" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a>inp_data, labels <span class="op">=</span> train_loader.dataset[<span class="dv">0</span>]</span>
<span id="cb20-2"><a></a><span class="bu">print</span>(<span class="st">"Input data:"</span>, inp_data)</span>
<span id="cb20-3"><a></a><span class="bu">print</span>(<span class="st">"Labels:    "</span>, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input data: tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9])
Labels:     tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-4" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training: embeddings</strong></p>
</div>
<div class="callout-content">
<ul>
<li>During training, we pass the input sequence through the Transformer encoder and predict the output for each input token.</li>
<li>We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector.</li>
<li>An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module nn.Embedding.</li>
<li>However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (self.input_net maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).</li>
</ul>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-5" class="slide level2">
<h2>Sequence to Sequence</h2>
<div id="90009ddb" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a><span class="kw">class</span> ReversePredictor(TransformerPredictor):</span>
<span id="cb22-2"><a></a>    </span>
<span id="cb22-3"><a></a>    <span class="kw">def</span> _calculate_loss(<span class="va">self</span>, batch, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb22-4"><a></a>        <span class="co"># Fetch data and transform categories to one-hot vectors</span></span>
<span id="cb22-5"><a></a>        inp_data, labels <span class="op">=</span> batch</span>
<span id="cb22-6"><a></a>        inp_data <span class="op">=</span> F.one_hot(inp_data, num_classes<span class="op">=</span><span class="va">self</span>.hparams.num_classes).<span class="bu">float</span>()</span>
<span id="cb22-7"><a></a>        </span>
<span id="cb22-8"><a></a>        <span class="co"># Perform prediction and calculate loss and accuracy</span></span>
<span id="cb22-9"><a></a>        preds <span class="op">=</span> <span class="va">self</span>.forward(inp_data, add_positional_encoding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-10"><a></a>        loss <span class="op">=</span> F.cross_entropy(preds.view(<span class="op">-</span><span class="dv">1</span>,preds.size(<span class="op">-</span><span class="dv">1</span>)), labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb22-11"><a></a>        acc <span class="op">=</span> (preds.argmax(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">==</span> labels).<span class="bu">float</span>().mean()</span>
<span id="cb22-12"><a></a>        </span>
<span id="cb22-13"><a></a>        <span class="co"># Logging</span></span>
<span id="cb22-14"><a></a>        <span class="va">self</span>.log(<span class="ss">f"</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_loss"</span>, loss)</span>
<span id="cb22-15"><a></a>        <span class="va">self</span>.log(<span class="ss">f"</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_acc"</span>, acc)</span>
<span id="cb22-16"><a></a>        <span class="cf">return</span> loss, acc</span>
<span id="cb22-17"><a></a>        </span>
<span id="cb22-18"><a></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb22-19"><a></a>        loss, _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb22-20"><a></a>        <span class="cf">return</span> loss</span>
<span id="cb22-21"><a></a>    </span>
<span id="cb22-22"><a></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb22-23"><a></a>        _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"val"</span>)</span>
<span id="cb22-24"><a></a>    </span>
<span id="cb22-25"><a></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb22-26"><a></a>        _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sequence-to-sequence-6" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training: gradient clipping</strong></p>
</div>
<div class="callout-content">
<ul>
<li><code>gradient_clip_val</code>: this clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like <a href="https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping">DeepAI glossary</a>).</li>
<li>For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward.</li>
<li>In plain PyTorch, you can apply gradient clipping via <code>torch.nn.utils.clip_grad_norm_(...)</code> (see <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_">documentation</a>).</li>
<li>The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-7" class="slide level2">
<h2>Sequence to Sequence</h2>
<div id="dd32dde6" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a><span class="kw">def</span> train_reverse(<span class="op">**</span>kwargs):</span>
<span id="cb23-2"><a></a>    <span class="co"># Create a PyTorch Lightning trainer with the generation callback</span></span>
<span id="cb23-3"><a></a>    root_dir <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"ReverseTask"</span>)</span>
<span id="cb23-4"><a></a>    os.makedirs(root_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-5"><a></a>    trainer <span class="op">=</span> pl.Trainer(default_root_dir<span class="op">=</span>root_dir, </span>
<span id="cb23-6"><a></a>                         callbacks<span class="op">=</span>[ModelCheckpoint(save_weights_only<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">"max"</span>, monitor<span class="op">=</span><span class="st">"val_acc"</span>)],</span>
<span id="cb23-7"><a></a>                         accelerator<span class="op">=</span><span class="st">"gpu"</span> <span class="cf">if</span> <span class="bu">str</span>(device).startswith(<span class="st">"cuda"</span>) <span class="cf">else</span> <span class="st">"cpu"</span>,</span>
<span id="cb23-8"><a></a>                         devices<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb23-9"><a></a>                         max_epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb23-10"><a></a>                         gradient_clip_val<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb23-11"><a></a>    trainer.logger._default_hp_metric <span class="op">=</span> <span class="va">None</span> <span class="co"># Optional logging argument that we don't need</span></span>
<span id="cb23-12"><a></a>    </span>
<span id="cb23-13"><a></a>    <span class="co"># Check whether pretrained model exists. If yes, load it and skip training</span></span>
<span id="cb23-14"><a></a>    pretrained_filename <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"ReverseTask.ckpt"</span>)</span>
<span id="cb23-15"><a></a>    <span class="cf">if</span> os.path.isfile(pretrained_filename):</span>
<span id="cb23-16"><a></a>        <span class="bu">print</span>(<span class="st">"Found pretrained model, loading..."</span>)</span>
<span id="cb23-17"><a></a>        model <span class="op">=</span> ReversePredictor.load_from_checkpoint(pretrained_filename)</span>
<span id="cb23-18"><a></a>    <span class="cf">else</span>:</span>
<span id="cb23-19"><a></a>        model <span class="op">=</span> ReversePredictor(max_iters<span class="op">=</span>trainer.max_epochs<span class="op">*</span><span class="bu">len</span>(train_loader), <span class="op">**</span>kwargs)</span>
<span id="cb23-20"><a></a>        trainer.fit(model, train_loader, val_loader)</span>
<span id="cb23-21"><a></a>        </span>
<span id="cb23-22"><a></a>    <span class="co"># Test best model on validation and test set</span></span>
<span id="cb23-23"><a></a>    val_result <span class="op">=</span> trainer.test(model, val_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-24"><a></a>    test_result <span class="op">=</span> trainer.test(model, test_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-25"><a></a>    result <span class="op">=</span> {<span class="st">"test_acc"</span>: test_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>], <span class="st">"val_acc"</span>: val_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>]}</span>
<span id="cb23-26"><a></a>    </span>
<span id="cb23-27"><a></a>    model <span class="op">=</span> model.to(device)</span>
<span id="cb23-28"><a></a>    <span class="cf">return</span> model, result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sequence-to-sequence-8" class="slide level2">
<h2>Sequence to Sequence</h2>
<div id="1a64b307" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>reverse_model, reverse_result <span class="op">=</span> train_reverse(input_dim<span class="op">=</span>train_loader.dataset.num_categories,</span>
<span id="cb24-2"><a></a>                                              model_dim<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb24-3"><a></a>                                              num_heads<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-4"><a></a>                                              num_classes<span class="op">=</span>train_loader.dataset.num_categories,</span>
<span id="cb24-5"><a></a>                                              num_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-6"><a></a>                                              dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb24-7"><a></a>                                              lr<span class="op">=</span><span class="fl">5e-4</span>,</span>
<span id="cb24-8"><a></a>                                              warmup<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found pretrained model, loading...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5ef2684a564d4ab6ae9b1e8a6e877a41","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c405c02b2b8e453fb3cd22113fd6e19c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="13c8e506" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a><span class="bu">print</span>(<span class="ss">f"Val accuracy:  </span><span class="sc">{</span>(<span class="fl">100.0</span> <span class="op">*</span> reverse_result[<span class="st">'val_acc'</span>])<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span>
<span id="cb26-2"><a></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>(<span class="fl">100.0</span> <span class="op">*</span> reverse_result[<span class="st">'test_acc'</span>])<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Val accuracy:  100.00%
Test accuracy: 100.00%</code></pre>
</div>
</div>
</section>
<section id="sequence-to-sequence-9" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualizing attention</strong></p>
</div>
<div class="callout-content">
<div id="f1fa5305" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>data_input, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb28-2"><a></a>inp_data <span class="op">=</span> F.one_hot(data_input, num_classes<span class="op">=</span>reverse_model.hparams.num_classes).<span class="bu">float</span>()</span>
<span id="cb28-3"><a></a>inp_data <span class="op">=</span> inp_data.to(device)</span>
<span id="cb28-4"><a></a>attention_maps <span class="op">=</span> reverse_model.get_attention_maps(inp_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="969eb833" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a>attention_maps[<span class="dv">0</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="173">
<pre><code>torch.Size([128, 1, 16, 16])</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sequence-to-sequence-10" class="slide level2">
<h2>Sequence to Sequence</h2>
<div id="c11f8a66" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a><span class="kw">def</span> plot_attention_maps(input_data, attn_maps, idx<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb31-2"><a></a>    <span class="cf">if</span> input_data <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-3"><a></a>        input_data <span class="op">=</span> input_data[idx].detach().cpu().numpy()</span>
<span id="cb31-4"><a></a>    <span class="cf">else</span>:</span>
<span id="cb31-5"><a></a>        input_data <span class="op">=</span> np.arange(attn_maps[<span class="dv">0</span>][idx].shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb31-6"><a></a>    attn_maps <span class="op">=</span> [m[idx].detach().cpu().numpy() <span class="cf">for</span> m <span class="kw">in</span> attn_maps]</span>
<span id="cb31-7"><a></a>    </span>
<span id="cb31-8"><a></a>    num_heads <span class="op">=</span> attn_maps[<span class="dv">0</span>].shape[<span class="dv">0</span>]</span>
<span id="cb31-9"><a></a>    num_layers <span class="op">=</span> <span class="bu">len</span>(attn_maps)</span>
<span id="cb31-10"><a></a>    seq_len <span class="op">=</span> input_data.shape[<span class="dv">0</span>]</span>
<span id="cb31-11"><a></a>    fig_size <span class="op">=</span> <span class="dv">4</span> <span class="cf">if</span> num_heads <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">3</span></span>
<span id="cb31-12"><a></a>    fig, ax <span class="op">=</span> plt.subplots(num_layers, num_heads, figsize<span class="op">=</span>(num_heads<span class="op">*</span>fig_size, num_layers<span class="op">*</span>fig_size))</span>
<span id="cb31-13"><a></a>    <span class="cf">if</span> num_layers <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb31-14"><a></a>        ax <span class="op">=</span> [ax]</span>
<span id="cb31-15"><a></a>    <span class="cf">if</span> num_heads <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb31-16"><a></a>        ax <span class="op">=</span> [[a] <span class="cf">for</span> a <span class="kw">in</span> ax]</span>
<span id="cb31-17"><a></a>    <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(num_layers):</span>
<span id="cb31-18"><a></a>        <span class="cf">for</span> column <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb31-19"><a></a>            ax[row][column].imshow(attn_maps[row][column], origin<span class="op">=</span><span class="st">'lower'</span>, vmin<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb31-20"><a></a>            ax[row][column].set_xticks(<span class="bu">list</span>(<span class="bu">range</span>(seq_len)))</span>
<span id="cb31-21"><a></a>            ax[row][column].set_xticklabels(input_data.tolist())</span>
<span id="cb31-22"><a></a>            ax[row][column].set_yticks(<span class="bu">list</span>(<span class="bu">range</span>(seq_len)))</span>
<span id="cb31-23"><a></a>            ax[row][column].set_yticklabels(input_data.tolist())</span>
<span id="cb31-24"><a></a>            ax[row][column].set_title(<span class="ss">f"Layer </span><span class="sc">{</span>row<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Head </span><span class="sc">{</span>column<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-25"><a></a>    fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb31-26"><a></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sequence-to-sequence-11" class="slide level2">
<h2>Sequence to Sequence</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualization</strong></p>
</div>
<div class="callout-content">
<div id="3fe205d7" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>plot_attention_maps(data_input, attention_maps, idx<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-29-output-1.svg"></p>
</figure>
</div>
</div>
</div>
<p><strong>Result</strong>: the model has learned to attend to the token that is on the flipped index of itself.</p>
</div>
</div>
</div>
</section>
<section id="set-anomaly-detection" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Issues with order</strong></p>
</div>
<div class="callout-content">
<ul>
<li>RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data.</li>
<li><a href="https://arxiv.org/abs/1511.06391">Vinyals et al.&nbsp;(2015)</a> and other papers have shown that the assumed order can have a <strong>significant impact</strong> on the model’s performance, and hence, we should try to not use RNNs on sets.</li>
<li>Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p><strong>Set Anomaly Detection</strong>: we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where <span class="math inline">\(N-1\)</span> images belong to the same category/have the same high-level features while one belongs to another category.</p>
</div>
</div>
</div>
</section>
<section id="set-anomaly-detection-1" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dataset notes</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>We will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.</p></li>
<li><p>Instead of raw images, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images.</p></li>
</ul>
</div>
</div>
</div>
<p><img data-src="img/cifar100_example_anomaly.png"> <!-- The ResNet model has been pre-trained on the [ImageNet](http://image-net.org/) dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model.--></p>
</section>
<section id="set-anomaly-detection-2" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="e4d7185f" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a><span class="co"># ImageNet statistics</span></span>
<span id="cb33-2"><a></a>DATA_MEANS <span class="op">=</span> np.array([<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>])</span>
<span id="cb33-3"><a></a>DATA_STD <span class="op">=</span> np.array([<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb33-4"><a></a><span class="co"># As torch tensors for later preprocessing</span></span>
<span id="cb33-5"><a></a>TORCH_DATA_MEANS <span class="op">=</span> torch.from_numpy(DATA_MEANS).view(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb33-6"><a></a>TORCH_DATA_STD <span class="op">=</span> torch.from_numpy(DATA_STD).view(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb33-7"><a></a></span>
<span id="cb33-8"><a></a><span class="co"># Resize to 224x224, and normalize to ImageNet statistic</span></span>
<span id="cb33-9"><a></a>transform <span class="op">=</span> transforms.Compose([transforms.Resize((<span class="dv">224</span>,<span class="dv">224</span>)),</span>
<span id="cb33-10"><a></a>                                transforms.ToTensor(),</span>
<span id="cb33-11"><a></a>                                transforms.Normalize(DATA_MEANS, DATA_STD)</span>
<span id="cb33-12"><a></a>                                ])</span>
<span id="cb33-13"><a></a><span class="co"># Loading the training dataset. </span></span>
<span id="cb33-14"><a></a>train_set <span class="op">=</span> CIFAR100(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-15"><a></a></span>
<span id="cb33-16"><a></a><span class="co"># Loading the test set</span></span>
<span id="cb33-17"><a></a>test_set <span class="op">=</span> CIFAR100(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-3" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="74568e43" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a><span class="im">import</span> os</span>
<span id="cb34-2"><a></a>os.environ[<span class="st">"TORCH_HOME"</span>] <span class="op">=</span> CHECKPOINT_PATH</span>
<span id="cb34-3"><a></a>pretrained_model <span class="op">=</span> torchvision.models.resnet34(weights<span class="op">=</span><span class="st">'IMAGENET1K_V1'</span>)</span>
<span id="cb34-4"><a></a><span class="co"># Remove classification layer</span></span>
<span id="cb34-5"><a></a><span class="co"># In some models, it is called "fc", others have "classifier"</span></span>
<span id="cb34-6"><a></a><span class="co"># Setting both to an empty sequential represents an identity map of the final features.</span></span>
<span id="cb34-7"><a></a>pretrained_model.fc <span class="op">=</span> nn.Sequential()</span>
<span id="cb34-8"><a></a>pretrained_model.classifier <span class="op">=</span> nn.Sequential()</span>
<span id="cb34-9"><a></a><span class="co"># To GPU</span></span>
<span id="cb34-10"><a></a>pretrained_model <span class="op">=</span> pretrained_model.to(device)</span>
<span id="cb34-11"><a></a></span>
<span id="cb34-12"><a></a><span class="co"># Only eval, no gradient required</span></span>
<span id="cb34-13"><a></a>pretrained_model.<span class="bu">eval</span>()</span>
<span id="cb34-14"><a></a><span class="cf">for</span> p <span class="kw">in</span> pretrained_model.parameters():</span>
<span id="cb34-15"><a></a>    p.requires_grad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-4" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="386936cd" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb35-2"><a></a><span class="kw">def</span> extract_features(dataset, save_file):</span>
<span id="cb35-3"><a></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(save_file):</span>
<span id="cb35-4"><a></a>        data_loader <span class="op">=</span> data.DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-5"><a></a>        extracted_features <span class="op">=</span> []</span>
<span id="cb35-6"><a></a>        <span class="cf">for</span> imgs, _ <span class="kw">in</span> tqdm(data_loader):</span>
<span id="cb35-7"><a></a>            imgs <span class="op">=</span> imgs.to(device)</span>
<span id="cb35-8"><a></a>            feats <span class="op">=</span> pretrained_model(imgs)</span>
<span id="cb35-9"><a></a>            extracted_features.append(feats)</span>
<span id="cb35-10"><a></a>        extracted_features <span class="op">=</span> torch.cat(extracted_features, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-11"><a></a>        extracted_features <span class="op">=</span> extracted_features.detach().cpu()</span>
<span id="cb35-12"><a></a>        torch.save(extracted_features, save_file)</span>
<span id="cb35-13"><a></a>    <span class="cf">else</span>:</span>
<span id="cb35-14"><a></a>        extracted_features <span class="op">=</span> torch.load(save_file)</span>
<span id="cb35-15"><a></a>    <span class="cf">return</span> extracted_features</span>
<span id="cb35-16"><a></a></span>
<span id="cb35-17"><a></a>train_feat_file <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"train_set_features.tar"</span>)</span>
<span id="cb35-18"><a></a>train_set_feats <span class="op">=</span> extract_features(train_set, train_feat_file)</span>
<span id="cb35-19"><a></a></span>
<span id="cb35-20"><a></a>test_feat_file <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"test_set_features.tar"</span>)</span>
<span id="cb35-21"><a></a>test_feats <span class="op">=</span> extract_features(test_set, test_feat_file)</span>
<span id="cb35-22"><a></a></span>
<span id="cb35-23"><a></a><span class="bu">print</span>(<span class="st">"Train:"</span>, train_set_feats.shape)</span>
<span id="cb35-24"><a></a><span class="bu">print</span>(<span class="st">"Test: "</span>, test_feats.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train: torch.Size([50000, 512])
Test:  torch.Size([10000, 512])</code></pre>
</div>
</div>
</section>
<section id="set-anomaly-detection-5" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Validation set</strong></p>
</div>
<div class="callout-content">
<div id="4973406d" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a><span class="co">## Split train into train+val</span></span>
<span id="cb37-2"><a></a><span class="co"># Get labels from train set</span></span>
<span id="cb37-3"><a></a>labels <span class="op">=</span> train_set.targets</span>
<span id="cb37-4"><a></a></span>
<span id="cb37-5"><a></a><span class="co"># Get indices of images per class</span></span>
<span id="cb37-6"><a></a>labels <span class="op">=</span> torch.LongTensor(labels)</span>
<span id="cb37-7"><a></a>num_labels <span class="op">=</span> labels.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb37-8"><a></a>sorted_indices <span class="op">=</span> torch.argsort(labels).reshape(num_labels, <span class="op">-</span><span class="dv">1</span>) <span class="co"># [classes, num_imgs per class]</span></span>
<span id="cb37-9"><a></a></span>
<span id="cb37-10"><a></a><span class="co"># Determine number of validation images per class</span></span>
<span id="cb37-11"><a></a>num_val_exmps <span class="op">=</span> sorted_indices.shape[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">10</span></span>
<span id="cb37-12"><a></a></span>
<span id="cb37-13"><a></a><span class="co"># Get image indices for validation and training</span></span>
<span id="cb37-14"><a></a>val_indices   <span class="op">=</span> sorted_indices[:,:num_val_exmps].reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-15"><a></a>train_indices <span class="op">=</span> sorted_indices[:,num_val_exmps:].reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-16"><a></a></span>
<span id="cb37-17"><a></a><span class="co"># Group corresponding image features and labels</span></span>
<span id="cb37-18"><a></a>train_feats, train_labels <span class="op">=</span> train_set_feats[train_indices], labels[train_indices]</span>
<span id="cb37-19"><a></a>val_feats,   val_labels   <span class="op">=</span> train_set_feats[val_indices],   labels[val_indices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="set-anomaly-detection-6" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="5237a350" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="kw">class</span> SetAnomalyDataset(data.Dataset):</span>
<span id="cb38-2"><a></a>    </span>
<span id="cb38-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_feats, labels, set_size<span class="op">=</span><span class="dv">10</span>, train<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb38-4"><a></a>        <span class="co">"""</span></span>
<span id="cb38-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb38-6"><a></a><span class="co">            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.</span></span>
<span id="cb38-7"><a></a><span class="co">            labels - Tensor of shape [num_imgs], containing the class labels for the images</span></span>
<span id="cb38-8"><a></a><span class="co">            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.</span></span>
<span id="cb38-9"><a></a><span class="co">            train - If True, a new set will be sampled every time __getitem__ is called.</span></span>
<span id="cb38-10"><a></a><span class="co">        """</span></span>
<span id="cb38-11"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb38-12"><a></a>        <span class="va">self</span>.img_feats <span class="op">=</span> img_feats</span>
<span id="cb38-13"><a></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb38-14"><a></a>        <span class="va">self</span>.set_size <span class="op">=</span> set_size<span class="op">-</span><span class="dv">1</span> <span class="co"># The set size is here the size of correct images</span></span>
<span id="cb38-15"><a></a>        <span class="va">self</span>.train <span class="op">=</span> train</span>
<span id="cb38-16"><a></a>        </span>
<span id="cb38-17"><a></a>        <span class="co"># Tensors with indices of the images per class</span></span>
<span id="cb38-18"><a></a>        <span class="va">self</span>.num_labels <span class="op">=</span> labels.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb38-19"><a></a>        <span class="va">self</span>.img_idx_by_label <span class="op">=</span> torch.argsort(<span class="va">self</span>.labels).reshape(<span class="va">self</span>.num_labels, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb38-20"><a></a>        </span>
<span id="cb38-21"><a></a>        <span class="cf">if</span> <span class="kw">not</span> train:</span>
<span id="cb38-22"><a></a>            <span class="va">self</span>.test_sets <span class="op">=</span> <span class="va">self</span>._create_test_sets()</span>
<span id="cb38-23"><a></a>            </span>
<span id="cb38-24"><a></a>            </span>
<span id="cb38-25"><a></a>    <span class="kw">def</span> _create_test_sets(<span class="va">self</span>):</span>
<span id="cb38-26"><a></a>        <span class="co"># Pre-generates the sets for each image for the test set</span></span>
<span id="cb38-27"><a></a>        test_sets <span class="op">=</span> []</span>
<span id="cb38-28"><a></a>        num_imgs <span class="op">=</span> <span class="va">self</span>.img_feats.shape[<span class="dv">0</span>]</span>
<span id="cb38-29"><a></a>        np.random.seed(<span class="dv">42</span>)</span>
<span id="cb38-30"><a></a>        test_sets <span class="op">=</span> [<span class="va">self</span>.sample_img_set(<span class="va">self</span>.labels[idx]) <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(num_imgs)]</span>
<span id="cb38-31"><a></a>        test_sets <span class="op">=</span> torch.stack(test_sets, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-32"><a></a>        <span class="cf">return</span> test_sets</span>
<span id="cb38-33"><a></a>            </span>
<span id="cb38-34"><a></a>        </span>
<span id="cb38-35"><a></a>    <span class="kw">def</span> sample_img_set(<span class="va">self</span>, anomaly_label):</span>
<span id="cb38-36"><a></a>        <span class="co">"""</span></span>
<span id="cb38-37"><a></a><span class="co">        Samples a new set of images, given the label of the anomaly. </span></span>
<span id="cb38-38"><a></a><span class="co">        The sampled images come from a different class than anomaly_label</span></span>
<span id="cb38-39"><a></a><span class="co">        """</span></span>
<span id="cb38-40"><a></a>        <span class="co"># Sample class from 0,...,num_classes-1 while skipping anomaly_label as class</span></span>
<span id="cb38-41"><a></a>        set_label <span class="op">=</span> np.random.randint(<span class="va">self</span>.num_labels<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb38-42"><a></a>        <span class="cf">if</span> set_label <span class="op">&gt;=</span> anomaly_label:</span>
<span id="cb38-43"><a></a>            set_label <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-44"><a></a>            </span>
<span id="cb38-45"><a></a>        <span class="co"># Sample images from the class determined above</span></span>
<span id="cb38-46"><a></a>        img_indices <span class="op">=</span> np.random.choice(<span class="va">self</span>.img_idx_by_label.shape[<span class="dv">1</span>], size<span class="op">=</span><span class="va">self</span>.set_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb38-47"><a></a>        img_indices <span class="op">=</span> <span class="va">self</span>.img_idx_by_label[set_label, img_indices]</span>
<span id="cb38-48"><a></a>        <span class="cf">return</span> img_indices</span>
<span id="cb38-49"><a></a>        </span>
<span id="cb38-50"><a></a>        </span>
<span id="cb38-51"><a></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb38-52"><a></a>        <span class="cf">return</span> <span class="va">self</span>.img_feats.shape[<span class="dv">0</span>]</span>
<span id="cb38-53"><a></a>    </span>
<span id="cb38-54"><a></a>    </span>
<span id="cb38-55"><a></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb38-56"><a></a>        anomaly <span class="op">=</span> <span class="va">self</span>.img_feats[idx]</span>
<span id="cb38-57"><a></a>        <span class="cf">if</span> <span class="va">self</span>.train: <span class="co"># If train =&gt; sample</span></span>
<span id="cb38-58"><a></a>            img_indices <span class="op">=</span> <span class="va">self</span>.sample_img_set(<span class="va">self</span>.labels[idx])</span>
<span id="cb38-59"><a></a>        <span class="cf">else</span>: <span class="co"># If test =&gt; use pre-generated ones</span></span>
<span id="cb38-60"><a></a>            img_indices <span class="op">=</span> <span class="va">self</span>.test_sets[idx]</span>
<span id="cb38-61"><a></a>            </span>
<span id="cb38-62"><a></a>        <span class="co"># Concatenate images. The anomaly is always the last image for simplicity</span></span>
<span id="cb38-63"><a></a>        img_set <span class="op">=</span> torch.cat([<span class="va">self</span>.img_feats[img_indices], anomaly[<span class="va">None</span>]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-64"><a></a>        indices <span class="op">=</span> torch.cat([img_indices, torch.LongTensor([idx])], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-65"><a></a>        label <span class="op">=</span> img_set.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span></span>
<span id="cb38-66"><a></a>        </span>
<span id="cb38-67"><a></a>        <span class="co"># We return the indices of the images for visualization purpose. "Label" is the index of the anomaly</span></span>
<span id="cb38-68"><a></a>        <span class="cf">return</span> img_set, indices, label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-7" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="ee528598" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a>SET_SIZE <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb39-2"><a></a>test_labels <span class="op">=</span> torch.LongTensor(test_set.targets)</span>
<span id="cb39-3"><a></a></span>
<span id="cb39-4"><a></a>train_anom_dataset <span class="op">=</span> SetAnomalyDataset(train_feats, train_labels, set_size<span class="op">=</span>SET_SIZE, train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-5"><a></a>val_anom_dataset   <span class="op">=</span> SetAnomalyDataset(val_feats,   val_labels,   set_size<span class="op">=</span>SET_SIZE, train<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-6"><a></a>test_anom_dataset  <span class="op">=</span> SetAnomalyDataset(test_feats,  test_labels,  set_size<span class="op">=</span>SET_SIZE, train<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-7"><a></a></span>
<span id="cb39-8"><a></a>train_anom_loader <span class="op">=</span> data.DataLoader(train_anom_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>,  drop_last<span class="op">=</span><span class="va">True</span>,  num_workers<span class="op">=</span><span class="dv">0</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-9"><a></a>val_anom_loader   <span class="op">=</span> data.DataLoader(val_anom_dataset,   batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-10"><a></a>test_anom_loader  <span class="op">=</span> data.DataLoader(test_anom_dataset,  batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-8" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div class="panel-tabset">
<ul id="tabset-4" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-4-1">Code</a></li><li><a href="#tabset-4-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1">
<div id="504ba4c8" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a><span class="kw">def</span> visualize_exmp(indices, orig_dataset):</span>
<span id="cb40-2"><a></a>    images <span class="op">=</span> [orig_dataset[idx][<span class="dv">0</span>] <span class="cf">for</span> idx <span class="kw">in</span> indices.reshape(<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb40-3"><a></a>    images <span class="op">=</span> torch.stack(images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb40-4"><a></a>    images <span class="op">=</span> images <span class="op">*</span> TORCH_DATA_STD <span class="op">+</span> TORCH_DATA_MEANS</span>
<span id="cb40-5"><a></a>    </span>
<span id="cb40-6"><a></a>    img_grid <span class="op">=</span> torchvision.utils.make_grid(images, nrow<span class="op">=</span>SET_SIZE, normalize<span class="op">=</span><span class="va">True</span>, pad_value<span class="op">=</span><span class="fl">0.5</span>, padding<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb40-7"><a></a>    img_grid <span class="op">=</span> img_grid.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb40-8"><a></a></span>
<span id="cb40-9"><a></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">8</span>))</span>
<span id="cb40-10"><a></a>    plt.title(<span class="st">"Anomaly examples on CIFAR100"</span>)</span>
<span id="cb40-11"><a></a>    plt.imshow(img_grid)</span>
<span id="cb40-12"><a></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb40-13"><a></a>    plt.show()</span>
<span id="cb40-14"><a></a>    plt.close()</span>
<span id="cb40-15"><a></a></span>
<span id="cb40-16"><a></a>_, indices, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_anom_loader))</span>
<span id="cb40-17"><a></a>visualize_exmp(indices[:<span class="dv">4</span>], test_set)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-4-2">
<div id="253798dd" class="cell" data-execution_count="37">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-37-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="set-anomaly-detection-9" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="f670cfe6" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a></a><span class="kw">class</span> AnomalyPredictor(TransformerPredictor):</span>
<span id="cb41-2"><a></a>    </span>
<span id="cb41-3"><a></a>    <span class="kw">def</span> _calculate_loss(<span class="va">self</span>, batch, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb41-4"><a></a>        img_sets, _, labels <span class="op">=</span> batch</span>
<span id="cb41-5"><a></a>        preds <span class="op">=</span> <span class="va">self</span>.forward(img_sets, add_positional_encoding<span class="op">=</span><span class="va">False</span>) <span class="co"># No positional encodings as it is a set, not a sequence!</span></span>
<span id="cb41-6"><a></a>        preds <span class="op">=</span> preds.squeeze(dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># Shape: [Batch_size, set_size]</span></span>
<span id="cb41-7"><a></a>        loss <span class="op">=</span> F.cross_entropy(preds, labels) <span class="co"># Softmax/CE over set dimension</span></span>
<span id="cb41-8"><a></a>        acc <span class="op">=</span> (preds.argmax(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">==</span> labels).<span class="bu">float</span>().mean()</span>
<span id="cb41-9"><a></a>        <span class="va">self</span>.log(<span class="ss">f"</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_loss"</span>, loss)</span>
<span id="cb41-10"><a></a>        <span class="va">self</span>.log(<span class="ss">f"</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_acc"</span>, acc, on_step<span class="op">=</span><span class="va">False</span>, on_epoch<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-11"><a></a>        <span class="cf">return</span> loss, acc</span>
<span id="cb41-12"><a></a>        </span>
<span id="cb41-13"><a></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb41-14"><a></a>        loss, _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb41-15"><a></a>        <span class="cf">return</span> loss</span>
<span id="cb41-16"><a></a>    </span>
<span id="cb41-17"><a></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb41-18"><a></a>        _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"val"</span>)</span>
<span id="cb41-19"><a></a>    </span>
<span id="cb41-20"><a></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb41-21"><a></a>        _ <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-10" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="638b890f" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a><span class="kw">def</span> train_anomaly(<span class="op">**</span>kwargs):</span>
<span id="cb42-2"><a></a>    <span class="co"># Create a PyTorch Lightning trainer with the generation callback</span></span>
<span id="cb42-3"><a></a>    root_dir <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"SetAnomalyTask"</span>)</span>
<span id="cb42-4"><a></a>    os.makedirs(root_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-5"><a></a>    trainer <span class="op">=</span> pl.Trainer(default_root_dir<span class="op">=</span>root_dir, </span>
<span id="cb42-6"><a></a>                         callbacks<span class="op">=</span>[ModelCheckpoint(save_weights_only<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">"max"</span>, monitor<span class="op">=</span><span class="st">"val_acc"</span>)],</span>
<span id="cb42-7"><a></a>                         accelerator<span class="op">=</span><span class="st">"gpu"</span> <span class="cf">if</span> <span class="bu">str</span>(device).startswith(<span class="st">"cuda"</span>) <span class="cf">else</span> <span class="st">"cpu"</span>,</span>
<span id="cb42-8"><a></a>                         devices<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb42-9"><a></a>                         max_epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb42-10"><a></a>                         gradient_clip_val<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb42-11"><a></a>    trainer.logger._default_hp_metric <span class="op">=</span> <span class="va">None</span> <span class="co"># Optional logging argument that we don't need</span></span>
<span id="cb42-12"><a></a>    </span>
<span id="cb42-13"><a></a>    <span class="co"># Check whether pretrained model exists. If yes, load it and skip training</span></span>
<span id="cb42-14"><a></a>    pretrained_filename <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"SetAnomalyTask.ckpt"</span>)</span>
<span id="cb42-15"><a></a>    <span class="cf">if</span> os.path.isfile(pretrained_filename):</span>
<span id="cb42-16"><a></a>        <span class="bu">print</span>(<span class="st">"Found pretrained model, loading..."</span>)</span>
<span id="cb42-17"><a></a>        model <span class="op">=</span> AnomalyPredictor.load_from_checkpoint(pretrained_filename)</span>
<span id="cb42-18"><a></a>    <span class="cf">else</span>:</span>
<span id="cb42-19"><a></a>        model <span class="op">=</span> AnomalyPredictor(max_iters<span class="op">=</span>trainer.max_epochs<span class="op">*</span><span class="bu">len</span>(train_anom_loader), <span class="op">**</span>kwargs)</span>
<span id="cb42-20"><a></a>        trainer.fit(model, train_anom_loader, val_anom_loader)</span>
<span id="cb42-21"><a></a>        model <span class="op">=</span> AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)</span>
<span id="cb42-22"><a></a>    </span>
<span id="cb42-23"><a></a>    <span class="co"># Test best model on validation and test set</span></span>
<span id="cb42-24"><a></a>    train_result <span class="op">=</span> trainer.test(model, train_anom_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-25"><a></a>    val_result <span class="op">=</span> trainer.test(model, val_anom_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-26"><a></a>    test_result <span class="op">=</span> trainer.test(model, test_anom_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-27"><a></a>    result <span class="op">=</span> {<span class="st">"test_acc"</span>: test_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>], <span class="st">"val_acc"</span>: val_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>], <span class="st">"train_acc"</span>: train_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>]}</span>
<span id="cb42-28"><a></a>    </span>
<span id="cb42-29"><a></a>    model <span class="op">=</span> model.to(device)</span>
<span id="cb42-30"><a></a>    <span class="cf">return</span> model, result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="set-anomaly-detection-11" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="aa776395" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a></a>anomaly_model, anomaly_result <span class="op">=</span> train_anomaly(input_dim<span class="op">=</span>train_anom_dataset.img_feats.shape[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb43-2"><a></a>                                              model_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb43-3"><a></a>                                              num_heads<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb43-4"><a></a>                                              num_classes<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb43-5"><a></a>                                              num_layers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb43-6"><a></a>                                              dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb43-7"><a></a>                                              input_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb43-8"><a></a>                                              lr<span class="op">=</span><span class="fl">5e-4</span>,</span>
<span id="cb43-9"><a></a>                                              warmup<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found pretrained model, loading...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f224515cc90d42f1a88ae0e7049cc8a9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0cdddac2120d47809d93627bf7a3001d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"65ca8cec48274678b8b522f5c8103718","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="set-anomaly-detection-12" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="0e4cd363" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a></a><span class="bu">print</span>(<span class="ss">f"Train accuracy: </span><span class="sc">{</span>(<span class="fl">100.0</span><span class="op">*</span>anomaly_result[<span class="st">'train_acc'</span>])<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span>
<span id="cb45-2"><a></a><span class="bu">print</span>(<span class="ss">f"Val accuracy:   </span><span class="sc">{</span>(<span class="fl">100.0</span><span class="op">*</span>anomaly_result[<span class="st">'val_acc'</span>])<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span>
<span id="cb45-3"><a></a><span class="bu">print</span>(<span class="ss">f"Test accuracy:  </span><span class="sc">{</span>(<span class="fl">100.0</span><span class="op">*</span>anomaly_result[<span class="st">'test_acc'</span>])<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train accuracy: 96.27%
Val accuracy:   96.40%
Test accuracy:  94.34%</code></pre>
</div>
</div>
</section>
<section id="set-anomaly-detection-13" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="3fac072d" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a></a>inp_data, indices, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_anom_loader))</span>
<span id="cb47-2"><a></a>inp_data <span class="op">=</span> inp_data.to(device)</span>
<span id="cb47-3"><a></a>anomaly_model.<span class="bu">eval</span>()</span>
<span id="cb47-4"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-5"><a></a>    preds <span class="op">=</span> anomaly_model.forward(inp_data, add_positional_encoding<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb47-6"><a></a>    preds <span class="op">=</span> F.softmax(preds.squeeze(dim<span class="op">=-</span><span class="dv">1</span>), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb47-7"><a></a>    <span class="co"># Permut input data</span></span>
<span id="cb47-8"><a></a>    permut <span class="op">=</span> np.random.permutation(inp_data.shape[<span class="dv">1</span>])</span>
<span id="cb47-9"><a></a>    perm_inp_data <span class="op">=</span> inp_data[:,permut]</span>
<span id="cb47-10"><a></a>    perm_preds <span class="op">=</span> anomaly_model.forward(perm_inp_data, add_positional_encoding<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb47-11"><a></a>    perm_preds <span class="op">=</span> F.softmax(perm_preds.squeeze(dim<span class="op">=-</span><span class="dv">1</span>), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb47-12"><a></a><span class="cf">assert</span> (preds[:,permut] <span class="op">-</span> perm_preds).<span class="bu">abs</span>().<span class="bu">max</span>() <span class="op">&lt;</span> <span class="fl">1e-5</span>, <span class="st">"Predictions are not permutation equivariant"</span></span>
<span id="cb47-13"><a></a><span class="bu">print</span>(<span class="st">"Preds</span><span class="ch">\n</span><span class="st">"</span>, preds[<span class="dv">0</span>,permut].cpu().numpy())</span>
<span id="cb47-14"><a></a><span class="bu">print</span>(<span class="st">"Permuted preds</span><span class="ch">\n</span><span class="st">"</span>, perm_preds[<span class="dv">0</span>].cpu().numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Preds
 [9.9983788e-01 2.8534807e-05 1.6281172e-05 1.1398807e-05 1.9029028e-05
 1.4160971e-05 1.1995394e-05 2.8615961e-05 1.4995643e-05 1.7090044e-05]
Permuted preds
 [9.9983788e-01 2.8534861e-05 1.6281203e-05 1.1398840e-05 1.9029065e-05
 1.4161012e-05 1.1995416e-05 2.8616014e-05 1.4995686e-05 1.7090077e-05]</code></pre>
</div>
</div>
</section>
<section id="set-anomaly-detection-14" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div class="panel-tabset">
<ul id="tabset-5" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-5-1">Code</a></li><li><a href="#tabset-5-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1">
<div id="c09b313d" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a></a>attention_maps <span class="op">=</span> anomaly_model.get_attention_maps(inp_data, add_positional_encoding<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb49-2"><a></a>predictions <span class="op">=</span> preds.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb49-3"><a></a></span>
<span id="cb49-4"><a></a><span class="kw">def</span> visualize_prediction(idx):</span>
<span id="cb49-5"><a></a>    visualize_exmp(indices[idx:idx<span class="op">+</span><span class="dv">1</span>], test_set)</span>
<span id="cb49-6"><a></a>    <span class="bu">print</span>(<span class="st">"Prediction:"</span>, predictions[idx].item())</span>
<span id="cb49-7"><a></a>    plot_attention_maps(input_data<span class="op">=</span><span class="va">None</span>, attn_maps<span class="op">=</span>attention_maps, idx<span class="op">=</span>idx)</span>
<span id="cb49-8"><a></a></span>
<span id="cb49-9"><a></a>visualize_prediction(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-5-2">
<div id="ad9d6f3b" class="cell" data-execution_count="44">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-44-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction: 9</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-44-output-3.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="set-anomaly-detection-15" class="slide level2">
<h2>Set Anomaly Detection</h2>
<div id="48fd59ce" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a></a>mistakes <span class="op">=</span> torch.where(predictions <span class="op">!=</span> <span class="dv">9</span>)[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb51-2"><a></a><span class="bu">print</span>(<span class="st">"Indices with mistake:"</span>, mistakes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Indices with mistake: [11 30]</code></pre>
</div>
</div>
<div id="ea4b227c" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a>visualize_prediction(mistakes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb53-2"><a></a><span class="bu">print</span>(<span class="st">"Probabilities:"</span>)</span>
<span id="cb53-3"><a></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(preds[mistakes[<span class="op">-</span><span class="dv">1</span>]].cpu().numpy()):</span>
<span id="cb53-4"><a></a>    <span class="bu">print</span>(<span class="ss">f"Image </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="fl">100.0</span><span class="op">*</span>p<span class="sc">:4.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-46-output-1.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction: 3</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-46-output-3.svg"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Probabilities:
Image 0: 0.15%
Image 1: 0.09%
Image 2: 0.06%
Image 3: 93.35%
Image 4: 0.09%
Image 5: 0.95%
Image 6: 0.07%
Image 7: 0.04%
Image 8: 0.24%
Image 9: 4.97%</code></pre>
</div>
</div>
</section>
<section id="vision-transformers" class="slide level2">
<h2>Vision Transformers</h2>
<div id="7d3dd2bb" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a></a><span class="co">## Standard libraries</span></span>
<span id="cb56-2"><a></a><span class="im">import</span> os</span>
<span id="cb56-3"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-4"><a></a><span class="im">import</span> random</span>
<span id="cb56-5"><a></a><span class="im">import</span> math</span>
<span id="cb56-6"><a></a><span class="im">import</span> json</span>
<span id="cb56-7"><a></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb56-8"><a></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb56-9"><a></a></span>
<span id="cb56-10"><a></a><span class="co">## Imports for plotting</span></span>
<span id="cb56-11"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-12"><a></a>plt.set_cmap(<span class="st">'cividis'</span>)</span>
<span id="cb56-13"><a></a><span class="op">%</span>matplotlib inline</span>
<span id="cb56-14"><a></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb56-15"><a></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb56-16"><a></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> to_rgb</span>
<span id="cb56-17"><a></a><span class="im">import</span> matplotlib</span>
<span id="cb56-18"><a></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb56-19"><a></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-20"><a></a>sns.reset_orig()</span>
<span id="cb56-21"><a></a></span>
<span id="cb56-22"><a></a><span class="co">## tqdm for loading bars</span></span>
<span id="cb56-23"><a></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb56-24"><a></a></span>
<span id="cb56-25"><a></a><span class="co">## PyTorch</span></span>
<span id="cb56-26"><a></a><span class="im">import</span> torch</span>
<span id="cb56-27"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb56-28"><a></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb56-29"><a></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb56-30"><a></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb56-31"><a></a></span>
<span id="cb56-32"><a></a><span class="co">## Torchvision</span></span>
<span id="cb56-33"><a></a><span class="im">import</span> torchvision</span>
<span id="cb56-34"><a></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> CIFAR10</span>
<span id="cb56-35"><a></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb56-36"><a></a></span>
<span id="cb56-37"><a></a><span class="co"># PyTorch Lightning</span></span>
<span id="cb56-38"><a></a><span class="cf">try</span>:</span>
<span id="cb56-39"><a></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb56-40"><a></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary</span></span>
<span id="cb56-41"><a></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet pytorch<span class="op">-</span>lightning<span class="op">&gt;=</span><span class="fl">1.4</span></span>
<span id="cb56-42"><a></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb56-43"><a></a><span class="im">from</span> pytorch_lightning.callbacks <span class="im">import</span> LearningRateMonitor, ModelCheckpoint</span>
<span id="cb56-44"><a></a></span>
<span id="cb56-45"><a></a><span class="co"># Import tensorboard</span></span>
<span id="cb56-46"><a></a><span class="op">%</span>load_ext tensorboard</span>
<span id="cb56-47"><a></a></span>
<span id="cb56-48"><a></a><span class="co"># Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)</span></span>
<span id="cb56-49"><a></a>DATASET_PATH <span class="op">=</span> <span class="st">"../data"</span></span>
<span id="cb56-50"><a></a><span class="co"># Path to the folder where the pretrained models are saved</span></span>
<span id="cb56-51"><a></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../saved_models/tutorial15"</span></span>
<span id="cb56-52"><a></a></span>
<span id="cb56-53"><a></a><span class="co"># Setting the seed</span></span>
<span id="cb56-54"><a></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb56-55"><a></a></span>
<span id="cb56-56"><a></a><span class="co"># Ensure that all operations are deterministic on GPU (if used) for reproducibility</span></span>
<span id="cb56-57"><a></a>torch.backends.mps.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb56-58"><a></a>torch.backends.mps.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The tensorboard extension is already loaded. To reload it, use:
  %reload_ext tensorboard</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 960x480 with 0 Axes&gt;</code></pre>
</div>
</div>
</section>
<section id="vision-transformers-1" class="slide level2">
<h2>Vision Transformers</h2>
<div id="92d1df5a" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a></a><span class="im">import</span> urllib.request</span>
<span id="cb59-2"><a></a><span class="im">from</span> urllib.error <span class="im">import</span> HTTPError</span>
<span id="cb59-3"><a></a><span class="co"># Github URL where saved models are stored for this tutorial</span></span>
<span id="cb59-4"><a></a>base_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/phlippe/saved_models/main/"</span></span>
<span id="cb59-5"><a></a><span class="co"># Files to download</span></span>
<span id="cb59-6"><a></a>pretrained_files <span class="op">=</span> [<span class="st">"tutorial15/ViT.ckpt"</span>, <span class="st">"tutorial15/tensorboards/ViT/events.out.tfevents.ViT"</span>,</span>
<span id="cb59-7"><a></a>                    <span class="st">"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet"</span>]</span>
<span id="cb59-8"><a></a><span class="co"># Create checkpoint path if it doesn't exist yet</span></span>
<span id="cb59-9"><a></a>os.makedirs(CHECKPOINT_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-10"><a></a></span>
<span id="cb59-11"><a></a><span class="co"># For each file, check whether it already exists. If not, try downloading it.</span></span>
<span id="cb59-12"><a></a><span class="cf">for</span> file_name <span class="kw">in</span> pretrained_files:</span>
<span id="cb59-13"><a></a>    file_path <span class="op">=</span> os.path.join(CHECKPOINT_PATH, file_name.split(<span class="st">"/"</span>,<span class="dv">1</span>)[<span class="dv">1</span>])</span>
<span id="cb59-14"><a></a>    <span class="cf">if</span> <span class="st">"/"</span> <span class="kw">in</span> file_name.split(<span class="st">"/"</span>,<span class="dv">1</span>)[<span class="dv">1</span>]:</span>
<span id="cb59-15"><a></a>        os.makedirs(file_path.rsplit(<span class="st">"/"</span>,<span class="dv">1</span>)[<span class="dv">0</span>], exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-16"><a></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(file_path):</span>
<span id="cb59-17"><a></a>        file_url <span class="op">=</span> base_url <span class="op">+</span> file_name</span>
<span id="cb59-18"><a></a>        <span class="bu">print</span>(<span class="ss">f"Downloading </span><span class="sc">{</span>file_url<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb59-19"><a></a>        <span class="cf">try</span>:</span>
<span id="cb59-20"><a></a>            urllib.request.urlretrieve(file_url, file_path)</span>
<span id="cb59-21"><a></a>        <span class="cf">except</span> HTTPError <span class="im">as</span> e:</span>
<span id="cb59-22"><a></a>            <span class="bu">print</span>(<span class="st">"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:</span><span class="ch">\n</span><span class="st">"</span>, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vision-transformers-2" class="slide level2">
<h2>Vision Transformers</h2>
<div class="panel-tabset">
<ul id="tabset-6" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-6-1">Code</a></li><li><a href="#tabset-6-2">Result</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1">
<div id="3e0d6ad6" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a></a>test_transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(),</span>
<span id="cb60-2"><a></a>                                     transforms.Normalize([<span class="fl">0.49139968</span>, <span class="fl">0.48215841</span>, <span class="fl">0.44653091</span>], [<span class="fl">0.24703223</span>, <span class="fl">0.24348513</span>, <span class="fl">0.26158784</span>])</span>
<span id="cb60-3"><a></a>                                     ])</span>
<span id="cb60-4"><a></a><span class="co"># For training, we add some augmentation. Networks are too powerful and would overfit.</span></span>
<span id="cb60-5"><a></a>train_transform <span class="op">=</span> transforms.Compose([transforms.RandomHorizontalFlip(),</span>
<span id="cb60-6"><a></a>                                      transforms.RandomResizedCrop((<span class="dv">32</span>,<span class="dv">32</span>),scale<span class="op">=</span>(<span class="fl">0.8</span>,<span class="fl">1.0</span>),ratio<span class="op">=</span>(<span class="fl">0.9</span>,<span class="fl">1.1</span>)),</span>
<span id="cb60-7"><a></a>                                      transforms.ToTensor(),</span>
<span id="cb60-8"><a></a>                                      transforms.Normalize([<span class="fl">0.49139968</span>, <span class="fl">0.48215841</span>, <span class="fl">0.44653091</span>], [<span class="fl">0.24703223</span>, <span class="fl">0.24348513</span>, <span class="fl">0.26158784</span>])</span>
<span id="cb60-9"><a></a>                                     ])</span>
<span id="cb60-10"><a></a><span class="co"># Loading the training dataset. We need to split it into a training and validation part</span></span>
<span id="cb60-11"><a></a><span class="co"># We need to do a little trick because the validation set should not use the augmentation.</span></span>
<span id="cb60-12"><a></a>train_dataset <span class="op">=</span> CIFAR10(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>train_transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-13"><a></a>val_dataset <span class="op">=</span> CIFAR10(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>test_transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-14"><a></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb60-15"><a></a>train_set, _ <span class="op">=</span> torch.utils.data.random_split(train_dataset, [<span class="dv">45000</span>, <span class="dv">5000</span>])</span>
<span id="cb60-16"><a></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb60-17"><a></a>_, val_set <span class="op">=</span> torch.utils.data.random_split(val_dataset, [<span class="dv">45000</span>, <span class="dv">5000</span>])</span>
<span id="cb60-18"><a></a></span>
<span id="cb60-19"><a></a><span class="co"># Loading the test set</span></span>
<span id="cb60-20"><a></a>test_set <span class="op">=</span> CIFAR10(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>test_transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-21"><a></a></span>
<span id="cb60-22"><a></a><span class="co"># We define a set of data loaders that we can use for various purposes later.</span></span>
<span id="cb60-23"><a></a>train_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb60-24"><a></a>val_loader <span class="op">=</span> data.DataLoader(val_set, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb60-25"><a></a>test_loader <span class="op">=</span> data.DataLoader(test_set, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb60-26"><a></a></span>
<span id="cb60-27"><a></a><span class="co"># Visualize some examples</span></span>
<span id="cb60-28"><a></a>NUM_IMAGES <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb60-29"><a></a>CIFAR_images <span class="op">=</span> torch.stack([val_set[idx][<span class="dv">0</span>] <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(NUM_IMAGES)], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb60-30"><a></a>img_grid <span class="op">=</span> torchvision.utils.make_grid(CIFAR_images, nrow<span class="op">=</span><span class="dv">4</span>, normalize<span class="op">=</span><span class="va">True</span>, pad_value<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb60-31"><a></a>img_grid <span class="op">=</span> img_grid.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb60-32"><a></a></span>
<span id="cb60-33"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb60-34"><a></a>plt.title(<span class="st">"Image examples of the CIFAR10 dataset"</span>)</span>
<span id="cb60-35"><a></a>plt.imshow(img_grid)</span>
<span id="cb60-36"><a></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb60-37"><a></a>plt.show()</span>
<span id="cb60-38"><a></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-6-2">
<div id="bce14079" class="cell" data-execution_count="50">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-50-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="vision-transformers-3" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Transformers for image classification</strong></p>
</div>
<div class="callout-content">
<p>The Vision Transformer is a model for image classification that views images as sequences of smaller patches.</p>
<ul>
<li>As a preprocessing step, we split an image of, for example, <span class="math inline">\(48\times 48\)</span> pixels into 9 <span class="math inline">\(16\times 16\)</span> patches.</li>
<li>Each of those patches is considered to be a “word”/“token” and projected to a feature space.</li>
<li>With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="vision-transformers-4" class="slide level2">
<h2>Vision Transformers</h2>

<img data-src="img/vit.gif" class="r-stretch"></section>
<section id="vision-transformers-5" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Image preprocessing</strong></p>
</div>
<div class="callout-content">
<p>An image of size <span class="math inline">\(N\times N\)</span> has to be split into <span class="math inline">\((N/M)^2\)</span> patches of size <span class="math inline">\(M\times M\)</span>. These represent the input words to the Transformer.</p>
</div>
</div>
</div>
<div id="6039fc70" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a></a><span class="kw">def</span> img_to_patch(x, patch_size, flatten_channels<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb61-2"><a></a>    <span class="co">"""</span></span>
<span id="cb61-3"><a></a><span class="co">    Inputs:</span></span>
<span id="cb61-4"><a></a><span class="co">        x - torch.Tensor representing the image of shape [B, C, H, W]</span></span>
<span id="cb61-5"><a></a><span class="co">        patch_size - Number of pixels per dimension of the patches (integer)</span></span>
<span id="cb61-6"><a></a><span class="co">        flatten_channels - If True, the patches will be returned in a flattened format</span></span>
<span id="cb61-7"><a></a><span class="co">                           as a feature vector instead of a image grid.</span></span>
<span id="cb61-8"><a></a><span class="co">    """</span></span>
<span id="cb61-9"><a></a>    B, C, H, W <span class="op">=</span> x.shape</span>
<span id="cb61-10"><a></a>    x <span class="op">=</span> x.reshape(B, C, H<span class="op">//</span>patch_size, patch_size, W<span class="op">//</span>patch_size, patch_size)</span>
<span id="cb61-11"><a></a>    x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>) <span class="co"># [B, H', W', C, p_H, p_W]</span></span>
<span id="cb61-12"><a></a>    x <span class="op">=</span> x.flatten(<span class="dv">1</span>,<span class="dv">2</span>)              <span class="co"># [B, H'*W', C, p_H, p_W]</span></span>
<span id="cb61-13"><a></a>    <span class="cf">if</span> flatten_channels:</span>
<span id="cb61-14"><a></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>,<span class="dv">4</span>)          <span class="co"># [B, H'*W', C*p_H*p_W]</span></span>
<span id="cb61-15"><a></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vision-transformers-6" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Visualization</strong></p>
</div>
<div class="callout-content">
<div id="2d8a14ba" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a></a>img_patches <span class="op">=</span> img_to_patch(CIFAR_images, patch_size<span class="op">=</span><span class="dv">4</span>, flatten_channels<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb62-2"><a></a></span>
<span id="cb62-3"><a></a>fig, ax <span class="op">=</span> plt.subplots(CIFAR_images.shape[<span class="dv">0</span>], <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">3</span>))</span>
<span id="cb62-4"><a></a>fig.suptitle(<span class="st">"Images as input sequences of patches"</span>)</span>
<span id="cb62-5"><a></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(CIFAR_images.shape[<span class="dv">0</span>]):</span>
<span id="cb62-6"><a></a>    img_grid <span class="op">=</span> torchvision.utils.make_grid(img_patches[i], nrow<span class="op">=</span><span class="dv">64</span>, normalize<span class="op">=</span><span class="va">True</span>, pad_value<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb62-7"><a></a>    img_grid <span class="op">=</span> img_grid.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb62-8"><a></a>    ax[i].imshow(img_grid)</span>
<span id="cb62-9"><a></a>    ax[i].axis(<span class="st">'off'</span>)</span>
<span id="cb62-10"><a></a>plt.show()</span>
<span id="cb62-11"><a></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="dl_lec11_files/figure-revealjs/cell-52-output-1.svg"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="vision-transformers-7" class="slide level2">
<h2>Vision Transformers</h2>

<img data-src="img/pre_layer_norm.svg" class="r-stretch quarto-figure-center"><p class="caption">Pre-Layer Normalization: the idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage.</p></section>
<section id="vision-transformers-8" class="slide level2">
<h2>Vision Transformers</h2>
<div id="ce5e634d" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a></a><span class="kw">class</span> AttentionBlock(nn.Module):</span>
<span id="cb63-2"><a></a>    </span>
<span id="cb63-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, hidden_dim, num_heads, dropout<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb63-4"><a></a>        <span class="co">"""</span></span>
<span id="cb63-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb63-6"><a></a><span class="co">            embed_dim - Dimensionality of input and attention feature vectors</span></span>
<span id="cb63-7"><a></a><span class="co">            hidden_dim - Dimensionality of hidden layer in feed-forward network </span></span>
<span id="cb63-8"><a></a><span class="co">                         (usually 2-4x larger than embed_dim)</span></span>
<span id="cb63-9"><a></a><span class="co">            num_heads - Number of heads to use in the Multi-Head Attention block</span></span>
<span id="cb63-10"><a></a><span class="co">            dropout - Amount of dropout to apply in the feed-forward network</span></span>
<span id="cb63-11"><a></a><span class="co">        """</span></span>
<span id="cb63-12"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-13"><a></a>        </span>
<span id="cb63-14"><a></a>        <span class="va">self</span>.layer_norm_1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb63-15"><a></a>        <span class="va">self</span>.attn <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads, </span>
<span id="cb63-16"><a></a>                                          dropout<span class="op">=</span>dropout)</span>
<span id="cb63-17"><a></a>        <span class="va">self</span>.layer_norm_2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb63-18"><a></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Sequential(</span>
<span id="cb63-19"><a></a>            nn.Linear(embed_dim, hidden_dim),</span>
<span id="cb63-20"><a></a>            nn.GELU(),</span>
<span id="cb63-21"><a></a>            nn.Dropout(dropout),</span>
<span id="cb63-22"><a></a>            nn.Linear(hidden_dim, embed_dim),</span>
<span id="cb63-23"><a></a>            nn.Dropout(dropout)</span>
<span id="cb63-24"><a></a>        )</span>
<span id="cb63-25"><a></a>        </span>
<span id="cb63-26"><a></a>        </span>
<span id="cb63-27"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-28"><a></a>        inp_x <span class="op">=</span> <span class="va">self</span>.layer_norm_1(x)</span>
<span id="cb63-29"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(inp_x, inp_x, inp_x)[<span class="dv">0</span>]</span>
<span id="cb63-30"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.linear(<span class="va">self</span>.layer_norm_2(x))</span>
<span id="cb63-31"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vision-transformers-9" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Modules</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Encoder</strong></li>
<li>A <strong>linear projection</strong> layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each <span class="math inline">\(M\times M\)</span> patch independently as input.</li>
<li>A <strong>classification token</strong> that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.</li>
<li>Learnable <strong>positional encodings</strong> that are added to the tokens before being processed by the Transformer. We can learn them instead of having the pattern of sine and cosine functions.</li>
<li>An <strong>MLP head</strong> that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="vision-transformers-10" class="slide level2">
<h2>Vision Transformers</h2>
<div id="058196e8" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb64-2"><a></a>    </span>
<span id="cb64-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb64-4"><a></a>        <span class="co">"""</span></span>
<span id="cb64-5"><a></a><span class="co">        Inputs:</span></span>
<span id="cb64-6"><a></a><span class="co">            embed_dim - Dimensionality of the input feature vectors to the Transformer</span></span>
<span id="cb64-7"><a></a><span class="co">            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks</span></span>
<span id="cb64-8"><a></a><span class="co">                         within the Transformer</span></span>
<span id="cb64-9"><a></a><span class="co">            num_channels - Number of channels of the input (3 for RGB)</span></span>
<span id="cb64-10"><a></a><span class="co">            num_heads - Number of heads to use in the Multi-Head Attention block</span></span>
<span id="cb64-11"><a></a><span class="co">            num_layers - Number of layers to use in the Transformer</span></span>
<span id="cb64-12"><a></a><span class="co">            num_classes - Number of classes to predict</span></span>
<span id="cb64-13"><a></a><span class="co">            patch_size - Number of pixels that the patches have per dimension</span></span>
<span id="cb64-14"><a></a><span class="co">            num_patches - Maximum number of patches an image can have</span></span>
<span id="cb64-15"><a></a><span class="co">            dropout - Amount of dropout to apply in the feed-forward network and </span></span>
<span id="cb64-16"><a></a><span class="co">                      on the input encoding</span></span>
<span id="cb64-17"><a></a><span class="co">        """</span></span>
<span id="cb64-18"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-19"><a></a>        </span>
<span id="cb64-20"><a></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb64-21"><a></a>        </span>
<span id="cb64-22"><a></a>        <span class="co"># Layers/Networks</span></span>
<span id="cb64-23"><a></a>        <span class="va">self</span>.input_layer <span class="op">=</span> nn.Linear(num_channels<span class="op">*</span>(patch_size<span class="op">**</span><span class="dv">2</span>), embed_dim)</span>
<span id="cb64-24"><a></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.Sequential(<span class="op">*</span>[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout<span class="op">=</span>dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb64-25"><a></a>        <span class="va">self</span>.mlp_head <span class="op">=</span> nn.Sequential(</span>
<span id="cb64-26"><a></a>            nn.LayerNorm(embed_dim),</span>
<span id="cb64-27"><a></a>            nn.Linear(embed_dim, num_classes)</span>
<span id="cb64-28"><a></a>        )</span>
<span id="cb64-29"><a></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb64-30"><a></a>        </span>
<span id="cb64-31"><a></a>        <span class="co"># Parameters/Embeddings</span></span>
<span id="cb64-32"><a></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>,<span class="dv">1</span>,embed_dim))</span>
<span id="cb64-33"><a></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>,<span class="dv">1</span><span class="op">+</span>num_patches,embed_dim))</span>
<span id="cb64-34"><a></a>    </span>
<span id="cb64-35"><a></a>    </span>
<span id="cb64-36"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-37"><a></a>        <span class="co"># Preprocess input</span></span>
<span id="cb64-38"><a></a>        x <span class="op">=</span> img_to_patch(x, <span class="va">self</span>.patch_size)</span>
<span id="cb64-39"><a></a>        B, T, _ <span class="op">=</span> x.shape</span>
<span id="cb64-40"><a></a>        x <span class="op">=</span> <span class="va">self</span>.input_layer(x)</span>
<span id="cb64-41"><a></a>        </span>
<span id="cb64-42"><a></a>        <span class="co"># Add CLS token and positional encoding</span></span>
<span id="cb64-43"><a></a>        cls_token <span class="op">=</span> <span class="va">self</span>.cls_token.repeat(B, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb64-44"><a></a>        x <span class="op">=</span> torch.cat([cls_token, x], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-45"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embedding[:,:T<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb64-46"><a></a>        </span>
<span id="cb64-47"><a></a>        <span class="co"># Apply Transforrmer</span></span>
<span id="cb64-48"><a></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb64-49"><a></a>        x <span class="op">=</span> x.transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb64-50"><a></a>        x <span class="op">=</span> <span class="va">self</span>.transformer(x)</span>
<span id="cb64-51"><a></a>        </span>
<span id="cb64-52"><a></a>        <span class="co"># Perform classification prediction</span></span>
<span id="cb64-53"><a></a>        cls <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb64-54"><a></a>        out <span class="op">=</span> <span class="va">self</span>.mlp_head(cls)</span>
<span id="cb64-55"><a></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vision-transformers-11" class="slide level2">
<h2>Vision Transformers</h2>
<div id="787ccf08" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a></a><span class="kw">class</span> ViT(pl.LightningModule):</span>
<span id="cb65-2"><a></a>    </span>
<span id="cb65-3"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_kwargs, lr):</span>
<span id="cb65-4"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-5"><a></a>        <span class="va">self</span>.save_hyperparameters()</span>
<span id="cb65-6"><a></a>        <span class="va">self</span>.model <span class="op">=</span> VisionTransformer(<span class="op">**</span>model_kwargs)</span>
<span id="cb65-7"><a></a>        <span class="va">self</span>.example_input_array <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))[<span class="dv">0</span>]</span>
<span id="cb65-8"><a></a>        </span>
<span id="cb65-9"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-10"><a></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb65-11"><a></a>    </span>
<span id="cb65-12"><a></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>):</span>
<span id="cb65-13"><a></a>        optimizer <span class="op">=</span> optim.AdamW(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="va">self</span>.hparams.lr)</span>
<span id="cb65-14"><a></a>        lr_scheduler <span class="op">=</span> optim.lr_scheduler.MultiStepLR(optimizer, milestones<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">150</span>], gamma<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb65-15"><a></a>        <span class="cf">return</span> [optimizer], [lr_scheduler]   </span>
<span id="cb65-16"><a></a>    </span>
<span id="cb65-17"><a></a>    <span class="kw">def</span> _calculate_loss(<span class="va">self</span>, batch, mode<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb65-18"><a></a>        imgs, labels <span class="op">=</span> batch</span>
<span id="cb65-19"><a></a>        preds <span class="op">=</span> <span class="va">self</span>.model(imgs)</span>
<span id="cb65-20"><a></a>        loss <span class="op">=</span> F.cross_entropy(preds, labels)</span>
<span id="cb65-21"><a></a>        acc <span class="op">=</span> (preds.argmax(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">==</span> labels).<span class="bu">float</span>().mean()</span>
<span id="cb65-22"><a></a>        </span>
<span id="cb65-23"><a></a>        <span class="va">self</span>.log(<span class="ss">f'</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_loss'</span>, loss)</span>
<span id="cb65-24"><a></a>        <span class="va">self</span>.log(<span class="ss">f'</span><span class="sc">{</span>mode<span class="sc">}</span><span class="ss">_acc'</span>, acc)</span>
<span id="cb65-25"><a></a>        <span class="cf">return</span> loss</span>
<span id="cb65-26"><a></a></span>
<span id="cb65-27"><a></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb65-28"><a></a>        loss <span class="op">=</span> <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb65-29"><a></a>        <span class="cf">return</span> loss</span>
<span id="cb65-30"><a></a></span>
<span id="cb65-31"><a></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb65-32"><a></a>        <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"val"</span>)</span>
<span id="cb65-33"><a></a></span>
<span id="cb65-34"><a></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb65-35"><a></a>        <span class="va">self</span>._calculate_loss(batch, mode<span class="op">=</span><span class="st">"test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vision-transformers-12" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<div id="3d8a24cd" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a></a><span class="kw">def</span> train_model(<span class="op">**</span>kwargs):</span>
<span id="cb66-2"><a></a>    trainer <span class="op">=</span> pl.Trainer(default_root_dir<span class="op">=</span>os.path.join(CHECKPOINT_PATH, <span class="st">"ViT"</span>), </span>
<span id="cb66-3"><a></a>                         accelerator<span class="op">=</span><span class="st">"gpu"</span> <span class="cf">if</span> <span class="bu">str</span>(device).startswith(<span class="st">"cuda"</span>) <span class="cf">else</span> <span class="st">"cpu"</span>,</span>
<span id="cb66-4"><a></a>                         devices<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb66-5"><a></a>                         max_epochs<span class="op">=</span><span class="dv">180</span>,</span>
<span id="cb66-6"><a></a>                         callbacks<span class="op">=</span>[ModelCheckpoint(save_weights_only<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">"max"</span>, monitor<span class="op">=</span><span class="st">"val_acc"</span>),</span>
<span id="cb66-7"><a></a>                                    LearningRateMonitor(<span class="st">"epoch"</span>)])</span>
<span id="cb66-8"><a></a>    trainer.logger._log_graph <span class="op">=</span> <span class="va">True</span>         <span class="co"># If True, we plot the computation graph in tensorboard</span></span>
<span id="cb66-9"><a></a>    trainer.logger._default_hp_metric <span class="op">=</span> <span class="va">None</span> <span class="co"># Optional logging argument that we don't need</span></span>
<span id="cb66-10"><a></a></span>
<span id="cb66-11"><a></a>    <span class="co"># Check whether pretrained model exists. If yes, load it and skip training</span></span>
<span id="cb66-12"><a></a>    pretrained_filename <span class="op">=</span> os.path.join(CHECKPOINT_PATH, <span class="st">"ViT.ckpt"</span>)</span>
<span id="cb66-13"><a></a>    <span class="cf">if</span> os.path.isfile(pretrained_filename):</span>
<span id="cb66-14"><a></a>        <span class="bu">print</span>(<span class="ss">f"Found pretrained model at </span><span class="sc">{</span>pretrained_filename<span class="sc">}</span><span class="ss">, loading..."</span>)</span>
<span id="cb66-15"><a></a>        model <span class="op">=</span> ViT.load_from_checkpoint(pretrained_filename) <span class="co"># Automatically loads the model with the saved hyperparameters</span></span>
<span id="cb66-16"><a></a>    <span class="cf">else</span>:</span>
<span id="cb66-17"><a></a>        pl.seed_everything(<span class="dv">42</span>) <span class="co"># To be reproducable</span></span>
<span id="cb66-18"><a></a>        model <span class="op">=</span> ViT(<span class="op">**</span>kwargs)</span>
<span id="cb66-19"><a></a>        trainer.fit(model, train_loader, val_loader)</span>
<span id="cb66-20"><a></a>        model <span class="op">=</span> ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) <span class="co"># Load best checkpoint after training</span></span>
<span id="cb66-21"><a></a></span>
<span id="cb66-22"><a></a>    <span class="co"># Test best model on validation and test set</span></span>
<span id="cb66-23"><a></a>    val_result <span class="op">=</span> trainer.test(model, val_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb66-24"><a></a>    test_result <span class="op">=</span> trainer.test(model, test_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb66-25"><a></a>    result <span class="op">=</span> {<span class="st">"test"</span>: test_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>], <span class="st">"val"</span>: val_result[<span class="dv">0</span>][<span class="st">"test_acc"</span>]}</span>
<span id="cb66-26"><a></a></span>
<span id="cb66-27"><a></a>    <span class="cf">return</span> model, result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="vision-transformers-13" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Considerations</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><strong>patch size</strong>: the smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to its quadratic memory usage in the attention layer.</p></li>
<li><p>the <strong>embedding</strong> and <strong>hidden dimensionality</strong> have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of <code>embed_dim/num_heads</code>.</p></li>
<li><p>the <strong>learning rate</strong> for Transformers is usually relatively small, and in papers, a common value to use is 3e-5.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="vision-transformers-14" class="slide level2">
<h2>Vision Transformers</h2>
<div id="5709a921" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a></a>model, results <span class="op">=</span> train_model(model_kwargs<span class="op">=</span>{</span>
<span id="cb67-2"><a></a>                                <span class="st">'embed_dim'</span>: <span class="dv">256</span>,</span>
<span id="cb67-3"><a></a>                                <span class="st">'hidden_dim'</span>: <span class="dv">512</span>,</span>
<span id="cb67-4"><a></a>                                <span class="st">'num_heads'</span>: <span class="dv">8</span>,</span>
<span id="cb67-5"><a></a>                                <span class="st">'num_layers'</span>: <span class="dv">6</span>,</span>
<span id="cb67-6"><a></a>                                <span class="st">'patch_size'</span>: <span class="dv">4</span>,</span>
<span id="cb67-7"><a></a>                                <span class="st">'num_channels'</span>: <span class="dv">3</span>,</span>
<span id="cb67-8"><a></a>                                <span class="st">'num_patches'</span>: <span class="dv">64</span>,</span>
<span id="cb67-9"><a></a>                                <span class="st">'num_classes'</span>: <span class="dv">10</span>,</span>
<span id="cb67-10"><a></a>                                <span class="st">'dropout'</span>: <span class="fl">0.2</span></span>
<span id="cb67-11"><a></a>                            },</span>
<span id="cb67-12"><a></a>                            lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb67-13"><a></a><span class="bu">print</span>(<span class="st">"ViT results"</span>, results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found pretrained model at ../saved_models/tutorial15/ViT.ckpt, loading...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"09e178ad366843f9afd462d70c2c1ffa","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"14ba4caa7d8a444490fe166d93a0828a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ViT results {'test': 0.7713000178337097, 'val': 0.7781999707221985}</code></pre>
</div>
</div>
</section>
<section id="vision-transformers-15" class="slide level2">
<h2>Vision Transformers</h2>

<img data-src="img/tensorboard_screenshot_vit.png" class="r-stretch"></section>
<section id="vision-transformers-16" class="slide level2">
<h2>Vision Transformers</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p><strong>Inductive biases</strong>:</p>
<ul>
<li>CNNs have been designed with the assumption that images are translation invariant. Hence, we apply convolutions with shared filters across the image.</li>
<li>a CNN architecture integrates the concept of distance in an image: two pixels that are close to each other are more related than two distant pixels. Local patterns are combined into larger patterns until we perform our classification prediction.</li>
</ul>
</div>
</div>
</div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":"95761eb3fe859a695fac299f81c28bc2","id":"15f642a77454b78d504c20e6d9a59446d424f89bc89eb611a80d3d30cd045894"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="application/vnd.jupyter.widget-state+json">
    {"state":{"018ee0a5ba1f4664a15c66b7e28e67fb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01b0e143be6b4ca1896fd7c07899a390":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9b84d25fe3f548a8ae92eb4c9d0232ca","placeholder":"​","style":"IPY_MODEL_6f9d46edddfc4004b95b7e0dd41006c6","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"023bc36955994e5db74d44033a4c3d1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0d6df428614844e5accc9cdc03afb382","placeholder":"​","style":"IPY_MODEL_0f48dda67d0e4dbb8ce0b3aa594e301b","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 450.37it/s]"}},"041c9164178c44a7b9bd6c58a8fcd5c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_87cc25a5354a4248ae7e9eaeec34011b","placeholder":"​","style":"IPY_MODEL_b9696720254143259b2e4b6bbb281488","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"0429eb01255042b5b8c5c681df5edb74":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"049c2e676b2b408199660d1ed8f8224d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"068a035a355d484ca98814a0ed358563":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06ef1e644a1a46a7be9b0e0b76568995":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8b138b011f5d41d59b2896a98f9c715e","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fab94e55f9504c709fc31f58cc26d974","tabbable":null,"tooltip":null,"value":8}},"075b0a72a8fc4548b0a8e67378650bca":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07ab0fc2c96441e8ac80b60312470331":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"07b03412abcf45b1be6d33e245c22463":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09e178ad366843f9afd462d70c2c1ffa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f441259e1be4b098065e764db5b7a28","IPY_MODEL_2f7b5074323f4736b155987c1dc1b0dc","IPY_MODEL_368590b6c0d240fbb0656050486043b2"],"layout":"IPY_MODEL_9600a121b4c048e087aab0ec32c506ac","tabbable":null,"tooltip":null}},"0cdddac2120d47809d93627bf7a3001d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01b0e143be6b4ca1896fd7c07899a390","IPY_MODEL_57f33b54eaf545c1b69727ab43dc6ccc","IPY_MODEL_54003c453f24483994aa1ecbdf42ef6f"],"layout":"IPY_MODEL_cf8f46da5c8e4e8b9f33676110afafb2","tabbable":null,"tooltip":null}},"0ce340ffe4e846d8acb21cc23f6d077c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e53d706a742a47c59cb6c59d3c6cbd49","placeholder":"​","style":"IPY_MODEL_3cd8adc33b8e466282a6ab5bb91cd999","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"0d3eef2db4f4477fa3c173644e341f87":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d6df428614844e5accc9cdc03afb382":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e1d2fb5324e4156b6a8f93519618218":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0f722074e58643139e26d8597b4aba88","placeholder":"​","style":"IPY_MODEL_67402d39c36f42e5bbec250bc5c6833a","tabbable":null,"tooltip":null,"value":" 157/157 [00:01&lt;00:00, 108.80it/s]"}},"0f48dda67d0e4dbb8ce0b3aa594e301b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0f722074e58643139e26d8597b4aba88":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"107c78314c3040318bfcdb2206443e1e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1176520bd5b7468385cd5812a435a703":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c01ad232bc498c9634db596707c52c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9d628571aa647328237a19d022ac26d","IPY_MODEL_72fb74597733494f86c852767c0563bc","IPY_MODEL_4849dd441f094e6ab98c25963422293d"],"layout":"IPY_MODEL_b5337e824d0d4b4fba50aa469a76c02b","tabbable":null,"tooltip":null}},"13d3458c157c4e1795b3b3e23b01a993":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14ba4caa7d8a444490fe166d93a0828a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8dffc77092394eed8a39f254399d6207","IPY_MODEL_68658112254b40dbb8d951b63f9cabb1","IPY_MODEL_4fd575bd3052441d9d5e06192b940744"],"layout":"IPY_MODEL_d749310c211e49d9930a8304d9aeb928","tabbable":null,"tooltip":null}},"16f5e3976da84e9f9f53c0787cdbc0ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1873e4741cb44edc8ca177cd53d1d536":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"1877adad0237432b8921f6b6f2c5dc34":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c190471dc46d42fc8128933c1507d6dd","IPY_MODEL_1cf2dd884a274dcf9c5f7ac9577b25ef","IPY_MODEL_ede895a0189f4273af502776c6a59319"],"layout":"IPY_MODEL_e759badae5f84de294d71190dc30bccd","tabbable":null,"tooltip":null}},"187d7489c7ce4c8db5d6da465d4fa23d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1942b5011e344a78a977c39e93e7688a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194bd131446f4ea59daf15b2f2e7eb5f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"199551fb720843569d7cece81dd9aace":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_40e51303bfea4a6c9f70b69ebd2ff010","placeholder":"​","style":"IPY_MODEL_07ab0fc2c96441e8ac80b60312470331","tabbable":null,"tooltip":null,"value":" 79/79 [00:13&lt;00:00,  5.93it/s]"}},"1b3c95c97bc2459aacd57d0d8481d33b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_416ea5d241a749c8856a400f25816599","placeholder":"​","style":"IPY_MODEL_fd1299daa81942c1ad847bb853884d1b","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"1c5b309787524c35b66c808aa8916672":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8b2e3663dc6b4d6ab4be2e997fa8e886","placeholder":"​","style":"IPY_MODEL_ae3930f4f9a740dd8d975ace189e1ddd","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"1cb42f360466497783e3810e3975db53":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e8d1e3670cbc43a2a3c2d6be2c3594e8","placeholder":"​","style":"IPY_MODEL_3070bc063f4b4c088cd1ad8f8a1b6f70","tabbable":null,"tooltip":null,"value":" 8/8 [00:00&lt;00:00, 368.37it/s]"}},"1cf2dd884a274dcf9c5f7ac9577b25ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_79bc0d3a9c3649f9b1ea8875ce61a574","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80130edfe1f6403998f8e30dc2af1415","tabbable":null,"tooltip":null,"value":79}},"1e98bdb290924b11930cc4ec15684dc3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4c0b7d9ae247bc87cb0f9d98c78743":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"229bc20fdc6049b7abb9671a74bd7e85":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d8414d98f6c5428fa2d4f7520e3628fb","placeholder":"​","style":"IPY_MODEL_e4d785751bdd418a8cc4ee9319655631","tabbable":null,"tooltip":null,"value":" 157/157 [00:01&lt;00:00, 114.29it/s]"}},"23a1d9bde4b747f4b34af4cd88a4f015":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"25245fb8280e437e9a9dd34fd58acec2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"26cbcd89698f4a9f99fddab3a5b88c56":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"274662aa97fd4deb9ea2020ded142717":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"278f676a010843e5867e34055b23efd3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"2b7084a2f40f4856b8d9e94fc6fbbe88":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"2bc0a4999af14e9b91bef7635f1e8245":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c8e55e9e642d47f7890b61c406e32f3e","placeholder":"​","style":"IPY_MODEL_25245fb8280e437e9a9dd34fd58acec2","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"2c8b57b574f64069ba78be21e29a1acf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"2f6d030911274325beae64b7ecef2bf5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f7b5074323f4736b155987c1dc1b0dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_9ce52310e7934b6eb57df401811297c5","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ebcaa7d60944a28ae95d27b06e1234f","tabbable":null,"tooltip":null,"value":40}},"3070bc063f4b4c088cd1ad8f8a1b6f70":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3182059b2e3a4815af5be5468a2999ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"328f194e95694708a2a33a1318013b56":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54b9655b455e4ba98d7dab272e2e07b2","IPY_MODEL_43725ec419c340b796b35123b6ae2353","IPY_MODEL_199551fb720843569d7cece81dd9aace"],"layout":"IPY_MODEL_fa9cf0e3e0dd4c029df500e8a4f90bae","tabbable":null,"tooltip":null}},"3298fc3f6b3046eea7bdb48c5c5e72f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_dc1d471600164693baba77b91d6a28b7","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c567c33972da4342beb6c6a225b4f90b","tabbable":null,"tooltip":null,"value":40}},"337ddfc2d5b042d59b0c9fa983e03bb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33aa54fb7bb0463bacc321916389971d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33d20ca1824d43edb825baf31962616e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_9aaeb45ac2344927ae8565e5e5c3ccc1","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16f5e3976da84e9f9f53c0787cdbc0ef","tabbable":null,"tooltip":null,"value":79}},"3460bdde314b41d3a5041ebc4f32b461":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_075b0a72a8fc4548b0a8e67378650bca","placeholder":"​","style":"IPY_MODEL_187d7489c7ce4c8db5d6da465d4fa23d","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"3519f6ec5a4a4490bfdc77b2f8a6f136":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71835fe9fea34c2d902a27c78637ea51","IPY_MODEL_ee3e3a492b1f453493387d3bc24bb6f3","IPY_MODEL_54f39c4fd10a4a159b5c3c188559a7fe"],"layout":"IPY_MODEL_66795ca4fd5a4c6792bc3f2cf81b47b2","tabbable":null,"tooltip":null}},"360a4398dd0b4335a8520c0a0e10d9dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"366af1eae4804f23920460f465a2608a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368590b6c0d240fbb0656050486043b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a1ef4e722da74769b1f80b55e8ee3f76","placeholder":"​","style":"IPY_MODEL_3182059b2e3a4815af5be5468a2999ec","tabbable":null,"tooltip":null,"value":" 40/40 [00:06&lt;00:00,  6.17it/s]"}},"38202e7bd8de45989572437ff348b1e5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"383a3ed3679347b9929504d2d5ba88f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3e1e8638d74e41fa8e060688933c3092","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43588800d5ae46aaafa32275e3e06095","tabbable":null,"tooltip":null,"value":79}},"3a296b69f9094690bde0904554de172b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_d008d9ee0e9f4248bfcafead43061adf","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33aa54fb7bb0463bacc321916389971d","tabbable":null,"tooltip":null,"value":79}},"3b3dff8ace4c47bda355351e008efd87":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b39b5bd0b161481fba7ab3f268c798f7","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9dcf5665342349dea5635a30046146cd","tabbable":null,"tooltip":null,"value":157}},"3cd8adc33b8e466282a6ab5bb91cd999":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3cede17f6e754a17bd7d7235b87faba4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3da4721ce5664e7d81e003c3be0c2a3e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3dd52f972e4443c9a94ee280f6863c5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3e1e8638d74e41fa8e060688933c3092":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec01bb357cd4991b815d6c123454741":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3eee37e6baed4d498c1270b794cb50a6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3ff2234622d54a989a978577402dcbce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40b9fe7bed1e4953acc77084c8f9f7d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d077fc3f4fd34399af50a30eae77440e","IPY_MODEL_dab4975a67d74f429436e89c39db463d","IPY_MODEL_7064821f7c2a49aba7fdd1314a62da57"],"layout":"IPY_MODEL_049c2e676b2b408199660d1ed8f8224d","tabbable":null,"tooltip":null}},"40e51303bfea4a6c9f70b69ebd2ff010":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"416ea5d241a749c8856a400f25816599":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426b0d3a5f9b44ef8938839868b3e4a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1942b5011e344a78a977c39e93e7688a","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6e4718f88b643a393980cab0b9b9915","tabbable":null,"tooltip":null,"value":8}},"4291fd75e0514117bba7a319115a2647":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43588800d5ae46aaafa32275e3e06095":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43725ec419c340b796b35123b6ae2353":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_865fde05f68544149765b5b6e3cb8de8","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b272b8097cf4b25accda3ebb5248cd7","tabbable":null,"tooltip":null,"value":79}},"45fa117ce0fd4224b5abf5d741375106":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4849dd441f094e6ab98c25963422293d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9f81af01ed174d21b9426f9da2728d38","placeholder":"​","style":"IPY_MODEL_c49d26ee978948ea9eb834d1b8986db5","tabbable":null,"tooltip":null,"value":" 703/703 [00:06&lt;00:00, 101.85it/s]"}},"487048581b3f46fa808212d49eff7f95":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8de95e4b697845f9a84d19b141bf5f03","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ecebbcb66f3048b3817d6228dc06c181","tabbable":null,"tooltip":null,"value":79}},"48b5aa7ae186486f977abc995266456d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a83a4002d55476e98f661671f4f93ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_041c9164178c44a7b9bd6c58a8fcd5c0","IPY_MODEL_3a296b69f9094690bde0904554de172b","IPY_MODEL_80227244b4dd43bf888fa74e3bf8af2d"],"layout":"IPY_MODEL_3eee37e6baed4d498c1270b794cb50a6","tabbable":null,"tooltip":null}},"4b272b8097cf4b25accda3ebb5248cd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e130b2cd078494e93089a854ff489cd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"4e92d5711c934b48b0c79ec689fbbc57":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7f918713a096456b9332530c3101d202","placeholder":"​","style":"IPY_MODEL_62fa2310aa85419e95e1c1cb09e761b9","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"4f7ea66a8d384fc59eddb0f97fedb4e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"4fd575bd3052441d9d5e06192b940744":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_068a035a355d484ca98814a0ed358563","placeholder":"​","style":"IPY_MODEL_6de2e98eeca64ed6872b8597d3039a31","tabbable":null,"tooltip":null,"value":" 79/79 [00:12&lt;00:00,  6.24it/s]"}},"5022daec54fe4290a5a2604f447a2231":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c9485671b1c14595b1431ad8a3dee338","placeholder":"​","style":"IPY_MODEL_d69b7d760df0486ea48de765ebbadd33","tabbable":null,"tooltip":null,"value":" 157/157 [00:01&lt;00:00, 106.71it/s]"}},"54003c453f24483994aa1ecbdf42ef6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0d3eef2db4f4477fa3c173644e341f87","placeholder":"​","style":"IPY_MODEL_6d52e2174e9945b9a7f4b025f050e7c3","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 114.03it/s]"}},"54b9655b455e4ba98d7dab272e2e07b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_107c78314c3040318bfcdb2206443e1e","placeholder":"​","style":"IPY_MODEL_5b221829f8f74db98312ff41ef992952","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"54f39c4fd10a4a159b5c3c188559a7fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7f99461bd86443fb8707e31da87db7af","placeholder":"​","style":"IPY_MODEL_c2662bbfe7334423bc94f03d78a573e6","tabbable":null,"tooltip":null,"value":" 40/40 [00:07&lt;00:00,  5.56it/s]"}},"579bc1d7b2cc4d5cb87e728f82022cbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ef4876f6f7854b78aac8e8d3585497da","placeholder":"​","style":"IPY_MODEL_98ead05c6a6a4e758ba218de6a736a66","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 426.33it/s]"}},"57f33b54eaf545c1b69727ab43dc6ccc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f695246c4e554f2dbe9bbebdfd120e95","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2c7b836b4ce4cd18be0c2bd34939b55","tabbable":null,"tooltip":null,"value":79}},"58b6c898ffa34fd3a6abe3298ba2c05c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"597a559fc62f48529f13eb2bcb733cb7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59ce5b4da4bd43efbcc2600a409ffa51":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b111cf6cdc94ba3b902df29d5344c73":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b221829f8f74db98312ff41ef992952":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"5c8b192f4144459b8dd2162e7b7d8f1f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d720bfdd4574df2b1be362127983d04":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_48b5aa7ae186486f977abc995266456d","placeholder":"​","style":"IPY_MODEL_90faeed6585248bfae664d18cc8c7d32","tabbable":null,"tooltip":null,"value":" 40/40 [00:06&lt;00:00,  6.14it/s]"}},"5e61ee46763b476f933b72deaf1bf700":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"5ef2684a564d4ab6ae9b1e8a6e877a41":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bc0a4999af14e9b91bef7635f1e8245","IPY_MODEL_06ef1e644a1a46a7be9b0e0b76568995","IPY_MODEL_f18a4c1d36cc499fbc5bbf7b5862a0c0"],"layout":"IPY_MODEL_2c8b57b574f64069ba78be21e29a1acf","tabbable":null,"tooltip":null}},"5f298f35523746ad889dc10c0a9d3c08":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b0de6c6dcc904910bb344cb9bb3214e0","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74d4c69f2c3b4779811e9549b2a00277","tabbable":null,"tooltip":null,"value":8}},"5fa3554f02a1447594b4763fce66e6a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5fb670148f1047ab8d1d171844129d62":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"605e4d53602b4067ad7d6d723f5603d2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"62f6235293a242e188358f737d63acd3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62fa2310aa85419e95e1c1cb09e761b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"63bd7a76e2a345f2beb9cb1c0fd9f966":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"649e2d50a8154c558a864c7c1c17daca":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d2982edab66c47f9a9b5951ec76dd84d","placeholder":"​","style":"IPY_MODEL_3dd52f972e4443c9a94ee280f6863c5a","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"64ed18c0311a4a848da73a16b228ad78":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"65ca8cec48274678b8b522f5c8103718":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6ba960fd84d4376942443ae3b0da1e7","IPY_MODEL_7b9b1f4bd34f4d3e9286e31c9058148f","IPY_MODEL_229bc20fdc6049b7abb9671a74bd7e85"],"layout":"IPY_MODEL_2b7084a2f40f4856b8d9e94fc6fbbe88","tabbable":null,"tooltip":null}},"66795ca4fd5a4c6792bc3f2cf81b47b2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6692875a7d8b409ca04485f1558f6851":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fac3d100b8c7461daf06d88d4983b1bc","IPY_MODEL_d462fc687ee74420b6e82ba906390a14","IPY_MODEL_5d720bfdd4574df2b1be362127983d04"],"layout":"IPY_MODEL_4e130b2cd078494e93089a854ff489cd","tabbable":null,"tooltip":null}},"67402d39c36f42e5bbec250bc5c6833a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"682cd64a984843068f5767eceb025d82":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"68658112254b40dbb8d951b63f9cabb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_d2b150e600674cb1b8221221a70a0c63","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a70cdd1fc9e5496abab73f548ef8d105","tabbable":null,"tooltip":null,"value":79}},"6aaafeec48ca40f49e58b474ff37c18a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6d2dc70d147442b9ae6cb964340559da":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6d52e2174e9945b9a7f4b025f050e7c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6d74863305fe42ec957171b30be1f0fe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6de2e98eeca64ed6872b8597d3039a31":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6ebcaa7d60944a28ae95d27b06e1234f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f9d46edddfc4004b95b7e0dd41006c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6fa9ad890cca48ffae90e5f1d512ef47":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7050e3bcdda245a4a71822f001a9b969":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a16a4ddf6a794525b30db897a73490e9","IPY_MODEL_95188a207d4d4612ab627e4eedb0ed60","IPY_MODEL_7e89578e47e149e7a7e4542f14dfffef"],"layout":"IPY_MODEL_3da4721ce5664e7d81e003c3be0c2a3e","tabbable":null,"tooltip":null}},"7064821f7c2a49aba7fdd1314a62da57":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8891b7aae2a8432aa87ca8fcd78cf2c2","placeholder":"​","style":"IPY_MODEL_5e61ee46763b476f933b72deaf1bf700","tabbable":null,"tooltip":null,"value":" 79/79 [00:13&lt;00:00,  5.98it/s]"}},"70be5c68abb64c54998b0598ff19ef48":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c3036eea6633421d841db90c789b9976","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5358a8179e74135a9d82db55a91d838","tabbable":null,"tooltip":null,"value":79}},"71835fe9fea34c2d902a27c78637ea51":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5b111cf6cdc94ba3b902df29d5344c73","placeholder":"​","style":"IPY_MODEL_f3a771920e1e4703bb03874b57f4f19e","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"72a0e66b44954a3db37e9cc14909dfcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e92d5711c934b48b0c79ec689fbbc57","IPY_MODEL_487048581b3f46fa808212d49eff7f95","IPY_MODEL_77a3e76e207547ff8dc6baf77d7bca56"],"layout":"IPY_MODEL_1873e4741cb44edc8ca177cd53d1d536","tabbable":null,"tooltip":null}},"72fb74597733494f86c852767c0563bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_2f6d030911274325beae64b7ecef2bf5","max":703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab0498681d0942c9bdc1e224b623213c","tabbable":null,"tooltip":null,"value":703}},"7357f5d59b7542aab3d1c1eb20cec31c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"73f29d7a66924de58f80d97c0e8c10d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_26cbcd89698f4a9f99fddab3a5b88c56","placeholder":"​","style":"IPY_MODEL_4f7ea66a8d384fc59eddb0f97fedb4e3","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"73f8aea1b3c84541a55edb68ac8aeea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73f9903b81274ddaac72d9a93a6a2a9e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"745ab8707ce645749c250ae6d5b6c317":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"74d4c69f2c3b4779811e9549b2a00277":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7592b6a256b84b25a902371e025d531d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_274662aa97fd4deb9ea2020ded142717","placeholder":"​","style":"IPY_MODEL_c717d29b071a442cb3b989d576d6df8f","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 427.34it/s]"}},"7663a2d686a748d291e8d4892c39ca60":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"76ee838254034cda918c3ebeccd2f6db":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"770355de70874cd0b9017114b9266831":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_92e427e2bdcf48dd82c880cd2e93bce4","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99179eca93d14d1cb4f3b79b90991a2a","tabbable":null,"tooltip":null,"value":157}},"77a3e76e207547ff8dc6baf77d7bca56":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_45fa117ce0fd4224b5abf5d741375106","placeholder":"​","style":"IPY_MODEL_745ab8707ce645749c250ae6d5b6c317","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 111.31it/s]"}},"77e17cfd21e04933ab995a9b9dd5331a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"79156a3ac5224384a9a2bd770a16c390":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84183c7a02b949fda66dce21957ff78c","IPY_MODEL_5f298f35523746ad889dc10c0a9d3c08","IPY_MODEL_ebe275bb8f2147e7aaa350cf0333a7c2"],"layout":"IPY_MODEL_1f4c0b7d9ae247bc87cb0f9d98c78743","tabbable":null,"tooltip":null}},"7919bc2b9cd04a018deb15f48ad2186f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_62f6235293a242e188358f737d63acd3","placeholder":"​","style":"IPY_MODEL_a9ead4bd773a4ba0acb6fd54e5acd8fd","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"79ba18af59844472bbf43ae8eb1d53e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"79bc0d3a9c3649f9b1ea8875ce61a574":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9b1f4bd34f4d3e9286e31c9058148f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_990e7c1037e641a392fd79862e318d29","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07b03412abcf45b1be6d33e245c22463","tabbable":null,"tooltip":null,"value":157}},"7c4a55bb0d9c49e5991585d8ea2a9033":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7cc4442fa48e433f995346372bf98a88":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cd43f6dbcfb4c20ba4bdf71bad6d853":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7e89578e47e149e7a7e4542f14dfffef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_194bd131446f4ea59daf15b2f2e7eb5f","placeholder":"​","style":"IPY_MODEL_acf050c1ada7424bb1e6c64431f988e9","tabbable":null,"tooltip":null,"value":" 703/703 [00:07&lt;00:00, 94.61it/s]"}},"7f918713a096456b9332530c3101d202":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f99461bd86443fb8707e31da87db7af":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80130edfe1f6403998f8e30dc2af1415":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80227244b4dd43bf888fa74e3bf8af2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fc2c2f3fc5184a56b71fb5f872321168","placeholder":"​","style":"IPY_MODEL_e5e647986a7e40cfa146490b72caa69d","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 116.06it/s]"}},"8057bc8cdf0b4d1080fbcc6ddd1b7b0e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82ee923da21e4f82b70a88c9b9b2e4bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"833b5556d0b0482fb083eee189e695d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_fc3d1cd6e177494180e2d2c96dbdd844","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_360a4398dd0b4335a8520c0a0e10d9dd","tabbable":null,"tooltip":null,"value":79}},"84183c7a02b949fda66dce21957ff78c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ff2d517588a74f9497ddf7ccf0e066ae","placeholder":"​","style":"IPY_MODEL_7cd43f6dbcfb4c20ba4bdf71bad6d853","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"846f90bb45594b6b866a02d8bc50bd6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_366af1eae4804f23920460f465a2608a","max":703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87f3d0af2aac41aea3568571a1efb213","tabbable":null,"tooltip":null,"value":703}},"86592d2066d143f3ba821fde0207e6a5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865fde05f68544149765b5b6e3cb8de8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87cc25a5354a4248ae7e9eaeec34011b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87f3d0af2aac41aea3568571a1efb213":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8891b7aae2a8432aa87ca8fcd78cf2c2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88e1b1c159b3430c997fb29a7dca8032":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b138b011f5d41d59b2896a98f9c715e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b2e3663dc6b4d6ab4be2e997fa8e886":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dbaa61c8c1049278e41bfbd0a2f0c39":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ef63e19d56df4a5a9c9391013c5f7a1b","placeholder":"​","style":"IPY_MODEL_5fb670148f1047ab8d1d171844129d62","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"8de95e4b697845f9a84d19b141bf5f03":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dffc77092394eed8a39f254399d6207":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_018ee0a5ba1f4664a15c66b7e28e67fb","placeholder":"​","style":"IPY_MODEL_77e17cfd21e04933ab995a9b9dd5331a","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"8e0bc77d5d374d11a14f1682e5fa718e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"8f04de6e7cf54324b3419bd5bf89877a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9072a9ad01764f1f9df06a5912cbd4a9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"90faeed6585248bfae664d18cc8c7d32":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"91a99a718dd54efb9c94759b698aa853":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"91d188e136f648748f1a03e01e6ede00":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7919bc2b9cd04a018deb15f48ad2186f","IPY_MODEL_a87c68dc26e24813965d07ef19c91d0c","IPY_MODEL_1cb42f360466497783e3810e3975db53"],"layout":"IPY_MODEL_8e0bc77d5d374d11a14f1682e5fa718e","tabbable":null,"tooltip":null}},"92e427e2bdcf48dd82c880cd2e93bce4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c2cafcb9e342eb9475aa379f0de87a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c5b309787524c35b66c808aa8916672","IPY_MODEL_383a3ed3679347b9929504d2d5ba88f8","IPY_MODEL_7592b6a256b84b25a902371e025d531d"],"layout":"IPY_MODEL_278f676a010843e5867e34055b23efd3","tabbable":null,"tooltip":null}},"95188a207d4d4612ab627e4eedb0ed60":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_73f9903b81274ddaac72d9a93a6a2a9e","max":703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ff2234622d54a989a978577402dcbce","tabbable":null,"tooltip":null,"value":703}},"9600a121b4c048e087aab0ec32c506ac":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"97d81566c8e14396817ca8eaf298587f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98a04d0e864f479a8353710eebaa8c77":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73f29d7a66924de58f80d97c0e8c10d9","IPY_MODEL_846f90bb45594b6b866a02d8bc50bd6c","IPY_MODEL_e7e0a1e9b18f4b5ea36c85d5f2314151"],"layout":"IPY_MODEL_bdef22db6f404352b67dd826865d21f0","tabbable":null,"tooltip":null}},"98ead05c6a6a4e758ba218de6a736a66":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"990e7c1037e641a392fd79862e318d29":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99179eca93d14d1cb4f3b79b90991a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9aaeb45ac2344927ae8565e5e5c3ccc1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b5666e7d6434614832076819ef8486f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ed3c0ad61df14f75ab630c2b2d16053c","placeholder":"​","style":"IPY_MODEL_7c4a55bb0d9c49e5991585d8ea2a9033","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"9b77b674322a4e67a3186ceb426d18eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b84d25fe3f548a8ae92eb4c9d0232ca":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc4259935234bc8b81c00f34559f74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8dbaa61c8c1049278e41bfbd0a2f0c39","IPY_MODEL_3b3dff8ace4c47bda355351e008efd87","IPY_MODEL_d7cac59fbfff489eaa2f77a88a6307dc"],"layout":"IPY_MODEL_be77bbe7f34948228b3c98d236073ef0","tabbable":null,"tooltip":null}},"9ce52310e7934b6eb57df401811297c5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dcf5665342349dea5635a30046146cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f441259e1be4b098065e764db5b7a28":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a7899707ef574b409d446e5cc94ce576","placeholder":"​","style":"IPY_MODEL_3ec01bb357cd4991b815d6c123454741","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"9f81af01ed174d21b9426f9da2728d38":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fc5999795ef45958f984c36c36b5682":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a067b379a66c4cdca08b9e9db6053a07":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a52c045c717d40d481778d066d1921e4","placeholder":"​","style":"IPY_MODEL_7663a2d686a748d291e8d4892c39ca60","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"a079b23abee645b5af3dd59ae2f5ed40":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a16a4ddf6a794525b30db897a73490e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_97d81566c8e14396817ca8eaf298587f","placeholder":"​","style":"IPY_MODEL_91a99a718dd54efb9c94759b698aa853","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"a1ef4e722da74769b1f80b55e8ee3f76":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2c7b836b4ce4cd18be0c2bd34939b55":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a429b42b47ca446c8294df1501f5486f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a52c045c717d40d481778d066d1921e4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5358a8179e74135a9d82db55a91d838":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a56335085ba148b0b9b3ac52c0c3bb53":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_649e2d50a8154c558a864c7c1c17daca","IPY_MODEL_833b5556d0b0482fb083eee189e695d8","IPY_MODEL_023bc36955994e5db74d44033a4c3d1d"],"layout":"IPY_MODEL_605e4d53602b4067ad7d6d723f5603d2","tabbable":null,"tooltip":null}},"a70cdd1fc9e5496abab73f548ef8d105":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7899707ef574b409d446e5cc94ce576":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a87bf86073804612958b5c048e0e0c0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a87c68dc26e24813965d07ef19c91d0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_86592d2066d143f3ba821fde0207e6a5","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63bd7a76e2a345f2beb9cb1c0fd9f966","tabbable":null,"tooltip":null,"value":8}},"a9ead4bd773a4ba0acb6fd54e5acd8fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ab0498681d0942c9bdc1e224b623213c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab9fc1da51144a7e80b9f3086292b000":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8f04de6e7cf54324b3419bd5bf89877a","placeholder":"​","style":"IPY_MODEL_82ee923da21e4f82b70a88c9b9b2e4bc","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 428.20it/s]"}},"acf050c1ada7424bb1e6c64431f988e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ad6e232faf684cdda135e09e7564b826":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae3930f4f9a740dd8d975ace189e1ddd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"aee36f32933c4017bbbca3658492cef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4291fd75e0514117bba7a319115a2647","placeholder":"​","style":"IPY_MODEL_eb474a1227fe44e593efa1b8c80b933e","tabbable":null,"tooltip":null,"value":" 8/8 [00:00&lt;00:00, 226.03it/s]"}},"b092b439d86245118df32c0c81f1e4d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3460bdde314b41d3a5041ebc4f32b461","IPY_MODEL_3298fc3f6b3046eea7bdb48c5c5e72f5","IPY_MODEL_ecbb73c2f5c947cea1d435516c468ea9"],"layout":"IPY_MODEL_6aaafeec48ca40f49e58b474ff37c18a","tabbable":null,"tooltip":null}},"b0d7ae79c62e451484ddb49de54f7d93":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b0de6c6dcc904910bb344cb9bb3214e0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1adc9b413f5488eb4b25ada3d33bbd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b39b5bd0b161481fba7ab3f268c798f7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4ccbe9e109a4bc39a1dc5f5088068e9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b5337e824d0d4b4fba50aa469a76c02b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b698ca40ecac4d37af6057c9a079c0c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5c8b192f4144459b8dd2162e7b7d8f1f","max":703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b77b674322a4e67a3186ceb426d18eb","tabbable":null,"tooltip":null,"value":703}},"b6e4718f88b643a393980cab0b9b9915":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7ef209e60964737a778b405e00e5b1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b9696720254143259b2e4b6bbb281488":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"bdef22db6f404352b67dd826865d21f0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"be77bbe7f34948228b3c98d236073ef0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"c190471dc46d42fc8128933c1507d6dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_eff20e9d177f40c8b4c6073c9290a65a","placeholder":"​","style":"IPY_MODEL_a87bf86073804612958b5c048e0e0c0f","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"c2662bbfe7334423bc94f03d78a573e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c3036eea6633421d841db90c789b9976":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d9400f841842ec9708bba7a043f324":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c405c02b2b8e453fb3cd22113fd6e19c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a067b379a66c4cdca08b9e9db6053a07","IPY_MODEL_70be5c68abb64c54998b0598ff19ef48","IPY_MODEL_579bc1d7b2cc4d5cb87e728f82022cbb"],"layout":"IPY_MODEL_b4ccbe9e109a4bc39a1dc5f5088068e9","tabbable":null,"tooltip":null}},"c4200b16cfe74646b4d7467f1d0065f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ad6e232faf684cdda135e09e7564b826","placeholder":"​","style":"IPY_MODEL_a429b42b47ca446c8294df1501f5486f","tabbable":null,"tooltip":null,"value":" 79/79 [00:13&lt;00:00,  5.73it/s]"}},"c49d26ee978948ea9eb834d1b8986db5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c567c33972da4342beb6c6a225b4f90b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6ba960fd84d4376942443ae3b0da1e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fbcb55d196bf470797955ee70efbcbea","placeholder":"​","style":"IPY_MODEL_64ed18c0311a4a848da73a16b228ad78","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"c717d29b071a442cb3b989d576d6df8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c8e55e9e642d47f7890b61c406e32f3e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8fc4389533b480480388b1bf5ef8d9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c9485671b1c14595b1431ad8a3dee338":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caccd62e6232451c9ccd7f7974909e69":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"cb9fcfcbb9c34a0a96c1cd9c98959d15":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b3c95c97bc2459aacd57d0d8481d33b","IPY_MODEL_dbd039dc429743528d11e56fdd5b4fbc","IPY_MODEL_0e1d2fb5324e4156b6a8f93519618218"],"layout":"IPY_MODEL_9072a9ad01764f1f9df06a5912cbd4a9","tabbable":null,"tooltip":null}},"ce132d75b80e4e0e83520d9d40a4d139":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_597a559fc62f48529f13eb2bcb733cb7","placeholder":"​","style":"IPY_MODEL_b0d7ae79c62e451484ddb49de54f7d93","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"cf8f46da5c8e4e8b9f33676110afafb2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d008d9ee0e9f4248bfcafead43061adf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d077fc3f4fd34399af50a30eae77440e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a079b23abee645b5af3dd59ae2f5ed40","placeholder":"​","style":"IPY_MODEL_6d2dc70d147442b9ae6cb964340559da","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"d1bd2ac4b3e249dd91e93daca0fb0716":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2982edab66c47f9a9b5951ec76dd84d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2b150e600674cb1b8221221a70a0c63":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d462fc687ee74420b6e82ba906390a14":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_db73d56f18a14254a488918171ff2267","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73f8aea1b3c84541a55edb68ac8aeea6","tabbable":null,"tooltip":null,"value":40}},"d69b7d760df0486ea48de765ebbadd33":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d749310c211e49d9930a8304d9aeb928":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d7cac59fbfff489eaa2f77a88a6307dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9fc5999795ef45958f984c36c36b5682","placeholder":"​","style":"IPY_MODEL_682cd64a984843068f5767eceb025d82","tabbable":null,"tooltip":null,"value":" 157/157 [00:01&lt;00:00, 109.13it/s]"}},"d7db13e68e5e45c8bb7e5f19aedeb3fa":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8414d98f6c5428fa2d4f7520e3628fb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9d628571aa647328237a19d022ac26d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_38202e7bd8de45989572437ff348b1e5","placeholder":"​","style":"IPY_MODEL_c3d9400f841842ec9708bba7a043f324","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"dab4975a67d74f429436e89c39db463d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8057bc8cdf0b4d1080fbcc6ddd1b7b0e","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5fa3554f02a1447594b4763fce66e6a9","tabbable":null,"tooltip":null,"value":79}},"db73d56f18a14254a488918171ff2267":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db9a79a11b87463794a81f27c015e843":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b5666e7d6434614832076819ef8486f","IPY_MODEL_e33f8cc856c443e5a83b7a42b497da1f","IPY_MODEL_c4200b16cfe74646b4d7467f1d0065f0"],"layout":"IPY_MODEL_7357f5d59b7542aab3d1c1eb20cec31c","tabbable":null,"tooltip":null}},"dbd039dc429743528d11e56fdd5b4fbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_76ee838254034cda918c3ebeccd2f6db","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13d3458c157c4e1795b3b3e23b01a993","tabbable":null,"tooltip":null,"value":157}},"dc1d471600164693baba77b91d6a28b7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc4069c20634f52b4565be3cafcaf50":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d1bd2ac4b3e249dd91e93daca0fb0716","placeholder":"​","style":"IPY_MODEL_6fa9ad890cca48ffae90e5f1d512ef47","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"e33f8cc856c443e5a83b7a42b497da1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_fa0915395ce34736b42870bc0e78efab","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_337ddfc2d5b042d59b0c9fa983e03bb0","tabbable":null,"tooltip":null,"value":79}},"e4d785751bdd418a8cc4ee9319655631":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"e53d706a742a47c59cb6c59d3c6cbd49":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5e647986a7e40cfa146490b72caa69d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"e5edf508220a48fb84b894cf7e792b57":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfc4069c20634f52b4565be3cafcaf50","IPY_MODEL_33d20ca1824d43edb825baf31962616e","IPY_MODEL_ab9fc1da51144a7e80b9f3086292b000"],"layout":"IPY_MODEL_0429eb01255042b5b8c5c681df5edb74","tabbable":null,"tooltip":null}},"e759badae5f84de294d71190dc30bccd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e7e0a1e9b18f4b5ea36c85d5f2314151":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1176520bd5b7468385cd5812a435a703","placeholder":"​","style":"IPY_MODEL_f5907fcbf651423da153474f30fde7f2","tabbable":null,"tooltip":null,"value":" 703/703 [00:07&lt;00:00, 96.98it/s]"}},"e8d1e3670cbc43a2a3c2d6be2c3594e8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab12e1a5f3d4a1092332f2d54fc1e2c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"eb474a1227fe44e593efa1b8c80b933e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ebe275bb8f2147e7aaa350cf0333a7c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_3cede17f6e754a17bd7d7235b87faba4","placeholder":"​","style":"IPY_MODEL_b1adc9b413f5488eb4b25ada3d33bbd1","tabbable":null,"tooltip":null,"value":" 8/8 [00:00&lt;00:00, 298.63it/s]"}},"ecbb73c2f5c947cea1d435516c468ea9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7cc4442fa48e433f995346372bf98a88","placeholder":"​","style":"IPY_MODEL_ede3f35007ba449cbdf7ae4c2e77d288","tabbable":null,"tooltip":null,"value":" 40/40 [00:06&lt;00:00,  5.83it/s]"}},"ecebbcb66f3048b3817d6228dc06c181":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed3c0ad61df14f75ab630c2b2d16053c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9f70f4075d4ed0a296d7c7916e0228":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d7db13e68e5e45c8bb7e5f19aedeb3fa","placeholder":"​","style":"IPY_MODEL_c8fc4389533b480480388b1bf5ef8d9e","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"ede3f35007ba449cbdf7ae4c2e77d288":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ede895a0189f4273af502776c6a59319":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_88e1b1c159b3430c997fb29a7dca8032","placeholder":"​","style":"IPY_MODEL_58b6c898ffa34fd3a6abe3298ba2c05c","tabbable":null,"tooltip":null,"value":" 79/79 [00:00&lt;00:00, 106.80it/s]"}},"ee3e3a492b1f453493387d3bc24bb6f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f7cc718953c5477e95e948341a8a0ec1","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb27dd76ebe04d62be29b46729620cee","tabbable":null,"tooltip":null,"value":40}},"ee748612fb374e1084d639a0b812577a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef4876f6f7854b78aac8e8d3585497da":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef63e19d56df4a5a9c9391013c5f7a1b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff20e9d177f40c8b4c6073c9290a65a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f18a4c1d36cc499fbc5bbf7b5862a0c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ee748612fb374e1084d639a0b812577a","placeholder":"​","style":"IPY_MODEL_79ba18af59844472bbf43ae8eb1d53e3","tabbable":null,"tooltip":null,"value":" 8/8 [00:00&lt;00:00, 373.83it/s]"}},"f224515cc90d42f1a88ae0e7049cc8a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed9f70f4075d4ed0a296d7c7916e0228","IPY_MODEL_b698ca40ecac4d37af6057c9a079c0c6","IPY_MODEL_f6e84546cdc2443699fa66e000821f1b"],"layout":"IPY_MODEL_eab12e1a5f3d4a1092332f2d54fc1e2c","tabbable":null,"tooltip":null}},"f3a771920e1e4703bb03874b57f4f19e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f5907fcbf651423da153474f30fde7f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f6123049f94d41518bebfe178d443c6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ce340ffe4e846d8acb21cc23f6d077c","IPY_MODEL_426b0d3a5f9b44ef8938839868b3e4a4","IPY_MODEL_aee36f32933c4017bbbca3658492cef0"],"layout":"IPY_MODEL_6d74863305fe42ec957171b30be1f0fe","tabbable":null,"tooltip":null}},"f695246c4e554f2dbe9bbebdfd120e95":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e84546cdc2443699fa66e000821f1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1e98bdb290924b11930cc4ec15684dc3","placeholder":"​","style":"IPY_MODEL_b7ef209e60964737a778b405e00e5b1d","tabbable":null,"tooltip":null,"value":" 703/703 [00:07&lt;00:00, 95.84it/s]"}},"f7cc718953c5477e95e948341a8a0ec1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8192eb9fbe04a1fbed941decbfd27cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce132d75b80e4e0e83520d9d40a4d139","IPY_MODEL_770355de70874cd0b9017114b9266831","IPY_MODEL_5022daec54fe4290a5a2604f447a2231"],"layout":"IPY_MODEL_23a1d9bde4b747f4b34af4cd88a4f015","tabbable":null,"tooltip":null}},"fa0915395ce34736b42870bc0e78efab":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa9cf0e3e0dd4c029df500e8a4f90bae":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"fab94e55f9504c709fc31f58cc26d974":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fac3d100b8c7461daf06d88d4983b1bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_59ce5b4da4bd43efbcc2600a409ffa51","placeholder":"​","style":"IPY_MODEL_caccd62e6232451c9ccd7f7974909e69","tabbable":null,"tooltip":null,"value":"Testing DataLoader 0: 100%"}},"fb27dd76ebe04d62be29b46729620cee":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fbcb55d196bf470797955ee70efbcbea":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2c2f3fc5184a56b71fb5f872321168":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc3d1cd6e177494180e2d2c96dbdd844":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd1299daa81942c1ad847bb853884d1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ff2d517588a74f9497ddf7ccf0e066ae":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>