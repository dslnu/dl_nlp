<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.43">

  <meta name="author" content="Vitaly Vlasov">
  <title>Deep Learning/NLP course – Deep learning: logistic regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Deep learning: logistic regression</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Vitaly Vlasov 
</div>
        <p class="quarto-title-affiliation">
            Lviv University
          </p>
    </div>
</div>

</section>
<section id="neurons" class="slide level2">
<h2>Neurons</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>ANN <span class="math inline">\(\equiv\)</span> Artificial Neural Network</p>
<p>Remember - Neurons, axons, dendrites.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Inputs to neurons are scaled with <strong>weight</strong>.</p>
<p>Weight is similar to a strength of synaptic connection.</p>
</div>
</div>
</div>
</section>
<section id="neurons-1" class="slide level2">
<h2>Neurons</h2>

<img data-src="img/neuron.png" class="r-stretch"></section>
<section id="neurons-2" class="slide level2">
<h2>Neurons</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Computation</strong></p>
</div>
<div class="callout-content">
<p>ANN <strong>computes</strong> a function of the inputs by propagating the computed values from input neurons to output neurons, using weights as intermediate parameters.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Learning</strong></p>
</div>
<div class="callout-content">
<p><strong>Learning</strong> occurs by changing the weights. External stimuli are required for learning in bio-organisms, in case of ANNs they are provided by the training data.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Training</strong></p>
</div>
<div class="callout-content">
<p><strong>Training</strong> data contain input-output pairs. We compare predicted output with annotated output label from training data.</p>
</div>
</div>
</div>
</section>
<section id="neurons-3" class="slide level2">
<h2>Neurons</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Errors</strong></p>
</div>
<div class="callout-content">
<p>Errors are comparison failures. These are similar to unpleasant feedback modifying synaptic strengths. Goal of changing weights - make predictions better.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Model generalizations</strong></p>
</div>
<div class="callout-content">
<p>Ability to compute functions of unseen inputs accurately, even though given finite sets of input-output pairs.</p>
</div>
</div>
</div>
</section>
<section id="computation-graph" class="slide level2">
<h2>Computation graph</h2>
<p>Alternative view - <strong>computation graph</strong>.</p>
<p>When used in basic graph, NNs reduce to classical ML models.</p>
<ul>
<li>Least-squares regression</li>
<li>logistic regression</li>
<li>linear regression</li>
</ul>
<p>Nodes compute based on inputs and weights.</p>
</section>
<section id="goal" class="slide level2">
<h2>Goal</h2>
<p><strong>Goal</strong> of NN: <em>learn</em> a function that relates inputs to outputs with the use of training examples.</p>
<p>Settings the edge weights is <em>training</em>.</p>
</section>
<section id="structure" class="slide level2">
<h2>Structure</h2>
<p>Consider a simple case of <span class="math inline">\(d\)</span> inputs and a single binary output. <span class="math display">\[
(\overline{X}, y) - \text{training instance}
\]</span></p>
<p>Feature variables: <span class="math display">\[
\overline{X}=[x_1, \dots, x_d]
\]</span> Observed value: <span class="math inline">\(y \in {0,1}\)</span>, contained in target variable <span class="math inline">\(y\)</span>.</p>
</section>
<section id="objective" class="slide level2">
<h2>Objective</h2>
<ul>
<li><p>learn the function <span class="math inline">\(f(\cdot)\)</span>, such that <span class="math inline">\(y=f_{\overline{W}}(\overline{X})\)</span>.</p></li>
<li><p>minimize mismatch between <span class="math inline">\(y\)</span> and <span class="math inline">\(f_{\overline{W}}(\overline{X})\)</span>. <span class="math inline">\(W\)</span> - weight vector.</p></li>
</ul>
<p>In case of perceptron, we compute a linear function: <span class="math display">\[\begin{align*}
  &amp;\hat{y}=f(\overline{X}) = sign\left\{\overline{W}^T \overline{X}^T\right\} =  sign\left\{\sum\limits_{i=1}^d w_i x_i\right\}
\end{align*}\]</span> <span class="math inline">\(\hat{y}\)</span> means value, not observed value <span class="math inline">\(y\)</span>.</p>
</section>
<section id="perceptron" class="slide level2">
<h2>Perceptron</h2>
<p>A simplest NN.</p>

<img data-src="img/perceptron.png" class="r-stretch"></section>
<section id="perceptron-1" class="slide level2">
<h2>Perceptron</h2>
<p>We choose the basic form of the function, but strive to find some parameters.</p>
<ul>
<li><code>Sign</code> is an activation function</li>
<li>value of the node is also sometimes referred to as an <code>activation</code>.</li>
</ul>
<p>Perceptron is a single-layer network, as input nodes are not counted.</p>
</section>
<section id="perceptron-2" class="slide level2">
<h2>Perceptron</h2>
<p>How does perceptron learn? <span class="math display">\[
\overline{W} := \overline{W} + \alpha(y-\hat{y})\overline{X}^T.
\]</span> So, in case when <span class="math inline">\(y \neq \hat{y}\)</span>, we can write it as <span class="math display">\[
\overline{W} := \overline{W} + \alpha y \overline{X}^T.
\]</span></p>
</section>
<section id="perceptron-3" class="slide level2">
<h2>Perceptron</h2>
<p>We can show that perceptron works when data are linearly separable by a hyperplane <span class="math inline">\(\overline{W}^T X = 0\)</span>.</p>
<p><img data-src="img/linear_separation.png" height="350"></p>

<aside><div>
<p>Perceptron algorithm is not guaranteed to converge when data are not linearly separable.</p>
</div></aside></section>
<section id="bias" class="slide level2">
<h2>Bias</h2>
<p>Bias is needed when binary class distribution is imbalanced: <span class="math display">\[
\overline{W}^T \cdot \sum_i \overline{X_i}^T \neq \sum_i y_i
\]</span> Bias can be incorporated by using a bias neuron.</p>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problems</strong></p>
</div>
<div class="callout-content">
<p>In linearly separable data sets, a nonzero weight vector <span class="math inline">\(W\)</span> exists in which the <span class="math inline">\(sign(\overline{W}^T X) = sign(y_i)\; \forall (\overline{X}_i,y_i)\)</span>.</p>
<p>However, the behavior of the perceptron algorithm for data that are not linearly separable is rather arbitrary.</p>
</div>
</div>
</div>
</section>
<section id="loss-function" class="slide level2">
<h2>Loss function</h2>
<p>ML algorithms are loss optimization problems, where gradient descent updates are used to minimize the loss.</p>
<p>Original perceptron did not formally use a loss function.</p>
<p><em>Retrospectively</em> we can introduce it as: <span class="math display">\[
L_i \equiv \max\left\{-y_i(\overline{W}^T \overline{X_i}\right\}
\]</span></p>
</section>
<section id="loss-function-1" class="slide level2">
<h2>Loss function</h2>
<p>We differentiate: <span class="math display">\[\begin{align*}
&amp;\dfrac{\partial L_i}{\partial \overline{W}} = \left[\dfrac{\partial L_i}{\partial w_1}, \dots, \dfrac{\partial L_i}{\partial w_d}\right] = \\
&amp; = \begin{cases}
  -y_i \overline{X_i}, &amp; \text{if } sign\{W^T X_i\} \neq y_i,\\
  0, &amp; \text{otherwise}
\end{cases}
\end{align*}\]</span></p>
</section>
<section id="perceptron-update" class="slide level2">
<h2>Perceptron update</h2>
<p>Negative of the vector is the direction of the fastest rate of loss reduction, hence perceptron update: <span class="math display">\[\begin{align*}
   &amp;\overline{W} := \overline{W} - \alpha\dfrac{\partial L_i}{\partial \overline{W}} = \overline{W} + \alpha y_i \overline{X_i}^T.
\end{align*}\]</span></p>
</section>
<section id="activation-functions" class="slide level2">
<h2>Activation functions</h2>
<p>A network with weights <span class="math inline">\(\overline{W}\)</span> and input <span class="math inline">\(\overline{X}\)</span> will have a prediction of the form <span class="math display">\[\begin{align*}
   &amp;\hat{y}=\Phi\left( \overline{W}^T \overline{X}\right)
\end{align*}\]</span> where <span class="math inline">\(\Phi\)</span> denotes <em>activation function</em>.</p>
</section>
<section id="activation-functions-1" class="slide level2">
<h2>Activation functions</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Identity aka linear activation</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = v
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Sign function</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = sign(v)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="activation-functions-2" class="slide level2">
<h2>Activation functions</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Sigmoid function</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = \dfrac{1}{1+e^{-v}}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>tanh</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = \dfrac{e^{2v}-1}{e^{2v}+1}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="activation-functions-3" class="slide level2">
<h2>Activation functions</h2>
<p>Actually, neuron computes two functions:</p>

<img data-src="img/activation.png" class="r-stretch"></section>
<section id="activation-functions-4" class="slide level2">
<h2>Activation functions</h2>
<p>We have <em>pre-activation value</em> and <em>post-activation value</em>.</p>
<ul>
<li><strong>pre-activation</strong>: linear transformation</li>
<li><strong>post-activation</strong>: nonlinear transformation</li>
</ul>
</section>
<section id="activation-functions-5" class="slide level2">
<h2>Activation functions</h2>
<img data-src="dl_lec2_files/mediabag/65b24282599e327b820db5acef99ee53564568ac.svg" height="600">
</section>
<section id="activation-functions-6" class="slide level2">
<h2>Activation functions</h2>
<img data-src="dl_lec2_files/mediabag/d92afd5a5d15d1d12e9b7b1d0d42e8756d79fecf.svg" height="600">
</section>
<section id="activation-functions-7" class="slide level2">
<h2>Activation functions</h2>
<p>Two more functions that have become popular recently:</p>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rectified Linear Unit (ReLU)</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = \max\left\{v, 0\right\}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hard tanh</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\Phi(v) = \max\left\{\min\left[v, 1\right], -1 \right\}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="multiple-activation-fns" class="slide level2">
<h2>Multiple activation fns</h2>

<img data-src="img/different_activation_functions.png" class="r-stretch"></section>
<section id="activation-functions-8" class="slide level2">
<h2>Activation functions</h2>
<p>Properties:</p>
<ul>
<li>monotonic</li>
<li>saturation at large values</li>
<li>squashing</li>
</ul>
</section>
<section id="softmax-activation-function" class="slide level2">
<h2>Softmax activation function</h2>
<p>Used for k-way classification problems. Used in the output layer.</p>
<p><span class="math display">\[\begin{align*}
&amp;\Phi(v)_i = \dfrac{\exp(v_i)}{\sum\limits_{i=1}^k \exp(v_i)}.
\end{align*}\]</span></p>
<p>Softmax layer converts real values to <strong>probabilities</strong>.</p>
</section>
<section id="softmax-activation-function-1" class="slide level2">
<h2>Softmax activation function</h2>

<img data-src="img/softmax.png" class="r-stretch"></section>
<section id="loss-functions" class="slide level2">
<h2>Loss functions</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Least squares regression, numeric targets</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;L(\hat{y}, y) = (y-\hat{y})^2
\end{align*}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Logistic regression, binary targets</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;L(\hat{y}, y) = -\log \left|y/2 - 1/2 + \hat{y}\right|, \{-1,+1\}
\end{align*}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multinomial logistic regression, categorical targets</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{align*}
  &amp;L(\hat{y}, y) = -\log (\hat{y}_r) \text{ - cross-entropy loss}
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section>
<section id="multilayer-networks" class="title-slide slide level1 center">
<h1>Multilayer networks</h1>

</section>
<section id="multilayer-networks-1" class="slide level2">
<h2>Multilayer networks</h2>
<p>Suppose NN contains <span class="math inline">\(p_1, \dots, p_k\)</span> units in each of its <span class="math inline">\(k\)</span> layers.</p>
<p>Then column representations of these layers, denoted by <span class="math inline">\(\overline{h}_1, \dots, \overline{h}_k\)</span>, have <span class="math inline">\(p_1, \dots, p_k\)</span> units.</p>
<ul>
<li><p>Weights between input layer and first hidden layer: matrix <span class="math inline">\(W_1\)</span>, sized <span class="math inline">\(p_1 \times d\)</span>.</p></li>
<li><p>Weights between <span class="math inline">\(r\)</span>-th layer and <span class="math inline">\(r+1\)</span>-th layer: matrix <span class="math inline">\(W_r\)</span> sized <span class="math inline">\(p_{r+1}\times p_r\)</span>.</p></li>
</ul>
</section>
<section id="multilayer-networks-2" class="slide level2">
<h2>Multilayer networks</h2>

<img data-src="img/multi_layer.png" class="r-stretch"></section>
<section id="multilayer-networks-3" class="slide level2">
<h2>Multilayer networks</h2>
<p>Therefore, a <span class="math inline">\(d\)</span>-dimensional input vector <span class="math inline">\(\overline{x}\)</span> is transformed into the outputs using these equations: <span class="math display">\[\begin{align*}
  &amp;\overline{h}_1 = \Phi(W_1^T x),\\
  &amp;\overline{h}_{p+1} = \Phi(W_{p+1}^T \overline{h}_p), \forall p \in \left\{1 \dots k-1 \right\} \\
  &amp;\overline{o} = \Phi(W_{k+1}^T \overline{h}_k)
\end{align*}\]</span> Activation functions operate on vectors and are applied element-wise.</p>
</section>
<section id="multilayer-networks-4" class="slide level2">
<h2>Multilayer networks</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition (Aggarwal)</strong></p>
</div>
<div class="callout-content">
<p>A multilayer network computes a nested composition of parameterized multi-variate functions.</p>
<p>The overall function computed from the inputs to the outputs can be controlled very closely by the choice of parameters.</p>
<p>The notion of learning refers to the setting of the parameters to make the overall function consistent with observed input-output pairs.</p>
</div>
</div>
</div>
</section>
<section id="multilayer-networks-5" class="slide level2">
<h2>Multilayer networks</h2>
<p>Input-output function of NN is difficult to express explicitly. NN can also be called <em>universal function approximators</em>.</p>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Universal approximation theorem</strong></p>
</div>
<div class="callout-content">
<p>Given a family of neural networks, for each function <span class="math inline">\(\displaystyle f\)</span> from a certain function space, there exists a sequence of neural networks <span class="math inline">\(\phi_1,\phi_2,\dots\)</span> from the family, such that <span class="math inline">\(\phi_{n} \to f\)</span> according to some criterion.</p>
</div>
</div>
</div>
<p>In other words, the family of neural networks is <em>dense</em> in the function space.</p>

<aside><div>
<p>K. Hornik, M. Stinchcombe, and H. White. . Neural Networks, 2(5), pp.&nbsp;359–366, 1989.</p>
</div></aside></section>
<section id="nonlinear-activation-functions" class="slide level2">
<h2>Nonlinear activation functions</h2>
<p><strong>Theorem</strong>. A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network.</p>
<p><strong>Proof</strong>. Consider a network containing <span class="math inline">\(k\)</span> hidden layers, therefore containing a total of <span class="math inline">\((k+1)\)</span> computational layers (including the output layer).</p>
<p>The corresponding <span class="math inline">\((k+1)\)</span> weight matrices between successive layers are denoted by <span class="math inline">\(W_1 ...W_{k+1}\)</span>.</p>
</section>
<section id="nonlinear-activation-functions-1" class="slide level2">
<h2>Nonlinear activation functions</h2>
<p>Let:</p>
<ul>
<li><span class="math inline">\(\overline{x}\)</span> be the <span class="math inline">\(d\)</span>-dimensional column vector corresponding to the input</li>
<li><span class="math inline">\(\overline{h_1},\dots,\overline{h_k}\)</span> be the column vectors corresponding to the hidden layers</li>
<li>and <span class="math inline">\(\overline{o}\)</span> be the <span class="math inline">\(m\)</span>-dimensional column vector corresponding to the output.</li>
</ul>
</section>
<section id="nonlinear-activation-functions-2" class="slide level2">
<h2>Nonlinear activation functions</h2>
<p>Then, we have the following recurrence condition for multi-layer networks: <span class="math display">\[\begin{align*}
  &amp;\overline{h_1} = \Phi(W_1 x) = W_1 x,\\
  &amp;\overline{h}_{p+1} = \Phi(W_{p+1} \overline{h}_p) = W_{p+1}\overline{h}_p \;\; \forall p \in \left\{1 \dots k−1\right\}, \\
  &amp;\overline{o} = \Phi(W_{k+1} \overline{h}_k) = W_{k+1} \overline{h}_k.
\end{align*}\]</span></p>
</section>
<section id="nonlinear-activation-functions-3" class="slide level2">
<h2>Nonlinear activation functions</h2>
<p>In all the cases above, the activation function <span class="math inline">\(\Phi(\cdot)\)</span> has been set to the identity function. Then, by eliminating the hidden layer variables, we obtain the following: <span class="math display">\[\begin{align*}
&amp;\overline{o} = W_{k+1}W_k \dots W_1 \overline{x}
\end{align*}\]</span> Denote <span class="math inline">\(W_{xo}=W_{k+1}W_k \dots W_1\)</span>.</p>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>One can replace the matrix <span class="math inline">\(W_{k+1}W_k \dots W_1\)</span> with the new <span class="math inline">\(d\times m\)</span> matrix <span class="math inline">\(W_{xo}\)</span>, and learn the coefficients of <span class="math inline">\(W_{xo}\)</span> instead of those of all the matrices <span class="math inline">\(W_1, W_2, \dots W_{k+1}\)</span>, without loss of expressivity.</p>
</div>
</div>
</div>
</section>
<section id="nonlinear-activation-functions-4" class="slide level2">
<h2>Nonlinear activation functions</h2>
<p>In other words, we have the following: <span class="math display">\[\begin{align*}
&amp;\overline{o} = W_{xo} \overline{x}
\end{align*}\]</span> However, this condition is exactly identical to that of linear regression with multiple outputs. Therefore, a multilayer neural network with identity activations does not gain over a single-layer network in terms of expressivity.</p>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Linearity observation</strong></p>
</div>
<div class="callout-content">
<p>The composition of linear functions is always a linear function. The repeated composition of simple nonlinear functions can be a very complex nonlinear function.</p>
</div>
</div>
</div>
</section>
<section id="backpropagation" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>DAG Definition</strong></p>
</div>
<div class="callout-content">
<p>A <strong>directed acyclic computational graph</strong> is a directed acyclic graph of nodes, where each node contains a variable. Edges might be associated with learnable parameters.</p>
<p>A variable in a node is either fixed externally (for input nodes with no incoming edges), or it is a computed as a function of the variables in the tail ends of edges incoming into the node and the learnable parameters on the incoming edges.</p>
</div>
</div>
</div>
<p>DAG is a more general version of NN.</p>
</section>
<section id="backpropagation-1" class="slide level2">
<h2>Backpropagation</h2>
<p>A computational graph evaluates compositions of functions.</p>
<p>A path of length 2 in a computational graph in which the function <span class="math inline">\(f(\cdot)\)</span> follows <span class="math inline">\(g(\cdot)\)</span> can be considered a composition function <span class="math inline">\(f(g(\cdot))\)</span>.</p>
<p>In case of sigmoid function: <span class="math display">\[\begin{align*}
   &amp;f(x) = g(x) = \dfrac{1}{1+e^{-x}} \\
   &amp;f(g(x)) = \dfrac{1}{1 + e^{\left[-\dfrac{1}{1+e^{-x}}\right]}}
\end{align*}\]</span></p>
</section>
<section id="backpropagation-2" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The inability to easily express the optimization function in closed form in terms of the edge-specific parameters (as is common in all machine learning problems) causes difficulties in computing the derivatives needed for gradient descent.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>For example, if we have a computational graph which has 10 layers, and 2 nodes per layer, the overall composition function would have <span class="math inline">\(2^{10}\)</span> nested “terms”.</p>
</div>
</div>
</div>
</section>
<section id="backpropagation-3" class="slide level2 smaller">
<h2>Backpropagation</h2>
<ol type="1">
<li>Derivatives of the output with respect to various variables in the computational graph are related to one another with the use of the <strong>chain rule</strong> of differential calculus.</li>
<li>Therefore, the chain rule of differential calculus needs to be applied repeatedly to <strong>update derivatives</strong> of the output with respect to the variables in the computational graph.</li>
<li>This approach is referred to as the <strong>backpropagation algorithm</strong>, because the derivatives of the output with respect to the variables close to the output are simpler to compute (and are therefore computed first while propagating them backwards towards the inputs).</li>
</ol>
<p>Derivatives are computed <em>numerically</em>, not <em>algebraically</em>.</p>
</section>
<section id="backpropagation-4" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Forward phase</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Use the attribute values from the input portion of a training data point to fix the values in the input nodes.</li>
<li>Select a node for which the values in all incoming nodes have already been computed and apply the node-specific function to also compute its variable.</li>
<li>Repeat the process until the values in all nodes (including the output nodes) have been computed.</li>
<li>Compute loss value if the computed and observed values mismatch.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="backpropagation-5" class="slide level2">
<h2>Backpropagation</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backward phase</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Compute the gradient of the loss with respect to the weights on the edges.</li>
<li>Derivatives of the loss with respect to weights near the output (where the loss function is computed) are easier to compute and are computed first.</li>
<li>The derivatives become increasingly complex as we move towards edge weights away from the output (in the backwards direction) and the chain rule is used repeatedly to compute them.</li>
<li>Update the weights in the negative direction of the gradient.</li>
</ul>
</div>
</div>
</div>
<p>Single cycle through all training points is an <em>epoch</em>.</p>
</section></section>
<section>
<section id="logistic-regression-as-a-neural-network" class="title-slide slide level1 center">
<h1>Logistic Regression as a Neural Network</h1>

</section>
<section id="inputs" class="slide level2">
<h2>Inputs</h2>
<p>Logistic regression is an algorithm for binary classification.</p>
<p><span class="math inline">\(x \in \mathbb{R}^{n_x}, y \in \{0,1\}\)</span>.</p>
<p><span class="math inline">\(\left\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\right\}\)</span>- <span class="math inline">\(m\)</span> training examples.</p>
<!-- Let's use a superscript notation $x^{(i)}$ - $i$-th data set element. -->
<p><span class="math inline">\(X\)</span> matrix - <span class="math inline">\(m\)</span> columns and <span class="math inline">\(n_x\)</span> rows.</p>
<p><span class="math display">\[\begin{align*}
&amp;X = \begin{bmatrix}
  \vdots &amp; \vdots &amp; \dots &amp; \vdots \\
  x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)} \\
  \vdots &amp; \vdots &amp; \dots &amp; \vdots
\end{bmatrix}
\end{align*}\]</span></p>
<p><span class="math display">\[
Y = \left[y^{(1)}, y^{(2)}, \dots, y^{(m})\right]
\]</span></p>
</section>
<section id="logistic-regression" class="slide level2">
<h2>Logistic Regression</h2>
<p><span class="math inline">\(X \in \mathbb{R}^{n_x,m}\)</span>.</p>
<p>Using Numpy syntax:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a>X.shape <span class="op">=</span> (n_x,m).</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math inline">\(Y \in \mathbb{R}^{1,m}\)</span>.</p>
<p>Using Numpy syntax:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>Y.shape <span class="op">=</span> (<span class="dv">1</span>,m).</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="logistic-regression-1" class="slide level2">
<h2>Logistic Regression</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>We will strive to maximize <span class="math inline">\(\hat{y} = P(y=1 | x)\)</span>, where <span class="math inline">\(x \in \mathbb{R}^{n_x}\)</span>.</p>
<p>Obviously, <span class="math inline">\(0 \leq \hat{y} \leq 1\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>If doing linear regresssion, we can try <span class="math display">\[
\hat{y}=w^T x + b.
\]</span></p>
<p>But for logistic regression, we do <span class="math display">\[
\hat{y}=\sigma(w^T x + b)$, \; \text{where }\; \sigma=\dfrac{1}{1+e^{-z}}.
\]</span></p>
</div>
</div>
</div>
</section>
<section id="parameters" class="slide level2">
<h2>Parameters</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Input</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
w \in \mathbb{R}^{n_x},\\
b \in \mathbb{R}.
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Output</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\hat{y} = \sigma\left( w^T x + b\right),
\]</span></p>
<p><span class="math display">\[
z \equiv  w^T x + b.
\]</span></p>
</div>
</div>
</div>
<p><span class="math inline">\(w\)</span> - weights, <span class="math inline">\(b\)</span> - bias term (<em>intercept</em>)</p>
</section>
<section id="graphs" class="slide level2">
<h2>Graphs</h2>
<img data-src="dl_lec2_files/mediabag/65b24282599e327b820db5acef99ee53564568ac.svg" height="500">
<p><span class="math inline">\(\sigma=\dfrac{1}{1+e^{-z}}\)</span>.</p>
</section>
<section id="loss-function-2" class="slide level2">
<h2>Loss function</h2>
<p>For every <span class="math inline">\(\left\{(x^{(1)},y^{(1)}), ...(x^{(m)}, y^{(m)})\right\}\)</span>, we want to find <span class="math inline">\(\hat{y}^{(i)} \approx y^{(i)}\)</span>. <span class="math display">\[\begin{align*}
  &amp;\hat{y}^{(i)} = \sigma\left(w^T x^{(i)} + b\right)
\end{align*}\]</span> We have to define a <em>loss (error) function</em> - this will estimate our model.</p>
</section>
<section id="loss-function-3" class="slide level2">
<h2>Loss function</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Quadratic</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
L(\hat{y}, y) = \dfrac{1}{2}\left(\hat{y}-y)\right)^2.
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Log</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
L(\hat{y}, y) = -\left((y\log(\hat{y}) + (1 - y)\log(1 - \hat{y}))\right).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="cost-function" class="slide level2">
<h2>Cost function</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Why does it work well?</strong></p>
</div>
<div class="callout-content">
<p>Consider <span class="math inline">\(y=0\)</span> and <span class="math inline">\(y=1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
  &amp;y=1: P(y | x) = \hat{y},\\
  &amp;y=0: P(y | x) = 1-\hat{y}
\end{align*}\]</span></p>
<p>We select <span class="math inline">\(P(y|x) = \hat{y}^y(1-\hat{y})^{(1-y)}\)</span>.</p>
<p><span class="math display">\[
\log P(y|x) = y\log(\hat{y}) + (1-y)\log(1-\hat{y}) = -L(\hat{y}, y).
\]</span></p>
<p><span class="math display">\[\begin{align*}
  y=1:&amp; L(\hat{y},y) = -\log(\hat{y}),\\
  y=0:&amp; L(\hat{y},y) = -\log(1-\hat{y})
\end{align*}\]</span></p>
</div>
</div>
</div>
</section>
<section id="cost-function-1" class="slide level2">
<h2>Cost function</h2>
<p>Cost function show how well we’re doing across the whole training set: <span class="math display">\[\begin{align*}
&amp;J(w, b) = \dfrac{1}{m} \sum\limits_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) = \\
&amp; = -\dfrac{1}{m} \sum\limits_{i=1}^m \left[y\log(\hat{y}) + (1 - y)\log(1 - \hat{y})\right].
\end{align*}\]</span></p>
</section>
<section id="cost-function-2" class="slide level2">
<h2>Cost function</h2>
<p>On <span class="math inline">\(m\)</span> examples: <span class="math display">\[\begin{align*}
  &amp;\log P(m \dots) = \log \prod_{i=1}^m P(y^{(i)} | x^{(i)}) = \\
  &amp; = \sum\limits_{i=1}^m \log P(y^{(i)} | x^{(i)}) = -\sum\limits_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}).
\end{align*}\]</span></p>
</section>
<section id="gradient-descent" class="slide level2">
<h2>Gradient descent</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem</strong></p>
</div>
<div class="callout-content">
<p>Minimization problem: find <span class="math inline">\(w,b\)</span> that minimize <span class="math inline">\(J(w,b)\)</span>.</p>
</div>
</div>
</div>
<p><img data-src="img/gradient-descent-2d-diagram.png" height="500"></p>
</section>
<section id="gradient-descent-1" class="slide level2">
<h2>Gradient descent</h2>
<ul>
<li>We use <span class="math inline">\(J(w,b)\)</span> because it is convex.</li>
<li>We pick an initial point - anything might do, e.g.&nbsp;0.</li>
<li>Then we take steps in the direction of steepest descent.</li>
</ul>
<p><span class="math display">\[
w := w - \alpha \frac{d J(w,b)}{dw}, \\
b := b - \alpha \frac{d J(w,b)}{db}
\]</span></p>
<p><span class="math inline">\(\alpha\)</span> - learning rate</p>
</section>
<section id="gradient-descent-2" class="slide level2">
<h2>Gradient descent</h2>
<ul>
<li><em>forward pass</em>: compute output</li>
<li><em>backward pass</em>: compute derivatives</li>
</ul>
</section>
<section id="logistic-regression-gradient-descent" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p><span class="math display">\[\begin{align*}
&amp;z = w^T x + b ,\\
&amp;a \equiv \hat{y}  = \sigma(z),\\
&amp;L(a,y) = -\left[y\log(a) + (1 - y)\log(1 - a)\right].
\end{align*}\]</span></p>
<p>So, for <span class="math inline">\(n_x=2\)</span> we have a computation graph:</p>
<p><span class="math inline">\((x_1,x_2,w_1,w_2,b)\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(z =w_1 x_1+w_2 x_2 + b\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\hat{y}=a=\sigma(z)\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(L(a,y)\)</span>. <!-- TODO tikz graph --></p>
</section>
<section id="computation-graph-1" class="slide level2">
<h2>Computation graph</h2>
<p>Let’s compute the derivative for <span class="math inline">\(L\)</span> by a: <span class="math display">\[\begin{align*}
&amp;\frac{dL}{da} = -\dfrac{y}{a} + \dfrac{1-y}{1-a},\\
&amp;\frac{da}{dz} = a(1-a).
\end{align*}\]</span></p>
</section>
<section id="computation-graph-2" class="slide level2">
<h2>Computation graph</h2>
<p>After computing, we’ll have <span class="math display">\[\begin{align*}
&amp;dz \equiv \dfrac{dL}{dz} = \dfrac{dL}{da}\dfrac{da}{dz} = a-y,\\
&amp;dw_1 \equiv \frac{dL}{dw_1} = x_1 dz,\\
&amp;dw_2 \equiv \frac{dL}{dw_2} = x_2 dz, \\
&amp;db \equiv \frac{dL}{db} = dz.
\end{align*}\]</span></p>
</section>
<section id="logistic-regression-gradient-descent-1" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p>GD steps are computed via <span class="math display">\[\begin{align*}
&amp;w_1 := w_1 - \alpha \frac{dL}{dw_1},\\
&amp;w_2 := w_2 - \alpha \frac{dL}{dw_2},\\
&amp;b := b - \alpha \frac{dL}{db}.
\end{align*}\]</span></p>
</section>
<section id="logistic-regression-gradient-descent-2" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p>Consider now <span class="math inline">\(m\)</span> examples in the training set.</p>
<p>Let’s recall the definition of the cost function: <span class="math display">\[\begin{align*}
&amp;J(w,b) = \dfrac{1}{m}\sum\limits_{i=1}^{m} L(a^{(i)}, y^{(i)}, \\
&amp;a^{(i)} = \hat{y}^{(i)}=\sigma(w^T x^{(i)} + b).
\end{align*}\]</span> And also <span class="math display">\[
\frac{dJ}{dw_1} = \frac{1}{m}\sum\limits_{i=1}^{m}\frac{dL(a^{(i)}, y^{(i)})}{dw_1}.
\]</span></p>
</section>
<section id="logistic-regression-gradient-descent-3" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p>Let’s implement the algorithm. First, initialize <span class="math display">\[
J=0,\\
dw_1=0,\\
dw_2=0,\\
db=0
\]</span></p>
</section>
<section id="logistic-regression-gradient-descent-4" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p><code>for i=1 to m</code> <span class="math display">\[\begin{align*}
  &amp;z^{(i)} = w^T x^{(i)} + b, \\
  &amp;a^{(i)} = \sigma(z^{(i)}), \\
  &amp;J += -\left[y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)})\right], \\
  &amp;dz^{(i)} = a^{(i)} - y^{(i)}, \\
  &amp;dw_1 += x_1^{(i)} dz^{(i)},\\
  &amp;dw_2 += x_2^{(i)} dz^{(i)},\\
  &amp;db += dz^{(i)}.
\end{align*}\]</span></p>
</section>
<section id="logistic-regression-gradient-descent-5" class="slide level2">
<h2>Logistic Regression Gradient Descent</h2>
<p>Then compute averages:</p>
<p><span class="math display">\[
J = \dfrac{J}{m}, \\
dw_1 = \dfrac{dw_1}{m}, \; dw_2 = \dfrac{dw_2}{m}, \\
db = \dfrac{db}{m}.
\]</span></p>

<aside><div>
<p>Note that <span class="math inline">\(dw_i\)</span> don’t have a superscript - we use them as accumulators. (In this example feature count <span class="math inline">\(n_x=2\)</span>)</p>
</div></aside></section>
<section id="gd-step" class="slide level2">
<h2>GD step</h2>
<p><span class="math display">\[
w_1 := w_1 - \alpha dw_1,\\
w_2 := w_2 - \alpha dw_2,\\
b := b - \alpha db.
\]</span></p>
</section>
<section id="vectorization" class="slide level2">
<h2>Vectorization</h2>
<p>We only have 2 features <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>, so we don’t have an extra for loop. Turns out that for loops have a detrimental impact on performance.</p>
<p><strong>Vectorization</strong> techniques exist for this purpose - getting rid of for loops.</p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>We have to compute <span class="math inline">\(z=w^T x + b\)</span>, where <span class="math inline">\(w,x \in \mathbb{R}^{n_x}\)</span>, and for this we can naturally use a for loop.</p>
<p>A vectorized Python command is</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>z <span class="op">=</span> np.dot(w,x)<span class="op">+</span>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<!-- An jupyter example follows, comparing times of for-loop and vectorized code samples. -->
<!-- SIMD calculations on GPUs and CPUs. -->
</section>
<section id="vectorization-1" class="slide level2">
<h2>Vectorization</h2>
<p><strong>Programming guideline</strong> - avoid explicit for loops. <span class="math display">\[\begin{align*}
  &amp;u = Av,\\
  &amp;u_i = \sum_j\limits A_{ij} v_j
\end{align*}\]</span> To be replaced by</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>u <span class="op">=</span> np.dot(A, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

<aside><div>
<p>Numpy impl: https://numpy.org/doc/1.21/reference/simd/simd-optimizations.html</p>
</div></aside></section>
<section id="vectorization-2" class="slide level2">
<h2>Vectorization</h2>
<p>Another example. Let’s say we have a vector <span class="math display">\[\begin{align*}
    &amp;v = \begin{bmatrix}
      v_1 \\
      \vdots \\
      v_n
    \end{bmatrix},
    u = \begin{bmatrix}
      e^{v_1},\\
      \vdots \\
      e^{v_n}
    \end{bmatrix}
\end{align*}\]</span> A code listing is</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a></a>u <span class="op">=</span> np.exp(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So we can modify the above code to get rid of for loops (except for the one for <span class="math inline">\(m\)</span>).</p>
</section>
<section id="vectorizing-logistic-regression" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>Let’s examine the forward propagation step of LR. <span class="math display">\[\begin{align*}
  &amp;z^{(1)} = w^T x^{(1)} + b,\\
  &amp;a^{(1)} = \sigma(z^{(1)}),
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  &amp;z^{(2)} = w^T x^{(2)} + b,\\
  &amp;a^{(2)} = \sigma(z^{(2)}).
\end{align*}\]</span></p>
</section>
<section id="vectorizing-logistic-regression-1" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>Let’s recall what have we defined as our learning matrix: <span class="math display">\[
X = \begin{bmatrix}
  \vdots &amp; \vdots &amp; \dots &amp; \vdots \\
  x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)} \\
  \vdots &amp; \vdots &amp; \dots &amp; \vdots
\end{bmatrix}
\]</span></p>
</section>
<section id="vectorizing-logistic-regression-2" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>Next <span class="math display">\[
Z = [z^{(1)}, \dots, z^{(m)}] = w^T X + [b, b, \dots, b] =\\
= [w^T x^{(1)}+b, \dots, w^T x^{(m)}+b].
\]</span></p>
<p><span class="math display">\[
A = \left[a^{(1)}, \dots, a^{(m)}\right] = \sigma\left(Z\right)
\]</span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>  z <span class="op">=</span> np.dot(w.T, x) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

<aside><div>
<p><span class="math inline">\(b\)</span> is a raw number, Python will automatically take care of expanding it into a vector - this is called <strong>broadcasting.</strong></p>
</div></aside></section>
<section id="vectorizing-logistic-regression-3" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>Earlier on, we computed <span class="math display">\[\begin{align*}
&amp;dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, \dots
\end{align*}\]</span></p>
<p>We now define <span class="math display">\[\begin{align*}
&amp;Y = [y^{(1)}, \dots, y^{(m)}],\\
&amp;dZ = [dz^{(1)}, \dots, dz^{(m)}] =\\
&amp;= A-Y = [a^{(1)}-y^{(1)}, \dots, a^{(m)}-y^{(m)}]
\end{align*}\]</span></p>
</section>
<section id="vectorizing-logistic-regression-4" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>For <span class="math inline">\(db\)</span> we have <span class="math display">\[\begin{align*}
&amp;db = \frac{1}{m}np.sum(dZ),\\
&amp;dw = \frac{1}{m}X dZ^T = \\
&amp; \frac{1}{m}\begin{bmatrix}
  \vdots &amp; &amp; \vdots \\
  x^{(1)} &amp; \dots &amp; x^{(m)} \\
  \vdots &amp; &amp; \vdots \\
\end{bmatrix}
\begin{bmatrix}
  dz^{(1)} \\
  \vdots\\
  dz^{(m)}
\end{bmatrix} = \\
&amp; = \frac{1}{m}\left[x^{(1)}dz^{(1)} + \dots +x^{(m)}dz^{(m)}\right].
\end{align*}\]</span></p>
</section>
<section id="vectorizing-logistic-regression-5" class="slide level2">
<h2>Vectorizing logistic regression</h2>
<p>Now we can go back to the backward propagation algorithm again.</p>
<p>Multiple iterations of GD will still require a for loop.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb7-2"><a></a>  Z <span class="op">=</span> np.dot(w.T, X) <span class="op">+</span> B</span>
<span id="cb7-3"><a></a>  A <span class="op">=</span> sigma(Z)</span>
<span id="cb7-4"><a></a>  dZ <span class="op">=</span> A<span class="op">-</span>Y</span>
<span id="cb7-5"><a></a>  dw <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>m X <span class="op">*</span> dZ.T</span>
<span id="cb7-6"><a></a>  db <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>m np.<span class="bu">sum</span>(dZ)</span>
<span id="cb7-7"><a></a>  w <span class="op">:=</span> w <span class="op">-</span> alpha <span class="op">*</span> dw</span>
<span id="cb7-8"><a></a>  b <span class="op">:=</span> b <span class="op">-</span> alpha <span class="op">*</span> db</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"11e673604a90f9305f7da7d0313a2862688c0eb7568c4eadb64868141de66d23"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/dl_nlp\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>